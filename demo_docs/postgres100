filenr,sentence
0,Performance Optimization - PostgreSQL wiki
0,"Want to edit, but don't see an edit button when logged in?"
0,Click here.
0,Performance Optimization
0,From PostgreSQL wiki
0,"Jump to: navigation, search"
0,Contents
0,1 How to Effectively Ask Questions Regarding Performance on Postgres Lists
0,2 General Setup and Optimization
0,3 Critical maintenance for performance
0,4 Database architecture
0,5 Database Hardware Selection and Setup
0,6 Benchmark Workloads
0,How to Effectively Ask Questions Regarding Performance on Postgres Lists
0,Slow_Query_Questions
0,General Setup and Optimization
0,"Tuning Your PostgreSQL Server by Greg Smith, Robert Treat, and Christopher Browne"
0,PostgreSQL Query Profiler in dbForge Studio by Devart
0,Performance Tuning PostgreSQL by Frank Wiles
0,QuickStart Guide to Tuning
0,PostgreSQL by Christopher Browne
0,Performance Tuning by Josh Berkus and Shridhar Daithankar
0,Replacing Slow Loops in PostgreSQL by Joel Burton
0,PostgreSQL Hardware Performance Tuning by Bruce Momjian
0,The effects of data fragmentation in a mixed load database by Dmitry Dvoinikov
0,Understanding Postgres Performance by Craig Kerstiens
0,More on Postgres Performance by Craig Kerstiens
0,Faster PostgreSQL counting by Joe Nelson
0,Row count estimates in Postgres by David Conlin
0,Index-only scans in Postgres by David Conlin
0,Optimize PostgreSQL Server Performance Through Configuration by Tom Swartz
0,Performance courses are available from a number of companies. Check events and trainings for further details.
0,Critical maintenance for performance
0,"Introduction to VACUUM, ANALYZE, EXPLAIN, and COUNT by Jim Nasby."
0,VACUUM FULL and why you should avoid it
0,Planner Statistics
0,Using EXPLAIN
0,Logging Difficult Queries
0,Logging Checkpoints
0,Bulk Loading and Restores
0,Performance Analysis Tools by Craig Ringer
0,Database architecture
0,Limiting and prioritizing user/query/database resource usage by Craig Ringer
0,Prioritizing databases by separating into multiple clusters by Craig Ringer
0,Clustering
0,Shared Storage
0,Database Hardware Selection and Setup
0,Database Hardware
0,Reliable Writes
0,Benchmark Workloads
0,Category:Benchmarking
0,"Retrieved from ""https://wiki.postgresql.org/index.php?title=Performance_Optimization&oldid=35388"""
0,Categories: AdministrationPerformanceBenchmarkingGeneral articles and guides
0,Navigation menu
0,Views
0,Page
0,Discussion
0,View source
0,History
0,Personal tools
0,Log in
0,Navigation
0,Main Page
0,Random page
0,Recent changes
0,Help
0,Search
0,Tools
0,What links here
0,Related changes
0,Special pages
0,Printable version
0,Permanent link
0,Page information
0,"This page was last edited on 29 September 2020, at 13:37."
0,Privacy policy
0,About PostgreSQL wiki
0,Disclaimers
1,"High Performance PostgreSQL, Tuning and Optimization Guide - Ibrar Ahmed - PostgreSQL Tutorial - YouTubeAboutPressCopyrightContact usCreatorsAdvertiseDevelopersTermsPrivacyPolicy & SafetyHow YouTube worksTest new features© 2021 Google LLC"
2,Tips for PostgreSQL Query Optimization: Improve Performance with EXPLAIN ANALYZE | EnterpriseDB
2,Skip to main content
2,Barrio main menu
2,Why EDB?
2,EDB Supercharges PostgreSQL
2,wrapper
2,Use Cases
2,Oracle Migration
2,Hybrid Cloud
2,High Availability
2,Solutions for
2,IT Professionals
2,Database Architects
2,Developers
2,Database Admins
2,Products
2,Products Overview
2,wrapper
2,Databases
2,EDB Postgres Advanced Server
2,PostgreSQL
2,wrapper
2,Tools
2,Postgres Enterprise Manager
2,Backup and Recovery
2,Failover Manager
2,Open Source Projects
2,Cloud Native PostgreSQL
2,Migration Portal
2,Migration Toolkit
2,Replication
2,Services
2,Services Overview
2,Training
2,Getting Started
2,PostgreSQL Optimization
2,Enterprise Strategy
2,Custom Services
2,Support
2,Customer Support Portal
2,Support Overview
2,PostgreSQL Technical Support
2,Remote DBA Service
2,Cloud DBA Service
2,Technical Account Management
2,Resources
2,Docs
2,wrapper
2,Blog
2,Webinars
2,PostgreSQL Tutorials
2,Training
2,Partners
2,White Papers
2,Customer Stories
2,Product Compatibility
2,Plans
2,Barrio utility menu
2,Contact
2,Dashboard
2,Sign In
2,Downloads
2,Barrio Mobile Menu
2,Why EDB?
2,EDB Supercharges PostgreSQL
2,Use Cases
2,Oracle Migration
2,Hybrid Cloud
2,High Availability
2,Solutions for
2,IT Professionals
2,Database Architects
2,Developers
2,Database Admins
2,Products
2,Products Overview
2,Databases
2,EDB Postgres Advanced Server
2,PostgreSQL
2,Tools
2,Postgres Enterprise Manager
2,Backup and Recovery
2,Failover Manager
2,Open Source Projects
2,Cloud Native PostgreSQL
2,Migration Portal
2,Migration Toolkit
2,Replication
2,Services
2,Services Overview
2,Training
2,Getting Started
2,Postgres Optimization
2,Enterprise Strategy
2,Custom Services
2,Support
2,Support Overview
2,PostgreSQL Technical Support
2,Remote DBA Service
2,Cloud DBA Service
2,Technical Account Management
2,Resources
2,Docs
2,Blog
2,Webinars
2,Postgres Tutorials
2,Training
2,Partners
2,White Papers
2,Customer Stories
2,Product Compatibility
2,Plans
2,Downloads
2,Contact
2,Sign In
2,Dashboard
2,The EDB Blog
2,Back
2,How to Use EXPLAIN ANALYZE for Planning and Optimizing Query Performance in PostgreSQL
2,PostgreSQL
2,"With many people working from home these days because of the coronavirus pandemic, it can be a little challenging to get help from a colleague remotely. Sure, there’s Slack and all manner of collaboration tools, but it’s not quite the same as walking up to someone’s cubicle and getting a second pair of eyes to look at a problem, not to mention that our co-workers might be busy trying to juggle deadlines and unruly kids in the home. When it comes to dealing with poor database and query performance, it’s a daunting task to venture into the dark cavern of query planning and optimization, but fear not! EXPLAIN is our friend in those dark and lonely places."
2,"We recently received a request from one of our customers, concerned about a slow query on one of their JSON columns. They were seeing a slow performance in their development environments and were understandably worried about the impact that they’d see if they went to production with poor query performance. We got right to work to help them out, and our first stone to turn over was to have them send us their EXPLAIN ANALYZE output for the query, which yielded:"
2,postgres=# explain SELECT * FROM org where 'aa'::text IN (SELECT jsonb_array_elements(info -> 'dept') ->> 'name');
2,QUERY PLAN
2,-------------------------------------------------------------------------
2,Seq Scan on org
2,(cost=0.00..719572.55 rows=249996 width=1169)
2,Filter: (SubPlan 1)
2,SubPlan 1
2,Result
2,(cost=0.00..2.27 rows=100 width=32)
2,ProjectSet
2,(cost=0.00..0.52 rows=100 width=32)
2,Result
2,(cost=0.00..0.01 rows=1 width=0)
2,"They knew they had created an index, and were curious as to why the index was not being used. Our next data point to gather was information about the index itself, and it turned out that they had created their index like so:"
2,"CREATE INDEX idx_org_dept ON org ((info -> 'dept'::text) ->> 'name'::text));Notice anything? Their query was wrapping info -> 'dept' in a function called jsonb_array_elements(), which led the query planner to think that it shouldn’t use the index. The fix was simple, and we were able to get the customer back on their way after a rather quick adjustment to their query. Once the customer changed their query to the following, the Index started getting scanned:"
2,postgres=# SELECT * FROM org where 'aa'::text IN (info -> 'dept' ->> 'name');
2,postgres=# explain SELECT * FROM organization where 'aa'::text IN (info -> 'dept' ->> 'name');
2,QUERY PLAN
2,----------------------------------------------------------------------------------------------
2,Index Scan using idx_org_dept on org
2,(cost=0.42..8.44 rows=1 width=1169)
2,Index Cond: ('aa'::text = ((info -> 'dept'::text) ->> 'name'::text))
2,"(2 rows)As we can see, having and using EXPLAIN in your troubleshooting arsenal can be invaluable."
2,What is Explain?
2,"EXPLAIN is a keyword that gets prepended to a query to show a user how the query planner plans to execute the given query.  Depending on the complexity of the query, it will show the join strategy, method of extracting data from tables, estimated rows involved in executing the query, and a number of other bits of useful information.  Used with ANALYZE, EXPLAIN will also show the time spent on executing the query, sorts, and merges that couldn’t be done in-memory, and more.  This information is invaluable when it comes to identifying query performance bottlenecks and opportunities, and helps us understand what information the query planner is working with as it makes its decisions for us."
2,A Cost-Based Approach
2,"To the query planner, all the data on disk is basically the same.  To determine the fastest way to reach a particular piece of data requires some estimation of the amount of time it takes to do a full table scan, a merge of two tables, and other operations to get data back to the user.  PostgreSQL accomplishes this by assigning costs to each execution task, and these values are derived from the postgresql.conf file (see parameters ending in *_cost or beginning with enable_*).  When a query is sent to the database, the query planner calculates the cumulative costs for different execution strategies and selects the most optimal plan (which may not necessarily be the one with the lowest cost)."
2,bash $ pgbench -i && psql
2,<...>
2,postgres=# EXPLAIN SELECT * FROM pgbench_accounts a JOIN pgbench_branches b ON (a.bid=b.bid) WHERE a.aid < 100000;
2,QUERY PLAN
2,--------------------------------------------------------------------------------
2,Nested Loop
2,(cost=0.00..4141.00 rows=99999 width=461)
2,Join Filter: (a.bid = b.bid)
2,Seq Scan on pgbench_branches b
2,(cost=0.00..1.01 rows=1 width=364)
2,Seq Scan on pgbench_accounts a
2,(cost=0.00..2890.00 rows=99999 width=97)
2,Filter: (aid < 100000)
2,"(5 rows)Here, we see that the Seq Scan on pgbench_accounts has cost 2890 to execute the task.  Where does this value come from?  If we look at some settings and do the calculations, we find:"
2,cost = ( #blocks * seq_page_cost ) + ( #records * cpu_tuple_cost ) + ( #records * cpu_filter_cost )
2,postgres=# select pg_relation_size('pgbench_accounts');
2,pg_relation_size
2,------------------
2,13434880
2,block_size
2,= 8192
2,"(8kB, typical OS)"
2,#blocks
2,= 1640
2,(relation_size / block_size)
2,#records
2,= 100000
2,seq_page_cost
2,= 1
2,(default)
2,cpu_tuple_cost
2,= 0.01
2,(default)
2,cpu_filter_cost = 0.0025 (default)
2,cost = ( 1640 * 1 ) + ( 100000 * 0.01 ) + ( 100000 * 0.0025 ) = 2890
2,"As we can see, the costs are directly based on some internal statistics that the query planner can work with."
2,A Note About Statistics
2,"The query planner calculates costs based on statistics stored in pg_statistic (don’t look there--there’s nothing human-readable in there.  If you want to get visibility into the table and row statistics, try looking at pg_stats).  If any of these internal statistics are off (i.e., a bloated table or too many joins that cause the Genetic Query Optimizer to kick in), a sub-optimal plan may be selected, leading to poor query performance.  Having bad statistics isn’t necessarily a problem--the statistics aren’t always updated in real-time, and much of it depends on PostgreSQL’s internal maintenance.  As such, it’s imperative that database maintenance is conducted regularly--this means frequent VACUUM-ing and ANALYZE-ing.  Without good statistics, you could end up with something like this:"
2,postgres=# EXPLAIN SELECT * FROM pgbench_history WHERE aid < 100;
2,QUERY PLAN
2,-----------------------------------------------------------------------
2,Seq Scan on pgbench_history
2,(cost=0.00..2346.00 rows=35360 width=50)
2,"Filter: (aid < 100)In the example above, the database had gone through a fair amount of activity, and the statistics were inaccurate.  With an ANALYZE (not VACUUM ANALYZE or EXPLAIN ANALYZE, but just a plain ANALYZE), the statistics are fixed, and the query planner now chooses an Index Scan:"
2,postgres=# EXPLAIN SELECT * FROM pgbench_history WHERE aid < 100;
2,QUERY PLAN
2,----------------------------------------------------------------------
2,Index Scan using foo on pgbench_history
2,(cost=0.42..579.09 rows=153 width=50)
2,Index Cond: (aid < 100)
2,How Does EXPLAIN ANALYZE Help?
2,"When an EXPLAIN is prepended to a query, the query plan gets printed, but the query does not get run.  We won’t know whether the statistics stored in the database were correct or not, and we won’t know if some operations required expensive I/O instead of fully running in memory.  When used with ANALYZE, the query is actually run and the query plan, along with some under-the-hood activity is printed out."
2,"If we look at the first query above and run EXPLAIN ANALYZE instead of a plain EXPLAIN, we get:"
2,postgres=# EXPLAIN ANALYZE SELECT * FROM pgbench_accounts a JOIN pgbench_branches b ON (a.bid=b.bid) WHERE a.aid < 100000;
2,QUERY PLAN
2,-------------------------------------------------------------------------------------------------------------
2,Nested Loop
2,(cost=0.00..4141.00 rows=99999 width=461) (actual time=0.039..56.582 rows=99999 loops=1)
2,Join Filter: (a.bid = b.bid)
2,Seq Scan on pgbench_branches b
2,(cost=0.00..1.01 rows=1 width=364) (actual time=0.025..0.026 rows=1 loops=1)
2,Seq Scan on pgbench_accounts a
2,(cost=0.00..2890.00 rows=99999 width=97) (actual time=0.008..25.752 rows=99999 loops=1)
2,Filter: (aid < 100000)
2,Rows Removed by Filter: 1
2,Planning Time: 0.306 ms
2,Execution Time: 61.031 ms
2,"(8 rows)You’ll notice here that there’s more information -- actual time and rows, as well as planning and execution times.  If we add BUFFERS, like EXPLAIN (ANALYZE, BUFFERS), we’ll even get cache hit/miss statistics in the output:"
2,"postgres=# EXPLAIN (BUFFERS, ANALYZE) SELECT * FROM pgbench_accounts a JOIN pgbench_branches b ON (a.bid=b.bid) WHERE a.aid < 100000;"
2,QUERY PLAN
2,-------------------------------------------------------------------------------------------------------------
2,Nested Loop
2,(cost=0.00..4141.00 rows=99999 width=461) (actual time=0.039..56.582 rows=99999 loops=1)
2,Join Filter: (a.bid = b.bid)
2,Buffers: shared hit=3 read=1638
2,Seq Scan on pgbench_branches b
2,(cost=0.00..1.01 rows=1 width=364) (actual time=0.025..0.026 rows=1 loops=1)
2,Buffers: shared hit=1
2,Seq Scan on pgbench_accounts a
2,(cost=0.00..2890.00 rows=99999 width=97) (actual time=0.008..25.752 rows=99999 loops=1)
2,Filter: (aid < 100000)
2,Rows Removed by Filter: 1
2,Buffers: shared hit=2 read=1638
2,Planning Time: 0.306 ms
2,Execution Time: 61.031 ms
2,(8 rows)
2,"Very quickly, you can see that EXPLAIN can be a useful tool for people looking to understand their database performance behaviors."
2,A Quick Review of Scan Types and Joins
2,"It’s important to know that every join type and scan type have their time and place.  Some people look for the word “Sequential” scan and immediately jump back in fear, not considering whether it would be worthwhile to access data another.  Take, for example, a table with 2 rows -- it would not make sense to the query planner to scan the index, then go back and retrieve data from the disk when it could just quickly scan the table and pull data out without touching the index.  In this case, and in the case of most other small-ish tables, it would be more efficient to do a sequential scan.  To quickly review the join and scan types that PostgreSQL works with:"
2,Scan Types
2,Sequential Scan
2,Basically a brute-force retrieval from disk
2,Scans the whole table
2,Fast for small tables
2,Index Scan
2,Scan all/some rows in an index; look up rows in heap
2,"Causes random seek, which can be costly for old-school spindle-based disks"
2,Faster than a Sequential Scan when extracting a small number of rows for large tables
2,Index Only Scan
2,Scan all/some rows in index
2,No need to lookup rows in the table because the values we want are already stored in the index itself
2,Bitmap Heap Scan
2,"Scan index, building a bitmap of pages to visit"
2,"Then, look up only relevant pages in the table for desired rows"
2,Join Types
2,Nested Loops
2,"For each row in the outer table, scan for matching rows in the inner table"
2,"Fast to start, best for small tables"
2,Merge Join
2,Zipper-operation on _sorted_ data sets
2,Good for large tables
2,High startup cost if an additional sort is required
2,Hash Join
2,"Build hash of inner table values, scan outer table for matches"
2,Only usable for equality conditions
2,"High startup cost, but fast execution"
2,"As we can see, every scan type and join type has its place.  What’s most important is that the query planner has good statistics to work with, as mentioned earlier."
2,"We’ve only talked about one instance where EXPLAIN helped identify a problem and give an idea of how to solve it.  At EDB Support, we’ve seen many situations where EXPLAIN could help identify things like:"
2,Inaccurate statistics leading to poor join/scan choices
2,Maintenance activity (VACUUM and ANALYZE) not aggressive enough
2,Corrupted indexes requiring a REINDEX
2,Index definition v. query mismatch
2,"work_mem being set too low, preventing in-memory sorts and joins"
2,Poor performance due to join order listing when writing a query
2,Improper ORM configuration
2,"EXPLAIN is certainly one of the most invaluable tools for anyone working with PostgreSQL, and using it well will save you lots of time!"
2,Join Postgres Pulse Live!
2,"We make use of the problems we solve and the conversations we have in helping people with Postgres, and this was another example of that effort in motion.  EXPLAIN and the query planner doesn’t start and stop with what we’ve outlined here, so if you have other questions, we’re here for you.  You can find all of our blog and YouTube series here, and you can always join us for our next session."
2,"Join us on Monday, May 4th, for our next Pulse Live Session!  We’ll dive into this week’s questions and quagmires around EXPLAIN use, as well as take questions from anyone who participates.  You can ask your questions via email at postgrespulse@enterprisedb.com, hashtag on Twitter, or live during the event right here."
2,Richard
2,Yen
2,"Richard is a Senior Support Engineer at EnterpriseDB and supports the entire suite of EnterpriseDB's products. Prior to joining EnterpriseDB, Richard worked as a database engineer and web developer, functioning primarily in operations with a focus on scalability, performance, and rec ..."
2,Popular Posts
2,Connecting PostgreSQL using psql and pgAdmin
2,How to use PostgreSQL with Django
2,Microsoft SQL Server (MSSQL) vs. PostgreSQL Comparison in Details - What are the Differences? [2020]
2,10 Examples of PostgreSQL Stored Procedures
2,How to Install Postgres on Docker
2,Ready to take the next step with PostgreSQL? Contact Us
2,Barrio main menu
2,Why EDB?
2,Use Cases
2,Oracle Migration
2,Hybrid Cloud
2,High Availability
2,Solutions for
2,IT Professionals
2,Database Architects
2,Developers
2,Database Admins
2,Products
2,Databases
2,EDB Postgres Advanced Server
2,PostgreSQL
2,Tools
2,Postgres Enterprise Manager
2,Backup and Recovery
2,Failover Manager
2,Open Source Projects
2,Cloud Native PostgreSQL
2,Migration Portal
2,Migration Toolkit
2,Replication
2,Services
2,Services Overview
2,Training
2,Getting Started
2,PostgreSQL Optimization
2,Enterprise Strategy
2,Custom Services
2,Support
2,Customer Support Portal
2,Support Overview
2,PostgreSQL Technical Support
2,Remote DBA Service
2,Cloud DBA Service
2,Technical Account Management
2,Resources
2,Docs
2,Blog
2,Webinars
2,PostgreSQL Tutorials
2,Training
2,Partners
2,White Papers
2,Customer Stories
2,Product Compatibility
2,Plans
2,Company
2,About EDB
2,PostgreSQL Contributions
2,Careers
2,Events
2,Press Releases
2,Media Coverage
2,Customers
2,Follow Us
2,Twitter
2,LinkedIn
2,Facebook
2,YouTube
2,Barrio Footer Secondary Menu
2,© 2020 EDB
2,GDPR
2,Privacy Policy
2,Terms of Use
2,Trademarks
2,"Select LanguageEnglish (EN)DutchFrenchGermanJapaneseKoreanPortugueseSpanishThis automated translation should not be considered exact and only used to approximate the original English language content. EDB does not guarantee the accuracy, reliability, or timeliness of any information translated."
6,13 Tips to Improve PostgreSQL Insert Performance - DZone Database
6,Database Zone
6,"Thanks for visiting DZone today,"
6,Edit Profile
6,Manage Email Subscriptions
6,How to Post to DZone
6,Sign Out
6,View Profile
6,Post
6,Over 2 million developers have joined DZone.
6,Log In
6,Join
6,Refcardz
6,Research
6,Webinars
6,Zones
6,Agile
6,Big Data
6,Cloud
6,Database
6,DevOps
6,Integration
6,IoT
6,Java
6,Microservices
6,Open Source
6,Performance
6,Security
6,Web Dev
6,DZone
6,Database Zone
6,13 Tips to Improve PostgreSQL Insert Performance
6,13 Tips to Improve PostgreSQL Insert Performance
6,Get a cheatsheet full of ways to improve your database ingest (INSERT) performance and speed up your time-series queries using PostgreSQL.
6,Mike Freedman
6,"Jul. 08, 20"
6,Database Zone
6,Tutorial
6,Like
6,(2)
6,Comment
6,Save
6,Tweet
6,11.98K
6,Views
6,Join the DZone community and get the full member experience.
6,Join For Free
6,"Ingest performance is critical for many common PostgreSQL use cases, including application monitoring, application analytics, IoT monitoring, and more. While databases have long had time fields, there's a key difference in the type of data these use cases collect: unlike standard relational ""business"" data, changes are treated as inserts, not overwrites (in other words, every new value becomes a new row in the database, instead of replacing the row's prior value with the latest one)."
6,"If you're operating in a scenario where you need to retain all data v. overwriting past values, optimizing the speed in which your database can ingest new data becomes essential."
6,"We have a lot of experience optimizing performance for ourselves and our community members, and we’ve broken our top tips into two categories. First, we’ve outlined a few tips that are useful for improving PostgreSQL in general. After that, we’ve outlined a few that are specific to TimescaleDB."
6,Improve PostgreSQL Performance
6,Here are some best practices for improving ingest performance in vanilla PostgreSQL:
6,1. Use Indexes in Moderation
6,"Having the right indexes can speed up your queries, but they’re not a silver bullet. Incrementally maintaining indexes with each new row requires additional work. Check the number of indexes you’ve defined on your table (use the psql command \d table_name), and determine whether their potential query benefits outweigh the storage and insert overhead. Since every system is different, there aren’t any hard and fast rules or “magic number” of indexes – just be reasonable."
6,2. Reconsider Foreign Key Constraints
6,"Sometimes it's necessary to build foreign keys (FK) from one table to other relational tables. When you have an FK constraint, every INSERT will typically then need to read from your referenced table, which can degrade performance. Consider if you can denormalize your data – we sometimes see pretty extreme use of FK constraints, done from a sense of “elegance” rather than engineering tradeoffs."
6,3. Avoid Unnecessary UNIQUE Keys
6,"Developers are often trained to specify primary keys in database tables, and many ORMs love them. Yet, many use cases – including common monitoring or time-series applications – don’t require them, as each event or sensor reading can simply be logged as a separate event by inserting it at the tail of a hypertable's current chunk during write time."
6,"If a UNIQUE constraint is otherwise defined, that insert can necessitate an index lookup to determine if the row already exists, which will adversely impact the speed of your INSERT."
6,4. Use Separate Disks for WAL and Data
6,"While this is a more advanced optimization that isn't always needed, if your disk becomes a bottleneck, you can further increase throughput by using a separate disk (tablespace) for the database's write-ahead log (WAL) and data."
6,5. Use Performant Disks
6,"Sometimes developers deploy their database in environments with slower disks, whether due to poorly-performing HDD, remote SANs, or other types of configurations. And because when you are inserting rows, the data is durably stored to the write-ahead log (WAL) before the transaction completes, slow disks can impact insert performance. One thing to do is check your disk IOPS using the ioping command.Read test:"
6,SQL
6,$ ioping -q -c 10 -s 8k .
6,--- . (hfs /dev/disk1 930.7 GiB) ioping statistics ---
6,"9 requests completed in 208 us, 72 KiB read, 43.3 k iops, 338.0 MiB/s"
6,"generated 10 requests in 9.00 s, 80 KiB, 1 iops, 8.88 KiB/s"
6,min/avg/max/mdev = 18 us / 23.1 us / 35 us / 6.17 us
6,Write test:
6,Java
6,xxxxxxxxxx
6,$ ioping -q -c 10 -s 8k -W .
6,--- . (hfs /dev/disk1 930.7 GiB) ioping statistics ---
6,"9 requests completed in 10.8 ms, 72 KiB written, 830 iops, 6.49 MiB/s"
6,"generated 10 requests in 9.00 s, 80 KiB, 1 iops, 8.89 KiB/s"
6,min/avg/max/mdev = 99 us / 1.20 ms / 2.23 ms / 919.3 us
6,"You should see at least 1000s of read IOPS and many 100s of write IOPS.  If you are seeing far fewer, your INSERT performance is likely to be impacted by your disk hardware.  See if alternative storage configurations are feasible."
6,Choose and Configure TimescaleDB for Better Ingest Performance
6,"TimescaleDB is tuned to improve ingest performance. The most common uses for TimescaleDB involve storing massive amounts of data for cloud infrastructure metrics, product analytics, web analytics, IoT devices, and many other time-series use cases. As is typical with time-series data, these scenarios are time-centric, almost solely append-only (lots of INSERTs), and require fast ingestion of large amounts of data within small time windows."
6,"TimescaleDB is packaged as an extension to PostgreSQL and is purpose-built for time-series use cases. So, if getting faster ingest performance out of PostgreSQL is necessary for your applications or systems, consider using TimescaleDB (available fully-managed via Timescale Cloud – our database-as-a-service offering, or self-managed via our free Community Edition)."
6,...and here are 8 more techniques for improving ingest performance with TimescaleDB:
6,6. Use Parallel Writes
6,"Each INSERT or COPY command to TimescaleDB (as in PostgreSQL) is executed as a single transaction and thus runs in a single-threaded fashion. To achieve higher ingest, you should execute multiple INSERTS or COPY commands in parallel."
6,"For help with bulk loading large CSV files in parallel, check out  TimescaleDB's parallel copy command."
6,Pro tip: make sure your client machine has enough cores to execute this parallelism (running 32 client workers on a 2 vCPU machine doesn’t help much – the workers won’t actually be executed in parallel).
6,7. Insert Rows in Batches
6,"In order to achieve higher ingest rates, you should insert your data with many rows in each INSERT call (or else use some bulk insert command, like COPY or our parallel copy tool)."
6,"Don't insert your data row-by-row – instead, try at least hundreds (or thousands) of rows per INSERT. This allows the database to spend less time on connection management, transaction overhead, SQL parsing, etc., and more time on data processing."
6,8. Properly Configure shared_buffers
6,"We typically recommend 25% of available RAM. If you install TimescaleDB via a method that runs timescaledb-tune, it should automatically configure shared_buffers to something well-suited to your hardware specs."
6,"Note: in some cases, typically with virtualization and constrained cgroups memory allocation, these automatically-configured settings may not be ideal. To check that your shared_buffers are set to within the 25% range,  run SHOW shared_buffers from your psql connection."
6,9. Run our Docker Images on Linux Hosts
6,"If you are running a TimescaleDB Docker container (which runs Linux) on top of another Linux operating system, you're in great shape. The container is basically providing process isolation, and the overhead is extremely minimal."
6,"If you're running the container on a Mac or Windows machine, you'll see some performance hits for the OS virtualization, including for I/O."
6,"Instead, if you need to run on Mac or Windows, we recommend installing directly instead of using a Docker image."
6,10. Write Data in Loose Time Order
6,"When chunks are sized appropriately (see #11 and #12), the latest chunk(s) and their associated indexes are naturally maintained in memory. New rows inserted with recent timestamps will be written to these chunks and indexes already in memory."
6,"If a row with a sufficiently older timestamp is inserted – i.e., it's an out-of-order or backfilled write – the disk pages corresponding to the older chunk (and its indexes) will need to be read in from disk. This will significantly increase write latency and lower insert throughput."
6,"Particularly, when you are loading data for the first time, try to load data in sorted, increasing timestamp order."
6,"Be careful if you're bulk loading data about many different servers, devices, and so forth:"
6,"Do not bulk insert data sequentially by server  (i.e., all data for server A, then server B, then C, and so forth). This will cause disk thrashing as loading each server will walk through all chunks before starting anew."
6,"Instead, arrange your bulk load so that data from all servers are inserted in loose timestamp order (e.g., day 1 across all servers in parallel, then day 2 across all servers in parallel, etc.)"
6,11. Avoid “Too Large” Chunks
6,"To maintain higher ingest rates, you want your latest chunk, as well as all its associated indexes, to stay in memory, so that writes to the chunk and index updates merely update memory. (The write is still durable, as inserts are written to the WAL on disk before the database pages are updated.)"
6,"If your chunks are too large, then writes to even the latest chunk will start swapping to disk."
6,"As a rule-of-thumb, we recommend that the latest chunks and all their indexes fit comfortably within the database's shared_buffers. You can check your chunk sizes via the chunk_relation_size_pretty SQL command."
6,SQL
6,xxxxxxxxxx
6,"=> SELECT chunk_table, table_size, index_size, toast_size, total_sizeFROM chunk_relation_size_pretty('hypertable_name')ORDER BY ranges DESC LIMIT 4;"
6,chunk_table                             | table_size | index_size | toast_size | total_size
6,-----------------------------------------+------------+------------+------------+------------
6,_timescaledb_internal._hyper_1_96_chunk | 200 MB     | 64 MB
6,| 8192 bytes | 272 MB
6,_timescaledb_internal._hyper_1_95_chunk | 388 MB     | 108 MB     | 8192 bytes | 500 MB
6,_timescaledb_internal._hyper_1_94_chunk | 388 MB     | 108 MB     | 8192 bytes | 500 MB
6,_timescaledb_internal._hyper_1_93_chunk | 388 MB     | 108 MB     | 8192 bytes | 500 MB
6,"If your chunks are too large, you can update the range for future chunks via the set_chunk_time_interval command. However, this does not modify the range of existing chunks (e.g., by rewriting large chunks into multiple small chunks)."
6,"For configurations where individual chunks are much larger than your available memory, we recommend dumping and reloading your hypertable data to properly sized chunks."
6,"Keeping the latest chunk applies to all active hypertables; if you are actively writing to two hypertables, the latest chunks from both should fit within shared_buffers."
6,12. Avoid Too Many or Too Small Chunks
6,"Unless you're running multi-node TimescaleDB, we don't currently recommend using space partitioning. And if you do, remember that this number of chunks is created for every time interval."
6,"So, if you create 64 space partitions and daily chunks, you'll have 24,640 chunks per year. This may lead to a bigger performance hit during query time (due to planning overhead) compared to insert time, but something to consider nonetheless.Another thing to avoid: using an incorrect integer value when you specify the time interval range in create_hypertable."
6,Pro tip:
6,"If your time column uses a native timestamp type, then any integer value should be in terms of microseconds (so one day = 86400000000). We recommend using interval types ('1 day') to avoid potential for any confusion."
6,"If your time column is an integer or bigint itself,  use the appropriate range: if the integer timestamp is in seconds, use 86400; if the bigint timestamp is in nanoseconds, use 86400000000000.In both cases, you can use chunk_relation_size_pretty to make sure your chunk sizes or partition ranges seem reasonable:"
6,SQL
6,xxxxxxxxxx
6,"=> SELECT chunk_table, ranges, total_size"
6,FROM chunk_relation_size_pretty('hypertable_name')
6,ORDER BY ranges DESC LIMIT 4;
6,chunk_table                             |                         ranges
6,| total_size
6,-----------------------------------------+---------------------------------------------------------+------------
6,"_timescaledb_internal._hyper_1_96_chunk | {""['2020-02-13 23:00:00+00','2020-02-14 00:00:00+00')""} | 272 MB"
6,"_timescaledb_internal._hyper_1_95_chunk | {""['2020-02-13 22:00:00+00','2020-02-13 23:00:00+00')""} | 500 MB"
6,"_timescaledb_internal._hyper_1_94_chunk | {""['2020-02-13 21:30:00+00','2020-02-13 22:00:00+00')""} | 500 MB"
6,"_timescaledb_internal._hyper_1_93_chunk | {""['2020-02-13 20:00:00+00','2020-02-13 21:00:00+00')""} | 500 MB"
6,13. Watch Row Width
6,"The overhead from inserting a wide row (say, 50, 100, 250 columns) is going to be much higher than inserting a narrower row (more network I/O, more parsing and data processing, larger writes to WAL, etc.). Most of our published benchmarks are using TSBS, which uses 12 columns per row. So you'll correspondingly see lower insert rates if you have very wide rows."
6,"If you are considering very wide rows because you have different types of records, and each type has a disjoint set of columns, you might want to try using multiple hypertables (one per record type) – particularly if you don't often query across these types."
6,"Additionally, JSONB records are another good option if virtually all columns are sparse. That said, if you're using sparse wide rows, use NULLs for missing records whenever possible, not default values, for the most performance gains (NULLs are much cheaper to store and query)."
6,"Finally, the cost of wide rows are actually much less once you compress rows using TimescaleDB’s native compression.  Rows are converted into more columnar compressed form, sparse columns compress extremely well, and compressed columns aren’t read from disk for queries that don’t fetch individual columns."
6,Summary
6,"If ingest performance is critical to your use case, consider using TimescaleDB. You can get started with Timescale Cloud for free today, or download TimescaleDB to your own machine or cloud instance for free."
6,"Our approach to support is to address your whole solution, so we're here to help help you achieve your desired performance results (see more details about our Customer Care team and ethos)."
6,"Lastly, our Slack community is a great place to connect with 4K+ other developers with similar use cases, as well as myself, Timescale engineers, product team members, and Developer Advocates."
6,Topics:
6,"postgresql,"
6,"database performance,"
6,"time-series data,"
6,"sql (structured query language),"
6,"database,"
6,performance
6,Published at DZone with permission of Mike Freedman.
6,See the original article here.
6,Opinions expressed by DZone contributors are their own.
6,Popular on DZone
6,The State of Serverless Computing 2021
6,Top 20 Dockerfile Best Practices
6,Predicting Housing Prices Using Google AutoML Tables
6,An Examination of Open Source
6,Comments
6,Database Partner Resources
6,ABOUT US
6,About DZone
6,Send feedback
6,Careers
6,ADVERTISE
6,Developer
6,Marketing Blog
6,Advertise with DZone
6,+1 (919) 238-7100
6,CONTRIBUTE ON DZONE
6,MVB Program
6,Become a Contributor
6,Visit the Writers' Zone
6,LEGAL
6,Terms of Service
6,Privacy Policy
6,CONTACT US
6,600 Park Offices Drive
6,Suite 150
6,"Research Triangle Park, NC 27709"
6,support@dzone.com
6,+1 (919) 678-0300
6,Let's be friends:
6,DZone.com is powered by
8,Performance tips  |  Cloud SQL for PostgreSQL  |  Google Cloud
8,Why Google
8,close
8,Transform your business with innovative solutions
8,"Whether your business is early in its journey or well on its way to digital transformation, Google Cloud's solutions and technologies help solve your toughest challenges."
8,Learn more
8,Why Google Cloud
8,Choosing Google Cloud
8,Reasons why businesses choose us.
8,Multicloud
8,Run your apps wherever you need them.
8,Trust and security
8,Keep your data secure and compliant.
8,Global infrastructure
8,Build on the same infrastructure Google uses.
8,Data analytics
8,Make smarter decisions with the leading data platform.
8,Open cloud
8,"Scale with open, flexible technology."
8,Sustainability
8,Run on the cleanest cloud in the industry.
8,Analyst reports
8,See how Google Cloud ranks.
8,Customer stories
8,Learn how businesses use Google Cloud.
8,Google Cloud Blog
8,Read the latest story and product updates.
8,Solutions
8,close
8,Industry Solutions
8,"Reduce cost, increase operational agility, and capture new market opportunities."
8,Retail
8,Analytics and collaboration tools for the retail value chain.
8,Consumer Packaged Goods
8,Solutions for CPG digital transformation and brand growth.
8,Financial Services
8,"Computing, data management, and analytics tools for financial services."
8,Healthcare and Life Sciences
8,Health-specific solutions to enhance the patient experience.
8,Media and Entertainment
8,Solutions for content production and distribution operations.
8,Telecommunications
8,Hybrid and multi-cloud services to deploy and monetize 5G.
8,Gaming
8,AI-driven solutions to build and scale games faster.
8,Manufacturing
8,Migration and AI tools to optimize the manufacturing value chain.
8,Supply Chain and Logistics
8,Digital supply chain solutions built in the cloud.
8,Government
8,"Data storage, AI, and analytics solutions for government agencies."
8,Education
8,Teaching tools to provide more engaging learning experiences.
8,Small and Medium Business
8,"Explore SMB solutions for web hosting, app development, AI, analytics, and more."
8,Not seeing what you're looking for?
8,See all solutions
8,Application Modernization
8,"Develop and run applications anywhere, using cloud-native technologies like containers, serverless, and service mesh."
8,Hybrid and Multi-cloud Application Platform
8,Platform for modernizing legacy apps and building new apps.
8,Cloud-Native App Development
8,"End-to-end solution for building, deploying, and managing apps."
8,Serverless Solutions
8,"Fully managed environment for developing, deploying and scaling apps."
8,DevOps
8,Processes and resources for implementing DevOps in your org.
8,Continuous Delivery (CD)
8,End-to-end automation from source to production.
8,Continuous Integration (CI)
8,Fast feedback on code changes at scale.
8,Mainframe Modernization
8,Automated tools and prescriptive guidance for moving to the cloud.
8,Hosting
8,Services and infrastructure for building web apps and websites.
8,Artificial Intelligence
8,Add intelligence and efficiency to your business with AI and machine learning.
8,Build and Use AI
8,Products to build and use artificial intelligence.
8,Contact Center AI
8,AI model for speaking with customers and assisting human agents.
8,Document AI
8,Machine learning and AI to unlock insights from your documents.
8,Cloud Talent Solution
8,AI with job search and talent acquisition capabilities.
8,Business Application Platform
8,"Speed up the pace of innovation without coding, using APIs, apps, and automation."
8,New Business Channels Using APIs
8,Attract and empower an ecosystem of developers and partners.
8,Unlocking Legacy Applications Using APIs
8,Cloud services for extending and modernizing legacy apps.
8,Open Banking APIx
8,Simplify and accelerate secure delivery of open banking compliant APIs.
8,Databases
8,"Migrate and manage enterprise data with security, reliability, high availability, and fully managed data services."
8,Database Migration
8,Guides and tools to simplify your database migration life cycle.
8,Database Modernization
8,Upgrades to modernize your operational database infrastructure.
8,Google Cloud Databases
8,"Database services to migrate, manage, and modernize data."
8,Migrate Oracle workloads to Google Cloud
8,"Rehost, replatform, rewrite your Oracle workloads."
8,Open Source Databases
8,Fully managed open source databases with enterprise-grade support.
8,SQL Server on Google Cloud
8,Options for running SQL Server virtual machines on Google Cloud.
8,Digital Transformation
8,"Accelerate business recovery and ensure a better future with solutions that enable hybrid and multi-cloud, generate intelligent insights, and keep your workers connected."
8,Business Continuity
8,Proactively plan and prioritize workloads.
8,Digital Innovation
8,Reimagine your operations and unlock new opportunities.
8,Operational Efficiency
8,Prioritize investments and optimize costs.
8,COVID-19 Solutions
8,Get work done more safely and securely.
8,COVID-19 Solutions for the Healthcare Industry
8,How Google is helping healthcare meet extraordinary challenges.
8,Infrastructure Modernization
8,"Migrate quickly with solutions for SAP, VMware, Windows, Oracle, and other workloads."
8,Application Migration
8,Discovery and analysis tools for moving to the cloud.
8,SAP on Google Cloud
8,Certifications for running SAP applications and SAP HANA.
8,High Performance Computing
8,"Compute, storage, and networking options to support any workload."
8,Windows on Google Cloud
8,Tools and partners for running Windows workloads.
8,Data Center Migration
8,"Migration solutions for VMs, apps, databases, and more."
8,Active Assist
8,Automatic cloud resource optimization and increased security.
8,Virtual Desktops
8,Remote work solutions for desktops and applications (VDI & DaaS).
8,Rapid Assessment & Migration Program (RAMP)
8,End-to-end migration program to simplify your path to the cloud.
8,Productivity and Collaboration
8,Change the way teams work with solutions designed for humans and built for impact.
8,Google Workspace
8,Collaboration and productivity tools for enterprises.
8,Google Workspace Essentials
8,Secure video meetings and modern collaboration for teams.
8,Cloud Identity
8,Unified platform for IT admins to manage user devices and apps.
8,Chrome Enterprise
8,"Chrome OS, Chrome Browser, and Chrome devices built for business."
8,Cloud Search
8,Enterprise search for employees to quickly find company information.
8,Security
8,"Detect, investigate, and respond to online threats to help protect your business."
8,Security Analytics and Operations
8,Solution for analyzing petabytes of security telemetry.
8,Web App and API Protection
8,Threat and fraud protection for your web applications and APIs.
8,Smart Analytics
8,"Generate instant insights from data at any scale with a serverless, fully managed analytics platform that significantly simplifies analytics."
8,Data Warehouse Modernization
8,Data warehouse to jumpstart your migration and unlock insights.
8,Stream Analytics
8,"Insights from ingesting, processing, and analyzing event streams."
8,Marketing Analytics
8,"Solutions for collecting, analyzing, and activating customer data."
8,Data Lake Modernization
8,Services for building and modernizing your data lake.
8,Business Intelligence
8,"Data analytics tools for collecting, analyzing, and activating BI."
8,Products
8,close
8,Featured Products
8,Compute Engine
8,Virtual machines running in Google’s data center.
8,Cloud Storage
8,"Object storage that’s secure, durable, and scalable."
8,Cloud SDK
8,Command-line tools and libraries for Google Cloud.
8,Cloud SQL
8,"Relational database services for MySQL, PostgreSQL, and SQL server."
8,Google Kubernetes Engine
8,Managed environment for running containerized apps.
8,BigQuery
8,Data warehouse for business agility and insights.
8,Cloud CDN
8,Content delivery network for delivering web and video.
8,Dataflow
8,Streaming analytics for stream and batch
8,processing.
8,Operations
8,"Monitoring, logging, and application performance suite."
8,Cloud Run
8,Fully managed environment for running containerized apps.
8,Anthos
8,Platform for modernizing existing apps and building new ones.
8,Not seeing what you're looking for?
8,See all products (100+)
8,AI and Machine Learning
8,Speech-to-Text
8,Speech recognition and transcription supporting 125 languages.
8,Vision AI
8,"Custom and pre-trained models to detect emotion, text, more."
8,Text-to-Speech
8,Speech synthesis in 220+ voices and 40+ languages.
8,Cloud Translation
8,"Language detection, translation, and glossary support."
8,Cloud Natural Language
8,Sentiment analysis and classification of unstructured text.
8,AutoML
8,Custom machine learning model training and development.
8,AI Platform
8,"Platform for training, hosting, and managing ML models."
8,Video AI
8,Video classification and recognition using machine learning.
8,AI Infrastructure
8,Options for every business to train deep learning and machine learning models cost-effectively.
8,Dialogflow
8,Conversation applications and systems development suite for virtual agents.
8,AutoML Tables
8,Service for training ML models with structured data.
8,Not seeing what you're looking for?
8,See all AI and machine learning products
8,API Management
8,Apigee API Management
8,Manage the full life cycle of APIs anywhere with visibility and control.
8,Cloud Endpoints
8,Deployment and development management for APIs on Google Cloud.
8,Cloud Healthcare API
8,Solution to bridge existing care systems and apps on Google Cloud.
8,AppSheet
8,No-code development platform to build and extend applications.
8,API Gateway
8,"Develop, deploy, secure, and manage APIs with a fully managed gateway."
8,Compute
8,Compute Engine
8,Virtual machines running in Google’s data center.
8,App Engine
8,Serverless application platform for apps and back ends.
8,Cloud GPUs
8,"GPUs for ML, scientific computing, and 3D visualization."
8,Migrate for Compute Engine
8,Server and virtual machine migration to Compute Engine.
8,Preemptible VMs
8,Compute instances for batch jobs and fault-tolerant workloads.
8,Shielded VMs
8,Reinforced virtual machines on Google Cloud.
8,Sole-Tenant Nodes
8,"Dedicated hardware for compliance, licensing, and management."
8,Bare Metal
8,Infrastructure to run specialized workloads on Google Cloud.
8,Recommender
8,Usage recommendations for Google Cloud products and services.
8,VMware Engine
8,"Fully managed, native VMware Cloud Foundation software stack."
8,Cloud Run
8,Fully managed environment for running containerized apps.
8,Not seeing what you're looking for?
8,See all compute products
8,Containers
8,Google Kubernetes Engine
8,Managed environment for running containerized apps.
8,Container Registry
8,"Registry for storing, managing, and securing Docker images."
8,Container Security
8,Container environment security for each stage of the life cycle.
8,Cloud Build
8,Solution for running build steps in a Docker container.
8,Deep Learning Containers
8,"Containers with data science frameworks, libraries, and tools."
8,Kubernetes Applications
8,Containerized apps with prebuilt deployment and unified billing.
8,Artifact Registry
8,Package manager for build artifacts and dependencies.
8,Knative
8,Components to create Kubernetes-native cloud-based software.
8,Cloud Run
8,Fully managed environment for running containerized apps.
8,Cloud Code
8,"IDE support to write, run, and debug Kubernetes applications."
8,Data Analytics
8,BigQuery
8,Data warehouse for business agility and insights.
8,Looker
8,"Platform for BI, data applications, and embedded analytics."
8,Dataflow
8,Streaming analytics for stream and batch processing.
8,Pub/Sub
8,Messaging service for event ingestion and delivery.
8,Dataproc
8,Service for running Apache Spark and Apache Hadoop clusters.
8,Cloud Data Fusion
8,Data integration for building and managing data pipelines.
8,Cloud Composer
8,Workflow orchestration service built on Apache Airflow.
8,Data Catalog
8,"Metadata service for discovering, understanding and managing data."
8,Dataprep
8,Service to prepare data for analysis and machine learning.
8,Google Data Studio
8,"Interactive data suite for dashboarding, reporting, and analytics."
8,Google Marketing Platform
8,Marketing platform unifying advertising and analytics.
8,Cloud Life Sciences
8,"Tools for managing, processing, and transforming biomedical data."
8,Databases
8,Cloud Bigtable
8,"Cloud-native wide-column database for large scale, low-latency workloads."
8,Firestore
8,"Cloud-native document database for building rich mobile, web, and IoT apps."
8,Memorystore
8,In-memory database for managed Redis and Memcached.
8,Cloud Spanner
8,Cloud-native relational database with unlimited scale and 99.999% availability.
8,Cloud SQL
8,"Fully managed database for MySQL, PostgreSQL, and SQL Server."
8,Database Migration Service
8,"Serverless, minimal downtime migrations to Cloud SQL."
8,Bare Metal
8,Infrastructure to run specialized workloads on Google Cloud.
8,Firebase Realtime Database
8,NoSQL database for storing and syncing data in real time.
8,Developer Tools
8,Artifact Registry
8,Universal package manager for build artifacts and dependencies.
8,Cloud Build
8,Continuous integration and continuous delivery platform.
8,Cloud Code
8,"IDE support to write, run, and debug Kubernetes applications."
8,Cloud Deployment Manager
8,Service for creating and managing Google Cloud resources.
8,Cloud SDK
8,Command line tools and libraries for Google Cloud.
8,Cloud Scheduler
8,Cron job scheduler for task automation and management.
8,Cloud Source Repositories
8,"Private Git repository to store, manage, and track code."
8,Cloud Tasks
8,Task management service for asynchronous task execution.
8,Container Registry
8,Private Docker storage for container images on Google Cloud.
8,Tekton
8,Kubernetes-native resources for declaring CI/CD pipelines.
8,Not seeing what you're looking for?
8,See all developer tools
8,Healthcare and Life Sciences
8,Apigee Healthcare APIx
8,FHIR API-based digital service production.
8,Cloud Healthcare API
8,Solution for bridging existing care systems and apps on Google Cloud.
8,Cloud Life Sciences
8,"Tools for managing, processing, and transforming biomedical data."
8,Healthcare Natural Language AI
8,Real-time insights from unstructured medical text.
8,Hybrid and Multi-cloud
8,Anthos
8,Platform for modernizing existing apps and building new ones.
8,Looker
8,"Platform for BI, data applications, and embedded analytics."
8,Cloud Run for Anthos
8,Integration that provides a serverless development platform on GKE.
8,Google Cloud Marketplace for Anthos
8,Containerized apps with prebuilt deployment and unified billing.
8,Migrate for Anthos
8,Tool to move workloads and existing applications to GKE.
8,Operations
8,"Monitoring, logging, and application performance suite."
8,Cloud Build
8,Service for executing builds on Google Cloud infrastructure.
8,Traffic Director
8,Traffic control pane and management for open service mesh.
8,Apigee API Management
8,"API management, development, and security platform."
8,Internet of Things
8,Cloud IoT Core
8,"IoT device management, integration, and"
8,connection service.
8,Edge TPU
8,ASIC designed to run ML inference and AI at the edge.
8,Management Tools
8,Cloud Shell
8,Interactive shell environment with a built-in command line.
8,Cloud Console
8,Web-based interface for managing and monitoring cloud apps.
8,Cloud Deployment Manager
8,Service for creating and managing Google Cloud resources.
8,Cloud Mobile App
8,App to
8,manage Google Cloud services from your mobile device.
8,Cloud APIs
8,Programmatic interfaces for
8,Google Cloud services.
8,Private Catalog
8,Service catalog for admins managing internal enterprise solutions.
8,Cost Management
8,"Tools for monitoring, controlling, and optimizing your costs."
8,Intelligent Management
8,"Tools for easily managing performance, security, and cost."
8,Media and Gaming
8,Game Servers
8,Game server management service running on Google Kubernetes Engine.
8,OpenCue
8,Open source render manager for visual effects and animation.
8,Migration
8,Application Migration
8,App migration to the cloud for low-cost refresh cycles.
8,BigQuery Data Transfer Service
8,Data import service for scheduling and moving data into BigQuery.
8,Cloud Data Transfer
8,Tools and services for transferring your data to Google Cloud.
8,Cloud Foundation Toolkit
8,Reference templates for Deployment Manager and Terraform.
8,Database Migration Service
8,"Serverless, minimal downtime migrations to Cloud SQL."
8,Migrate for Anthos
8,Components for migrating VMs into system containers on GKE.
8,Migrate for Compute Engine
8,Components for migrating VMs and physical servers to Compute Engine.
8,Rapid Assessment & Migration Program (RAMP)
8,End-to-end migration program to simplify your path to the cloud.
8,Transfer Appliance
8,Storage server for moving large volumes of data to Google Cloud.
8,Transfer Service
8,Data transfers from online and on-premises sources to Cloud Storage.
8,VMware Engine
8,Migrate and run your VMware workloads natively on Google Cloud.
8,Networking
8,Cloud Armor
8,Security policies and defense against web and DDoS attacks.
8,Cloud CDN
8,Content delivery network for serving web and video content.
8,Cloud DNS
8,Domain name system for reliable and low-latency name lookups.
8,Cloud Load Balancing
8,Service for distributing traffic across applications and regions.
8,Cloud NAT
8,NAT service for giving private instances internet access.
8,Hybrid Connectivity
8,"Connectivity options for VPN, peering, and enterprise needs."
8,Network Intelligence Center
8,"Network monitoring, verification, and optimization platform."
8,Network Service Tiers
8,Cloud network options
8,"based on performance, availability, and cost."
8,Network Telemetry
8,"VPC flow logs for network monitoring, forensics, and security."
8,Traffic Director
8,Traffic control pane and management for open service mesh.
8,Virtual Private Cloud
8,Virtual network for Google Cloud resources and cloud-based services.
8,Service Directory
8,"Platform for discovering, publishing, and connecting services."
8,Operations
8,Cloud Logging
8,"Google Cloud audit, platform, and application logs management."
8,Cloud Monitoring
8,Infrastructure and application health with rich metrics.
8,Error Reporting
8,Application error identification and analysis.
8,Kubernetes Engine Monitoring
8,GKE app development and troubleshooting.
8,Cloud Trace
8,Tracing system collecting latency data from applications.
8,Cloud Profiler
8,CPU and heap profiler for analyzing application performance.
8,Cloud Debugger
8,Real-time application state inspection and in-production debugging.
8,Intelligent Operations
8,"Tools for easily optimizing performance, security, and cost."
8,Security and Identity
8,Cloud IAM
8,Permissions management system for Google Cloud resources.
8,Assured Workloads
8,Compliance and security controls for sensitive workloads.
8,Cloud Key Management
8,Manage encryption keys on Google Cloud.
8,Confidential Computing
8,Encrypt data in use with Confidential VMs.
8,Security Command Center
8,Platform for defending against threats to your Google Cloud assets.
8,Cloud Data Loss Prevention
8,"Sensitive data inspection, classification, and redaction platform."
8,Managed Service for Microsoft Active Directory
8,Hardened service running Microsoft® Active Directory (AD).
8,Access Transparency
8,Cloud provider visibility through near real-time logs.
8,Titan Security Key
8,Two-factor authentication device for user account protection.
8,Secret Manager
8,"Store API keys, passwords, certificates, and other sensitive data."
8,BeyondCorp Enterprise
8,Zero trust solution for secure application and resource access.
8,Not seeing what you're looking for?
8,See all security and identity products
8,Serverless Computing
8,Cloud Run
8,Fully managed environment for running containerized apps.
8,Cloud Functions
8,Platform for creating functions that respond to cloud events.
8,App Engine
8,Serverless application platform for apps and back ends.
8,Workflows
8,Workflow orchestration for serverless products and API services.
8,Storage
8,All Storage Products
8,Cloud-based storage services for your business.
8,Cloud Storage
8,"Object storage that’s secure, durable, and scalable."
8,Filestore
8,File storage that is highly scalable and secure.
8,Persistent Disk
8,Block storage for virtual machine instances running on Google Cloud.
8,Cloud Storage for Firebase
8,Object storage for storing and serving user-generated content.
8,Local SSD
8,Block storage that is locally attached for high-performance needs.
8,Archival Storage
8,Data archive that offers online access speed at ultra low cost.
8,Cloud Data Transfer
8,Tools and services for transferring your data to Google Cloud.
8,Google Workspace Essentials
8,Secure video meetings and modern collaboration for teams.
8,Pricing
8,close
8,Do more for less with Google Cloud
8,Our customer-friendly pricing means more overall value to your business.
8,Contact Us
8,Google Cloud Platform
8,Overview
8,Pay only for what you use with no lock-in
8,Price list
8,Pricing details on each Google Cloud product
8,Calculators
8,Calculate your cloud savings
8,Free on Google Cloud
8,Learn and build on Google Cloud for free
8,More Cloud Products
8,Google Workspace
8,Google Maps Platform
8,Cloud Identity
8,Apigee
8,Firebase
8,Zync Render
8,Getting started
8,close
8,Get started with Google Cloud
8,"Start building right away on our secure, intelligent platform. New customers can use a $300 free credit to get started with any GCP product."
8,Try GCP Free
8,Get Started
8,Resources to Start on Your Own
8,Quickstarts
8,View short tutorials to help you get started
8,GCP Marketplace
8,Deploy ready-to-go solutions in a few clicks
8,Training
8,Enroll in on-demand or classroom training
8,Certification
8,Become Google Cloud Certified
8,Get Help from an Expert
8,Consulting
8,Jump-start your project with help from Google
8,Technical Account Management
8,Get long-term guidance from Google
8,Find a Partner
8,Work with a Partner in our global network
8,Become a Partner
8,Join Google Cloud's Partner program
8,More ways to get started
8,Docs
8,Support
8,Docs
8,Support
8,Language
8,English
8,Deutsch
8,Español – América Latina
8,Français
8,Português – Brasil
8,中文 – 简体
8,日本語
8,한국어
8,Cloud SQL
8,Overview
8,Guides
8,Reference
8,Samples
8,Support
8,Resources
8,Contact Us
8,Get started for free
8,Why Google
8,More
8,Solutions
8,More
8,Products
8,More
8,Pricing
8,More
8,Getting started
8,More
8,Docs
8,Overview
8,Guides
8,Reference
8,Samples
8,Support
8,Resources
8,Support
8,Console
8,Contact Us
8,Get started for free
8,Cloud SQL for PostgreSQL
8,All APIs and reference
8,Error codes
8,Cloud SQL MetricsCloud SQL metricsInsights metrics
8,REST Reference
8,v1beta4REST Resources
8,backupRunsOverviewdeletegetinsertlist
8,databasesOverviewdeletegetinsertlistpatchupdate
8,flagsOverviewlist
8,instancesOverviewaddServerCaclonedeletedemoteMasterexportfailovergetimportinsertlistlistServerCaspatchpromoteReplicaresetSslConfigrestartrestoreBackuprotateServerCastartReplicastopReplicatruncateLogupdate
8,operationsOverviewgetlist
8,projects.instancesOverviewrescheduleMaintenancestartExternalSyncverifyExternalSyncSettings
8,sslCertsOverviewcreateEphemeraldeletegetinsertlist
8,tiersOverviewlist
8,usersOverviewdeleteinsertlistupdateTypesDiskEncryptionConfigurationDiskEncryptionStatusExternalSyncModeOperationErrorSqlDatabaseVersion
8,API overviewUsing the Cloud SQL Admin APIConfiguring VPC Service Controls
8,API basicsAuthorizing requestsPerformance tips
8,Client libraries and sample code
8,gcloud sqlOverviewReference
8,Transform your business with innovative solutions
8,Learn more
8,Why Google Cloud
8,Choosing Google Cloud
8,Multicloud
8,Trust and security
8,Global infrastructure
8,Data analytics
8,Open cloud
8,Sustainability
8,Analyst reports
8,Customer stories
8,Google Cloud Blog
8,Industry Solutions
8,Retail
8,Consumer Packaged Goods
8,Financial Services
8,Healthcare and Life Sciences
8,Media and Entertainment
8,Telecommunications
8,Gaming
8,Manufacturing
8,Supply Chain and Logistics
8,Government
8,Education
8,Small and Medium Business
8,See all solutions
8,Application Modernization
8,Hybrid and Multi-cloud Application Platform
8,Cloud-Native App Development
8,Serverless Solutions
8,DevOps
8,Continuous Delivery (CD)
8,Continuous Integration (CI)
8,Mainframe Modernization
8,Hosting
8,Artificial Intelligence
8,Build and Use AI
8,Contact Center AI
8,Document AI
8,Cloud Talent Solution
8,Business Application Platform
8,New Business Channels Using APIs
8,Unlocking Legacy Applications Using APIs
8,Open Banking APIx
8,Databases
8,Database Migration
8,Database Modernization
8,Google Cloud Databases
8,Migrate Oracle workloads to Google Cloud
8,Open Source Databases
8,SQL Server on Google Cloud
8,Digital Transformation
8,Business Continuity
8,Digital Innovation
8,Operational Efficiency
8,COVID-19 Solutions
8,COVID-19 Solutions for the Healthcare Industry
8,Infrastructure Modernization
8,Application Migration
8,SAP on Google Cloud
8,High Performance Computing
8,Windows on Google Cloud
8,Data Center Migration
8,Active Assist
8,Virtual Desktops
8,Rapid Assessment & Migration Program (RAMP)
8,Productivity and Collaboration
8,Google Workspace
8,Google Workspace Essentials
8,Cloud Identity
8,Chrome Enterprise
8,Cloud Search
8,Security
8,Security Analytics and Operations
8,Web App and API Protection
8,Smart Analytics
8,Data Warehouse Modernization
8,Stream Analytics
8,Marketing Analytics
8,Data Lake Modernization
8,Business Intelligence
8,Featured Products
8,Compute Engine
8,Cloud Storage
8,Cloud SDK
8,Cloud SQL
8,Google Kubernetes Engine
8,BigQuery
8,Cloud CDN
8,Dataflow
8,Operations
8,Cloud Run
8,Anthos
8,See all products (100+)
8,AI and Machine Learning
8,Speech-to-Text
8,Vision AI
8,Text-to-Speech
8,Cloud Translation
8,Cloud Natural Language
8,AutoML
8,AI Platform
8,Video AI
8,AI Infrastructure
8,Dialogflow
8,AutoML Tables
8,See all AI and machine learning products
8,API Management
8,Apigee API Management
8,Cloud Endpoints
8,Cloud Healthcare API
8,AppSheet
8,API Gateway
8,Compute
8,Compute Engine
8,App Engine
8,Cloud GPUs
8,Migrate for Compute Engine
8,Preemptible VMs
8,Shielded VMs
8,Sole-Tenant Nodes
8,Bare Metal
8,Recommender
8,VMware Engine
8,Cloud Run
8,See all compute products
8,Containers
8,Google Kubernetes Engine
8,Container Registry
8,Container Security
8,Cloud Build
8,Deep Learning Containers
8,Kubernetes Applications
8,Artifact Registry
8,Knative
8,Cloud Run
8,Cloud Code
8,Data Analytics
8,BigQuery
8,Looker
8,Dataflow
8,Pub/Sub
8,Dataproc
8,Cloud Data Fusion
8,Cloud Composer
8,Data Catalog
8,Dataprep
8,Google Data Studio
8,Google Marketing Platform
8,Cloud Life Sciences
8,Databases
8,Cloud Bigtable
8,Firestore
8,Memorystore
8,Cloud Spanner
8,Cloud SQL
8,Database Migration Service
8,Bare Metal
8,Firebase Realtime Database
8,Developer Tools
8,Artifact Registry
8,Cloud Build
8,Cloud Code
8,Cloud Deployment Manager
8,Cloud SDK
8,Cloud Scheduler
8,Cloud Source Repositories
8,Cloud Tasks
8,Container Registry
8,Tekton
8,See all developer tools
8,Healthcare and Life Sciences
8,Apigee Healthcare APIx
8,Cloud Healthcare API
8,Cloud Life Sciences
8,Healthcare Natural Language AI
8,Hybrid and Multi-cloud
8,Anthos
8,Looker
8,Cloud Run for Anthos
8,Google Cloud Marketplace for Anthos
8,Migrate for Anthos
8,Operations
8,Cloud Build
8,Traffic Director
8,Apigee API Management
8,Internet of Things
8,Cloud IoT Core
8,Edge TPU
8,Management Tools
8,Cloud Shell
8,Cloud Console
8,Cloud Deployment Manager
8,Cloud Mobile App
8,Cloud APIs
8,Private Catalog
8,Cost Management
8,Intelligent Management
8,Media and Gaming
8,Game Servers
8,OpenCue
8,Migration
8,Application Migration
8,BigQuery Data Transfer Service
8,Cloud Data Transfer
8,Cloud Foundation Toolkit
8,Database Migration Service
8,Migrate for Anthos
8,Migrate for Compute Engine
8,Rapid Assessment & Migration Program (RAMP)
8,Transfer Appliance
8,Transfer Service
8,VMware Engine
8,Networking
8,Cloud Armor
8,Cloud CDN
8,Cloud DNS
8,Cloud Load Balancing
8,Cloud NAT
8,Hybrid Connectivity
8,Network Intelligence Center
8,Network Service Tiers
8,Network Telemetry
8,Traffic Director
8,Virtual Private Cloud
8,Service Directory
8,Operations
8,Cloud Logging
8,Cloud Monitoring
8,Error Reporting
8,Kubernetes Engine Monitoring
8,Cloud Trace
8,Cloud Profiler
8,Cloud Debugger
8,Intelligent Operations
8,Security and Identity
8,Cloud IAM
8,Assured Workloads
8,Cloud Key Management
8,Confidential Computing
8,Security Command Center
8,Cloud Data Loss Prevention
8,Managed Service for Microsoft Active Directory
8,Access Transparency
8,Titan Security Key
8,Secret Manager
8,BeyondCorp Enterprise
8,See all security and identity products
8,Serverless Computing
8,Cloud Run
8,Cloud Functions
8,App Engine
8,Workflows
8,Storage
8,All Storage Products
8,Cloud Storage
8,Filestore
8,Persistent Disk
8,Cloud Storage for Firebase
8,Local SSD
8,Archival Storage
8,Cloud Data Transfer
8,Google Workspace Essentials
8,Do more for less with Google Cloud
8,Contact Us
8,Google Cloud Platform
8,Overview
8,Price list
8,Calculators
8,Free on Google Cloud
8,More Cloud Products
8,Google Workspace
8,Google Maps Platform
8,Cloud Identity
8,Apigee
8,Firebase
8,Zync Render
8,Get started with Google Cloud
8,Try GCP Free
8,Get Started
8,Resources to Start on Your Own
8,Quickstarts
8,GCP Marketplace
8,Training
8,Certification
8,Get Help from an Expert
8,Consulting
8,Technical Account Management
8,Find a Partner
8,Become a Partner
8,More ways to get started
8,Home
8,Docs
8,Cloud SQL
8,Documentation
8,PostgreSQL
8,Reference
8,Send feedback
8,Performance tips
8,"This document covers some techniques you can use to improve the performance of your application. In some cases, examples from other APIs or generic APIs are used to illustrate the ideas presented. However, the same concepts are applicable to the Cloud SQL Admin API."
8,Compression using gzip
8,"An easy and convenient way to reduce the bandwidth needed for each request is to enable gzip compression. Although this requires additional CPU time to uncompress the results, the trade-off with network costs usually makes it very worthwhile."
8,"In order to receive a gzip-encoded response you must do two things: Set an Accept-Encoding header, and modify your user agent to contain the string gzip. Here is an example of properly formed HTTP headers for enabling gzip compression:"
8,Accept-Encoding: gzip
8,User-Agent: my program (gzip)
8,Working with partial resources
8,Another way to improve the performance of your API calls is by sending and receiving only the portion of the
8,"data that you're interested in. This lets your application avoid transferring, parsing, and storing unneeded fields, so it can use resources including network, CPU, and memory more efficiently."
8,There are two types of partial requests:
8,Partial response: A request where you specify which fields to include in the response (use the fields request parameter).
8,Patch: An update request where you send only the fields you want to change (use the PATCH HTTP verb).
8,More details on making partial requests are provided in the following sections.
8,Partial response
8,"By default, the server sends back the full representation of a resource after processing requests. For better performance, you can ask the server to send only the fields you really need and get a partial response instead."
8,"To request a partial response, use the fields request parameter to specify the fields you want returned. You can use this parameter with any request that returns response data."
8,"Note that the fields parameter only affects the response data; it does not affect the data that you need to send, if any. To reduce the amount of data you send when modifying resources, use a patch request."
8,Example
8,"The following example shows the use of the fields parameter with a generic (fictional) ""Demo"" API."
8,Simple request: This HTTP GET request omits the fields parameter and returns the full resource.
8,https://www.googleapis.com/demo/v1
8,"Full resource response: The full resource data includes the following fields, along with many others that have been omitted for brevity."
8,"""kind"": ""demo"","
8,...
8,"""items"": ["
8,"""title"": ""First title"","
8,"""comment"": ""First comment."","
8,"""characteristics"": {"
8,"""length"": ""short"","
8,"""accuracy"": ""high"","
8,"""followers"": [""Jo"", ""Will""],"
8,"""status"": ""active"","
8,...
8,"""title"": ""Second title"","
8,"""comment"": ""Second comment."","
8,"""characteristics"": {"
8,"""length"": ""long"","
8,"""accuracy"": ""medium"""
8,"""followers"": [ ],"
8,"""status"": ""pending"","
8,...
8,...
8,Request for a partial response: The following request for this same resource uses the fields parameter to significantly reduce the amount of data returned.
8,"https://www.googleapis.com/demo/v1?fields=kind,items(title,characteristics/length)"
8,"Partial response: In response to the request above, the server sends back a response that contains only the kind information along with a pared-down items array that includes only HTML title and length characteristic information in each item."
8,200 OK
8,"""kind"": ""demo"","
8,"""items"": [{"
8,"""title"": ""First title"","
8,"""characteristics"": {"
8,"""length"": ""short"""
8,"}, {"
8,"""title"": ""Second title"","
8,"""characteristics"": {"
8,"""length"": ""long"""
8,...
8,Note that the response is a JSON object that includes only the selected fields and their enclosing parent objects.
8,"Details on how to format the fields parameter is covered next, followed by more details about what exactly gets returned in the response."
8,Fields parameter syntax summary
8,"The format of the fields request parameter value is loosely based on XPath syntax. The supported syntax is summarized below, and additional examples are provided in the following section."
8,Use a comma-separated list to select multiple fields.
8,Use a/b to select a field b that is nested within field a; use a/b/c to select a field c nested within b.
8,"Exception: For API responses that use ""data"" wrappers, where the response is nested within a data object that looks like data: { ... }, do not include ""data"" in the fields specification."
8,"Including the data object with a fields specification like data/a/b causes an error. Instead, just use a fields specification like a/b."
8,"Use a sub-selector to request a set of specific sub-fields of arrays or objects by placing expressions in parentheses ""( )""."
8,"For example: fields=items(id,author/email) returns only the item ID and author's email for each element in the items array. You can also specify a single sub-field, where fields=items(id) is equivalent to fields=items/id."
8,"Use wildcards in field selections, if needed."
8,For example: fields=items/pagemap/* selects all objects in a pagemap.
8,More examples of using the fields parameter
8,The examples below include descriptions of how the fields parameter value affects the response.
8,"Note: As with all query parameter values, the fields parameter value must be URL encoded. For better readability, the examples in this document omit the encoding."
8,"Identify the fields you want returned, or make field selections."
8,"The fields request parameter value is a comma-separated list of fields, and each field is specified relative to the root of the response. Thus, if you are performing a list operation, the response is a collection, and it generally includes an array of resources. If you are performing an operation that returns a single resource, fields are specified relative to that resource. If the field you select is (or is part of) an array, the server returns the selected portion of all elements in the array."
8,Here are some collection-level examples:
8,Examples
8,Effect
8,items
8,"Returns all elements in the items array, including all fields in each element, but no other fields."
8,"etag,items"
8,Returns both the etag field
8,and all elements in the items array.
8,items/title
8,Returns only the title field for all elements in the items array.
8,"Whenever a nested field is returned, the response includes the enclosing parent objects. The parent fields do not include any other child fields unless they are also selected explicitly."
8,context/facets/label
8,"Returns only the label field for all members of the facets array, which is itself nested under the context object."
8,items/pagemap/*/title
8,"For each element in the items array, returns only the title field (if present) of all objects that are children of pagemap."
8,Here are some resource-level examples:
8,Examples
8,Effect
8,title
8,Returns the title field of the requested resource.
8,author/uri
8,Returns the uri sub-field of the author object in the requested resource.
8,links/*/href
8,Returns the href field of all objects that are children of links.
8,Request only parts of specific fields using sub-selections.
8,"By default, if your request specifies particular fields, the server returns the objects or array elements in their entirety. You can specify a response that includes only certain sub-fields. You do this using ""( )"" sub-selection syntax, as in the example below."
8,Example
8,Effect
8,"items(title,author/uri)"
8,Returns only the values of the title and author's uri for each element in the items array.
8,Handling partial responses
8,"After a server processes a valid request that includes the fields query parameter, it sends back an HTTP 200 OK status code, along with the requested data. If the fields query parameter has an error or is otherwise invalid, the server returns an HTTP 400 Bad Request status code, along with an error message telling the user what was wrong with their fields selection (for example, ""Invalid field selection a/b"")."
8,Here is the partial response example shown in the introductory section above. The request uses the fields parameter to specify which fields to return.
8,"https://www.googleapis.com/demo/v1?fields=kind,items(title,characteristics/length)"
8,The partial response looks like this:
8,200 OK
8,"""kind"": ""demo"","
8,"""items"": [{"
8,"""title"": ""First title"","
8,"""characteristics"": {"
8,"""length"": ""short"""
8,"}, {"
8,"""title"": ""Second title"","
8,"""characteristics"": {"
8,"""length"": ""long"""
8,...
8,"Note: For APIs that support query parameters for data pagination (maxResults and nextPageToken, for example), use those parameters to reduce the results of each query to a manageable size. Otherwise, the performance gains possible with partial response might not be realized."
8,Patch (partial update)
8,"You can also avoid sending unnecessary data when modifying resources. To send updated data only for the specific fields that you’re changing, use the HTTP PATCH verb. The patch semantics described in this document are different (and simpler) than they were for the older, GData implementation of partial update."
8,The short example below shows how using patch minimizes the data you need to send to make a small update.
8,Example
8,"This example shows a simple patch request to update only the title of a generic (fictional) ""Demo"" API resource. The"
8,"resource also has a comment, a set of characteristics, status, and many other fields, but this request only sends the title field, since that's the only field being modified:"
8,PATCH https://www.googleapis.com/demo/v1/324
8,Authorization: Bearer your_auth_token
8,Content-Type: application/json
8,"""title"": ""New title"""
8,Response:
8,200 OK
8,"""title"": ""New title"","
8,"""comment"": ""First comment."","
8,"""characteristics"": {"
8,"""length"": ""short"","
8,"""accuracy"": ""high"","
8,"""followers"": [""Jo"", ""Will""],"
8,"""status"": ""active"","
8,...
8,"The server returns a 200 OK status code, along with the full representation of the updated resource."
8,"Since only the title field was included in the patch request, that's the only value that is different from before."
8,"Note: If you use the partial response fields parameter in combination with patch, you can increase the efficiency of your update requests even further. A patch request only reduces the size of the request. A partial response reduces the size of the response. So to reduce the amount of data sent in both directions, use a patch request with the fields parameter."
8,Semantics of a patch request
8,"The body of the patch request includes only the resource fields you want to modify. When you specify a field, you must include any enclosing parent objects, just as the enclosing parents are returned with a partial response. The modified data you send is merged into the data for the parent object, if there is one."
8,"Add: To add a field that doesn't already exist, specify the new field and its value."
8,"Modify: To change the value of an existing field, specify the field and set it to the new value."
8,"Delete: To delete a field, specify the field and set it to null. For example, ""comment"": null. You can also delete an entire object (if it is mutable) by setting it to null. If you are using the"
8,"Java API Client Library, use Data.NULL_STRING instead; for"
8,"details, see JSON null."
8,"Note about arrays: Patch requests that contain arrays replace the existing array with the one you provide. You cannot modify, add, or delete items in an array in a piecemeal fashion."
8,Using patch in a read-modify-write cycle
8,It can be a useful practice to start by retrieving a partial response with the data you want to modify. This is especially important for resources that
8,"use ETags, since you must provide the current ETag value in the If-Match HTTP header in order to update the resource successfully. After you get the data, you can then modify the values you want to change and send the modified partial representation back with a patch request. Here is an example that assumes the Demo resource uses ETags:"
8,"GET https://www.googleapis.com/demo/v1/324?fields=etag,title,comment,characteristics"
8,Authorization: Bearer your_auth_token
8,This is the partial response:
8,200 OK
8,"""etag"": ""ETagString"""
8,"""title"": ""New title"""
8,"""comment"": ""First comment."","
8,"""characteristics"": {"
8,"""length"": ""short"","
8,"""level"": ""5"","
8,"""followers"": [""Jo"", ""Will""],"
8,"The following patch request is based on that response. As shown below, it also uses the fields parameter to limit the data returned in the patch response:"
8,"PATCH https://www.googleapis.com/demo/v1/324?fields=etag,title,comment,characteristics"
8,Authorization: Bearer your_auth_token
8,Content-Type: application/json
8,"If-Match: ""ETagString"""
8,"""etag"": ""ETagString"""
8,"""title"": """","
8,/* Clear the value of the title by setting it to the empty string. */
8,"""comment"": null,"
8,/* Delete the comment by replacing its value with null. */
8,"""characteristics"": {"
8,"""length"": ""short"","
8,"""level"": ""10"","
8,/* Modify the level value. */
8,"""followers"": [""Jo"", ""Liz""], /* Replace the followers array to delete Will and add Liz. */"
8,"""accuracy"": ""high"""
8,/* Add a new characteristic. */
8,"The server responds with a 200 OK HTTP status code, and the partial representation of the updated resource:"
8,200 OK
8,"""etag"": ""newETagString"""
8,"""title"": """","
8,/* Title is cleared; deleted comment field is missing. */
8,"""characteristics"": {"
8,"""length"": ""short"","
8,"""level"": ""10"","
8,/* Value is updated.*/
8,"""followers"": [""Jo"" ""Liz""], /* New follower Liz is present; deleted Will is missing. */"
8,"""accuracy"": ""high"""
8,/* New characteristic is present. */
8,Constructing a patch request directly
8,"For some patch requests, you need to base them on the data you previously retrieved. For example, if you want to add an item to an array and don't want to lose any of the existing array elements, you must get the existing data first. Similarly, if an API uses ETags, you need to send the previous ETag value with your request in order to update the resource successfully."
8,"Note: You can use an ""If-Match: *"" HTTP header to force a patch to go through when ETags are in use.  If you do this, you don't need to do the read before the write."
8,"For other situations, however, you can construct the patch request directly, without first retrieving the existing data. For example, you can easily set up a patch request that updates a field to a new value or adds a new field. Here is an example:"
8,"PATCH https://www.googleapis.com/demo/v1/324?fields=comment,characteristics"
8,Authorization: Bearer your_auth_token
8,Content-Type: application/json
8,"""comment"": ""A new comment"","
8,"""characteristics"": {"
8,"""volume"": ""loud"","
8,"""accuracy"": null"
8,"With this request, if the comment field has an existing value, the new value overwrites it; otherwise it is set to the new value. Similarly, if there was a volume characteristic, its value is overwritten; if not, it is created. The accuracy field, if set, is removed."
8,Handling the response to a patch
8,"After processing a valid patch request, the API returns a 200 OK HTTP response code along with the complete representation of the modified resource. If ETags are used by the API, the server updates ETag values when it successfully processes a patch request, just as it does with PUT."
8,The patch request returns the entire resource representation unless you use the fields parameter to reduce the amount of data it returns.
8,"If a patch request results in a new resource state that is syntactically or semantically invalid, the server returns a 400 Bad Request or 422 Unprocessable Entity HTTP status code, and the resource state remains unchanged. For example, if you attempt to delete the value for a required field, the server returns an error."
8,Alternate notation when PATCH HTTP verb is not supported
8,"If your firewall does not allow HTTP PATCH requests, then do an HTTP POST request and set the override header to PATCH, as shown below:"
8,POST https://www.googleapis.com/...
8,X-HTTP-Method-Override: PATCH
8,...
8,Difference between patch and update
8,"In practice, when you send data for an update request that uses the HTTP PUT verb, you only need to send those fields which are either required or optional; if you send values for fields that are set by the server, they are ignored. Although this might seem like another way to do a partial update, this approach has some limitations. With updates that use the HTTP PUT verb, the request fails if you don't supply required parameters, and it clears previously set data if you don't supply optional parameters."
8,"It's much safer to use patch for this reason. You only supply data for the fields you want to change; fields that you omit are not cleared. The only exception to this rule occurs with repeating elements or arrays: If you omit all of them, they stay just as they are; if you provide any of them, the whole set is replaced with the set that you provide."
8,Send feedback
8,"Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates."
8,Last updated 2020-11-16 UTC.
8,Why Google
8,Choosing Google Cloud
8,Trust and security
8,Open cloud
8,Global infrastructure
8,Customers and case studies
8,Analyst reports
8,Whitepapers
8,Products and pricing
8,Google Cloud pricing
8,Google Workspace pricing
8,Maps Platform pricing
8,See all products
8,Solutions
8,Application modernization
8,Artificial Intelligence
8,Business application platform
8,Database solutions
8,Infrastructure modernization
8,Productivity & collaboration
8,Security
8,Smart analytics
8,DevOps
8,Industries
8,Small business
8,See all solutions
8,Resources
8,Google Cloud documentation
8,Google Cloud quickstarts
8,Google Cloud Marketplace
8,Google Workspace Marketplace
8,Support
8,Code samples
8,Tutorials
8,Training
8,Certifications
8,Google Developers
8,Google Cloud for Startups
8,System status
8,Release Notes
8,Engage
8,Contact sales
8,Find a Partner
8,Become a Partner
8,Blog
8,Events
8,Podcast
8,Community
8,Press center
8,Google Cloud on YouTube
8,Google Cloud Platform on YouTube
8,Google Workspace on YouTube
8,Follow on Twitter
8,Join User Research
8,We're hiring. Join Google Cloud!
8,About Google
8,Privacy
8,Site terms
8,Google Cloud terms
8,Carbon neutral since 2007
8,Sign up for the Google Cloud newsletter
8,Subscribe
8,Language
8,English
8,Deutsch
8,Español – América Latina
8,Français
8,Português – Brasil
8,中文 – 简体
8,日本語
8,한국어
10,Efficient Use of PostgreSQL Indexes | Heroku Dev Center
10,Skip NavigationShow navHeroku Dev CenterGet StartedDocumentationChangelogSearchGet StartedNode.jsRuby on RailsRubyPythonJavaPHPGoScalaClojureDocumentationChangelogMoreAdditional ResourcesHomeElementsProductsPricingCareersHelpStatusEventsPodcastsCompliance CenterHeroku BlogHeroku BlogFind out what's new with Heroku on our blog.Visit BlogLog inorSign up
10,View categories
10,Categories
10,Heroku Architecture
10,Dynos (app containers)
10,Stacks (operating system images)
10,Networking & DNS
10,Platform Policies
10,Platform Principles
10,Command Line
10,Deployment
10,Deploying with Git
10,Deploying with Docker
10,Deployment Integrations
10,Continuous Delivery
10,Continuous Integration
10,Language Support
10,Node.js
10,Ruby
10,Working with Bundler
10,Rails Support
10,Python
10,Working with Django
10,Background Jobs in Python
10,Java
10,Working with Maven
10,Java Database Operations
10,Working with the Play Framework
10,Java Advanced Topics
10,Working with Spring Boot
10,PHP
10,Go Dependency Management
10,Scala
10,Clojure
10,Databases & Data Management
10,Heroku Postgres
10,Postgres Basics
10,Postgres Performance
10,Postgres Data Transfer & Preservation
10,Postgres Availability
10,Postgres Special Topics
10,Heroku Redis
10,Apache Kafka on Heroku
10,Other Data Stores
10,Monitoring & Metrics
10,Logging
10,App Performance
10,Add-ons
10,All Add-ons
10,Collaboration
10,Security
10,App Security
10,Identities & Authentication
10,Compliance
10,Heroku Enterprise
10,Private Spaces
10,Infrastructure Networking
10,Enterprise Accounts
10,Enterprise Teams
10,Heroku Connect (Salesforce sync)
10,Single Sign-on (SSO)
10,Patterns & Best Practices
10,Extending Heroku
10,Platform API
10,App Webhooks
10,Heroku Labs
10,Building Add-ons
10,Add-on Development Tasks
10,Add-on APIs
10,Add-on Guidelines & Requirements
10,Building CLI Plugins
10,Developing Buildpacks
10,Dev Center
10,Accounts & Billing
10,Troubleshooting & Support
10,Databases & Data ManagementHeroku PostgresPostgres PerformanceEfficient Use of PostgreSQL Indexes
10,Efficient Use of PostgreSQL Indexes
10,English — 日本語に切り替える
10,"Last updated January 25, 2021"
10,Table of ContentsIndex TypesWhy is my query not using an index?Partial IndexesExpression IndexesUnique IndexesMulti-column IndexesB-Trees and sortingManaging and Maintaining indexes
10,"There are many types of indexes in Postgres, as well as different ways to use them. In this article we give an overview of the types of indexes available, and explain different ways of using and maintaining the most common index type: B-Trees."
10,An index is a way to efficiently retrieve a relatively small number of rows from a table. It is only useful if the number of rows to be retrieved from a table is relatively small (i.e. the condition for retrieving rows - the WHERE clause - is selective). B-Tree indexes are also useful for avoiding sorting.
10,Index Types
10,Postgres supports many different index types:
10,"B-Tree is the default that you get when you do CREATE INDEX. Virtually all databases will have some B-tree indexes. B-trees attempt to remain balanced, with the amount of data in each branch of the tree being roughly the same. Therefore the number of levels that must be traversed to find rows is always in the same ballpark. B-tree indexes can be used for equality and range queries efficiently. They can operate against all datatypes, and can also be used to retrieve NULL values. B-trees are designed to work very well with caching, even when only partially cached."
10,"Hash Indexes pre-Postgres 10 are only useful for equality comparisons, but you pretty much never want to use them since they are not transaction safe, need to be manually rebuilt after crashes, and are not replicated to followers, so the advantage over using a B-Tree is rather small. In Postgres 10 and above, hash indexes are now write-ahead logged and replicated to followers."
10,"Generalized Inverted Indexes (GIN) are useful when an index must map many values to one row, whereas B-Tree indexes are optimized for when a row has a single key value. GINs are good for indexing array values as well as for implementing full-text search."
10,"Generalized Search Tree (GiST) indexes allow you to build general balanced tree structures, and can be used for operations beyond equality and range comparisons. They are used to index the geometric data types, as well as full-text search."
10,"This article is about how to get the most out of default B-Tree indexes. For examples of GIN and GiST index usage, refer to the contrib packages."
10,Why is my query not using an index?
10,"There are many reasons why the Postgres planner may choose to not use an index. Most of the time, the planner chooses correctly, even if it isn’t obvious why. It’s okay if the same query uses an index scan on some occasions but not others. The number of rows retrieved from the table may vary based on the particular constant values the query retrieves. So, for example, it might be correct for the query planner to use an index for the query select * from foo where bar = 1, and yet not use one for the query select * from foo where bar = 2, if there happened to be far more rows with “bar” values of 2. When this happens, a sequential scan is actually most likely much faster than an index scan, so the query planner has in fact correctly judged that the cost of performing the query that way is lower."
10,Partial Indexes
10,"A partial index covers just a subset of a table’s data. It is an index with a WHERE clause. The idea is to increase the efficiency of the index by reducing its size. A smaller index takes less storage, is easier to maintain, and is faster to scan."
10,"For example, suppose you allow users to flag comments on your site, which in turn sets the flagged boolean to true. You then process flagged comments in batches. You may want to create an index like so:"
10,CREATE INDEX articles_flagged_created_at_index ON articles(created_at) WHERE flagged IS TRUE;
10,"This index will remain fairly small, and can also be used along other indexes on the more complex queries that may require it."
10,Expression Indexes
10,"Expression indexes are useful for queries that match on some function or modification of your data. Postgres allows you to index the result of that function so that searches become as efficient as searching by raw data values. For example, you may require users to store their email addresses for signing in, but you want case insensitive authentication. In that case it’s possible to store the email address as is, but do searches on WHERE lower(email) = '<lowercased-email>'. The only way to use an index in such a query is with an expression index like so:"
10,CREATE INDEX users_lower_email ON users(lower(email));
10,"Another common example is for finding rows for a given date, where we’ve stored timestamps in a datetime field but want to find them by a date casted value. An index like CREATE INDEX articles_day ON articles ( date(published_at) ) can be used by a query containing WHERE date(articles.published_at) = date('2011-03-07')."
10,Unique Indexes
10,A unique index guarantees that the table won’t have more than one row with the same value. It’s advantageous to create unique indexes for two reasons: data integrity and performance. Lookups on a unique index are generally very fast.
10,"In terms of data integrity, using a validates_uniqueness_of validation on an ActiveModel class does not really guarantee uniqueness because there can and will be concurrent users creating invalid records. Therefore you should always create the constraint at the database level - either with an index or a unique constraint."
10,"There is little distinction between unique indexes and unique constraints. Unique indexes can be thought of as lower level, since expression indexes and partial indexes cannot be created as unique constraints. Even partial unique indexes on expressions are possible."
10,Multi-column Indexes
10,"While Postgres has the ability to create multi-column indexes, it’s important to understand when it makes sense to do so. The Postgres query planner has the ability to combine and use multiple single-column indexes in a multi-column query by performing a bitmap index scan. In general, you can create an index on every column that covers query conditions and in most cases Postgres will use it, so make sure to benchmark and justify the creation of a multi-column index before you create one. As always, indexes come with a cost, and multi-column indexes can only optimize the queries that reference the columns in the index in the same order, while multiple single column indexes provide performance improvements to a larger number of queries."
10,"However there are cases where a multi-column index clearly makes sense. An index on columns (a, b) can be used by queries containing WHERE a = x AND b = y, or queries using WHERE a = x only, but will not be used by a query using WHERE b = y. So if this matches the query patterns of your application, the multi-column index approach is worth considering. Also note that in this case creating an index on a alone would be redundant."
10,B-Trees and sorting
10,"B-Tree index entries are sorted in ascending order by default. In some cases it makes sense to supply a different sort order for an index. Take the case when you’re showing a paginated list of articles, sorted by most recent published first. We may have a published_at column on our articles table. For unpublished articles, the published_at value is NULL."
10,In this case we can create an index like so:
10,CREATE INDEX articles_published_at_index ON articles(published_at DESC NULLS LAST);
10,"In Postgres 9.2 and above, it’s of note that indexes are not always required to go to the table, provided we can get everything needed from the index (i.e. no unindexed columns are of interest). This feature is called “Index-only scans”."
10,"Since we will be querying the table in sorted order by published_at and limiting the result, we may get some benefit out of creating an index in the same order. Postgres will find the rows it needs from the index in the correct order, and then go to the data blocks to retrieve the data. If the index wasn’t sorted, there’s a good chance that Postgres would read the data blocks sequentially and sort the results."
10,"This technique is mostly relevant with single column indexes when you require “nulls to sort last” behavior, because otherwise the order is already available since an index can be scanned in any direction. It becomes even more relevant when used against a multi-column index when a query requests a mixed sort order, like a ASC, b DESC."
10,Managing and Maintaining indexes
10,"Indexes in Postgres do not hold all row data. Even when an index is used in a query and matching rows where found, Postgres will go to disk to fetch the row data. Additionally, row visibility information (discussed in the MVCC article) is not stored on the index either, therefore Postgres must also go to disk to fetch that information."
10,"Having that in mind, you can see how in some cases using an index doesn’t really make sense. An index must be selective enough to reduce the number of disk lookups for it to be worth it. For example, a primary key lookup with a big enough table makes good use of an index: instead of sequentially scanning the table matching the query conditions, Postgres is able to find the targeted rows in an index, and then fetch them from disk selectively. For very small tables, for example a cities lookup table, an index may be undesirable, even if you search by city name. In that case, Postgres may decide to ignore the index in favor of a sequential scan. Postgres will decide to perform a sequential scan on any query that will hit a significant portion of a table. If you do have an index on that column, it will be a dead index that’s never used - and indexes are not free: they come at a cost in terms of storage and maintenance."
10,"For more on running production, staging, and other environments for your Heroku application, take a look at our Managing Multiple Environments article."
10,"When tuning a query and understanding what indexes make the most sense, be sure to use a database as similar as possible to what exists, or will exist in production. Whether an index is used or not depends on a number of factors, including the Postgres server configuration, the data in the table, the index and the query. For instance, trying to make a query use an index on your development machine with a small subset of “test data” will be frustrating: Postgres will determine that the dataset is so small that it’s not worth the overhead of reading through the index and then fetching the data from disk. Random I/O is much slower than sequential, so the cost of a sequential scan is lower than that of the random I/O introduced by reading the index and selectively finding the data on disk. Performing index tuning should be done on production, or on a staging environment that is as close to production as possible. On the Heroku Postgres database platform it is possible to copy your production database to a different environment quite easily."
10,"When you are ready to apply an index on your production database, keep in mind that creating an index locks the table against writes. For big tables that can mean your site is down for hours. Fortunately Postgres allows you to CREATE INDEX CONCURRENTLY, which will take much longer to build, but does not require a lock that blocks writes. Ordinary CREATE INDEX commands require a lock that blocks writes but not reads."
10,"Finally, indexes will become fragmented and unoptimized after some time, especially if the rows in the table are often updated or deleted. In those cases it may be required to perform a REINDEX leaving you with a balanced and optimized index. However be cautious about reindexing big indexes as write locks are obtained on the parent table. One strategy to achieve the same result on a live site is to build an index concurrently on the same table and columns but with a different name, and then dropping the original index and renaming the new one. This procedure, while much longer, won’t require any long running locks on the live tables."
10,"Postgres provides a lot of flexibility when it comes to creating B-tree indexes that are optimized to your specific use cases, as well as options for managing the ever-growing database behind your applications. These tips should help you keep your database healthy, and your queries snappy."
10,Keep readingPostgres Performance
10,FeedbackLog in to submit feedback.
10,Understanding Heroku Postgres Data Caching
10,Expensive Queries
10,Information & SupportGetting StartedDocumentationChangelogCompliance CenterTraining & EducationBlogPodcastsSupport ChannelsStatusLanguage ReferenceNode.jsRubyJavaPHPPythonGoScalaClojureOther ResourcesCareersElementsProductsPricingSubscribe to our monthly newsletterYour email address: RSSDev Center ArticlesDev Center ChangelogHeroku BlogHeroku News BlogHeroku Engineering Blog Heroku Podcasts TwitterDev Center ArticlesDev Center ChangelogHerokuHeroku Status Facebook Instagram Github LinkedInYouTubeHeroku is acompany © Salesforce.comheroku.comTerms of ServicePrivacyCookies
12,PostgreSQL Performance Tuning Tips - Ubiq BI
12,Toggle navigation
12,Build dashboards & reports in minutes
12,Features
12,Pricing
12,Blog
12,What is Ubiq
12,Free Trial
12,PostgreSQL Performance Tuning Tips
12,"October 20, 2020October 20, 2020"
12,Ubiq
12,"PostgreSQL performance tuning helps in database maintenance and update. It allows you to speed up your database and optimize PostgreSQL performance. Otherwise, your databases and queries will slow down over time and affect application performance. Here are the top 5 PostgreSQL performance tuning tips to help you optimize your databases and tables."
12,Best PostgreSQL Performance Tuning Tips
12,Here are some simple PostgreSQL performance tuning tips to help you improve database performance.
12,1. Using ANALYZE
12,"When we run a SQL query in PostgreSQL, it creates a query plan after parsing your query string, and based on certain database metrics and statistics that it collects based on all queries that it has run so far. These metrics need to be updated periodically, to ensure that PostgreSQL creates query execution plan based on latest information and data."
12,"ANALYZE command allows PostgreSQL to update these statistics based on latest table schema, indexes and other information. This improves query speed and performance. So every time you update table or schema, or add/update index, make sure that to run ANALYZE command."
12,2. Using EXPLAIN ANALYZE
12,"EXPLAIN command explains how the PostgreSQL query planner will execute your SQL query, which joins it will use, how it will extract data, and estimated rows of information in result."
12,When used with ANALYZE command it even provides the amount of time each of these query operations will take. It will also tell you which operations will be done in-memory. This is very useful in identifying performance bottlenecks and optimization opportunities.
12,3. Using Slow Query Log
12,"PostgreSQL even provides the ability to log slow running queries. By logging long running queries into log file, you can easily identify which queries take most of your server’s time."
12,Here are the detailed steps to enable slow query log in PostgreSQL.
12,4. Using Indexing
12,"Indexes make it easy for PostgreSQL to do lookups which are useful for WHERE conditions and JOINS. Otherwise, each of these conditions will lead to a full table lookup, which is time consuming."
12,"PostgreSQL supports various types of indexes such as B-Tree (default), Hash, GiST, SP-GiST, and GIN. Here are the detailed steps to create PostgreSQL index."
12,5. Increase maximum connections
12,"By default, PostgreSQL supports a maximum of 100 concurrent connections. This is stored in max_connections server variable. You can increase this number to support more concurrent connections and keep users from waiting. However, each connection consumes memory, so don’t increase it, unless required."
12,Some more performance tips
12,You must also consider regularly updating your PostgreSQL to the latest version. Each update is faster than its predecessor and contains important performance updates.
12,"Similarly, if possible, run your database and application on different servers. Many times, application bugs consume a lot of memory and slow down the memory available to run database queries."
12,"Hopefully, the above performance tuning tips will help you improve PostgreSQL speed and performance."
12,"About AuthorAbout UbiqUbiq is a powerful dashboard & reporting platform. Build dashboards, charts & reports for your business in minutes. Try it for free!Related posts:How to Check if PostgreSQL Array Contains ValueHow to Update Array in PostgreSQLHow to Setup Remote Connection to PostgreSQL"
12,PostgreSQL
12,postgresql performance tuning.
12,permalink.
12,Post navigation
12,How To Enable MySQL Slow Query Log in MySQLHow To Enable MySQL Query Cache
12,"About UsUbiq is a business intelligence & reporting software. Build business dashboards, charts & reports in minutes. Get insights from data quickly. Try it for free!"
12,Data Reporting
12,MySQL Reporting
12,PostgreSQL Reporting
12,Online Reporting
12,Web Reporting
12,Redshift BI Reporting
12,SQL Reporting
12,Business Intelligence
12,BI Solution
12,BI Reporting Software
12,MySQL BI Reporting Tools
12,Self Service BI
12,SaaS BI
12,Data Visualization
12,Data Visualization Tools
12,Data Analysis Tools
12,Visual Analytics
12,MySQL Charts
12,MySQL Graph Generator
12,MySQL Report Builder
12,Online Report Generator
12,Redshift Data Visualization
12,Dashboards
12,Dashboard Builder
12,Dashboard Reporting Software
12,Dashboard Creator
12,KPI Dashboard Software
12,Quicklinks
12,Contact Us
12,Docs
12,Jobs
12,BI Blog
12,Database Blog
12,Tech Blog
12,Resources
12,Security
12,Privacy
12,T&C
12,Sitemap
12,dazzling					Theme by Colorlib Powered by WordPress
14,Hibernate - Vlad Mihalcea
14,Vlad Mihalcea
14,Home
14,Blog
14,Store
14,Books
14,Courses
14,Hypersistence Optimizer
14,Documentation
14,Installation Guide
14,User Guide
14,Examples
14,Release Notes
14,Issue Tracker
14,Trial Version
14,Full Version
14,Training
14,High-Performance SQL
14,High-Performance Java Persistence
14,Consulting
14,Tutorials
14,Hibernate
14,SQL
14,Spring
14,Videos
14,Talks
14,Hibernate
14,Last modified:
14,Follow @vlad_mihalcea
14,Imagine having a tool that can automatically detect JPA and Hibernate performance issues.
14,Hypersistence Optimizer is that tool!
14,High-Performance Hibernate Tutorial
14,"I’ve been using Hibernate for almost a decade and I admit it was not an easy journey. These tutorials are snippets from my High-Performance Java Persistence book, whose main goal is to show you how to make your data access layer run a high-speeds."
14,"This material is useful for both beginners and experienced developers, so enjoy reading it."
14,"The best Tutorials on High-Performance Hibernate #Hibernate #Java #Tutorials #NewYearsResolution #Career https://t.co/wQNjz6kK24 pic.twitter.com/4wDmQYvswq— Java (@java) January 1, 2019"
14,Tips and Best Practices
14,Why and when you should use JPA
14,The best way to prevent JPA and Hibernate performance issues
14,How to detect JPA and Hibernate performance issues automatically using Hypersistence Optimizer
14,Tuning Spring Petclinic JPA and Hibernate configuration with Hypersistence Optimizer
14,Hibernate Query Performance Tuning
14,Spring Boot performance tuning
14,Spring Boot performance monitoring
14,A beginner’s guide to the high-performance-java-persistence GitHub repository
14,Hibernate performance tuning tips
14,14 High-Performance Java Persistence tips
14,9 High-Performance Tips when using MySQL with JPA and Hibernate
14,9 High-Performance Tips when using PostgreSQL with JPA and Hibernate
14,How to detect the Hibernate N+1 query problem during testing
14,Hibernate slow query log
14,A beginner’s guide to SQL injection and how you should prevent it
14,"How to store date, time, and timestamps in UTC time zone with JDBC and Hibernate"
14,The fastest way to update a table row when using Hibernate and Oracle
14,How to use database-specific or Hibernate-specific features without sacrificing portability
14,How to use the Hibernate Session doWork and doReturningWork methods
14,JPA providers market share
14,Bootstrapping
14,A beginner’s guide to JPA persistence.xml file
14,How to bootstrap Hibernate without the persistence.xml configuration file
14,How to bootstrap JPA programmatically without the persistence.xml configuration file
14,JDBC Driver Connection URL strings
14,JDBC Driver Maven dependency list
14,How to get access to database table metadata with Hibernate 5
14,How to get the entity mapping to database table binding metadata from Hibernate
14,Schema Management
14,Flyway Database Schema Migrations
14,Hibernate hbm2ddl.auto schema generation
14,Mappings
14,Basic Types
14,A beginner’s guide to Hibernate Types
14,How to implement a custom basic type using Hibernate UserType
14,JPA AttributeConverter – A Beginner’s Guide
14,How to map calculated properties with JPA and Hibernate @Formula annotation
14,How to map calculated properties with Hibernate @Generated annotation
14,How to emulate @CreatedBy and @LastModifiedBy from Spring Data using the @GeneratorType Hibernate annotation
14,How to map Date and Timestamp with JPA and Hibernate
14,What’s new in JPA 2.2 – Java 8 Date and Time Types
14,The best way to map a Java 1.8 Optional entity attribute with JPA and Hibernate
14,The best way to map an Enum Type with JPA and Hibernate
14,How to map a JPA entity to a View or SQL query using Hibernate
14,How to map the PostgreSQL inet type with JPA and Hibernate
14,How to map a PostgreSQL Range column type with JPA and Hibernate
14,How to map the Java YearMonth type with JPA and Hibernate
14,How to map java.time.Year and java.time.Month with JPA and Hibernate
14,How to map a PostgreSQL Interval to a Java Duration with Hibernate
14,How to escape SQL reserved keywords with JPA and Hibernate
14,JSON
14,"The hibernate-types open-source project offers extra Hibernate Types (e.g. JSON, ARRAY)"
14,How to map JSON objects using generic Hibernate Types
14,How to map Oracle JSON columns using JPA and Hibernate
14,How to map SQL Server JSON columns using JPA and Hibernate
14,How to store schema-less EAV (Entity-Attribute-Value) data using JSON and Hibernate
14,How to map a String JPA property to a JSON column using Hibernate
14,How to map JSON collections using JPA and Hibernate
14,Java Map to JSON mapping with JPA and Hibernate
14,How to map Java Records to JSON columns using Hibernate
14,How to encrypt and decrypt JSON properties with JPA and Hibernate
14,How to customize the Jackson ObjectMapper used by Hibernate-Types
14,How to customize the JSON Serializer used by Hibernate-Types
14,How to fix the Hibernate “No Dialect mapping for JDBC type” issue
14,How to fix the Hibernate “column is of type jsonb but expression is of type bytes” issue
14,ARRAY
14,How to map a PostgreSQL ARRAY to a Java List with JPA and Hibernate
14,How to map Java and SQL arrays with JPA and Hibernate
14,How to map a PostgreSQL Enum ARRAY to a JPA entity property using Hibernate
14,Multidimensional array mapping with JPA and Hibernate
14,Hibernate HSQLDB ARRAY Type
14,Equals and HashCode
14,"The best way to implement equals, hashCode, and toString with JPA and Hibernate"
14,How to implement equals and hashCode using the entity identifier (primary key)
14,How to implement equals and hashCode using the entity natural identifier
14,Relationships
14,A beginner’s guide to database table relationships
14,ManyToOne JPA and Hibernate association best practices
14,The best way to map a @OneToOne relationship with JPA and Hibernate
14,How to change the @OneToOne shared primary key column name with JPA and Hibernate
14,The best way to map a @OneToMany relationship with JPA and Hibernate
14,The best way to use the @ManyToMany annotation with JPA and Hibernate
14,The best way to map a many-to-many association with extra columns when using JPA and Hibernate
14,The best way to map a Composite Primary Key with JPA and Hibernate
14,How to map a composite identifier using an automatically @GeneratedValue with JPA and Hibernate
14,How to synchronize bidirectional entity associations with JPA and Hibernate
14,How to map a @ManyToOne association using a non-Primary Key column
14,How to customize an entity association JOIN ON clause with Hibernate @JoinFormula
14,How to map a JPA @ManyToOne relationship to a SQL query using the Hibernate @JoinFormula annotation
14,How to optimize unidirectional collections with JPA and Hibernate
14,How do Set and List collections behave with JPA and Hibernate
14,Advanced mapping techniques
14,Fluent API entity building with JPA and Hibernate
14,How to map an immutable entity with JPA and Hibernate
14,How to map the latest child of a parent entity using Hibernate @JoinFormula
14,How to map multiple JPA entities to one database table with Hibernate
14,How to update only a subset of entity attributes using JPA and Hibernate @DynamicUpdate
14,How to use external XML mappings files with JPA and Hibernate
14,How to encrypt and decrypt data with Hibernate
14,The best way to soft delete with Hibernate
14,How to fix “wrong column type encountered” schema-validation errors with JPA and Hibernate
14,"How to audit entity modifications using the JPA @EntityListeners, @Embedded, and @Embeddable annotations"
14,How to use @PrePersist and @PreUpdate on Embeddable with JPA and Hibernate
14,How to map camelCase properties to snake_case column names with Hibernate
14,Identifiers
14,A beginner’s guide to natural and surrogate database keys
14,"Hibernate Identity, Sequence, and Table (Sequence) generator"
14,How to generate JPA entity identifier values using a database sequence
14,The hi/lo algorithm
14,Hibernate pooled and pooled-lo identifier generators
14,A beginner’s guide to Hibernate enhanced identifier generators
14,How to migrate the hilo Hibernate identifier optimizer to the pooled strategy
14,Why you should never use the TABLE identifier generator with JPA and Hibernate
14,Why should not use the AUTO JPA GenerationType with MySQL and Hibernate
14,How to replace the TABLE identifier generator with either SEQUENCE or IDENTITY in a portable way
14,PostgreSQL SERIAL column and Hibernate IDENTITY generator
14,How to combine the Hibernate assigned generator with a sequence or an identity column
14,How to implement a custom String-based sequence identifier generator with Hibernate
14,MariaDB 10.3 supports database sequences
14,Hibernate and UUID identifiers
14,How to use a JVM or database auto-generated UUID identifier with JPA and Hibernate
14,The best way to map a @NaturalId business key with JPA and Hibernate
14,Inheritance
14,The best way to use entity inheritance with JPA and Hibernate
14,The best way to map the SINGLE_TABLE inheritance with JPA and Hibernate
14,MySQL 8 support for custom SQL CHECK constraints simplifies SINGLE_TABLE inheritance data integrity validation rules
14,The best way to map the @DiscriminatorColumn with JPA and Hibernate
14,How to inherit properties from a base class entity using @MappedSuperclass with JPA and Hibernate
14,How to order entity subclasses by their class type using JPA and Hibernate
14,Connection Management
14,The simple scalability equation
14,The anatomy of Connection Pooling
14,Why you should use FlexyPool
14,How to monitor your connection pool with FlexyPool
14,How to monitor a Java EE DataSource
14,Why you should always use hibernate.connection.provider_disables_autocommit for resource-local JPA transactions
14,How does aggressive connection release work in Hibernate
14,Persistence Context
14,The JPA and Hibernate first-level cache
14,A beginner’s guide to JPA/Hibernate entity state transitions
14,A beginner’s guide to JPA and Hibernate Cascade Types
14,How does orphanRemoval work with JPA and Hibernate
14,A beginner’s guide to flush strategies in JPA and Hibernate
14,How does persist and merge work in JPA
14,"How do JPA persist, merge and Hibernate save, update, saveOrUpdate work"
14,How to merge entity collections with JPA and Hibernate
14,How does AUTO flush strategy work in Hibernate
14,How to override the default Hibernate Session FlushMode
14,How do JPA and Hibernate define the AUTO flush mode
14,A beginner’s guide to Hibernate flush operation order
14,The best way to clone or duplicate an entity with JPA and Hibernate
14,How to intercept entity changes with Hibernate event listeners
14,The anatomy of Hibernate dirty checking mechanism
14,How to customize Hibernate dirty checking mechanism
14,Fetching
14,Pagination best practices
14,JPA Default Fetch Plan
14,How do find and getReference EntityManager methods work when using JPA and Hibernate
14,N+1 query problem with JPA and Hibernate
14,The best way to fetch multiple entities by id using JPA and Hibernate
14,A beginner’s guide to Hibernate fetching strategies
14,EAGER fetching is a code smell
14,The best way to map a projection query to a DTO (Data Transfer Object) with JPA and Hibernate
14,How to write a compact DTO projection query with JPA
14,How to fetch a one-to-many DTO projection with JPA and Hibernate
14,The best way to handle the LazyInitializationException
14,The best way to lazy load entity attributes using JPA and Hibernate
14,Hibernate LazyToOne annotation
14,The best way to initialize LAZY entity and collection proxies with JPA and Hibernate
14,Why you should avoid EXTRA Lazy Collections with Hibernate
14,ResultSet statement fetching with JDBC and Hibernate
14,The best way to fix the Hibernate MultipleBagFetchException
14,How to fetch entities multiple levels deep with Hibernate
14,The Open Session In View Anti-Pattern
14,The hibernate.enable_lazy_load_no_trans Anti-Pattern
14,How does MySQL result set streaming perform vs fetching the whole JDBC ResultSet at once
14,How does a JPA Proxy work and how to unproxy it with Hibernate
14,The best way to fix the Hibernate “HHH000104: firstResult/maxResults specified with collection fetch; applying in memory!” warning message
14,How to detect HHH000104 issues with the hibernate.query.fail_on_pagination_over_collection_fetch configuration property
14,Bytecode Enhancement
14,Maven and Gradle Hibernate Enhance Plugin
14,How to enable bytecode enhancement dirty checking in Hibernate
14,How does the bytecode enhancement dirty checking mechanism work in Hibernate 4.3
14,Concurrency Control
14,Data knowledge stack
14,A beginner’s guide to ACID and database transactions
14,Optimistc vs. Pessimistic Locking
14,A beginner’s guide to database deadlock
14,How does the 2PL (Two-Phase Locking) algorithm work
14,How does MVCC (Multi-Version Concurrency Control) work
14,How to get the current database transaction id
14,How to log the database transaction id using MDC (Mapped Diagnostic Context)
14,How does the entity version property work when merging with JPA and Hibernate
14,Optimistic locking version property with JPA and Hibernate
14,The best way to map an entity version property with JPA and Hibernate
14,How do PostgreSQL advisory locks work
14,How to implement a database job queue using SKIP LOCKED
14,A beginner’s guide to database locking and the lost update phenomena
14,How to prevent lost updates in long conversations
14,A beginner’s guide to Dirty Read anomaly
14,A beginner’s guide to Non-Repeatable Read anomaly
14,A beginner’s guide to Phantom Read anomaly
14,A beginner’s guide to Read and Write Skew phenomena
14,"A beginner’s guide to the Write Skew anomaly, and how it differs between 2PL and MVCC"
14,"How does database pessimistic locking interact with INSERT, UPDATE, and DELETE SQL statements"
14,"How do UPSERT and MERGE work in Oracle, SQL Server, PostgreSQL, and MySQL"
14,Logical vs physical clock optimistic locking
14,How to retry JPA transactions after an OptimisticLockException
14,How does Hibernate guarantee application-level repeatable reads
14,Hibernate collections optimistic locking
14,How to address the OptimisticLockException in JPA and Hibernate
14,How to prevent OptimisticLockException with Hibernate versionless optimistic locking
14,A beginner’s guide to transaction isolation levels in enterprise Java
14,A beginner’s guide to Java Persistence locking
14,How does LockModeType.OPTIMISTIC work in JPA and Hibernate
14,How to fix optimistic locking race conditions with pessimistic locking
14,How does LockModeType.OPTIMISTIC_FORCE_INCREMENT work in JPA and Hibernate
14,How does LockModeType.PESSIMISTIC_FORCE_INCREMENT work in JPA and Hibernate
14,How do LockModeType.PESSIMISTIC_READ and LockModeType.PESSIMISTIC_WRITE work in JPA and Hibernate
14,How does CascadeType.LOCK works in JPA and Hibernate
14,How to increment the parent entity version whenever a child entity gets modified with JPA and Hibernate
14,Spring read-only transaction Hibernate optimization
14,Read-write and read-only transaction routing with Spring
14,Batching
14,Batch processing best practices
14,The best way to do batch processing with JPA and Hibernate
14,How to batch INSERT and UPDATE statements with Hibernate
14,How to batch DELETE statements with Hibernate
14,How to customize the JDBC batch size for each Persistence Context with Hibernate
14,How to find which statement failed in a JDBC Batch Update
14,How to optimize the merge operation using update while batching with JPA and Hibernate
14,How to enable multi-row inserts with the PostgreSQL reWriteBatchedInserts configuration property
14,Queries
14,A beginner’s guide to JPA and Hibernate Query setParameter method
14,A beginner’s guide to JPA and Hibernate query hints
14,Query timeout with JPA and Hibernate
14,The best way to use the JPA SqlResultSetMapping
14,How to return a Map result from a JPA or Hibernate query
14,How to improve statement caching efficiency with IN clause parameter padding
14,A beginner’s guide to the Hibernate JPQL and Native Query Plan Cache
14,How to optimize JPQL and Criteria API query plans with Hibernate Statistics
14,How to intercept and modify SQL queries with the Hibernate StatementInspector
14,The best way to use the JPQL DISTINCT keyword with JPA and Hibernate
14,How to JOIN unrelated entities with JPA and Hibernate
14,How to resolve the Hibernate global database schema and catalog for native SQL queries
14,How to map table rows to columns using SQL PIVOT or CASE expressions
14,The JPA EntityManager createNativeQuery is a Magic Wand
14,Why you should use the Hibernate ResultTransformer to customize result set mappings
14,The best way to use a Hibernate ResultTransformer
14,Why you should definitely learn SQL Window Functions
14,Query pagination with JPA and Hibernate
14,What’s new in JPA 2.2 – Stream the result of a Query execution
14,How to get the actual execution plan for an Oracle SQL query using Hibernate query hints
14,How to solve the PostgreSQL :: cast operator issue with JPA and Hibernate
14,The best way to use SQL functions in JPQL or Criteria API queries with JPA and Hibernate
14,How to execute SQL functions with multiple parameters in a JPQL query with Hibernate
14,How to query parent rows when all children must match the filtering criteria with SQL and Hibernate
14,How to bind custom Hibernate parameter types to JPA queries
14,Statement Caching
14,How does a relational database execute SQL statements and prepared statements
14,MySQL JDBC Statement Caching
14,Bulk Processing
14,Bulk Update and Delete with JPA and Hibernate
14,JPA Criteria API Bulk Update and Delete
14,Bulk Update optimistic locking with JPA and Hibernate
14,Criteria Queries
14,How to write JPA Criteria API queries using Codota
14,JPA Criteria Metamodel Generation and Usage Guide
14,How to query by entity type using JPA Criteria API
14,Why you should always check the SQL statements generated by Criteria API
14,The performance penalty of Class.forName when parsing JPQL and Criteria queries
14,How does Hibernate handle JPA Criteria API literals
14,Stored Procedures
14,The best way to call a stored procedure with JPA and Hibernate
14,How to call Oracle stored procedures and functions with JPA and Hibernate
14,How to call SQL Server stored procedures and functions with JPA and Hibernate
14,How to call PostgreSQL functions (stored procedures) with JPA and Hibernate
14,How to call MySQL stored procedures and functions with JPA and Hibernate
14,Caching
14,A beginner’s guide to Cache synchronization strategies
14,Things to consider before jumping to enterprise caching
14,Caching best practices
14,How does Hibernate store second-level cache entries
14,How does Hibernate READ_ONLY CacheConcurrencyStrategy work
14,How does Hibernate NONSTRICT_READ_WRITE CacheConcurrencyStrategy work
14,How does Hibernate READ_WRITE CacheConcurrencyStrategy work
14,How does Hibernate TRANSACTIONAL CacheConcurrencyStrategy work
14,How does Hibernate Collection Cache work
14,How does Hibernate Query Cache work
14,How to use the Hibernate Query Cache for DTO projections
14,How to avoid the Hibernate Query Cache N+1 issue
14,How to cache non-existing entity fetch results with JPA and Hibernate
14,Statistics
14,A beginner’s guide to Hibernate Statistics
14,How to expose Hibernate Statistics via JMX
14,Audit Logging
14,A beginner’s guide to CDC (Change Data Capture)
14,MySQL audit logging using triggers
14,The best way to implement an audit log using Hibernate Envers
14,How to extract change data events from MySQL to Kafka using Debezium
14,Multitenancy
14,A beginner’s guide to database multitenancy
14,Hibernate database catalog multitenancy
14,Hibernate database schema multitenancy
14,Testing
14,The minimal configuration for testing Hibernate
14,Hibernate integration testing strategies
14,How to run database integration tests 20 times faster
14,How to run integration tests at warp-speed using Docker and tmpfs
14,The best way to log JDBC statements
14,The best way to detect database connection leaks
14,How to install DB2 Express-C on Docker and set up the JDBC connection properties
14,How to get started with CockroachDB and Hibernate
14,Online Workshops
14,"If you enjoyed this article, I bet you are going to love my upcoming 4-day x 4 hours High-Performance Java Persistence Online Workshop"
14,Follow @vlad_mihalcea
14,Insert details about how the information is going to be processedDOWNLOAD NOW
14,Leave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment Name *
14,Email *
14,Website
14,This site uses Akismet to reduce spam. Learn how your comment data is processed.
14,Let’s connect
14,Twitter
14,YouTube
14,LinkedIn
14,Email
14,Facebook
14,Amazon
14,GitHub
14,Find Article
14,Search
14,Book
14,Video Course
14,Hypersistence Optimizer
14,Training
14,ERP Contact
14,TutorialsHibernate
14,SQL
14,Spring
14,Git
14,FlexyPool
14,Social MediaTwitter
14,Facebook
14,YouTube
14,GitHub
14,LinkedIn
14,AboutAbout
14,FAQ
14,Archive
14,Privacy Policy
14,Terms of Service
14,Meta
14,Log in
14,Entries feed
14,Comments feed
14,WordPress.org
14,"Privacy & Cookies: This site uses cookies. By continuing to use this website, you agree to their use."
14,"To find out more, including how to control cookies, see here:"
14,Our Cookie Policy
14,Vlad Mihalcea
14,Powered by WordPress.com.
14,High-Performance Java Persistence19th - 22nd of April
14,Prepare yourself to be amazed!The best way to map JPA and Hibernate entities and associationsBatch processing best practicesThe best way to fetch data with JPA and HibernateTransactions and Concurrency ControlCaching best practices
14,Save Your Seat!
16,Tuning Your Postgres Database for High Write Loads
16,Home
16,Cloud
16,Crunchy Bridge Overview
16,"Crunchy Bridge delivers a fully managed cloud Postgres service available on multiple clouds so you can focus on your application, not your database."
16,Documentation
16,"Read up on the capabilities provided by Crunchy Bridge, including supported extensions, pricing, and features."
16,Get Started
16,Get started with Crunchy Bridge by creating your account and provision your production ready database on the cloud of your choice.
16,Products
16,Featured Products
16,Crunchy Bridge
16,"A fully managed cloud Postgres service that allows you to focus on your application, not your database."
16,Crunchy PostgreSQL for Kubernetes
16,"Kubernetes-Native, containerized PostgreSQL-as-a-Service for your choice of public, private, or hybrid cloud."
16,Crunchy High Availability PostgreSQL
16,"Integrated high-availability PostgreSQL solution for enterprises with ""always on"" data requirements."
16,All Crunchy Products
16,Crunchy Bridge
16,Crunchy PostgreSQL for Kubernetes
16,Crunchy High Availability PostgreSQL
16,Crunchy Certified PostgreSQL
16,Crunchy PostgreSQL for Cloud Foundry
16,Crunchy MLS PostgreSQL
16,Crunchy Spatial
16,Services & Support
16,Learn
16,News
16,Press Releases
16,Read up on the latest product launches and company news from Crunchy Data.
16,Tutorials
16,Katacoda Learning Portal
16,Learn PostgreSQL by example with interactive courses designed by our experts.
16,Documentation
16,Software Documentation
16,Full product documentation of your favorite PostgreSQL tools.
16,Contact Us
16,Blog
16,Download
16,Login
16,Tuning Your Postgres Database for High Write Loads
16,"October 14, 2020"
16,Tom Swartz
16,Performance
16,"As a database grows and scales up from a proof of concept to a full-fledged production instance, there are always a variety of growing pains that database administrators and systems administrators will run into."
16,"Very often, the engineers on the Crunchy Data support team help support enterprise projects which start out as small, proof of concept systems, and are then promoted to large scale production uses.  As these systems receive increased traffic load beyond their original proof-of-concept sizes, one issue may be observed in the Postgres logs as the following:"
16,LOG:
16,checkpoints are occurring too frequently (9 seconds apart)
16,HINT:
16,"Consider increasing the configuration parameter ""max_wal_size""."
16,LOG:
16,checkpoints are occurring too frequently (2 seconds apart)
16,HINT:
16,"Consider increasing the configuration parameter ""max_wal_size""."
16,"This is a classic example of a database which has not been properly tuned for a high write load. In this post, we'll discuss what this means, some possible causes for this error, and some relatively simple ways to resolve the issue."
16,Systems Settings
16,"First, a look at the system settings and a brief discussion about what this error means."
16,"The Postgres logs mentioned two specific things, checkpoints and max_wal_size. Investigating the Postgres instance to observe any settings related to these two items, we see the following:"
16,"[local]:5433 user@exampledb=# select name, setting from pg_settings where name like '%wal_size%' or name like '%checkpoint%' order by name;"
16,name
16,setting
16,------------------------------+-----------
16,checkpoint_completion_target | 0.9
16,checkpoint_flush_after
16,| 32
16,checkpoint_timeout
16,| 300
16,checkpoint_warning
16,| 30
16,log_checkpoints
16,| off
16,max_wal_size
16,| 1024
16,min_wal_size
16,| 80
16,(7 rows)
16,"max_wal_size sets the maximum amount of Write-Ahead-Logging (WAL) to grow between automatic checkpoints. This is a soft limit; WAL size can exceed max_wal_size under special circumstances, such as heavy load, a failing archive_command, or a high wal_keep_segments setting."
16,It should also be noted that increasing this parameter can increase the amount of time needed for crash recovery. The default value is 1GB (1024 MB).
16,"As discussed in previous posts, the default configuration values for PostgreSQL are typically conservative, so as to work equally well on a large server as it would on a small, resource-constrained development machine. Because of this, it's likely that the default value observed here for max_wal_size is too low for the system generating the error messages we've seen."
16,Identifying the Issue
16,"Next, let's look at why this low value for max_wal_size might be the related to the cause of the issue."
16,"Obviously, the exact cause for this issue will vary from one situation to another, but generally speaking, when max_wal_size is low, and the database has a high number of updates or inserts happening quickly, it will tend to generate WAL faster than it can be archived, and faster than standard checkpoint processes can keep up."
16,"As a result, if you have disk usage monitoring on your Postgres instance (you should!) you may also observe that the pg_wal directory increases in size dramatically as these WAL files are retained."
16,A brief aside:
16,"There's a partner parameter for max_wal_size, which is it's opposite: min_wal_size. The parameter for min_wal_size defines the minimum size to shrink the WAL. As long as WAL disk usage stays below this setting while archiving, old WAL files are always recycled for future use at a checkpoint, rather than removed. This is useful to ensure that enough WAL space is reserved to handle spikes in WAL usage, for example when running large batch jobs. The default value for this is 80 MB."
16,How to Resolve
16,PostgreSQL helpfully informs us in the log file specifically what should be done: Increase the max_wal_size.
16,"So, as suggested, edit the instance configuration files to increase the max_wal_size value to match the system's work load."
16,"The ideal value, for most use cases, is to increase the value for max_wal_size such that it can hold at least one hour's worth of logs. The caveat here, however, is that you do not want to set this value extremely high, as it can increase the amount of time needed for crash recovery. If desired, the min_wal_size can also be increased, so that the system can handle spikes in WAL usage during batch jobs and other unusual circumstances. After making the appropriate configuration changes, and reloading Postgres, we can validate that the new settings are applied, as we expect:"
16,name
16,setting
16,------------------------------+-----------
16,checkpoint_completion_target | 0.9
16,checkpoint_flush_after
16,| 32
16,checkpoint_timeout
16,| 300
16,checkpoint_warning
16,| 30
16,log_checkpoints
16,| off
16,max_wal_size
16,| 16384
16,min_wal_size
16,| 4096
16,(7 rows)
16,"With these new settings in place, and with careful monitoring of the log files and system usage, the growing pains of scaling a system such as this up from a development device to a full-fledged production instance will be all but a distant memory."
16,"For more information, and some interactive workshops on configuring PostgreSQL settings, please visit the Crunchy Postgres Developer Portal."
16,Tweet
16,PostgreSQL Monitoring for App Developers: Alerts & Troubleshooting
16,Online Upgrades in Postgres
16,Like what you're reading? Stay informed by subscribing for our newsletter!
16,Newsletter
16,Like what you're reading? Stay informed by subscribing for our newsletter!
16,Read More
16,Crunchy News
16,Privacy Policy
16,"© 2018-2021 Crunchy Data Solutions, Inc."
16,Products
16,Crunchy Bridge
16,Crunchy PostgreSQL for Kubernetes
16,Crunchy High Availability PostgreSQL
16,Crunchy Certified PostgreSQL
16,Crunchy PostgreSQL for Cloud Foundry
16,Crunchy MLS PostgreSQL
16,Crunchy Spatial
16,Services & Support
16,Enterprise PostgreSQL Support
16,Red Hat Partner
16,Trusted PostgreSQL
16,Crunchy Data Subscription
16,Resources
16,Customer Portal
16,Software Documentation
16,Blog
16,Events
16,Videos
16,DISA STIG for PostgreSQL
16,CIS Benchmark for PostgreSQL
16,Company
16,About Crunchy Data
16,News
16,Careers
16,Contact Us
16,Newsletter
23,Install and Configure PostgreSQL for Cloudera Software | 6.3.x | Cloudera Documentation
23,Documentation
23,Products
23,Services & Support
23,Solutions
23,Cloudera Enterprise 6.3.x | Other versions
23,InstallationInstalling Cloudera Manager and
23,CDHStep 4: Install Databases
23,View All Categories
23,Getting Started
23,Cloudera Personas
23,Planning a New Cloudera Enterprise Deployment
23,CDH
23,Hive
23,Impala
23,Kudu
23,Sentry
23,Spark
23,External Documentation
23,Cloudera Manager
23,Software Management
23,Parcels
23,Navigator
23,Getting Started
23,FAQ
23,Navigator Encryption
23,Navigator Key Trustee Server
23,Navigator Key HSM
23,Navigator HSM KMS
23,Navigator Encrypt
23,Proof-of-Concept Installation Guide
23,Before You Begin
23,Installing a Proof-of-Concept Cluster
23,Step 1: Run the Cloudera Manager Installer
23,Step 2: Install CDH Using the Wizard
23,Step 3: Set Up a Cluster
23,Managing the Embedded Database
23,Migrating Embedded PostgreSQL Database to External PostgreSQL Database
23,Getting Support
23,FAQ
23,Release Notes
23,Requirements and Supported Versions
23,Installation
23,Before You Install
23,Storage Space Planning for Cloudera Manager
23,Configure Network Names
23,Disabling the Firewall
23,Setting SELinux mode
23,Enable an NTP Service
23,Install Python 2.7 on Hue Hosts
23,Impala Requirements
23,Required Privileges
23,Ports
23,Cloudera Manager and Navigator
23,Navigator Encryption
23,CDH Components
23,DistCp
23,Third-Party Components
23,Recommended Role Distribution
23,Custom Installation Solutions
23,Configuring a Local Parcel Repository
23,Configuring a Local Package Repository
23,Manually Install Cloudera Software Packages
23,Creating Virtual Images of Cluster Hosts
23,Configuring a Custom Java Home Location
23,Creating a CDH Cluster Using a Cloudera Manager Template
23,Service Dependencies in Cloudera Manager
23,Installing Cloudera Manager and CDH
23,Step 1: Configure a Repository
23,Step 2: Install JDK
23,Step 3: Install Cloudera Manager Server
23,Step 4: Install Databases
23,Install and Configure MariaDB
23,Install and Configure MySQL
23,Install and Configure PostgreSQL
23,Install and Configure Oracle Database
23,Step 5: Set up the Cloudera Manager Database
23,Step 6: Install CDH and Other Software
23,Step 7: Set Up a Cluster
23,Installing Navigator Data Management
23,Installing Navigator Encryption
23,Installing Cloudera Navigator Key Trustee Server
23,Installing Cloudera Navigator Key HSM
23,Installing Key Trustee KMS
23,Installing Navigator HSM KMS Backed by Thales HSM
23,Installing Navigator HSM KMS Backed by Luna HSM
23,Installing Cloudera Navigator Encrypt
23,After Installation
23,Deploying Clients
23,Testing the Installation
23,Installing the GPL Extras Parcel
23,Migrating from Packages to Parcels
23,Migrating from Parcels to Packages
23,Secure Your Cluster
23,Troubleshooting Installation Problems
23,Uninstalling Cloudera Software
23,Uninstalling a CDH Component From a Single Host
23,Upgrade Guide
23,Cluster Management
23,Cloudera Manager
23,Cloudera Manager Admin Console
23,Home Page
23,Documentation
23,Automatic Logout
23,FAQ
23,Cloudera Manager API
23,Cluster Automation
23,Cloudera Manager Administration
23,"Starting, Stopping, and Restarting the Cloudera Manager Server"
23,Configuring Cloudera Manager Server Ports
23,Moving the Cloudera Manager Server to a New Host
23,Migrating Embedded PostgreSQL Database to External PostgreSQL Database
23,Migrating from PostgreSQL Database Server to MySQL/Oracle Database Server
23,Managing the Cloudera Manager Server Log
23,Cloudera Manager Agents
23,"Starting, Stopping, and Restarting Cloudera Manager Agents"
23,Configuring Cloudera Manager Agents
23,Managing Cloudera Manager Agent Logs
23,Configuring Network Settings
23,Managing Licenses
23,Sending Usage and Diagnostic Data to Cloudera
23,Exporting and Importing Cloudera Manager Configuration
23,Backing Up Cloudera Manager
23,Other Tasks and Settings
23,Cloudera Management Service
23,Extending Cloudera Manager
23,Cluster Configuration Overview
23,Modifying Configuration Properties Using Cloudera Manager
23,Autoconfiguration
23,Custom Configuration
23,Stale Configurations
23,Client Configuration Files
23,Viewing and Reverting Configuration Changes
23,Exporting and Importing Cloudera Manager Configuration
23,Cloudera Manager Configuration Properties Reference
23,Managing Clusters
23,Adding and Deleting Clusters
23,"Starting, Stopping, Refreshing, and Restarting a Cluster"
23,Pausing a Cluster in AWS
23,Renaming a Cluster
23,Cluster-Wide Configuration
23,Virtual Private Clusters and Cloudera SDX
23,Compatibility Considerations for Virtual Private Clusters
23,"Tutorial: Using Impala, Hive and Hue with Virtual Private Clusters"
23,Networking Considerations for Virtual Private Clusters
23,Managing Services
23,HBase
23,HDFS
23,Data Durability
23,Enabling Erasure Coding
23,NameNodes
23,Backing Up and Restoring HDFS Metadata
23,Moving NameNode Roles
23,Sizing NameNode Heap Memory
23,Backing Up and Restoring NameNode Metadata
23,DataNodes
23,Configuring Storage Directories for DataNodes
23,Configuring Storage Balancing for DataNodes
23,Performing Disk Hot Swap for DataNodes
23,JournalNodes
23,Configuring Short-Circuit Reads
23,Configuring HDFS Trash
23,Preventing Inadvertent Deletion of Directories
23,HDFS Balancers
23,Enabling WebHDFS
23,Adding HttpFS
23,Adding and Configuring an NFS Gateway
23,Setting HDFS Quotas
23,Configuring Mountable HDFS
23,Configuring Centralized Cache Management in HDFS
23,Configuring Proxy Users to Access HDFS
23,Using CDH with Isilon Storage
23,Configuring Heterogeneous Storage in HDFS
23,Hive
23,Hue
23,Adding a Hue Service and Role Instance
23,Managing Hue Analytics Data Collection
23,Enabling Hue Applications Using Cloudera Manager
23,Impala
23,The Impala Service
23,Modifying Impala Startup Options
23,Post-Installation Configuration for Impala
23,Configuring Impala to Work with ODBC
23,Configuring Impala to Work with JDBC
23,Key-Value Store Indexer
23,Kudu
23,Solr
23,Spark
23,Managing Spark Using Cloudera Manager
23,Managing the Spark History Server
23,Sqoop 1 Client
23,YARN (MRv2) and MapReduce (MRv1)
23,Managing YARN
23,Managing YARN ACLs
23,Managing MapReduce
23,Managing ZooKeeper
23,Configuring Services to Use the GPL Extras Parcel
23,Managing Hosts
23,Viewing Host Details
23,Using the Host Inspector
23,Adding a Host to the Cluster
23,Specifying Racks for Hosts
23,Host Templates
23,Performing Maintenance on a Cluster Host
23,Tuning and Troubleshooting Host Decommissioning
23,Maintenance Mode
23,Changing Hostnames
23,Deleting Hosts
23,Moving a Host Between Clusters
23,Managing Services
23,Adding a Service
23,Comparing Configurations for a Service Between Clusters
23,Add-on Services
23,"Starting, Stopping, and Restarting Services"
23,Rolling Restart
23,Aborting a Pending Command
23,Deleting Services
23,Renaming a Service
23,Configuring Maximum File Descriptors
23,Exposing Hadoop Metrics to Graphite
23,Exposing Hadoop Metrics to Ganglia
23,Managing Roles
23,Role Instances
23,Role Groups
23,Monitoring and Diagnostics
23,Introduction to Cloudera Manager Monitoring
23,Time Line
23,Health Tests
23,Home Page
23,"Viewing Charts for Cluster, Service, Role, and Host Instances"
23,Configuring Monitoring Settings
23,Monitoring Clusters
23,Inspecting Network Performance
23,Monitoring Services
23,Monitoring Service Status
23,Viewing Service Status
23,Viewing Service Instance Details
23,Viewing Role Instance Status
23,The Processes Tab
23,Running Diagnostic Commands for Roles
23,Periodic Stacks Collection
23,Viewing Running and Recent Commands
23,Monitoring Resource Management
23,Monitoring Hosts
23,Host Details
23,Host Inspector
23,Monitoring Activities
23,Monitoring MapReduce Jobs
23,Viewing and Filtering MapReduce Activities
23,"Viewing the Jobs in a Pig, Oozie, or Hive Activity"
23,Task Attempts
23,Viewing Activity Details in a Report Format
23,Comparing Similar Activities
23,Viewing the Distribution of Task Attempts
23,Monitoring Impala Queries
23,Query Details
23,Monitoring YARN Applications
23,Monitoring Spark Applications
23,Events
23,Alerts
23,Managing Alerts
23,Configuring Alert Email Delivery
23,Configuring Alert SNMP Delivery
23,Configuring Custom Alert Scripts
23,Triggers
23,Cloudera Manager Trigger Use Cases
23,Lifecycle and Security Auditing
23,Charting Time-Series Data
23,Dashboards
23,tsquery Language
23,Metric Aggregation
23,Logs
23,Viewing the Cloudera Manager Server Log
23,Viewing the Cloudera Manager Agent Logs
23,Managing Disk Space for Log Files
23,Reports
23,Directory Usage Report
23,Disk Usage Reports
23,"Activity, Application, and Query Reports"
23,The File Browser
23,Downloading HDFS Directory Access Permission Reports
23,Troubleshooting Cluster Configuration and Operation
23,Monitoring Reference
23,Cloudera Manager Entity Types
23,Cloudera Manager Entity Type Attributes
23,Cloudera Manager Events
23,HEALTH_CHECK Category
23,SYSTEM Category
23,AUDIT_EVENT Category
23,HBASE Category
23,LOG_MESSAGE Category
23,ACTIVITY_EVENT Category
23,Cloudera Manager Health Tests
23,Active Database Health Tests
23,Active Key Trustee Server Health Tests
23,Activity Monitor Health Tests
23,Alert Publisher Health Tests
23,Authentication Server Health Tests
23,Authentication Server Load Balancer Health Tests
23,Authentication Service Health Tests
23,Cloudera Management Service Health Tests
23,DataNode Health Tests
23,Event Server Health Tests
23,Failover Controller Health Tests
23,Flume Health Tests
23,Flume Agent Health Tests
23,Garbage Collector Health Tests
23,HBase Health Tests
23,HBase REST Server Health Tests
23,HBase Thrift Server Health Tests
23,HDFS Health Tests
23,History Server Health Tests
23,Hive Health Tests
23,Hive Execution Health Tests
23,Hive Metastore Server Health Tests
23,HiveServer2 Health Tests
23,Host Health Tests
23,Host Monitor Health Tests
23,HttpFS Health Tests
23,Hue Health Tests
23,Hue Server Health Tests
23,Impala Health Tests
23,Impala Catalog Server Health Tests
23,Impala Daemon Health Tests
23,Impala Llama ApplicationMaster Health Tests
23,Impala StateStore Health Tests
23,JobHistory Server Health Tests
23,JobTracker Health Tests
23,JournalNode Health Tests
23,Kafka Health Tests
23,Kafka Broker Health Tests
23,Kafka MirrorMaker Health Tests
23,Kerberos Ticket Renewer Health Tests
23,Key Management Server Health Tests
23,Key Management Server Proxy Health Tests
23,Key-Value Store Indexer Health Tests
23,Kudu Health Tests
23,Lily HBase Indexer Health Tests
23,Load Balancer Health Tests
23,MapReduce Health Tests
23,Master Health Tests
23,Monitor Health Tests
23,NFS Gateway Health Tests
23,NameNode Health Tests
23,Navigator Audit Server Health Tests
23,Navigator Luna KMS Metastore Health Tests
23,Navigator Luna KMS Proxy Health Tests
23,Navigator Metadata Server Health Tests
23,Navigator Thales KMS Metastore Health Tests
23,Navigator Thales KMS Proxy Health Tests
23,NodeManager Health Tests
23,Oozie Health Tests
23,Oozie Server Health Tests
23,Passive Database Health Tests
23,Passive Key Trustee Server Health Tests
23,RegionServer Health Tests
23,Reports Manager Health Tests
23,ResourceManager Health Tests
23,SecondaryNameNode Health Tests
23,Sentry Health Tests
23,Sentry Server Health Tests
23,Service Monitor Health Tests
23,Solr Health Tests
23,Solr Server Health Tests
23,Spark Health Tests
23,Spark (Standalone) Health Tests
23,Tablet Server Health Tests
23,TaskTracker Health Tests
23,Telemetry Publisher Health Tests
23,Tracer Health Tests
23,WebHCat Server Health Tests
23,Worker Health Tests
23,YARN (MR2 Included) Health Tests
23,ZooKeeper Health Tests
23,ZooKeeper Server Health Tests
23,Cloudera Manager Metrics
23,Accumulo Metrics
23,Active Database Metrics
23,Active Key Trustee Server Metrics
23,Activity Metrics
23,Activity Monitor Metrics
23,Agent Metrics
23,Alert Publisher Metrics
23,Attempt Metrics
23,Authentication Server Metrics
23,Authentication Server Load Balancer Metrics
23,Authentication Service Metrics
23,Cloudera Management Service Metrics
23,Cloudera Manager Server Metrics
23,Cluster Metrics
23,DSSD DataNode Metrics
23,DataNode Metrics
23,Directory Metrics
23,Disk Metrics
23,Event Server Metrics
23,Failover Controller Metrics
23,Filesystem Metrics
23,Flume Metrics
23,Flume Channel Metrics
23,Flume Sink Metrics
23,Flume Source Metrics
23,Garbage Collector Metrics
23,HBase Metrics
23,HBase REST Server Metrics
23,HBase RegionServer Replication Peer Metrics
23,HBase Thrift Server Metrics
23,HDFS Metrics
23,HDFS Cache Directive Metrics
23,HDFS Cache Pool Metrics
23,HRegion Metrics
23,HTable Metrics
23,History Server Metrics
23,Hive Metrics
23,Hive Execution Metrics
23,Hive Metastore Server Metrics
23,HiveServer2 Metrics
23,Host Metrics
23,Host Monitor Metrics
23,HttpFS Metrics
23,Hue Metrics
23,Hue Server Metrics
23,Impala Metrics
23,Impala Catalog Server Metrics
23,Impala Daemon Metrics
23,Impala Daemon Resource Pool Metrics
23,Impala Llama ApplicationMaster Metrics
23,Impala Pool Metrics
23,Impala Pool User Metrics
23,Impala Query Metrics
23,Impala StateStore Metrics
23,Isilon Metrics
23,Java KeyStore KMS Metrics
23,JobHistory Server Metrics
23,JobTracker Metrics
23,JournalNode Metrics
23,Kafka Metrics
23,Kafka Broker Metrics
23,Kafka Broker Topic Metrics
23,Kafka Broker Topic Partition Metrics
23,Kafka Consumer Metrics
23,Kafka Consumer Group Metrics
23,Kafka MirrorMaker Metrics
23,Kafka Producer Metrics
23,Kafka Replica Metrics
23,Kerberos Ticket Renewer Metrics
23,Key Management Server Metrics
23,Key Management Server Proxy Metrics
23,Key Trustee KMS Metrics
23,Key Trustee Server Metrics
23,Key-Value Store Indexer Metrics
23,Kudu Metrics
23,Kudu Replica Metrics
23,Lily HBase Indexer Metrics
23,Load Balancer Metrics
23,MapReduce Metrics
23,Master Metrics
23,Monitor Metrics
23,NFS Gateway Metrics
23,NameNode Metrics
23,Navigator Audit Server Metrics
23,Navigator HSM KMS backed by SafeNet Luna HSM Metrics
23,Navigator HSM KMS backed by Thales HSM Metrics
23,Navigator Luna KMS Metastore Metrics
23,Navigator Luna KMS Proxy Metrics
23,Navigator Metadata Server Metrics
23,Navigator Thales KMS Metastore Metrics
23,Navigator Thales KMS Proxy Metrics
23,Network Interface Metrics
23,NodeManager Metrics
23,Oozie Metrics
23,Oozie Server Metrics
23,Passive Database Metrics
23,Passive Key Trustee Server Metrics
23,RegionServer Metrics
23,Reports Manager Metrics
23,ResourceManager Metrics
23,SecondaryNameNode Metrics
23,Sentry Metrics
23,Sentry Server Metrics
23,Server Metrics
23,Service Monitor Metrics
23,Solr Metrics
23,Solr Replica Metrics
23,Solr Server Metrics
23,Solr Shard Metrics
23,Spark Metrics
23,Spark (Standalone) Metrics
23,Sqoop 1 Client Metrics
23,Tablet Server Metrics
23,TaskTracker Metrics
23,Telemetry Publisher Metrics
23,Time Series Table Metrics
23,Tracer Metrics
23,User Metrics
23,WebHCat Server Metrics
23,Worker Metrics
23,YARN (MR2 Included) Metrics
23,YARN Pool Metrics
23,YARN Pool User Metrics
23,ZooKeeper Metrics
23,Disabling Metrics for Specific Roles
23,Performance Management
23,Optimizing Performance in CDH
23,Choosing and Configuring Data Compression
23,Tuning the Solr Server
23,Tuning Spark Applications
23,Tuning YARN
23,Tuning JVM Garbage Collection
23,Resource Management
23,Static Service Pools
23,Linux Control Groups (cgroups)
23,Dynamic Resource Pools
23,YARN (MRv2) and MapReduce (MRv1) Schedulers
23,Configuring the Fair Scheduler
23,Enabling and Disabling Fair Scheduler Preemption
23,Data Storage for Monitoring Data
23,Cluster Utilization Reports
23,Creating a Custom Cluster Utilization Report
23,High Availability
23,HDFS High Availability
23,Introduction to HDFS High Availability
23,Configuring Hardware for HDFS HA
23,Enabling HDFS HA
23,Disabling and Redeploying HDFS HA
23,Configuring Other CDH Components to Use HDFS HA
23,Administering an HDFS High Availability Cluster
23,Changing a Nameservice Name for Highly Available HDFS Using Cloudera Manager
23,MapReduce (MRv1) and YARN (MRv2) High Availability
23,YARN (MRv2) ResourceManager High Availability
23,Work Preserving Recovery for YARN Components
23,MapReduce (MRv1) JobTracker High Availability
23,Cloudera Navigator Key Trustee Server High Availability
23,Enabling Key Trustee KMS High Availability
23,Enabling Navigator HSM KMS High Availability
23,High Availability for Other CDH Components
23,HBase High Availability
23,HBase Read Replicas
23,Oozie High Availability
23,Search High Availability
23,Navigator Data Management in a High Availability Environment
23,Configuring Cloudera Manager for High Availability With a Load Balancer
23,Introduction to Cloudera Manager Deployment Architecture
23,Prerequisites for Setting up Cloudera Manager High Availability
23,Cloudera Manager Failover Protection
23,High-Level Steps to Configure Cloudera Manager High Availability
23,Step 1: Setting Up Hosts and the Load Balancer
23,Step 2: Installing and Configuring Cloudera Manager Server for High Availability
23,Step 3: Installing and Configuring Cloudera Management Service for High Availability
23,Step 4: Automating Failover with Corosync and Pacemaker
23,Database High Availability Configuration
23,TLS and Kerberos Configuration for Cloudera Manager High Availability
23,Backup and Disaster Recovery
23,Port Requirements for Backup and Disaster Recovery
23,Data Replication
23,Designating a Replication Source
23,HDFS Replication
23,Monitoring the Performance of HDFS Replications
23,Hive/Impala Replication
23,Monitoring the Performance of Hive/Impala Replications
23,Replicating Data to Impala Clusters
23,Using Snapshots with Replication
23,Enabling Replication Between Clusters with Kerberos Authentication
23,Replication of Encrypted Data
23,HBase Replication
23,Snapshots
23,Cloudera Manager Snapshot Policies
23,Managing HBase Snapshots
23,Managing HDFS Snapshots
23,BDR Tutorials
23,How To Back Up and Restore Apache Hive Data Using Cloudera Enterprise BDR
23,How To Back Up and Restore HDFS Data Using Cloudera Enterprise BDR
23,BDR Automation Examples
23,Migrating Data between Clusters Using distcp
23,Copying Cluster Data Using DistCp
23,Copying Data between a Secure and an Insecure Cluster using DistCp and WebHDFS
23,Post-migration Verification
23,Backing Up Databases
23,Cloudera Navigator Administration
23,Accessing Storage Using Amazon S3
23,Configuring the Amazon S3 Connector
23,"Using S3 Credentials with YARN, MapReduce, or Spark"
23,Using Fast Upload with Amazon S3
23,Configuring and Managing S3Guard
23,How to Configure a MapReduce Job to Access S3 with an HDFS Credstore
23,Importing Data into Amazon S3 Using Sqoop
23,Accessing Storage Using Microsoft ADLS
23,Configuring ADLS Access Using Cloudera Manager
23,Configuring ADLS Gen1 Connectivity
23,Configuring ADLS Gen2 Connectivity
23,Importing Data into Microsoft Azure Data Lake Store Using Sqoop
23,Configuring Google Cloud Storage Connectivity
23,How To Create a Multitenant Enterprise Data Hub
23,Security
23,Overview
23,Authentication Overview
23,Encryption Overview
23,Encryption Mechanisms
23,Authorization Overview
23,Auditing and Data Governance
23,Authentication
23,Kerberos Security Artifacts Overview
23,Configuring Authentication in Cloudera Manager
23,Cloudera Manager User Accounts
23,Configuring External Authentication and Authorization for Cloudera Manager
23,Enabling Kerberos Authentication for CDH
23,Step 1: Install Cloudera Manager and CDH
23,Step 2: Install JCE Policy Files for AES-256 Encryption
23,Step 3: Create the Kerberos Principal for Cloudera Manager Server
23,Step 4: Enabling Kerberos Using the Wizard
23,Step 5: Create the HDFS Superuser
23,Step 6: Get or Create a Kerberos Principal for Each User Account
23,Step 7: Prepare the Cluster for Each User
23,Step 8: Verify that Kerberos Security is Working
23,Step 9: (Optional) Enable Authentication for HTTP Web Consoles for Hadoop Roles
23,Kerberos Authentication for Non-Default Users
23,Customizing Kerberos Principals
23,Managing Kerberos Credentials Using Cloudera Manager
23,Using a Custom Kerberos Keytab Retrieval Script
23,Adding Trusted Realms to the Cluster
23,Using Auth-to-Local Rules to Isolate Cluster Users
23,Configuring Authentication for Cloudera Navigator
23,Cloudera Navigator and External Authentication
23,Configuring Cloudera Navigator for Active Directory
23,Configuring Cloudera Navigator for LDAP
23,Configuring Cloudera Navigator for SAML
23,Configuring Groups for Cloudera Navigator
23,Configuring Authentication for Other Components
23,Flume Authentication
23,Configuring Kerberos for Flume Thrift Source and Sink Using Cloudera Manager
23,Writing to a Secure HBase Cluster
23,Using Substitution Variables with Flume for Kerberos Artifacts
23,HBase Authentication
23,Configuring Kerberos Authentication for HBase
23,Configuring Secure HBase Replication
23,Configuring the HBase Client TGT Renewal Period
23,Hive Authentication
23,HiveServer2 Security Configuration
23,Using Hive to Run Queries on a Secure HBase Server
23,HttpFS Authentication
23,Hue Authentication
23,Enable Hue to Use Kerberos for Authentication
23,Impala Authentication
23,Enabling Kerberos Authentication for Impala
23,Enabling LDAP Authentication for Impala
23,Using Multiple Authentication Methods with Impala
23,Configuring Impala Delegation for Hue and BI Tools
23,Cloudera Search Authentication
23,Using Kerberos with Cloudera Search
23,Spark Authentication
23,Sqoop1 Authentication
23,ZooKeeper Authentication
23,Configuring a Dedicated MIT KDC for Cross-Realm Trust
23,Integrating MIT Kerberos and Active Directory
23,Hadoop Users (user:group) and Kerberos Principals
23,Mapping Kerberos Principals to Short Names
23,Authorization
23,Cloudera Manager User Roles
23,HDFS Extended ACLs
23,Authorization for HDFS Web UIs
23,Configuring LDAP Group Mappings
23,Authorization With Apache Sentry
23,Configuring HBase Authorization
23,Encrypting Data in Transit
23,Understanding Keystores and Truststores
23,Configuring TLS Encryption for Cloudera Manager and CDH Using Auto-TLS
23,Manually Configuring TLS Encryption for Cloudera Manager
23,Manually Configuring TLS Encryption on the Agent Listening Port
23,Manually Configuring TLS/SSL Encryption for CDH Services
23,"Configuring TLS/SSL for HDFS, YARN and MapReduce"
23,Configuring TLS/SSL for HBase
23,Configuring TLS/SSL for Flume
23,Configuring Encrypted Communication Between HiveServer2 and Client Drivers
23,Configuring TLS/SSL for Hue
23,Configuring TLS/SSL for Impala
23,Configuring TLS/SSL for Oozie
23,Configuring TLS/SSL for Solr
23,Spark Encryption
23,Configuring TLS/SSL for HttpFS
23,Configuring TLS/SSL for Navigator Audit Server
23,Configuring TLS/SSL for Navigator Metadata Server
23,Configuring TLS/SSL for Kafka (Navigator Event Broker)
23,Configuring Encrypted Transport for HDFS
23,Configuring Encrypted Transport for HBase
23,Encrypting Data at Rest
23,Data at Rest Encryption Reference Architecture
23,Data at Rest Encryption Requirements
23,Resource Planning for Data at Rest Encryption
23,HDFS Transparent Encryption
23,Optimizing Performance for HDFS Transparent Encryption
23,Enabling HDFS Encryption Using the Wizard
23,Managing Encryption Keys and Zones
23,Configuring the Key Management Server (KMS)
23,Securing the Key Management Server (KMS)
23,Configuring KMS Access Control Lists (ACLs)
23,Migrating from a Key Trustee KMS to an HSM KMS
23,Migrating Keys from a Java KeyStore to Cloudera Navigator Key Trustee Server
23,Migrating a Key Trustee KMS Server Role Instance to a New Host
23,Configuring CDH Services for HDFS Encryption
23,Cloudera Navigator Key Trustee Server
23,Backing Up and Restoring Key Trustee Server and Clients
23,Initializing Standalone Key Trustee Server
23,Configuring a Mail Transfer Agent for Key Trustee Server
23,Verifying Cloudera Navigator Key Trustee Server Operations
23,Managing Key Trustee Server Organizations
23,Managing Key Trustee Server Certificates
23,Cloudera Navigator Key HSM
23,Initializing Navigator Key HSM
23,HSM-Specific Setup for Cloudera Navigator Key HSM
23,Validating Key HSM Settings
23,Managing the Navigator Key HSM Service
23,Integrating Key HSM with Key Trustee Server
23,Cloudera Navigator Encrypt
23,Registering Cloudera Navigator Encrypt with Key Trustee Server
23,Preparing for Encryption Using Cloudera Navigator Encrypt
23,Encrypting and Decrypting Data Using Cloudera Navigator Encrypt
23,Converting from Device Names to UUIDs for Encrypted Devices
23,Navigator Encrypt Access Control List
23,Maintaining Cloudera Navigator Encrypt
23,Configuring Encryption for Data Spills
23,Configuring Encrypted On-disk File Channels for Flume
23,Impala Security Overview
23,Security Guidelines for Impala
23,Securing Impala Data and Log Files
23,Installation Considerations for Impala Security
23,Securing the Hive Metastore Database
23,Securing the Impala Web User Interface
23,Kudu Security Overview
23,How-To Guides
23,Add Root and Intermediate CAs to Truststore for TLS/SSL
23,Amazon S3 Security
23,Authenticate Kerberos Principals Using Java
23,Check Cluster Security Settings
23,Configure Antivirus Software on CDH Hosts
23,Configure Browser-based Interfaces to Require Authentication (SPNEGO)
23,Configure Browsers for Kerberos Authentication (SPNEGO)
23,Configure Cluster to Use Kerberos Authentication
23,"Convert DER, JKS, PEM Files for TLS/SSL Artifacts"
23,Configure Authentication for Amazon S3
23,Configure Encryption for Amazon S3
23,Configure AWS Credentials
23,Enable Sensitive Data Redaction
23,Log a Security Support Case
23,Obtain and Deploy Keys and Certificates for TLS/SSL
23,Renew and Redistribute Certificates
23,Set Up a Gateway Host to Restrict Access to the Cluster
23,Set Up Access to Cloudera EDH or Altus Director (Microsoft Azure Marketplace)
23,Use Self-Signed Certificates for TLS
23,Troubleshooting Security Issues
23,Error Messages
23,Authentication and Kerberos Issues
23,HDFS Encryption Issues
23,Key Trustee KMS Encryption Issues
23,TLS/SSL Issues
23,"YARN, MRv1, and Linux OS Security"
23,TaskController Error Codes (MRv1)
23,ContainerExecutor Error Codes (YARN)
23,Cloudera Navigator Data Management
23,Overview
23,Search
23,Performing Actions on Entities
23,Auditing
23,Using Audit Events to Understand Cluster Activity
23,Exploring Audit Data
23,Cloudera Navigator Audit Event Reports
23,Analytics
23,Policies
23,Lineage
23,Using the Lineage View
23,Using Lineage to Display Table Schema
23,Generating Lineage Diagrams
23,Business Metadata
23,Defining Managed Properties
23,Adding and Editing Metadata
23,Administration (Navigator Console)
23,Managing Metadata Storage with Purge
23,Administering Navigator User Roles
23,Navigator Configuration and Management
23,Accessing Navigator Data Management Logs
23,Backing Up Cloudera Navigator Data
23,Authentication and Authorization
23,Configuring Cloudera Navigator to work with Hue HA
23,Cloudera Navigator support for Virtual Private Clusters
23,Encryption (TLS/SSL) and Cloudera Navigator
23,Limiting Sensitive Data in Navigator Logs
23,Preventing Concurrent Logins from the Same User
23,Navigator Audit Server Management
23,Setting Up Navigator Audit Server
23,Enabling Audit and Log Collection for Services
23,Configuring Service Auditing Properties
23,Adding Audit Filters
23,Monitoring Navigator Audit Service Health
23,Publishing Audit Events
23,Maintaining Navigator Audit Server
23,Navigator Metadata Server Management
23,Setting Up Navigator Metadata Server
23,Navigator Metadata Server Tuning
23,Configuring and Managing Extraction
23,Hive and Impala Lineage Configuration
23,Configuring the Server for Policy Messages
23,Cloudera Navigator and the Cloud
23,Using Cloudera Navigator with Altus Clusters
23,Configuring Extraction for Altus Clusters on AWS
23,Using Cloudera Navigator with Amazon S3
23,Configuring Extraction for Amazon S3
23,Cloudera Navigator APIs
23,Navigator APIs Overview
23,Applying Metadata to HDFS and Hive Entities using the API
23,Using the Purge APIs for Metadata Maintenance Tasks
23,Cloudera Navigator Reference
23,Lineage Diagram Icons
23,Search Syntax and Properties
23,Service Audit Events
23,Service Metadata Entity Types
23,Metadata Policy Expressions
23,User Roles and Privileges Reference
23,Troubleshooting Navigator Data Management
23,CDH Component Guides
23,Crunch
23,Flume
23,Configuring
23,Configuring the Flume Properties File
23,Files Installed by the Flume RPM and Debian Packages
23,Configuring Flume Security with Kafka
23,Using & Managing
23,Running Flume
23,"Supported Sources, Sinks, and Channels"
23,Flume Kudu Sink
23,Viewing the Flume Documentation
23,HBase
23,Configuring
23,Accessing HBase by using the HBase Shell
23,HBase Online Merge
23,Using MapReduce with HBase
23,Configuring HBase Garbage Collection
23,Configuring the HBase Canary
23,Configuring the Blocksize for HBase
23,Configuring the HBase BlockCache
23,Configuring Quotas
23,Configuring the HBase Scanner Heartbeat
23,Limiting the Speed of Compactions
23,Configuring and Using the HBase REST API
23,Configuring HBase MultiWAL Support
23,Storing Medium Objects (MOBs) in HBase
23,Configuring the Storage Policy for the Write-Ahead Log (WAL)
23,Using & Managing
23,Starting and Stopping HBase
23,Accessing HBase by using the HBase Shell
23,Using HBase Command-Line Utilities
23,Using the HBCK2 Tool to Remediate HBase Clusters
23,Hedged Reads
23,Reading Data from HBase
23,HBase Filtering
23,Writing Data to HBase
23,Importing Data Into HBase
23,Exposing HBase Metrics to a Ganglia Server
23,Using HashTable and SyncTable Tool
23,Security
23,Troubleshooting
23,Hive
23,Installation and Upgrade
23,Configuring
23,Configuring HiveServer2
23,File System Permissions
23,"Starting, Stopping, & Using HS2"
23,Using Hive w/HBase
23,Installing JDBC/ODBC Drivers
23,Setting HADOOP_MAPRED_HOME
23,Using & Managing
23,Managing Hive with Cloudera Manager
23,Ingesting & Querying Data
23,Using Parquet Tables
23,Running Hive on Spark
23,Using HS2 Web UI
23,Using Query Plan Graph View
23,Accessing Table Statistics
23,Managing UDFs
23,Hive ETL Jobs on S3
23,Hive with ADLS
23,Erasure Coding with Hive
23,Removing the Hive Compilation Lock
23,Sqoop HS2 Import
23,Tuning
23,Tuning Hive on Spark
23,Tuning Hive on S3
23,Configuring HS2 HA
23,Enabling Query Vectorization
23,Hive Metastore (HMS)
23,Configuring
23,Configuring HMS
23,Configuring HMS HA
23,Configuring HMS for HDFS HA
23,Configuring Shared Amazon RDS as HMS
23,Using & Managing
23,Starting the Metastore
23,Using Metastore Schema Tool
23,Data Replication
23,Security
23,HCatalog
23,HCatalog Prerequisites
23,Configuration Change on Hosts Used with HCatalog
23,Accessing Table Information with the HCatalog Command-line API
23,Accessing Table Data with MapReduce
23,Accessing Table Data with Pig
23,Accessing Table Information with REST
23,Viewing the HCatalog Documentation
23,Troubleshooting
23,Hue
23,Hue Versions
23,Reference Architecture
23,Installation & Upgrade
23,Using
23,Enable SQL Editor Autocompleter
23,Use Governance-Based Data Discovery
23,Use S3 as Source or Sink in Hue
23,Administration
23,Configuring
23,Customize Hue Web UI
23,Enable Governance-Based Data Discovery
23,Enable S3 Cloud Storage
23,Run Shell Commands
23,Connecting a Database
23,Connect to MySQL or MariaDB
23,Connect to PostgreSQL
23,Connect to Oracle (Parcel)
23,Connect to Oracle (Package)
23,Custom Database Tutorial
23,Migrate the Database
23,Populate the Database
23,Performance Tuning
23,Add Load Balancer
23,Configure High Availability
23,Hue/HDFS High Availability
23,Security
23,User Permissions
23,Create Password Scripts
23,Authenticate Users with LDAP
23,Synchronize with LDAP Server
23,Authenticate Users with SAML
23,Authorize Groups with Sentry
23,Troubleshooting
23,Potential Misconfiguration
23,Unable to connect to database with provided credential
23,Unable to view Snappy-compressed files
23,“Unknown Attribute Name” exception while enabling SAML
23,Invalid query handle
23,Services backed by Postgres fail or hang
23,Downloading query results from Hue takes long time
23,Bad status: 3 (PLAIN auth failed: Error validating LDAP user)
23,502 Proxy Error while accessing Hue from the Load Balancer
23,Hue Load Balancer does not start after enabling TLS
23,Impala
23,Concepts and Architecture
23,Components
23,Developing Applications
23,Role in the Hadoop Ecosystem
23,Deployment Planning
23,Impala Requirements
23,Designing Schemas
23,Tutorials
23,Administration
23,Setting Timeouts
23,Load-Balancing Proxy for HA
23,Managing Disk Space
23,Auditing
23,Viewing Lineage Info
23,SQL Reference
23,Comments
23,Data Types
23,ARRAY Complex Type (CDH 5.5 or higher only)
23,BIGINT
23,BOOLEAN
23,CHAR
23,DECIMAL
23,DOUBLE
23,FLOAT
23,INT
23,MAP Complex Type (CDH 5.5 or higher only)
23,REAL
23,SMALLINT
23,STRING
23,STRUCT Complex Type (CDH 5.5 or higher only)
23,TIMESTAMP
23,Customizing Time Zones
23,TINYINT
23,VARCHAR
23,Complex Types (CDH 5.5 or higher only)
23,Literals
23,SQL Operators
23,Schema Objects and Object Names
23,Aliases
23,Databases
23,Functions
23,Identifiers
23,Tables
23,Views
23,SQL Statements
23,DDL Statements
23,DML Statements
23,ALTER DATABASE
23,ALTER TABLE
23,ALTER VIEW
23,COMMENT
23,COMPUTE STATS
23,CREATE DATABASE
23,CREATE FUNCTION
23,CREATE ROLE
23,CREATE TABLE
23,CREATE VIEW
23,DELETE
23,DESCRIBE
23,DROP DATABASE
23,DROP FUNCTION
23,DROP ROLE
23,DROP STATS
23,DROP TABLE
23,DROP VIEW
23,EXPLAIN
23,GRANT
23,INSERT
23,INVALIDATE METADATA
23,LOAD DATA
23,REFRESH
23,REFRESH AUTHORIZATION
23,REFRESH FUNCTIONS
23,REVOKE
23,SELECT
23,Joins
23,ORDER BY Clause
23,GROUP BY Clause
23,HAVING Clause
23,LIMIT Clause
23,OFFSET Clause
23,UNION Clause
23,Subqueries
23,TABLESAMPLE Clause
23,WITH Clause
23,DISTINCT Operator
23,SET
23,Query Options for the SET Statement
23,ABORT_ON_ERROR
23,ALLOW_ERASURE_CODED_FILES
23,ALLOW_UNSUPPORTED_FORMATS
23,APPX_COUNT_DISTINCT
23,BATCH_SIZE
23,BUFFER_POOL_LIMIT
23,COMPRESSION_CODEC
23,COMPUTE_STATS_MIN_SAMPLE_SIZE
23,DEBUG_ACTION
23,DECIMAL_V2
23,DEFAULT_JOIN_DISTRIBUTION_MODE
23,DEFAULT_SPILLABLE_BUFFER_SIZE
23,DISABLE_CODEGEN
23,DISABLE_CODEGEN_ROWS_THRESHOLD
23,DISABLE_ROW_RUNTIME_FILTERING
23,DISABLE_STREAMING_PREAGGREGATIONS
23,DISABLE_UNSAFE_SPILLS
23,ENABLE_EXPR_REWRITES
23,EXEC_SINGLE_NODE_ROWS_THRESHOLD
23,EXEC_TIME_LIMIT_S
23,EXPLAIN_LEVEL
23,HBASE_CACHE_BLOCKS
23,HBASE_CACHING
23,IDLE_SESSION_TIMEOUT
23,KUDU_READ_MODE
23,LIVE_PROGRESS
23,LIVE_SUMMARY
23,MAX_ERRORS
23,MAX_MEM_ESTIMATE_FOR_ADMISSION
23,MAX_NUM_RUNTIME_FILTERS
23,MAX_ROW_SIZE
23,MAX_SCAN_RANGE_LENGTH
23,MEM_LIMIT
23,MIN_SPILLABLE_BUFFER_SIZE
23,MT_DOP
23,NUM_NODES
23,NUM_ROWS_PRODUCED_LIMIT
23,NUM_SCANNER_THREADS
23,OPTIMIZE_PARTITION_KEY_SCANS
23,PARQUET_COMPRESSION_CODEC
23,PARQUET_ANNOTATE_STRINGS_UTF8
23,PARQUET_ARRAY_RESOLUTION
23,PARQUET_DICTIONARY_FILTERING
23,PARQUET_FALLBACK_SCHEMA_RESOLUTION
23,PARQUET_FILE_SIZE
23,PARQUET_READ_STATISTICS
23,PREFETCH_MODE
23,QUERY_TIMEOUT_S
23,REPLICA_PREFERENCE
23,REQUEST_POOL
23,RESOURCE_TRACE_RATIO
23,RUNTIME_BLOOM_FILTER_SIZE
23,RUNTIME_FILTER_MAX_SIZE
23,RUNTIME_FILTER_MIN_SIZE
23,RUNTIME_FILTER_MODE
23,RUNTIME_FILTER_WAIT_TIME_MS
23,S3_SKIP_INSERT_STAGING
23,SCAN_BYTES_LIMIT
23,SCHEDULE_RANDOM_REPLICA
23,SCRATCH_LIMIT
23,SHUFFLE_DISTINCT_EXPRS
23,SUPPORT_START_OVER
23,SYNC_DDL
23,THREAD_RESERVATION_AGGREGATE_LIMIT
23,THREAD_RESERVATION_LIMIT
23,TIMEZONE
23,TOPN_BYTES_LIMIT
23,SHOW
23,SHUTDOWN
23,TRUNCATE TABLE
23,UPDATE
23,UPSERT
23,USE
23,VALUES
23,Optimizer Hints
23,Built-In Functions
23,Mathematical Functions
23,Bit Functions
23,Type Conversion Functions
23,Date and Time Functions
23,Conditional Functions
23,String Functions
23,Miscellaneous Functions
23,Aggregate Functions
23,APPX_MEDIAN
23,AVG
23,COUNT
23,GROUP_CONCAT
23,MAX
23,MIN
23,NDV
23,"STDDEV, STDDEV_SAMP, STDDEV_POP"
23,SUM
23,"VARIANCE, VARIANCE_SAMP, VARIANCE_POP, VAR_SAMP, VAR_POP"
23,Analytic Functions
23,User-Defined Functions (UDFs)
23,SQL Differences Between Impala and Hive
23,Porting SQL
23,Resource Management
23,Admission Control and Query Queuing
23,Configuring Resource Pools and Admission Control
23,Admission Control Sample Scenario
23,Performance Tuning
23,Performance Best Practices
23,Join Performance
23,Table and Column Statistics
23,Benchmarking
23,Controlling Resource Usage
23,Runtime Filtering
23,HDFS Caching
23,HDFS Block Skew
23,Data Cache for Remote Reads
23,Testing Impala Performance
23,EXPLAIN Plans and Query Profiles
23,Scalability Considerations
23,Scaling Limits and Guidelines
23,Dedicated Coordinators
23,Metadata Management
23,Partitioning
23,File Formats
23,Text Data Files
23,Parquet Data Files
23,ORC Data Files
23,Avro Data Files
23,RCFile Data Files
23,SequenceFile Data Files
23,Using Impala to Query Kudu Tables
23,HBase Tables
23,S3 Tables
23,Configure with Cloudera Manager
23,Configure from Command Line
23,ADLS Tables
23,Logging
23,Impala Client Access
23,The Impala Shell
23,Configuration Options
23,Connecting to impalad
23,Running Commands and SQL Statements
23,Command Reference
23,Configuring Impala to Work with ODBC
23,Configuring Impala to Work with JDBC
23,Troubleshooting Impala
23,Web User Interface
23,Breakpad Minidumps
23,Ports Used by Impala
23,Impala Reserved Words
23,Impala Frequently Asked Questions
23,Kafka
23,Setup
23,Cloudera Manager
23,Clients
23,Brokers
23,Integration
23,Security
23,Managing Multiple Kafka Versions
23,Managing Topics across Multiple Kafka Clusters
23,Setting up an End-to-End Data Streaming Pipeline
23,Developing Kafka Clients
23,Metrics
23,Administration
23,Administration Basics
23,Broker Migration
23,User Limits for Kafka
23,Quotas
23,Kafka Command Line Tools
23,Disk Management
23,JBOD
23,Setup and Migration
23,Delegation Tokens
23,Enable Delegation Tokens
23,Managing Individual Delegation Tokens
23,Rotating the Master Key/Secret
23,Client Authentication
23,Kafka Security Hardening with Zookeeper ACLs
23,Kafka Streams
23,Performance Tuning
23,Handling Large Messages
23,Cluster Sizing
23,Broker Configuration
23,System-Level Broker Tuning
23,Kafka-ZooKeeper Performance Tuning
23,Reference
23,Metrics Reference
23,Useful Shell Command Reference
23,Kafka Public APIs
23,FAQ
23,Kudu
23,Concepts and Architecture
23,Usage Limitations
23,Installation and Upgrade
23,Configuration
23,Administration
23,Developing Applications with Kudu
23,Using Apache Impala with Kudu
23,Using the Hive Metastore with Kudu
23,Schema Design
23,Transaction Semantics
23,Background Tasks
23,Scaling Guide
23,Troubleshooting
23,More Resources
23,Oozie
23,Configuration
23,Configuring an External Database for Oozie
23,Oozie High Availability
23,Configuring Oozie to Use HDFS HA
23,Oozie Authentication
23,Using Sqoop Actions with Oozie
23,Configuring Oozie to Enable MapReduce Jobs To Read/Write from Amazon S3
23,Configuring Oozie to Enable MapReduce Jobs To Read/Write from Microsoft Azure (ADLS)
23,Oozie
23,"Starting, Stopping, and Accessing the Oozie Server"
23,Adding the Oozie Service Using Cloudera Manager
23,Redeploying the Oozie ShareLib
23,Configuring Oozie Data Purge Settings Using Cloudera Manager
23,Dumping and Loading an Oozie Database Using Cloudera Manager
23,Adding Schema to Oozie Using Cloudera Manager
23,Enabling the Oozie Web Console on Managed Clusters
23,Enabling Oozie SLA with Cloudera Manager
23,Setting the Oozie Database Timezone
23,Scheduling in Oozie Using Cron-like Syntax
23,Phoenix
23,Release Notes
23,Prerequisites
23,Installing Apache Phoenix using Cloudera Manager
23,Using Apache Phoenix to Store and Access Data
23,Orchestrating SQL and APIs with Apache Phoenix
23,Configuring Phoenix Query Server
23,Connecting to PQS
23,Creating and Using User-Defined Functions (UDFs) in Phoenix
23,Mapping Phoenix Schemas to HBase Namespaces
23,Associating Tables of a Schema to a Namespace
23,Using Phoenix Client to Load Data
23,Using the Index in Phoenix
23,Understanding Apache Phoenix-Spark Connector
23,Understanding Apache Phoenix-Hive Connector
23,Performance Tuning
23,Frequently Asked Questions
23,Uninstalling Phoenix Parcel
23,Search
23,Search
23,Understanding
23,Search and Other CDH Components
23,Architecture
23,Tasks and Processes
23,Tutorial
23,Validating Search Deployment
23,Preparing to Index Sample Tweets
23,Using MapReduce Batch Indexing to Index Sample Tweets
23,Near Real Time (NRT) Indexing Tweets Using Flume
23,Using Hue with Search
23,Deployment Planning
23,Schemaless Mode
23,Deploying
23,Using Search through a Proxy for High Availability
23,Using Custom JAR Files with Search
23,Cloudera Search Security
23,Enable Kerberos Authentication in Cloudera Search
23,Managing
23,Configuration
23,Collections
23,solrctl Reference
23,Example solrctl Usage
23,Migrating Solr Replicas
23,Backing Up and Restoring
23,ETL with Cloudera Morphlines
23,Example Morphline Usage
23,Indexing Data
23,Near Real Time Indexing
23,Flume NRT Indexing
23,Flume MorphlineSolrSink Configuration Options
23,Flume MorphlineInterceptor Configuration Options
23,Flume Solr UUIDInterceptor Configuration Options
23,Flume Solr BlobHandler Configuration Options
23,Flume Solr BlobDeserializer Configuration Options
23,Lily HBase NRT Indexing
23,Using the Lily HBase NRT Indexer Service
23,Configuring Lily HBase Indexer Security
23,Batch Indexing
23,Spark Indexing
23,MapReduce Indexing
23,MapReduceIndexerTool
23,Lily HBase Batch Indexing
23,FAQ
23,Troubleshooting
23,Configuration and Log Files
23,Identifying Problems
23,Solr Query Returns no Documents when Executed with a Non-Privileged User
23,Sentry
23,Before You Install Sentry
23,Installing and Upgrading the Sentry Service
23,Configuring
23,Sentry High Availability
23,Enabling Sentry Authorization for Impala
23,Configuring Sentry Authorization for Cloudera Search
23,Using & Managing
23,Synchronizing HDFS ACLs and Sentry Permissions
23,Authorization Privilege Model for Hive and Impala
23,Authorization Privilege Model for Cloudera Search
23,Hive SQL Syntax for Use with Sentry
23,Object Ownership
23,Using the Sentry Web Server
23,Sentry Debugging and Failure Scenarios
23,Troubleshooting
23,How-To Guides
23,Enabling High Availability
23,Verify HDFS ACL Sync
23,Managing Table Access in Hue
23,Spark
23,Running Your First Spark Application
23,Troubleshooting for Spark
23,Frequently Asked Questions about Apache Spark in CDH
23,Spark Application Overview
23,Developing Spark Applications
23,Developing and Running a Spark WordCount Application
23,Using Spark Streaming
23,Using Spark SQL
23,Using Spark MLlib
23,Accessing External Storage
23,Accessing Data Stored in Amazon S3 through Spark
23,Accessing Data Stored in Azure Data Lake Store (ADLS) through Spark
23,Accessing Avro Data Files From Spark SQL Applications
23,Accessing Parquet Files From Spark SQL Applications
23,Building Spark Applications
23,Configuring Spark Applications
23,Running Spark Applications
23,Running Spark Applications on YARN
23,Using PySpark
23,Running Spark Python Applications
23,Spark and IPython and Jupyter Notebooks
23,Tuning Spark Applications
23,Spark and Hadoop Integration
23,Building and Running a Crunch Application with Spark
23,File Formats and Compression
23,Parquet
23,Predicate Pushdown in Parquet
23,Avro
23,Data Compression
23,Snappy Compression
23,Glossary
23,"To read this documentation, you must turn JavaScript on."
23,Install and Configure PostgreSQL for Cloudera Software
23,"Note: The following instructions are for a dedicated PostgreSQL database for use in production environments, and are unrelated to the embedded"
23,PostgreSQL database provided by Cloudera for non-production installations.
23,"To use a PostgreSQL database, follow these procedures. For information on compatible versions of the PostgreSQL database, see Database Requirements."
23,Installing PostgreSQL Server
23,Installing the psycopg2 Python Package
23,Configuring and Starting the PostgreSQL Server
23,Creating Databases for Cloudera Software
23,Setting Up the Cloudera Manager Database
23,Installing PostgreSQL Server
23,Note:
23,"If you already have a PostgreSQL database set up, you can skip to the section Configuring and Starting the PostgreSQL"
23,Server to verify that your PostgreSQL configurations meet the requirements for Cloudera Manager.
23,"Make sure that the data directory, which by default is /var/lib/postgresql/data/, is on a partition that has sufficient free space."
23,"Cloudera Manager supports the use of a custom schema name for the Cloudera Manager Server database, but not the CDH component databases (such as Hive, Hue, Sentry, and so on). For more"
23,"information, see https://www.postgresql.org/docs/current/static/ddl-schemas.html."
23,Install the PostgreSQL packages as follows:
23,RHEL:
23,sudo yum install postgresql-server
23,SLES:
23,sudo zypper install --no-recommends postgresql96-server
23,"Note: This command installs PostgreSQL 9.6. If you want to install a different version, you can use zypper search"
23,postgresql to search for an available supported version. See Database Requirements.
23,Ubuntu:
23,sudo apt-get install postgresql
23,Installing the psycopg2 Python Package
23,"Hue in CDH 6 requires version 2.5.4 or higher of the psycopg2 Python package for connecting to a PostgreSQL database. The psycopg2 package is automatically installed as a dependency of Cloudera Manager Agent, but the version installed is often lower than 2.5.4."
23,"If you are installing or upgrading to CDH 6 and using PostgreSQL for the Hue database, you must install psycopg2 2.5.4 or higher on all Hue hosts as"
23,follows. These examples install version 2.7.5 (2.6.2 for RHEL 6):
23,RHEL 7 Compatible
23,Install the python-pip package:
23,sudo yum install python-pip
23,Install psycopg2 2.7.5 using pip:
23,sudo pip install psycopg2==2.7.5 --ignore-installed
23,RHEL 6 Compatible
23,Make sure that you have installed Python 2.7. You can verify this by running the following commands:
23,source /opt/rh/python27/enable
23,python --version
23,Install the python-pip package:
23,sudo yum install python-pip
23,Install the postgresql-devel package:
23,sudo yum install postgresql-devel
23,Install the gcc* packages:
23,sudo yum install gcc*
23,Install psycopg2 2.6.2 using pip:
23,"sudo bash -c ""source /opt/rh/python27/enable; pip install psycopg2==2.6.2 --ignore-installed"""
23,Ubuntu / Debian
23,Install the python-pip package:
23,sudo apt-get install python-pip
23,Install psycopg2 2.7.5 using pip:
23,sudo pip install psycopg2==2.7.5 --ignore-installed
23,SLES 12
23,Install the python-psycopg2 package:
23,sudo zypper install python-psycopg2
23,Configuring and Starting the PostgreSQL Server
23,"Note: If you are making changes to an existing database, make sure to stop any services that use the database before continuing."
23,"By default, PostgreSQL only accepts connections on the loopback interface. You must reconfigure PostgreSQL to accept connections from the fully qualified domain names (FQDN) of the hosts"
23,"hosting the services for which you are configuring databases. If you do not make these changes, the services cannot connect to and use the database on which they depend."
23,Make sure that LC_ALL is set to en_US.UTF-8 and initialize the database as follows:
23,RHEL 7:
23,"echo 'LC_ALL=""en_US.UTF-8""' >> /etc/locale.conf"
23,"sudo su -l postgres -c ""postgresql-setup initdb"""
23,RHEL 6:
23,"echo 'LC_ALL=""en_US.UTF-8""' >> /etc/default/locale"
23,sudo service postgresql initdb
23,SLES 12:
23,"sudo su -l postgres -c ""initdb --pgdata=/var/lib/pgsql/data --encoding=UTF-8"""
23,Ubuntu:
23,sudo service postgresql start
23,"Enable MD5 authentication. Edit pg_hba.conf, which is usually found in /var/lib/pgsql/data or /etc/postgresql/<version>/main. Add the following line:"
23,host all all 127.0.0.1/32 md5
23,If the default pg_hba.conf file contains the following line:
23,host all all 127.0.0.1/32 ident
23,"then the host line specifying md5 authentication shown above must be inserted before this ident line. Failure to do so may cause an authentication error when running the scm_prepare_database.sh script. You can modify the contents of the md5 line shown above to support different configurations. For example, if you"
23,"want to access PostgreSQL from a different host, replace 127.0.0.1 with your IP address and update postgresql.conf, which is typically"
23,"found in the same place as pg_hba.conf, to include:"
23,listen_addresses = '*'
23,Configure settings to ensure your system performs as expected. Update these settings in the /var/lib/pgsql/data/postgresql.conf or /var/lib/postgresql/data/postgresql.conf file. Settings vary based on cluster size and resources as follows:
23,"Small to mid-sized clusters - Consider the following settings as starting points. If resources are limited, consider reducing the buffer sizes and checkpoint segments further. Ongoing"
23,"tuning may be required based on each host's resource utilization. For example, if the Cloudera Manager Server is running on the same host as other roles, the following values may be acceptable:"
23,"max_connection - In general, allow each database on a host 100 maximum connections and then add 50 extra connections. You may have to increase the system"
23,"resources available to PostgreSQL, as described at Connection Settings."
23,shared_buffers - 256MB
23,wal_buffers - 8MB
23,checkpoint_segments - 16
23,"Note: The checkpoint_segments setting is removed in PostgreSQL 9.5 and higher, replaced by min_wal_size and max_wal_size. The PostgreSQL 9.5"
23,release notes provides the following formula for determining the new settings:
23,max_wal_size = (3 * checkpoint_segments) * 16MB
23,checkpoint_completion_target - 0.9
23,Large clusters - Can contain up to 1000 hosts. Consider the following settings as starting points.
23,"max_connection - For large clusters, each database is typically hosted on a different host. In general, allow each database on a host 100 maximum"
23,"connections and then add 50 extra connections. You may have to increase the system resources available to PostgreSQL, as described at Connection Settings."
23,shared_buffers - 1024 MB. This requires that the operating system can allocate sufficient shared memory. See PostgreSQL information on Managing Kernel Resources for more information on setting kernel resources.
23,wal_buffers - 16 MB. This value is derived from the shared_buffers value. Setting wal_buffers to be approximately 3% of shared_buffers up to a maximum of approximately 16 MB is sufficient in most cases.
23,checkpoint_segments - 128. The PostgreSQL Tuning
23,"Guide recommends values between 32 and 256 for write-intensive systems, such as this one."
23,"Note: The checkpoint_segments setting is removed in PostgreSQL 9.5 and higher, replaced by min_wal_size and max_wal_size. The PostgreSQL 9.5"
23,release notes provides the following formula for determining the new settings:
23,max_wal_size = (3 * checkpoint_segments) * 16MB
23,checkpoint_completion_target - 0.9.
23,Configure the PostgreSQL server to start at boot.
23,Command
23,RHEL 7 compatible
23,sudo systemctl enable postgresql
23,RHEL 6 compatible
23,sudo chkconfig postgresql on
23,SLES
23,sudo chkconfig --add postgresql
23,Ubuntu
23,sudo chkconfig postgresql on
23,Note: chkconfig may not be available on recent Ubuntu releases. You may need to use Upstart to configure PostgreSQL
23,"to start automatically when the system boots. For more information, see the Ubuntu documentation or the Upstart"
23,Cookbook.
23,Restart the PostgreSQL database:
23,RHEL 7 Compatible:
23,sudo systemctl restart postgresql
23,All Others:
23,sudo service postgresql restart
23,Creating Databases for Cloudera Software
23,Create databases and service accounts for components that require databases:
23,Cloudera Manager Server
23,Cloudera Management Service roles:
23,Activity Monitor (if using the MapReduce service in a CDH 5 cluster)
23,Reports Manager
23,Hue
23,Each Hive metastore
23,Sentry Server
23,Cloudera Navigator Audit Server
23,Cloudera Navigator Metadata Server
23,Oozie
23,The databases must be configured to support the PostgreSQL UTF8 character set encoding.
23,"Record the values you enter for database names, usernames, and passwords. The Cloudera Manager installation wizard requires this information to correctly connect to these databases."
23,Connect to PostgreSQL:
23,sudo -u postgres psql
23,Create databases for each service you are using from the below table:
23,CREATE ROLE <user> LOGIN PASSWORD '<password>';
23,CREATE DATABASE <database> OWNER <user> ENCODING 'UTF8';
23,"You can use any value you want for <database>, <user>, and <password>. The"
23,"following examples are the default names provided in the Cloudera Manager configuration settings, but you are not required to use them:"
23,Databases for Cloudera Software
23,Service
23,Database
23,User
23,Cloudera Manager Server
23,scm
23,scm
23,Activity Monitor
23,amon
23,amon
23,Reports Manager
23,rman
23,rman
23,Hue
23,hue
23,hue
23,Hive Metastore Server
23,metastore
23,hive
23,Sentry Server
23,sentry
23,sentry
23,Cloudera Navigator Audit Server
23,nav
23,nav
23,Cloudera Navigator Metadata Server
23,navms
23,navms
23,Oozie
23,oozie
23,oozie
23,"Record the databases, usernames, and passwords chosen because you will need them later."
23,"For PostgreSQL 8.4 and higher, set standard_conforming_strings=off for the Hive Metastore and Oozie databases:"
23,ALTER DATABASE <database> SET standard_conforming_strings=off;
23,Setting Up the Cloudera Manager Database
23,"After completing the above instructions to install and configure PostgreSQL databases for Cloudera software, continue to Step 5: Set up"
23,the Cloudera Manager Database to configure a database for Cloudera Manager.
23,Categories: Configuring | Databases | PostgreSQL | Starting and Stopping | All Categories
23,Install and Configure MySQL
23,Install and Configure Oracle Database
23,About Cloudera
23,Resources
23,Contact
23,Careers
23,Press
23,Documentation
23,United States: +1 888 789 1488
23,Outside the US: +1 650 362 0488
23,"© 2021 Cloudera, Inc. All rights reserved. Apache Hadoop and associated open source project names are trademarks of the Apache Software Foundation. For a complete list of trademarks, click here."
23,"If this documentation includes code, including but not limited to, code examples, Cloudera makes this available to you under the terms of the Apache License, Version 2.0, including any required"
23,notices. A copy of the Apache License Version 2.0 can be found here.
23,Terms & Conditions  |  Privacy Policy
23,"Page generated February 13, 2021."
24,Mapping data flow performance and tuning guide - Azure Data Factory | Microsoft Docs
24,Skip to main content
24,Contents
24,Exit focus mode
24,Bookmark
24,Feedback
24,Edit
24,Share
24,Twitter
24,LinkedIn
24,Facebook
24,Email
24,Table of contents
24,Mapping data flows performance and tuning guide
24,03/15/2021
24,21 minutes to read
24,In this article
24,APPLIES TO:
24,Azure Data Factory
24,Azure Synapse Analytics
24,"Mapping data flows in Azure Data Factory provide a code-free interface to design and run data transformations at scale. If you're not familiar with mapping data flows, see the Mapping Data Flow Overview. This article highlights various ways to tune and optimize your data flows so that they meet your performance benchmarks."
24,Watch the below video to see shows some sample timings transforming data with data flows.
24,Testing data flow logic
24,"When designing and testing data flows from the ADF UX, debug mode allows you to interactively test against a live Spark cluster. This allows you to preview data and execute your data flows without waiting for a cluster to warm up. For more information, see Debug Mode."
24,Monitoring data flow performance
24,"Once you verify your transformation logic using debug mode, run your data flow end-to-end as an activity in a pipeline. Data flows are operationalized in a pipeline using the execute data flow activity. The data flow activity has a unique monitoring experience compared to other Azure Data Factory activities that displays a detailed execution plan and performance profile of the transformation logic. To view detailed monitoring information of a data flow, click on the eyeglasses icon in the activity run output of a pipeline. For more information, see Monitoring mapping data flows."
24,"When monitoring data flow performance, there are four possible bottlenecks to look out for:"
24,Cluster start-up time
24,Reading from a source
24,Transformation time
24,Writing to a sink
24,"Cluster start-up time is the time it takes to spin up an Apache Spark cluster. This value is located in the top-right corner of the monitoring screen. Data flows run on a just-in-time model where each job uses an isolated cluster. This start-up time generally takes 3-5 minutes. For sequential jobs, this can be reduced by enabling a time to live value. For more information, see optimizing the Azure Integration Runtime."
24,"Data flows utilize a Spark optimizer that reorders and runs your business logic in 'stages' to perform as quickly as possible. For each sink that your data flow writes to, the monitoring output lists the duration of each transformation stage, along with the time it takes to write data into the sink. The time that is the largest is likely the bottleneck of your data flow. If the transformation stage that takes the largest contains a source, then you may want to look at further optimizing your read time. If a transformation is taking a long time, then you may need to repartition or increase the size of your integration runtime. If the sink processing time is large, you may need to scale up your database or verify you are not outputting to a single file."
24,"Once you have identified the bottleneck of your data flow, use the below optimizations strategies to improve performance."
24,Optimize tab
24,The Optimize tab contains settings to configure the partitioning scheme of the Spark cluster. This tab exists in every transformation of data flow and specifies whether you want to repartition the data after the transformation has completed. Adjusting the partitioning provides control over the distribution of your data across compute nodes and data locality optimizations that can have both positive and negative effects on your overall data flow performance.
24,"By default, Use current partitioning is selected which instructs Azure Data Factory keep the current output partitioning of the transformation. As repartitioning data takes time, Use current partitioning is recommended in most scenarios. Scenarios where you may want to repartition your data include after aggregates and joins that significantly skew your data or when using Source partitioning on a SQL DB."
24,"To change the partitioning on any transformation, select the Optimize tab and select the Set Partitioning radio button. You are presented with a series of options for partitioning. The best method of partitioning differs based on your data volumes, candidate keys, null values, and cardinality."
24,Important
24,Single partition combines all the distributed data into a single partition. This is a very slow operation that also significantly affects all downstream transformation and writes. The Azure Data Factory highly recommends against using this option unless there is an explicit business reason to do so.
24,The following partitioning options are available in every transformation:
24,Round robin
24,"Round robin distributes data equally across partitions. Use round-robin when you don't have good key candidates to implement a solid, smart partitioning strategy. You can set the number of physical partitions."
24,Hash
24,"Azure Data Factory produces a hash of columns to produce uniform partitions such that rows with similar values fall in the same partition. When you use the Hash option, test for possible partition skew. You can set the number of physical partitions."
24,Dynamic range
24,The dynamic range uses Spark dynamic ranges based on the columns or expressions that you provide. You can set the number of physical partitions.
24,Fixed range
24,"Build an expression that provides a fixed range for values within your partitioned data columns. To avoid partition skew, you should have a good understanding of your data before you use this option. The values you enter for the expression are used as part of a partition function. You can set the number of physical partitions."
24,Key
24,"If you have a good understanding of the cardinality of your data, key partitioning might be a good strategy. Key partitioning creates partitions for each unique value in your column. You can't set the number of partitions because the number is based on unique values in the data."
24,Tip
24,Manually setting the partitioning scheme reshuffles the data and can offset the benefits of the Spark optimizer. A best practice is to not manually set the partitioning unless you need to.
24,Logging level
24,"If you do not require every pipeline execution of your data flow activities to fully log all verbose telemetry logs, you can optionally set your logging level to ""Basic"" or ""None"". When executing your data flows in ""Verbose"" mode (default), you are requesting ADF to fully log activity at each individual partition level during your data transformation. This can be an expensive operation, so only enabling verbose when troubleshooting can improve your overall data flow and pipeline performance. ""Basic"" mode will only log transformation durations while ""None"" will only provide a summary of durations."
24,Optimizing the Azure Integration Runtime
24,"Data flows run on Spark clusters that are spun up at run-time. The configuration for the cluster used is defined in the integration runtime (IR) of the activity. There are three performance considerations to make when defining your integration runtime: cluster type, cluster size, and time to live."
24,"For more information how to create an Integration Runtime, see Integration Runtime in Azure Data Factory."
24,Cluster type
24,"There are three available options for the type of Spark cluster spun up: general purpose, memory optimized, and compute optimized."
24,General purpose clusters are the default selection and will be ideal for most data flow workloads. These tend to be the best balance of performance and cost.
24,"If your data flow has many joins and lookups, you may want to use a memory optimized cluster. Memory optimized clusters can store more data in memory and will minimize any out-of-memory errors you may get. Memory optimized have the highest price-point per core, but also tend to result in more successful pipelines. If you experience any out of memory errors when executing data flows, switch to a memory optimized Azure IR configuration."
24,"Compute optimized aren't ideal for ETL workflows and aren't recommended by the Azure Data Factory team for most production workloads. For simpler, non-memory intensive data transformations such as filtering data or adding derived columns, compute-optimized clusters can be used at a cheaper price per core."
24,Cluster size
24,Data flows distribute the data processing over different nodes in a Spark cluster to perform operations in parallel. A Spark cluster with more cores increases the number of nodes in the compute environment. More nodes increase the processing power of the data flow. Increasing the size of the cluster is often an easy way to reduce the processing time.
24,The default cluster size is four driver nodes and four worker nodes.
24,"As you process more data, larger clusters are recommended. Below are the possible sizing options:"
24,Worker cores
24,Driver cores
24,Total cores
24,Notes
24,Not available for compute optimized
24,128
24,144
24,256
24,272
24,"Data flows are priced at vcore-hrs meaning that both cluster size and execution-time factor into this. As you scale up, your cluster cost per minute will increase, but your overall time will decrease."
24,Tip
24,"There is a ceiling on how much the size of a cluster affects the performance of a data flow. Depending on the size of your data, there is a point where increasing the size of a cluster will stop improving performance. For example, If you have more nodes than partitions of data, adding additional nodes won't help."
24,A best practice is to start small and scale up to meet your performance needs.
24,Time to live
24,"By default, every data flow activity spins up a new cluster based upon the IR configuration. Cluster start-up time takes a few minutes and data processing can't start until it is complete. If your pipelines contain multiple sequential data flows, you can enable a time to live (TTL) value. Specifying a time to live value keeps a cluster alive for a certain period of time after its execution completes. If a new job starts using the IR during the TTL time, it will reuse the existing cluster and start up time will greatly reduced. After the second job completes, the cluster will again stay alive for the TTL time."
24,"Only one job can run on a single cluster at a time. If there is an available cluster, but two data flows start, only one will use the live cluster. The second job will spin up its own isolated cluster."
24,"If most of your data flows execute in parallel, it is not recommended that you enable TTL."
24,Note
24,Time to live is not available when using the auto-resolve integration runtime
24,Optimizing sources
24,"For every source except Azure SQL Database, it is recommended that you keep Use current partitioning as the selected value. When reading from all other source systems, data flows automatically partitions data evenly based upon the size of the data. A new partition is created for about every 128 MB of data. As your data size increases, the number of partitions increase."
24,"Any custom partitioning happens after Spark reads in the data and will negatively impact your data flow performance. As the data is evenly partitioned on read, this is not recommended."
24,Note
24,Read speeds can be limited by the throughput of your source system.
24,Azure SQL Database sources
24,Azure SQL Database has a unique partitioning option called 'Source' partitioning. Enabling source partitioning can improve your read times from Azure SQL DB by enabling parallel connections on the source system. Specify the number of partitions and how to partition your data. Use a partition column with high cardinality. You can also enter a query that matches the partitioning scheme of your source table.
24,Tip
24,"For source partitioning, the I/O of the SQL Server is the bottleneck. Adding too many partitions may saturate your source database. Generally four or five partitions is ideal when using this option."
24,Isolation level
24,"The isolation level of the read on an Azure SQL source system has an impact on performance. Choosing 'Read uncommitted' will provide the fastest performance and prevent any database locks. To learn more about SQL Isolation levels, please see Understanding isolation levels."
24,Read using query
24,"You can read from Azure SQL Database using a table or a SQL query. If you are executing a SQL query, the query must complete before transformation can start. SQL Queries can be useful to push down operations that may execute faster and reduce the amount of data read from a SQL Server such as SELECT, WHERE, and JOIN statements. When pushing down operations, you lose the ability to track lineage and performance of the transformations before the data comes into the data flow."
24,Azure Synapse Analytics sources
24,"When using Azure Synapse Analytics, a setting called Enable staging exists in the source options. This allows ADF to read from Synapse using Staging, which greatly improves read performance. Enabling Staging requires you to specify an Azure Blob Storage or Azure Data Lake Storage gen2 staging location in the data flow activity settings."
24,File-based sources
24,"While data flows support a variety of file types, the Azure Data Factory recommends using the Spark-native Parquet format for optimal read and write times."
24,"If you're running the same data flow on a set of files, we recommend reading from a folder, using wildcard paths or reading from a list of files. A single data flow activity run can process all of your files in batch. More information on how to set these settings can be found in the connector documentation such as Azure Blob Storage."
24,"If possible, avoid using the For-Each activity to run data flows over a set of files. This will cause each iteration of the for-each to spin up its own Spark cluster, which is often not necessary and can be expensive."
24,Optimizing sinks
24,"When data flows write to sinks, any custom partitioning will happen immediately before the write. Like the source, in most cases it is recommended that you keep Use current partitioning as the selected partition option. Partitioned data will write significantly quicker than unpartitioned data, even your destination is not partitioned. Below are the individual considerations for various sink types."
24,Azure SQL Database sinks
24,"With Azure SQL Database, the default partitioning should work in most cases. There is a chance that your sink may have too many partitions for your SQL database to handle. If you are running into this, reduce the number of partitions outputted by your SQL Database sink."
24,Impact of error row handling to performance
24,"When you enable error row handling (""continue on error"") in the sink transformation, ADF will take an additional step before writing the compatible rows to your destination table. This additional step will have a small performance penalty that can be in the range of 5% added for this step with an additional small performance hit also added if you set the option to also with the incompatible rows to a log file."
24,Disabling indexes using a SQL Script
24,Disabling indexes before a load in a SQL database can greatly improve performance of writing to the table. Run the below command before writing to your SQL sink.
24,ALTER INDEX ALL ON dbo.[Table Name] DISABLE
24,"After the write has completed, rebuild the indexes using the following command:"
24,ALTER INDEX ALL ON dbo.[Table Name] REBUILD
24,These can both be done natively using Pre and Post-SQL scripts within an Azure SQL DB or Synapse sink in mapping data flows.
24,Warning
24,"When disabling indexes, the data flow is effectively taking control of a database and queries are unlikely to succeed at this time. As a result, many ETL jobs are triggered in the middle of the night to avoid this conflict. For more information, learn about the constraints of disabling indexes"
24,Scaling up your database
24,"Schedule a resizing of your source and sink Azure SQL DB and DW before your pipeline run to increase the throughput and minimize Azure throttling once you reach DTU limits. After your pipeline execution is complete, resize your databases back to their normal run rate."
24,Azure Synapse Analytics sinks
24,"When writing to Azure Synapse Analytics, make sure that Enable staging is set to true. This enables ADF to write using SQL Copy Command which effectively loads the data in bulk. You will need to reference an Azure Data Lake Storage gen2 or Azure Blob Storage account for staging of the data when using Staging."
24,"Other than Staging, the same best practices apply to Azure Synapse Analytics as Azure SQL Database."
24,File-based sinks
24,"While data flows support a variety of file types, the Azure Data Factory recommends using the Spark-native Parquet format for optimal read and write times."
24,"If the data is evenly distributed, Use current partitioning will be the fastest partitioning option for writing files."
24,File name options
24,"When writing files, you have a choice of naming options that each have a performance impact."
24,Selecting the Default option will write the fastest. Each partition will equate to a file with the Spark default name. This is useful if you are just reading from the folder of data.
24,Setting a naming Pattern will rename each partition file to a more user-friendly name. This operation happens after write and is slightly slower than choosing the default. Per partition allows you to name each individual partition manually.
24,"If a column corresponds to how you wish to output the data, you can select As data in column. This reshuffles the data and can impact performance if the columns are not evenly distributed."
24,"Output to single file combines all the data into a single partition. This leads to long write times, especially for large datasets. The Azure Data Factory team highly recommends not choosing this option unless there is an explicit business reason to do so."
24,CosmosDB sinks
24,"When writing to CosmosDB, altering throughput and batch size during data flow execution can improve performance. These changes only take effect during the data flow activity run and will return to the original collection settings after conclusion."
24,"Batch size: Usually, starting with the default batch size is sufficient. To further tune this value, calculate the rough object size of your data, and make sure that object size * batch size is less than 2MB. If it is, you can increase the batch size to get better throughput."
24,Throughput: Set a higher throughput setting here to allow documents to write faster to CosmosDB. Keep in mind the higher RU costs based upon a high throughput setting.
24,"Write throughput budget: Use a value which is smaller than total RUs per minute. If you have a data flow with a high number of Spark partitions, setting a budget throughput will allow more balance across those partitions."
24,Optimizing transformations
24,"Optimizing Joins, Exists, and Lookups"
24,Broadcasting
24,"In joins, lookups, and exists transformations, if one or both data streams are small enough to fit into worker node memory, you can optimize performance by enabling Broadcasting. Broadcasting is when you send small data frames to all nodes in the cluster. This allows for the Spark engine to perform a join without reshuffling the data in the large stream. By default, the Spark engine will automatically decide whether or not to broadcast one side of a join. If you are familiar with your incoming data and know that one stream will be significantly smaller than the other, you can select Fixed broadcasting. Fixed broadcasting forces Spark to broadcast the selected stream."
24,"If the size of the broadcasted data is too large for the Spark node, you may get an out of memory error. To avoid out of memory errors, use memory optimized clusters. If you experience broadcast timeouts during data flow executions, you can switch off the broadcast optimization. However, this will result in slower performing data flows."
24,"When working with data sources that can take longer to query, like large database queries, it is recommended to turn broadcast off for joins. Source with long query times can cause Spark timeouts when the cluster attempts to broadcast to compute nodes. Another good choice for turning off broadcast is when you have a stream in your data flow that is aggregating values for use in a lookup transformation later. This pattern can confuse the Spark optimizer and cause timeouts."
24,Cross joins
24,"If you use literal values in your join conditions or have multiple matches on both sides of a join, Spark will run the join as a cross join. A cross join is a full cartesian product that then filters out the joined values. This is significantly slower than other join types. Ensure that you have column references on both sides of your join conditions to avoid the performance impact."
24,Sorting before joins
24,"Unlike merge join in tools like SSIS, the join transformation isn't a mandatory merge join operation. The join keys don't require sorting prior to the transformation. The Azure Data Factory team doesn't recommend using Sort transformations in mapping data flows."
24,Window transformation performance
24,"The Window transformation partitions your data by value in columns that you select as part of the over() clause in the transformation settings. There are a number of very popular aggregate and analytical functions that are exposed in the Windows transformation. However, if your use case is to generate a window over your entire dataset for the purpose of ranking rank() or row number rowNumber(), it is recommended that you instead use the Rank transformation and the Surrogate Key transformation. Those transformation will perform better again full dataset operations using those functions."
24,Repartitioning skewed data
24,Certain transformations such as joins and aggregates reshuffle your data partitions and can occasionally lead to skewed data. Skewed data means that data is not evenly distributed across the partitions. Heavily skewed data can lead to slower downstream transformations and sink writes. You can check the skewness of your data at any point in a data flow run by clicking on the transformation in the monitoring display.
24,"The monitoring display will show how the data is distributed across each partition along with two metrics, skewness and kurtosis. Skewness is a measure of how asymmetrical the data is and can have a positive, zero, negative, or undefined value. Negative skew means the left tail is longer than the right. Kurtosis is the measure of whether the data is heavy-tailed or light-tailed. High kurtosis values are not desirable. Ideal ranges of skewness lie between -3 and 3 and ranges of kurtosis are less than 10. An easy way to interpret these numbers is looking at the partition chart and seeing if 1 bar is significantly larger than the rest."
24,"If your data is not evenly partitioned after a transformation, you can use the optimize tab to repartition. Reshuffling data takes time and may not improve your data flow performance."
24,Tip
24,"If you repartition your data, but have downstream transformations that reshuffle your data, use hash partitioning on a column used as a join key."
24,Using data flows in pipelines
24,"When building complex pipelines with multiple data flows, your logical flow can have a big impact on timing and cost. This section covers the impact of different architecture strategies."
24,Executing data flows in parallel
24,"If you execute multiple data flows in parallel, ADF spins up separate Spark clusters for each activity. This allows for each job to be isolated and run in parallel, but will lead to multiple clusters running at the same time."
24,"If your data flows execute in parallel, its recommended to not enable the Azure IR time to live property as it will lead to multiple unused warm pools."
24,Tip
24,"Instead of running the same data flow multiple times in a for each activity, stage your data in a data lake and use wildcard paths to process the data in a single data flow."
24,Execute data flows sequentially
24,"If you execute your data flow activities in sequence, it is recommended that you set a TTL in the Azure IR configuration. ADF will reuse the compute resources resulting in a faster cluster start up time. Each activity will still be isolated receive a new Spark context for each execution."
24,"Running jobs sequentially will likely take the longest time to execute end-to-end, but provides a clean separation of logical operations."
24,Overloading a single data flow
24,"If you put all of your logic inside of a single data flow, ADF will execute the entire job on a single Spark instance. While this may seem like a way to reduce costs, it mixes together different logical flows and can be difficult to monitor and debug. If one component fails, all other parts of the job will fail as well. The Azure Data Factory team recommends organizing data flows by independent flows of business logic. If your data flow becomes too large, splitting it into separates components will make monitoring and debugging easier. While there is no hard limit on the number of transformations in a data flow, having too many will make the job complex."
24,Execute sinks in parallel
24,"The default behavior of data flow sinks is to execute each sink sequentially, in a serial manner, and to fail the data flow when an error is encountered in the sink. Additionally, all sinks are defaulted to the same group unless you go into the data flow properties and set different priorities for the sinks."
24,"Data flows allow you to group sinks together into groups from the data flow properties tab in the UI designer. You can both set the order of execution of your sinks as well as to group sinks together using the same group number. To help manage groups, you can ask ADF to run sinks in the same group, to run in parallel."
24,"On the pipeline execute data flow activity under the ""Sink Properties"" section is an option to turn on parallel sink loading. When you enable ""run in parallel"", you are instructing data flows write to connected sinks at the same time rather than in a sequential manner. In order to utilize the parallel option, the sinks must be group together and connected to the same stream via a New Branch or Conditional Split."
24,Next steps
24,See other Data Flow articles related to performance:
24,Data Flow activity
24,Monitor Data Flow performance
24,Is this page helpful?
24,Yes
24,Any additional feedback?
24,Skip
24,Submit
24,Thank you.
24,Feedback
24,Submit and view feedback for
24,This product
24,This page
24,View all page feedback
24,Theme
24,Light
24,Dark
24,High contrast
24,Previous Version Docs
24,Blog
24,Contribute
24,Privacy & Cookies
24,Terms of Use
24,Trademarks
24,© Microsoft 2021
24,Is this page helpful?
24,Yes
24,Any additional feedback?
24,Skip
24,Submit
24,Thank you.
24,In this article
24,Theme
24,Light
24,Dark
24,High contrast
24,Previous Version Docs
24,Blog
24,Contribute
24,Privacy & Cookies
24,Terms of Use
24,Trademarks
24,© Microsoft 2021
25,Performance tuning - OpenZFS
25,Performance tuning
25,From OpenZFS
25,"Jump to:					navigation, 					search"
25,This page was moved to: https://openzfs.github.io/openzfs-docs/Performance%20and%20Tuning/Workload%20Tuning.html
25,"Retrieved from ""https://openzfs.org/w/index.php?title=Performance_tuning&oldid=2948"""
25,Navigation menu
25,Personal tools
25,Create accountLog in
25,Namespaces
25,Page
25,Discussion
25,Variants
25,Views
25,Read
25,View source
25,View history
25,More
25,Search
25,Navigation
25,Developer SummitCode of ConductDistributionsCompaniesParticipateNewcomer resourcesDeveloper resourcesDocumentation
25,Tools
25,What links hereRelated changesSpecial pagesPrintable versionPermanent linkPage information
25,"This page was last modified on 21 September 2020, at 16:35."
25,Content is available under Creative Commons Attribution-ShareAlike license unless otherwise noted.
25,About OpenZFS
25,Mobile view
26,Top 10 performance tuning techniques for Amazon Redshift | AWS Big Data Blog
26,Click here to return to Amazon Web Services homepage
26,Contact Sales
26,Support
26,English
26,My Account
26,Create an AWS Account
26,Products
26,Solutions
26,Pricing
26,Documentation
26,Learn
26,Partner Network
26,AWS Marketplace
26,Customer Enablement
26,Events
26,Explore More
26,عربي
26,Bahasa Indonesia
26,Deutsch
26,English
26,Español
26,Français
26,Italiano
26,Português
26,Tiếng Việt
26,Türkçe
26,Ρусский
26,ไทย
26,日本語
26,한국어
26,中文 (简体)
26,中文 (繁體)
26,AWS Management Console
26,Account Settings
26,Billing & Cost Management
26,Security Credentials
26,AWS Personal Health Dashboard
26,Support Center
26,Knowledge Center
26,AWS Support Overview
26,Click here to return to Amazon Web Services homepage
26,Products
26,Solutions
26,Pricing
26,Introduction to AWS
26,Getting Started
26,Documentation
26,Training and Certification
26,Developer Center
26,Customer Success
26,Partner Network
26,AWS Marketplace
26,Support
26,Log into Console
26,Download the Mobile App
26,Blog Home
26,Category
26,Edition
26,Follow
26,Architecture
26,AWS Cost Management
26,AWS Partner Network
26,AWS Podcast
26,AWS Marketplace
26,AWS News
26,Big Data
26,Business Productivity
26,Compute
26,Contact Center
26,Containers
26,Database
26,Desktop & Application Streaming
26,Developer
26,DevOps
26,Enterprise Strategy
26,Front-End Web & Mobile
26,Game Tech
26,HPC
26,Infrastructure & Automation
26,Industries
26,Internet of Things
26,Machine Learning
26,Management & Governance
26,Media
26,Messaging & Targeting
26,Modernizing with AWS
26,Networking & Content Delivery
26,Open Source
26,Public Sector
26,Quantum Computing
26,Robotics
26,SAP
26,"Security, Identity, & Compliance"
26,Startups
26,Storage
26,Training & Certification
26,中国版
26,Édition Française
26,Deutsche Edition
26,日本版
26,한국 에디션
26,Edição em Português
26,Edición en Español
26,English Edition
26,Версия на русском
26,Edisi Bahasa Indonesia
26,Mailing List
26,RSS Feed
26,AWS Big Data Blog
26,Top 10 performance tuning techniques for Amazon Redshift
26,"Matt Scaer,"
26,"Manish Vazirani, and"
26,Tarun Chaudhary | on
26,28 AUG 2020 | in
26,"Amazon Redshift, Amazon Redshift, Analytics, AWS Big Data, Database |"
26,Permalink |
26,Comments |
26,Share
26,"Customers use Amazon Redshift for everything from accelerating existing database environments, to ingesting weblogs for big data analytics. Amazon Redshift is a fully managed, petabyte-scale, massively parallel data warehouse that offers simple operations and high performance. Amazon Redshift provides an open standard JDBC/ODBC driver interface, which allows you to connect your existing business intelligence (BI) tools and reuse existing analytics queries."
26,"Amazon Redshift can run any type of data model, from a production transaction system third-normal-form model to star and snowflake schemas, data vault, or simple flat tables."
26,This post takes you through the most common performance-related opportunities when adopting Amazon Redshift and gives you concrete guidance on how to optimize each one.
26,What’s new
26,"This post refreshes the Top 10 post from early 2019. We’re pleased to share the advances we’ve made since then, and want to highlight a few key points."
26,Query throughput is more important than query concurrency.
26,"Configuring concurrency, like memory management, can be relegated to Amazon Redshift’s internal ML models through Automatic WLM with Query Priorities. On production clusters across the fleet, we see the automated process assigning a much higher number of active statements for certain workloads, while a lower number for other types of use-cases. This is done to maximize throughput, a measure of how much work the Amazon Redshift cluster can do over a period of time. Examples are 300 queries a minute, or 1,500 SQL statements an hour. It’s recommended to focus on increasing throughput over concurrency, because throughput is the metric with much more direct impact on the cluster’s users."
26,"In addition to the optimized Automatic WLM settings to maximize throughput, the concurrency scaling functionality in Amazon Redshift extends the throughput capability of the cluster to up to 10 times greater than what’s delivered with the original cluster. The tenfold increase is a current soft limit, you can reach out to your account team to increase it."
26,Investing in the Amazon Redshift driver.
26,"AWS now recommends the Amazon Redshift JDBC or ODBC driver for improved performance. Each driver has optional configurations to further tune it for higher or lower number of statements, with either fewer or greater row counts in the result set."
26,Ease of use by automating all the common DBA tasks.
26,"In 2018, the SET DW “backronym” summarized the key considerations to drive performance (sort key, encoding, table maintenance, distribution, and workload management). Since then, Amazon Redshift has added automation to inform 100% of SET DW, absorbed table maintenance into the service’s (and no longer the user’s) responsibility, and enhanced out-of-the-box performance with smarter default settings. Amazon Redshift Advisor continuously monitors the cluster for additional optimization opportunities, even if the mission of a table changes over time. AWS publishes the benchmark used to quantify Amazon Redshift performance, so anyone can reproduce the results."
26,Scaling compute separately from storage with RA3 nodes and Amazon Redshift Spectrum.
26,"Although the convenient cluster building blocks of the Dense Compute and Dense Storage nodes continue to be available, you now have a variety of tools to further scale compute and storage separately. Amazon Redshift Managed Storage (the RA3 node family) allows for focusing on using the right amount of compute, without worrying about sizing for storage. Concurrency scaling lets you specify entire additional clusters of compute to be applied dynamically as-needed. Amazon Redshift Spectrum uses the functionally-infinite capacity of Amazon Simple Storage Service (Amazon S3) to support an on-demand compute layer up to 10 times the power of the main cluster, and is now bolstered with materialized view support."
26,Pause and resume feature to optimize cost of environments
26,"All Amazon Redshift clusters can use the pause and resume feature. For clusters created using On Demand, the per-second grain billing is stopped when the cluster is paused. Reserved Instance clusters can use the pause and resume feature to define access times or freeze a dataset at a point in time."
26,Tip #1: Precomputing results with Amazon Redshift materialized views
26,"Materialized views can significantly boost query performance for repeated and predictable analytical workloads such as dash-boarding, queries from BI tools, and extract, load, transform (ELT) data processing. Data engineers can easily create and maintain efficient data-processing pipelines with materialized views while seamlessly extending the performance benefits to data analysts and BI tools."
26,"Materialized views are especially useful for queries that are predictable and repeated over and over. Instead of performing resource-intensive queries on large tables, applications can query the pre-computed data stored in the materialized view."
26,"When the data in the base tables changes, you refresh the materialized view by issuing the Amazon Redshift SQL statement “refresh materialized view“. After issuing a refresh statement, your materialized view contains the same data as a regular view. Refreshes can be incremental or full refreshes (recompute). When possible, Amazon Redshift incrementally refreshes data that changed in the base tables since the materialized view was last refreshed."
26,"To demonstrate how it works, we can create an example schema to store sales information, each sale transaction and details about the store where the sales took place."
26,"To view the total amount of sales per city, we create a materialized view with the create materialized view SQL statement (city_sales) joining records from two tables and aggregating sales amount (sum(sales.amount)) per city (group by city):"
26,CREATE MATERIALIZED VIEW city_sales AS
26,"SELECT st.city, SUM(sa.amount) as total_sales"
26,"FROM sales sa, store st"
26,WHERE sa.store_id = st.id
26,GROUP BY st.city
26,"Now we can query the materialized view just like a regular view or table and issue statements like “SELECT city, total_sales FROM city_sales” to get the following results. The join between the two tables and the aggregate (sum and group by) are already computed, resulting in significantly less data to scan."
26,"When the data in the underlying base tables changes, the materialized view doesn’t automatically reflect those changes. You can refresh the data stored in the materialized view on demand with the latest changes from the base tables using the SQL refresh materialized view command. For example, see the following code:"
26,!-- let's add a row in the sales base table
26,"INSERT INTO sales (id, item, store_id, customer_id, amount)"
26,"VALUES(8, 'Gaming PC Super ProXXL', 1, 1, 3000);"
26,"SELECT city, total_sales FROM city_sales WHERE city = 'Paris'"
26,|city |total_sales|
26,|-----|-----------|
26,|Paris|
26,690|
26,!-- the new sale is not taken into account !!
26,-- let's refresh the materialized view
26,REFRESH MATERIALIZED VIEW city_sales;
26,"SELECT city, total_sales FROM city_sales WHERE city = 'Paris'"
26,|city |total_sales|
26,|-----|-----------|
26,|Paris|
26,3690|
26,!-- now the view has the latest sales data
26,The full code for this use case is available as a gist in GitHub.
26,"You can also extend the benefits of materialized views to external data in your Amazon S3 data lake and federated data sources. With materialized views, you can easily store and manage the pre-computed results of a SELECT statement referencing both external tables and Amazon Redshift tables. Subsequent queries referencing the materialized views run much faster because they use the pre-computed results stored in Amazon Redshift, instead of accessing the external tables. This also helps you reduce the associated costs of repeatedly accessing the external data sources, because you can only access them when you explicitly refresh the materialized views."
26,Tip #2: Handling bursts of workload with concurrency scaling and elastic resize
26,"The legacy, on-premises model requires you to estimate what the system will need 3-4 years in the future to make sure you’re leasing enough horsepower at the time of purchase. But the ability to resize a cluster allows for right-sizing your resources as you go. Amazon Redshift extends this ability with elastic resize and concurrency scaling."
26,"Elastic resize lets you quickly increase or decrease the number of compute nodes, doubling or halving the original cluster’s node count, or even change the node type. You can expand the cluster to provide additional processing power to accommodate an expected increase in workload, such as Black Friday for internet shopping, or a championship game for a team’s web business. Choose classic resize when you’re resizing to a configuration that isn’t available through elastic resize. Classic resize is slower but allows you to change the node type or expand beyond the doubling or halving size limitations of an elastic resize."
26,"Elastic resize completes in minutes and doesn’t require a cluster restart. For anticipated workload spikes that occur on a predictable schedule, you can automate the resize operation using the elastic resize scheduler feature on the Amazon Redshift console, the AWS Command Line Interface (AWS CLI), or API."
26,Concurrency scaling allows your Amazon Redshift cluster to add capacity dynamically in response to the workload arriving at the cluster.
26,"By default, concurrency scaling is disabled, and you can enable it for any workload management (WLM) queue to scale to a virtually unlimited number of concurrent queries, with consistently fast query performance. You can control the maximum number of concurrency scaling clusters allowed by setting the “max_concurrency_scaling_clusters” parameter value from 1 (default) to 10 (contact support to raise this soft limit). The free billing credits provided for concurrency scaling is often enough and the majority of customers using this feature don’t end up paying extra for it. For more information about the concurrency scaling billing model see Concurrency Scaling pricing."
26,"You can monitor and control the concurrency scaling usage and cost by creating daily, weekly, or monthly usage limits and instruct Amazon Redshift to automatically take action (such as logging, alerting or disabling further usage) if those limits are reached. For more information, see Managing usage limits in Amazon Redshift."
26,"Together, these options open up new ways to right-size the platform to meet demand. Before these options, you needed to size your WLM queue, or even an entire Amazon Redshift cluster, beforehand in anticipation of upcoming peaks."
26,Tip #3: Using the Amazon Redshift Advisor to minimize administrative work
26,Amazon Redshift Advisor offers recommendations specific to your Amazon Redshift cluster to help you improve its performance and decrease operating costs.
26,"Advisor bases its recommendations on observations regarding performance statistics or operations data. Advisor develops observations by running tests on your clusters to determine if a test value is within a specified range. If the test result is outside of that range, Advisor generates an observation for your cluster. At the same time, Advisor creates a recommendation about how to bring the observed value back into the best-practice range. Advisor only displays recommendations that can have a significant impact on performance and operations. When Advisor determines that a recommendation has been addressed, it removes it from your recommendation list. In this section, we share some examples of Advisor recommendations:"
26,Distribution key recommendation
26,"Advisor analyzes your cluster’s workload to identify the most appropriate distribution key for the tables that can significantly benefit from a KEY distribution style. Advisor provides ALTER TABLE statements that alter the DISTSTYLE and DISTKEY of a table based on its analysis. To realize a significant performance benefit, make sure to implement all SQL statements within a recommendation group."
26,The following screenshot shows recommendations regarding distribution keys.
26,"If you don’t see a recommendation, that doesn’t necessarily mean that the current distribution styles are the most appropriate. Advisor doesn’t provide recommendations when there isn’t enough data or the expected benefit of redistribution is small."
26,Sort key recommendation
26,"Sorting a table on an appropriate sort key can accelerate query performance, especially queries with range-restricted predicates, by requiring fewer table blocks to be read from disk."
26,Advisor analyzes your cluster’s workload over several days to identify a beneficial sort key for your tables. See the following screenshot.
26,"If you don’t see a recommendation for a table, that doesn’t necessarily mean that the current configuration is the best. Advisor doesn’t provide recommendations when there isn’t enough data or the expected benefit of sorting is small."
26,Table compression recommendation
26,"Amazon Redshift is optimized to reduce your storage footprint and improve query performance by using compression encodings. When you don’t use compression, data consumes additional space and requires additional disk I/O. Applying compression to large uncompressed columns can have a big impact on your cluster."
26,The compression analysis in Advisor tracks uncompressed storage allocated to permanent user tables. It reviews storage metadata associated with large uncompressed columns that aren’t sort key columns.
26,The following screenshot shows an example of table compression recommendation.
26,Table statistics recommendation
26,"Maintaining current statistics helps complex queries run in the shortest possible time. The Advisor analysis tracks tables whose statistics are out-of-date or missing. It reviews table access metadata associated with complex queries. If tables that are frequently accessed with complex patterns are missing statistics, Amazon Redshift Advisor creates a critical recommendation to run ANALYZE. If tables that are frequently accessed with complex patterns have out-of-date statistics, Advisor creates a suggested recommendation to run ANALYZE."
26,The following screenshot shows a table statistics recommendation.
26,Tip #4: Using Auto WLM with priorities to increase throughput
26,"Auto WLM simplifies workload management and maximizes query throughput by using ML to dynamically manage memory and concurrency, which ensures optimal utilization of the cluster resources"
26,Amazon Redshift runs queries using the queuing system (WLM). You can define up to eight queues to separate workloads from each other.
26,Amazon Redshift Advisor automatically analyzes the current WLM usage and can make recommendations to get more throughput from your cluster. Periodically reviewing the suggestions from Advisor helps you get the best performance.
26,"Query priorities is a feature of Auto WLM that lets you assign priority ranks to different user groups or query groups, to ensure that higher priority workloads get more resources for consistent query performance, even during busy times. It is a good practice to set up query monitoring rules (QMR) to monitor and manage resource intensive or runaway queries. QMR also enables you to dynamically change a query’s priority based on its runtime performance and metrics-based rules you define."
26,"For more information on migrating from manual to automatic WLM with query priorities, see Modifying the WLM configuration."
26,"It’s recommended to take advantage of Amazon Redshift’s short query acceleration (SQA). SQA uses ML to run short-running jobs in their own queue. This keeps small jobs processing, rather than waiting behind longer-running SQL statements. SQA is enabled by default in the default parameter group and for all new parameter groups. You can enable and disable SQA via a check box on the Amazon Redshift console, or by using the Amazon Redshift CLI."
26,"If you enable concurrency scaling, Amazon Redshift can automatically and quickly provision additional clusters should your workload begin to back up. This is an important consideration when deciding the cluster’s WLM configuration."
26,"A common pattern is to optimize the WLM configuration to run most SQL statements without the assistance of supplemental memory, reserving additional processing power for short jobs. Some queueing is acceptable because additional clusters spin up if your needs suddenly expand. To enable concurrency scaling on a WLM queue, set the concurrency scaling mode value to AUTO. You can best inform your decisions by reviewing the concurrency scaling billing model. You can also monitor and control the concurrency scaling usage and cost by using the Amazon Redshift usage limit feature."
26,"In some cases, unless you enable concurrency scaling for the queue, the user or query’s assigned queue may be busy, and you must wait for a queue slot to open. During this time, the system isn’t running the query at all. If this becomes a frequent problem, you may have to increase concurrency."
26,"First, determine if any queries are queuing, using the queuing_queries.sql admin script. Review the maximum concurrency that your cluster needed in the past with wlm_apex.sql, or get an hour-by-hour historical analysis with wlm_apex_hourly.sql. Keep in mind that increasing concurrency allows more queries to run, but each query gets a smaller share of the memory. You may find that by increasing concurrency, some queries must use temporary disk storage to complete, which is also sub-optimal."
26,Tip #5: Taking advantage of Amazon Redshift data lake integration
26,Amazon Redshift is tightly integrated with other AWS-native services such as Amazon S3 which let’s the Amazon Redshift cluster interact with the data lake in several useful ways.
26,"Amazon Redshift Spectrum lets you query data directly from files on Amazon S3 through an independent, elastically sized compute layer. Use these patterns independently or apply them together to offload work to the Amazon Redshift Spectrum compute layer, quickly create a transformed or aggregated dataset, or eliminate entire steps in a traditional ETL process."
26,"Use the Amazon Redshift Spectrum compute layer to offload workloads from the main cluster, and apply more processing power to the specific SQL statement. Amazon Redshift Spectrum automatically assigns compute power up to approximately 10 times the processing power of the main cluster. This may be an effective way to quickly process large transform or aggregate jobs."
26,"Skip the load in an ELT process and run the transform directly against data on Amazon S3. You can run transform logic against partitioned, columnar data on Amazon S3 with an INSERT … SELECT statement. It’s easier than going through the extra work of loading a staging dataset, joining it to other tables, and running a transform against it."
26,"Use Amazon Redshift Spectrum to run queries as the data lands in Amazon S3, rather than adding a step to load the data onto the main cluster. This allows for real-time analytics."
26,"Land the output of a staging or transformation cluster on Amazon S3 in a partitioned, columnar format. The main or reporting cluster can either query from that Amazon S3 dataset directly or load it via an INSERT … SELECT statement."
26,"Within Amazon Redshift itself, you can export the data into the data lake with the UNLOAD command, or by writing to external tables. Both options export SQL statement output to Amazon S3 in a massively parallel fashion. You can do the following:"
26,"Using familiar CREATE EXTERNAL TABLE AS SELECT and INSERT INTO SQL commands, create and populate external tables on Amazon S3 for subsequent use by Amazon Redshift or other services participating in the data lake without the need to manually maintain partitions. Materialized views can also cover external tables, further enhancing the accessibility and utility of the data lake."
26,"Using the UNLOAD command, Amazon Redshift can export SQL statement output to Amazon S3 in a massively parallel fashion. This technique greatly improves the export performance and lessens the impact of running the data through the leader node. You can compress the exported data on its way off the Amazon Redshift cluster. As the size of the output grows, so does the benefit of using this feature. For writing columnar data to the data lake, UNLOAD can write partition-aware Parquet data."
26,Tip #6: Improving the efficiency of temporary tables
26,"Amazon Redshift provides temporary tables, which act like normal tables but have a lifetime of a single SQL session. The proper use of temporary tables can significantly improve performance of some ETL operations. Unlike regular permanent tables, data changes made to temporary tables don’t trigger automatic incremental backups to Amazon S3, and they don’t require synchronous block mirroring to store a redundant copy of data on a different compute node. Due to these reasons, data ingestion on temporary tables involves reduced overhead and performs much faster. For transient storage needs like staging tables, temporary tables are ideal."
26,"You can create temporary tables using the CREATE TEMPORARY TABLE syntax, or by issuing a SELECT … INTO #TEMP_TABLE query. The CREATE TABLE statement gives you complete control over the definition of the temporary table. The SELECT … INTO and C(T)TAS commands use the input data to determine column names, sizes and data types, and use default storage properties. Consider default storage properties carefully, because they may cause problems. By default, for temporary tables, Amazon Redshift applies EVEN table distribution with no column encoding (such as RAW compression) for all columns. This data structure is sub-optimal for many types of queries."
26,"If you employ the SELECT…INTO syntax, you can’t set the column encoding, column distribution, or sort keys. The CREATE TABLE AS (CTAS) syntax instead lets you specify a distribution style and sort keys, and Amazon Redshift automatically applies LZO encoding for everything other than sort keys, Booleans, reals, and doubles. You can exert additional control by using the CREATE TABLE syntax rather than CTAS."
26,"If you create temporary tables, remember to convert all SELECT…INTO syntax into the CREATE statement. This ensures that your temporary tables have column encodings and don’t cause distribution errors within your workflow. For example, you may want to convert a statement using this syntax:"
26,"SELECT column_a, column_b INTO #my_temp_table FROM my_table;"
26,You need to analyze the temporary table for optimal column encoding:
26,Master=# analyze compression #my_temp_table;
26,Table | Column | Encoding
26,----------------+----------+---------
26,#my_temp_table | columb_a | lzo
26,#my_temp_table | columb_b | bytedict
26,(2 rows)
26,You can then convert the SELECT INTO a statement to the following:
26,BEGIN;
26,CREATE TEMPORARY TABLE my_temp_table(
26,"column_a varchar(128) encode lzo,"
26,column_b char(4) encode bytedict)
26,distkey (column_a) -- Assuming you intend to join this table on column_a
26,sortkey (column_b) -- Assuming you are sorting or grouping by column_b
26,"INSERT INTO my_temp_table SELECT column_a, column_b FROM my_table;"
26,COMMIT;
26,"If you create a temporary staging table by using a CREATE TABLE LIKE statement, the staging table inherits the distribution key, sort keys, and column encodings from the parent target table. In this case, merge operations that join the staging and target tables on the same distribution key performs faster because the joining rows are collocated. To verify that the query uses a collocated join, run the query with EXPLAIN and check for DS_DIST_NONE on all the joins."
26,"You may also want to analyze statistics on the temporary table, especially when you use it as a join table for subsequent queries. See the following code:"
26,ANALYZE my_temp_table;
26,"With this trick, you retain the functionality of temporary tables but control data placement on the cluster through distribution key assignment. You also take advantage of the columnar nature of Amazon Redshift by using column encoding."
26,Tip #7: Using QMR and Amazon CloudWatch metrics to drive additional performance improvements
26,"In addition to the Amazon Redshift Advisor recommendations, you can get performance insights through other channels."
26,"The Amazon Redshift cluster continuously and automatically collects query monitoring rules metrics, whether you institute any rules on the cluster or not. This convenient mechanism lets you view attributes like the following:"
26,The CPU time for a SQL statement (query_cpu_time)
26,The amount of temporary space a job might ‘spill to disk’ (query_temp_blocks_to_disk)
26,The ratio of the highest number of blocks read over the average (io_skew)
26,"It also makes Amazon Redshift Spectrum metrics available, such as the number of Amazon Redshift Spectrum rows and MBs scanned by a query (spectrum_scan_row_count and spectrum_scan_size_mb, respectively). The Amazon Redshift system view SVL_QUERY_METRICS_SUMMARY shows the maximum values of metrics for completed queries, and STL_QUERY_METRICS and STV_QUERY_METRICS carry the information at 1-second intervals for the completed and running queries respectively."
26,"The Amazon Redshift CloudWatch metrics are data points for use with Amazon CloudWatch monitoring. These can be cluster-wide metrics, such as health status or read/write, IOPS, latency, or throughput. It also offers compute node–level data, such as network transmit/receive throughput and read/write latency. At the WLM queue grain, there are the number of queries completed per second, queue length, and others. CloudWatch facilitates monitoring concurrency scaling usage with the metrics ConcurrencyScalingSeconds and ConcurrencyScalingActiveClusters."
26,"It’s recommended to consider the CloudWatch metrics (and the existing notification infrastructure built around them) before investing time in creating something new. Similarly, the QMR metrics cover most metric use cases and likely eliminate the need to write custom metrics."
26,"Tip #8: Federated queries connect the OLAP, OLTP and data lake worlds"
26,"The new Federated Query feature in Amazon Redshift allows you to run analytics directly against live data residing on your OLTP source system databases and Amazon S3 data lake, without the overhead of performing ETL and ingesting source data into Amazon Redshift tables. This feature gives you a convenient and efficient option for providing realtime data visibility on operational reports, as an alternative to micro-ETL batch ingestion of realtime data into the data warehouse. By combining historical trend data from the data warehouse with live developing trends from the source systems, you can gather valuable insights to drive real-time business decision making."
26,"For example, consider sales data residing in three different data stores:"
26,Live sales order data stored on an Amazon RDS for PostgreSQL database (represented as “ext_postgres” in the following external schema)
26,Historical sales data warehoused in a local Amazon Redshift database (represented as “local_dwh”)
26,"Archived, “cold” sales data older than 5 years stored on Amazon S3 (represented as “ext_spectrum”)"
26,We can create a late binding view in Amazon Redshift that allows you to merge and query data from all three sources. See the following code:
26,CREATE VIEW store_sales_integrated AS
26,SELECT * FROM ext_postgres.store_sales_live
26,UNION ALL
26,SELECT * FROM local_dwh.store_sales_current
26,UNION ALL
26,"SELECT ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_customer_sk, ss_cdemo_sk,"
26,"ss_hdemo_sk, ss_addr_sk, ss_store_sk, ss_promo_sk, ss_ticket_number, ss_quantity,"
26,"ss_wholesale_cost, ss_list_price, ss_sales_price, ss_ext_discount_amt,"
26,"ss_ext_sales_price, ss_ext_wholesale_cost, ss_ext_list_price, ss_ext_tax,"
26,"ss_coupon_amt, ss_net_paid, ss_net_paid_inc_tax, ss_net_profit"
26,FROM ext_spectrum.store_sales_historical
26,WITH NO SCHEMA BINDING
26,"Currently, direct federated querying is supported for data stored in Amazon Aurora PostgreSQL and Amazon RDS for PostgreSQL databases, with support for other major RDS engines coming soon. You can also use the federated query feature to simplify the ETL and data-ingestion process. Instead of staging data on Amazon S3, and performing a COPY operation, federated queries allow you to ingest data directly into an Amazon Redshift table in one step, as part of a federated CTAS/INSERT SQL query."
26,"For example, the following code shows an upsert/merge operation in which the COPY operation from Amazon S3 to Amazon Redshift is replaced with a federated query sourced directly from PostgreSQL:"
26,BEGIN;
26,CREATE TEMP TABLE staging (LIKE ods.store_sales);
26,-- replace the following COPY from S3:
26,/*COPY staging FROM 's3://yourETLbucket/daily_store_sales/'
26,IAM_ROLE 'arn:aws:iam::<account_id>:role/<s3_reader_role>'
26,DELIMITER '|' COMPUPDATE OFF; */
26,-- with this federated query to load staging data directly from PostgreSQL source
26,INSERT INTO staging SELECT * FROM pg.store_sales p
26,WHERE p.last_updated_date > (SELECT MAX(last_updated_date) FROM ods.store_sales);
26,DELETE FROM ods.store_sales USING staging s WHERE ods.store_sales.id = s.id;
26,INSERT INTO ods.store_sales SELECT * FROM staging;
26,DROP TABLE staging;
26,COMMIT;
26,"For more information about setting up the preceding federated queries, see Build a Simplified ETL and Live Data Query Solution using Redshift Federated Query. For additional tips and best practices on federated queries, see Best practices for Amazon Redshift Federated Query."
26,Tip #9: Maintaining efficient data loads
26,"Amazon Redshift best practices suggest using the COPY command to perform data loads of file-based data. Single-row INSERTs are an anti-pattern. The COPY operation uses all the compute nodes in your cluster to load data in parallel, from sources such as Amazon S3, Amazon DynamoDB, Amazon EMR HDFS file systems, or any SSH connection."
26,"When performing data loads, compress the data files whenever possible. For row-oriented (CSV) data, Amazon Redshift supports both GZIP and LZO compression. It’s more efficient to load a large number of small files than one large one, and the ideal file count is a multiple of the cluster’s total slice count. Columnar data, such as Parquet and ORC, is also supported. You can achieve best performance when the compressed files are between 1MB-1GB each."
26,"The number of slices per node depends on the cluster’s node size (and potentially elastic resize history). By ensuring an equal number of files per slice, you know that the COPY command evenly uses cluster resources and complete as quickly as possible. Query for the cluster’s current slice count with SELECT COUNT(*) AS number_of_slices FROM stv_slices;."
26,"Another script in the amazon-redshift-utils GitHub repo, CopyPerformance, calculates statistics for each load. Amazon Redshift Advisor also warns of missing compression or too few files based on the number of slices (see the following screenshot):"
26,"Conducting COPY operations efficiently reduces the time to results for downstream users, and minimizes the cluster resources utilized to perform the load."
26,Tip #10: Using the latest Amazon Redshift drivers from AWS
26,"Because Amazon Redshift is based on PostgreSQL, we previously recommended using JDBC4 PostgreSQL driver version 8.4.703 and psql ODBC version 9.x drivers. If you’re currently using those drivers, we recommend moving to the new Amazon Redshift–specific drivers. For more information about drivers and configuring connections, see JDBC and ODBC drivers for Amazon Redshift in the Amazon Redshift Cluster Management Guide."
26,"While rarely necessary, the Amazon Redshift drivers do permit some parameter tuning that may be useful in some circumstances. Downstream third-party applications often have their own best practices for driver tuning that may lead to additional performance gains."
26,"For JDBC, consider the following:"
26,"To avoid client-side out-of-memory errors when retrieving large data sets using JDBC, you can enable your client to fetch data in batches by setting the JDBC fetch size parameter or BlockingRowsMode."
26,"Amazon Redshift doesn’t recognize the JDBC maxRows parameter. Instead, specify a LIMIT clause to restrict the result set. You can also use an OFFSET clause to skip to a specific starting point in the result set."
26,"For ODBC, consider the following:"
26,A cursor is enabled on the cluster’s leader node when useDelareFecth is enabled. The cursor fetches up to fetchsize/cursorsize and then waits to fetch more rows when the application request more rows.
26,"The CURSOR command is an explicit directive that the application uses to manipulate cursor behavior on the leader node. Unlike the JDBC driver, the ODBC driver doesn’t have a BlockingRowsMode mechanism."
26,It’s recommended that you do not undertake driver tuning unless you have a clear need. AWS Support is available to help on this topic as well.
26,Conclusion
26,"Amazon Redshift is a powerful, fully managed data warehouse that can offer increased performance and lower cost in the cloud. As Amazon Redshift grows based on the feedback from its tens of thousands of active customers world-wide, it continues to become easier to use and extend its price-for-performance value proposition. Staying abreast of these improvements can help you get more value (with less effort) from this core AWS service."
26,We hope you learned a great deal about making the most of your Amazon Redshift account with the resources in this post.
26,"If you have questions or suggestions, please leave a comment."
26,About the Authors
26,"Matt Scaer is a Principal Data Warehousing Specialist Solution Architect, with over 20 years of data warehousing experience, with 11+ years at both AWS and Amazon.com."
26,Manish Vazirani is an Analytics Specialist Solutions Architect at Amazon Web Services.
26,Tarun Chaudhary is an Analytics Specialist Solutions Architect at AWS.
26,TAGS:
26,Amazon Redshift
26,View Comments
26,Resources
26,Amazon Athena
26,Amazon EMR
26,Amazon Kinesis
26,Amazon MSK
26,Amazon QuickSight
26,Amazon Redshift
26,AWS Glue
26,Follow
26,Twitter
26,Facebook
26,LinkedIn
26,Twitch
26,Email Updates
26,Sign In to the Console
26,Learn About AWS
26,What Is AWS?
26,What Is Cloud Computing?
26,"AWS Inclusion, Diversity & Equity"
26,What Is DevOps?
26,What Is a Container?
26,What Is a Data Lake?
26,AWS Cloud Security
26,What's New
26,Blogs
26,Press Releases
26,Resources for AWS
26,Getting Started
26,Training and Certification
26,AWS Solutions Portfolio
26,Architecture Center
26,Product and Technical FAQs
26,Analyst Reports
26,AWS Partner Network
26,Developers on AWS
26,Developer Center
26,SDKs & Tools
26,.NET on AWS
26,Python on AWS
26,Java on AWS
26,PHP on AWS
26,Javascript on AWS
26,Help
26,Contact Us
26,AWS Careers
26,File a Support Ticket
26,Knowledge Center
26,AWS Support Overview
26,Legal
26,Create an AWS Account
26,Amazon is an Equal Opportunity Employer:
26,Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.
26,Language
26,عربي
26,Bahasa Indonesia
26,Deutsch
26,English
26,Español
26,Français
26,Italiano
26,Português
26,Tiếng Việt
26,Türkçe
26,Ρусский
26,ไทย
26,日本語
26,한국어
26,中文 (简体)
26,中文 (繁體)
26,Privacy
26,Site Terms
26,Cookie Preferences
26,"© 2021, Amazon Web Services, Inc. or its affiliates. All rights reserved."
27,Performance Tuning Guide for Cisco UCS M5 Servers White Paper - Cisco
27,Skip to content
27,Skip to footer
27,Cisco.com Worldwide
27,MENU
27,CLOSE
27,Products
27,Support & Learn
27,Partners
27,Events & Videos
27,Search
27,Log In
27,Log Out
27,Choose Language Selection
27,More
27,Log In
27,Log Out
27,Log In
27,Have an account?
27,Personalized content
27,Your products and support
27,Log In
27,Forgot your user ID and/or password?
27,Manage account
27,Need an account?
27,Create an account
27,Help
27,Choose Language Selection
27,Choose Language Selection
27,Products
27,Support
27,Partners
27,Events & Videos
27,Products & ServicesServers - Unified ComputingCisco UCS B-Series Blade ServersWhite Papers
27,Performance Tuning Guide for Cisco UCS M5 Servers White Paper
27,White Paper
27,Download
27,Print
27,Available Languages
27,Download Options
27,PDF
27,(3.7 MB)
27,View with Adobe Reader on a variety of devices
27,"Updated:February 4, 2021"
27,See Revisions
27,Contact Cisco
27,Get a call from Sales
27,Product / Technical Support
27,Training & Certification
27,1-800-553-6387
27,US/CAN | 5am-5pm PT
27,Download
27,Print
27,Available Languages
27,Download Options
27,PDF
27,(3.7 MB)
27,View with Adobe Reader on a variety of devices
27,"Updated:February 4, 2021"
27,See Revisions
27,Table of Contents
27,Purpose and scope
27,What you will learn
27,BIOS tuning scenarios
27,High performance
27,Low latency
27,Cisco UCS BIOS options
27,Processor configuration
27,Power technology
27,Energy performance
27,Workload configuration
27,Memory settings
27,Configuring the BIOS for optimized CPU hardware power management
27,Operating system tuning guidance for best performance
27,BIOS recommendations for various workload types
27,Online transaction processing workloads
27,Virtualization workloads
27,High-performance computing workloads
27,Java Enterprise Edition application server workloads
27,Analytics database decision-support system workloads
27,Conclusion
27,For more information
27,Purpose and scope
27,The Basic Input and Output System (BIOS) tests and initializes the hardware components of a system and boots the operating system from a storage device. A typical computational system has several BIOS settings that control the system’s behavior. Some of these settings are directly related to the performance of the system.
27,"This document explains the BIOS settings that are valid for the Cisco Unified Computing System™ (Cisco UCS®) M5 server generation (Cisco UCS B200 and B480 M5 Blade Servers and C220, C240, and C480 M5 Rack Servers) using Intel® Xeon® Scalable processor family CPUs. It describes how to optimize the BIOS settings to meet requirements for the best performance and energy efficiency for the Cisco UCS M5 generation of blade and rack servers."
27,This document also discusses the BIOS settings that can be selected for various workload types on Cisco UCS M5 servers that use Intel Xeon Scalable processor family CPUs. Understanding the BIOS options will help you select appropriate values to achieve optimal system performance.
27,This document does not discuss the BIOS options for specific firmware releases of Cisco UCS servers. The settings demonstrated here are generic.
27,What you will learn
27,"The process of setting performance options in your system BIOS can be daunting and confusing, and some of the options you can choose are obscure. For most options, you must choose between optimizing a server for power savings or for performance. This document provides some general guidelines and suggestions to help you achieve optimal performance from your Cisco UCS blade and rack servers that use Intel Xeon Scalable processor family CPUs."
27,BIOS tuning scenarios
27,This document focuses on two main scenarios: how to tune the BIOS for high performance and how to tune it for low latency.
27,High performance
27,"With the latest multiprocessor, multicore, and multithreading technologies in conjunction with current operating systems and applications, today's Cisco UCS servers based on the Intel Xeon Scalable processor deliver the highest levels of performance, as demonstrated by the numerous industry-standard benchmark publications from the Standard Performance Evaluation Corporation (SPEC), SAP, and the Transaction Processing Performance Council (TPC)."
27,"Cisco UCS servers with standard settings already provide an optimal ratio of performance to energy efficiency. However, through BIOS settings you can further optimize the system with higher performance and less energy efficiency. Basically, this optimization operates all the components in the system at the maximum speed possible and prevents the energy-saving options from slowing down the system. In general, optimization to achieve greater performance is associated with increased consumption of electrical power. This document explains how to configure the BIOS settings to achieve optimal computing performance."
27,Low latency
27,"The BIOS offers a variety of options to reduce latency. In some cases, the corresponding application does not make efficient use of all the threads available in the hardware. To improve performance, you can disable threads that are not needed (hyperthreading) or even cores in the BIOS to reduce the small fluctuations in the performance of computing operations that especially occur in some High-Performance Computing (HPC) applications and analytical database applications. Furthermore, by disabling cores that are not needed, you can improve turbo-mode performance in the remaining cores under certain operating conditions."
27,"However, other scenarios require performance that is as constant as possible. Although the current generation of Intel processors delivers better turbo-mode performance than the preceding generation, the maximum turbo-mode frequency is not guaranteed under certain operating conditions. In such cases, disabling the turbo mode can help prevent changes in frequency."
27,"Energy-saving functions, whose aim is to save energy whenever possible through frequency and voltage reduction and through the disabling of certain function blocks and components, also have a negative impact on response time. The higher the settings for the energy saving modes, the lower the performance. Furthermore, in each energy-saving mode, the processor requires a certain amount of time to change back from reduced performance to maximum performance."
27,"This document explains how to configure the power and energy saving modes to reduce system latency. The optimization of server latency, particularly in an idle state, results in substantially greater consumption of electrical power."
27,Cisco UCS BIOS options
27,This section describes the options you can configure in the Cisco UCS BIOS.
27,Processor configuration
27,This section describes processor options you can configure.
27,Enhanced Intel SpeedStep Technology
27,Intel SpeedStep Technology is designed to save energy by adjusting the CPU clock frequency up or down depending on how busy the system is. Intel Turbo Boost Technology provides the capability for the CPU to adjust itself to run higher than its stated clock speed if it has enough power to do so.
27,"You can specify whether the processor uses Enhanced Intel SpeedStep Technology, which allows the system to dynamically adjust processor voltage and core frequency. This technology can result in decreased average power consumption and decreased average heat production."
27,The setting can be one of the following:
27,●     Disabled: The processor never dynamically adjusts its voltage or frequency.
27,●     Enabled: The processor uses Enhanced Intel SpeedStep Technology and enables all supported processor sleep states to further conserve power.
27,Intel Turbo Boost Technology
27,"Intel Turbo Boost Technology depends on Intel SpeedStep: if you want to enable Intel Turbo Boost, you must enable Intel SpeedStep first. If you disable Intel SpeedStep, you lose the capability to use Intel Turbo Boost."
27,"Intel Turbo Boost is especially useful for latency-sensitive applications and for scenarios in which the system is nearing saturation and would benefit from a temporary increase in the CPU speed. If your system is not running at this saturation level and you want the best performance at a utilization rate of less than 90 percent, you should disable Intel SpeedStep to help ensure that the system is running at its stated clock speed at all times."
27,Intel Hyper-Threading Technology
27,"You can specify whether the processor uses Intel Hyper-Threading Technology, which allows multithreaded software applications to process threads in parallel within each processor. You should test the CPU hyperthreading option both enabled and disabled in your specific environment. If you are running a single-threaded application, you should disable hyperthreading."
27,The setting can be one of the following:
27,●     Disabled: The processor does not permit hyperthreading.
27,●     Enabled: The processor allows parallel processing of multiple threads.
27,Core multiprocessing and latency-sensitive single-threaded applications
27,The core multiprocessing option is designed to enable the user to disable cores. This option may affect the pricing of certain software packages that are licensed by the core. You should consult your software license and software vendor about whether disabling cores qualifies you for any particular pricing policies. Set core multiprocessing to All if pricing policy is not an issue for you.
27,"For latency-sensitive single-threaded applications, you can optimize performance by disabling unnecessary cores, disabling hyperthreading, enabling all C-states, enabling Intel SpeedStep, and enabling Intel Turbo Boost. With this configuration, the remaining cores often will benefit from higher turbo speeds and better use of the shared Layer 3 cache."
27,CPU performance
27,"Intel Xeon processors have several layers of cache. Each core has a tiny Layer 1 cache, sometimes referred to as the Data Cache Unit (DCU), that has 32 KB for instructions and 32 KB for data. Slightly bigger is the Layer 2 cache, with 256 KB shared between data and instructions for each core. In addition, all cores on a chip share a much larger Layer 3 cache, which is about 10 to 45 MB in size (depending on the processor model and number of cores)."
27,The prefetcher settings provided by Intel primarily affect the Layer 1 and Layer 2 caches on a processor core (Table 1). You will likely need to perform some testing with your individual workload to find the combination that works best for you. Testing on the Intel Xeon Scalable processor has shown that most applications run best with all prefetchers enabled. See Tables 2 and 3 for guidance.
27,Table 1.           CPU performance and prefetch options from Intel
27,Performance option
27,Cache affected
27,Hardware prefetcher
27,Layer 2
27,Adjacent-cache-line prefetcher
27,Layer 2
27,DCU prefetcher
27,Layer 1
27,DCU instruction pointer (DCU-IP) prefetcher
27,Layer 1
27,Table 2.           Cisco UCS CPU performance options
27,Option
27,Description
27,CPU performance
27,Sets the CPU performance profile for the server. This can be one of the following:
27,●  Enterprise: All prefetchers are enabled.
27,"●  High throughput: DCU IP prefetcher are enabled, and all other prefetchers are disabled."
27,●  HPC: All prefetchers are enabled. This setting is also known as HPC.
27,●  Platform default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.
27,Table 3.           Cisco UCS CPU prefetcher options and target benchmarks and workloads
27,Prefetchers
27,Target benchmarks and workloads
27,All enabled
27,"HPC benchmarks, web server, SAP application server, virtualization, and TPC-E"
27,DCU-IP enabled; all others disabled
27,SPECjbb2005 and certain server-side Java application-server applications
27,Hardware prefetcher
27,"The hardware prefetcher prefetches additional streams of instructions and data into the Layer 2 cache upon detection of an access stride. This behavior is more likely to occur during operations that sort through sequential data, such as database table scans and clustered index scans, or that run a tight loop in code."
27,You can specify whether the processor allows the Intel hardware prefetcher to fetch streams of data and instructions from memory into the unified second-level cache when necessary.
27,The setting can be one of the following:
27,●     Disabled: The hardware prefetcher is not used.
27,●     Enabled: The processor uses the hardware prefetcher when cache problems are detected.
27,Adjacent-cache-line prefetcher
27,"The adjacent-cache-line prefetcher always prefetches the next cache line. Although this approach works well when data is accessed sequentially in memory, it can quickly litter the small Layer 2 cache with unneeded instructions and data if the system is not accessing data sequentially, causing frequently accessed instructions and code to leave the cache to make room for the adjacent-line data or instructions."
27,You can specify whether the processor fetches cache lines in even or odd pairs instead of fetching just the required line.
27,The setting can be one of the following:
27,●     Disabled: The processor fetches only the required line.
27,●     Enabled: The processor fetches both the required line and its paired line.
27,DCU streamer prefetcher
27,"Like the hardware prefetcher, the DCU streamer prefetcher prefetches additional streams of instructions or data upon detection of an access stride; however, it stores the streams in the tiny Layer 1 cache instead of the Layer 2 cache."
27,"This prefetcher is a Layer 1 data cache prefetcher. It detects multiple loads from the same cache line that occur within a time limit. Making the assumption that the next cache line is also required, the prefetcher loads the next line in advance to the Layer 1 cache from the Layer 2 cache or the main memory."
27,The setting can be one of the following:
27,●     Disabled: The processor does not try to anticipate cache read requirements and fetches only explicitly requested lines.
27,●     Enabled: The DCU prefetcher analyzes the cache read pattern and prefetches the next line in the cache if it determines that it may be needed.
27,DCU-IP prefetcher
27,The DCU-IP prefetcher predictably prefetches data into the Layer 1 cache on the basis of the recent instruction pointer load instruction history.
27,You can specify whether the processor uses the DCU-IP prefetch mechanism to analyze historical cache access patterns and preload the most relevant lines in the Layer 1 cache.
27,The setting can be one of the following:
27,●     Disabled: The processor does not preload any cache data.
27,●     Enabled: The DCU-IP prefetcher preloads the Layer 1 cache with the data it determines to be the most relevant.
27,Low-level cache prefetch
27,"This BIOS option configures the processor’s Last-Level Cache (LLC) prefetch feature as a result of the noninclusive cache architecture. The LLC prefetcher exists on top of other prefetchers that can prefetch data into the core DCU and Mid-Level Cache (MLC). In some cases, setting this option to disabled can improve performance."
27,The setting for this BIOS option can be one of the following:
27,●     Disabled: The LLC prefetcher is disabled. The other core prefetchers are not affected.
27,●     Enabled: The core prefetcher can prefetch data directly to the LLC.
27,"By default, the LLC prefetch option is disabled."
27,Direct cache access
27,"The Direct-Cache Access (DCA) mechanism is a system-level protocol in a multiprocessor system to improve I/O network performance, thereby providing higher system performance. The basic goal is to reduce cache misses when a demand read operation is performed. This goal is accomplished by placing the data from the I/O devices directly into the CPU cache through hints to the processor to perform a data prefetch operation and install the data in its local caches."
27,Execute Disable Bit feature
27,"The Intel Execute Disable Bit feature classifies memory areas on the server to specify where the application code can run. As a result of this classification, the processor disables code processing if a malicious worm attempts to insert code in the buffer. This setting helps prevent damage, worm propagation, and certain classes of malicious buffer overflow attacks."
27,The setting can be one of the following:
27,●     Disabled: The processor does not classify memory areas.
27,●     Enabled: The processor classifies memory areas.
27,Intel VT for Directed I/O
27,"You can specify whether the processor uses Intel Virtualization Technology (VT) for Directed I/O (VT-d), which allows a platform to run multiple operating systems and applications in independent partitions."
27,The setting can be one of the following:
27,●     Disabled: The processor does not permit virtualization.
27,●     Enabled: The processor allows multiple operating systems in independent partitions.
27,"Note:      If you change this option, you must power the server off and on before the setting takes effect."
27,Power technology
27,Enables you to configure the CPU power management settings for the following options:
27,●     Enhanced Intel Speedstep Technology
27,●     Intel Turbo Boost Technology
27,●     Processor Power State C6
27,"For best performance, set the power technology option to Performance or Custom. If it is not set to Custom, the individual settings for Intel SpeedStep and Turbo Boost and the C6 power state are ignored."
27,This option can be set to one of the following:
27,●     Custom: The server uses the individual settings for the BIOS parameters in the preceding section. You must select this option if you want to change any of these BIOS parameters.
27,●     Performance: The server determines the best settings for the BIOS parameters and provides optimal CPU power performance in the preceding section.
27,"●     Disabled: The server does not perform any CPU power management, and any settings for the BIOS parameters in the preceding section are ignored."
27,●     Energy Efficient: The server determines the best settings for the BIOS parameters in the preceding section and ignores the individual settings for these parameters.
27,You can set the processor C-states.
27,Processor C1E
27,"Enabling the C1E option allows the processor to transition to its minimum frequency upon entering the C1 state. This setting does not take effect until after you have rebooted the server. When this option is disabled, the CPU continues to run at its maximum frequency in the C1 state. Users should disable this option to perform application benchmarking."
27,You can specify whether the CPU transitions to its minimum frequency when entering the C1 state.
27,The setting can be one of the following:
27,●     Disabled: The CPU continues to run at its maximum frequency in the C1 state.
27,●     Enabled: The CPU transitions to its minimum frequency. This option saves the maximum amount of power in the C1 state.
27,Processor C3 report
27,"You can specify whether the BIOS sends the C3 report to the operating system. When the OS receives the report, it can transition the processor into the lower C3 power state to decrease energy use while maintaining optimal processor performance."
27,The setting can be one of the following:
27,●     Disabled: The BIOS does not send the C3 report.
27,"●     Enabled: The BIOS sends the C3 report, allowing the OS to transition the processor to the C3 low-power state."
27,Processor C6 report
27,"The C6 state is power-saving halt and sleep state that a CPU can enter when it is not busy. Unfortunately, it can take some time for the CPU to leave these states and return to a running condition. If you are concerned about performance (for all but latency-sensitive single-threaded applications), and if you can do so, disable anything related to C-states."
27,"You can specify whether the BIOS sends the C6 report to the operating system. When the OS receives the report, it can transition the processor into the lower C6 power state to decrease energy use while maintaining optimal processor performance."
27,The setting can be one of the following:
27,●     Disabled: The BIOS does not send the C6 report.
27,"●     Enabled: The BIOS sends the C6 report, allowing the OS to transition the processor to the C6 low-power state."
27,P-state coordination
27,"You can define the way that the BIOS communicates the P-state support model to the operating system. Three models are available, as defined by the Advanced Configuration and Power Interface (ACPI) specification:"
27,●     HW_ALL: The processor hardware is responsible for coordinating the P-state among logical processors with dependencies (all the logical processors in a package).
27,●     SW_ALL: The OS Power Manager (OSPM) is responsible for coordinating the P-state among logical processors with dependencies (all the logical processors in a physical package) and must initiate the transition on all the logical processors.
27,●     SW_ANY: The OSPM is responsible for coordinating the P-state among logical processors with dependencies (all the logical processors in a package) and can initiate the transition on any of the logical processors in the domain.
27,"Note:      The power technology option must be set to Custom; otherwise, the server ignores the setting for this parameter."
27,Package C-state limit
27,"When power technology is set to Custom, use this option to configure the lowest processor idle power state (C-state). The processor automatically transitions into package C-states based on the core C-states to which cores on the processor have transitioned. The higher the package C-state, the lower the power use of that idle package state. The default setting, Package C6 (nonretention), is the lowest power idle package state supported by the processor."
27,You can specify the amount of power available to the server components when they are idle.
27,The possible settings are as follows:
27,"●     C0/C1 State: When the CPU is idle, the system slightly reduces the power consumption. This option requires less power than C0 and allows the server to return quickly to high-performance mode."
27,"●     C2 State: When the CPU is idle, the system reduces power consumption more than with the C1 option. This option requires less power than C1 or C0, but the server takes slightly longer to return to high-performance mode."
27,"●     C6 Nonretention: When the CPU is idle, the system reduces the power consumption more than with the C3 option. This option saves more power than C0, C1, or C3, but the system may experience performance problems until the server returns to full power."
27,"●     C6 Retention: When the CPU is idle, the system reduces power consumption more than with the C3 option. This option consumes slightly more power than the C6 Nonretention option, because the processor is operating at Pn voltage to reduce the package’s C-state exit latency."
27,Energy performance
27,You can specify whether system performance or energy efficiency is more important on this server.
27,The setting can be one of the following:
27,●     Performance: The server provides all server components with full power at all times. This option maintains the highest level of performance and requires the greatest amount of power.
27,●     Balanced Performance: The server provides all server components with enough power to keep a balance between performance and power.
27,●     Balanced Energy: The server provides all server components with enough power to keep a balance between performance and power.
27,●     Energy Efficient: The server provides all server components with less power to reduce power consumption.
27,Note:      Power Technology must be set to Custom or the server ignores the setting for this parameter
27,Autonomous Core C-state
27,"When the operating system requests CPU core C1 state, system hardware automatically changes the request to core C6 state"
27,"This BIOS switch allows 2 options: ""Enabled"" and ""Disabled""."
27,●     Enabled: HALT and C1 request get converted to C6 requests in hardware.
27,●     Disabled: only C0 and C1 are used by the OS. C1 gets enabled automatically when an OS autohalts.
27,"By default, Autonomous Core C-state is disabled."
27,Workload configuration
27,You can configure workload optimization.
27,You can configure the following options:
27,●     Balanced
27,●     I/O Sensitive
27,●     Non-Uniform Memory Access (NUMA)
27,●     Uniform Memory Access (UMA)
27,"By default, I/O Sensitive is enabled."
27,Memory settings
27,You can use several settings to optimize memory performance.
27,"Memory reliability, availability, and serviceability configuration"
27,"Always set the memory reliability, availability, and serviceability (RAS) configuration to Maximum Performance for systems that require the highest performance and do not require memory fault-tolerance options."
27,The following settings are available:
27,●     Maximum Performance: System performance is optimized.
27,●     Mirror Mode 1LM (one-level memory): System reliability is optimized by using half the system memory as backup.
27,Note:      For the optimal balance of performance and system stability it is recommended to use “Platform Default” (ADDDC Sparing enabled) for the Memory RAS configuration. ADDDC Sparing will incur a small performance penalty. If maximum performance is desired independently of system stability the “Maximum-Performance” Memory RAS setting can be used.
27,Non-uniform memory access
27,"Most modern operating systems, particularly virtualization hypervisors, support NUMA because in the latest server designs, a processor is attached to a memory controller: therefore, half the memory belongs to one processor, and half belongs to the other processor. If a core needs to access memory that resides in another processor, a longer latency period is needed to access that part of memory. Operating systems and hypervisors recognize this architecture and are designed to reduce such trips. For hypervisors such as those from VMware and for modern applications designed for NUMA, keep this option enabled."
27,Integrated memory controller interleaving
27,"The Integrated Memory Controller (IMC) BIOS option controls the interleaving between the integrated memory controllers. There are two integrated memory controllers per CPU socket in a x86 server running Intel Xeon scalable processors. If integrated memory controller Interleaving is set to 2-way, addresses will be interleaved between the two-integrated memory controller. If integrated memory controller interleaving is set to 1-way, there will be no interleaving."
27,"Note:      If Sub-NUMA Clustering (SNC) is disabled, integrated memory controller interleaving should be set to Auto. If SNC is enabled, integrated memory controller interleaving should be set to 1-way."
27,The following settings are available:
27,●     1-way Interleave: There is no interleaving.
27,●     2-way Interleave: Addresses are interleaved between the two integrated memory controllers.
27,●     Auto: The CPU determines the integrated memory controller interleaving mode.
27,Sub-NUMA clustering
27,"The SNC BIOS option provides localization benefits similar to the Cluster-on-Die (CoD) option, without some of the disadvantages of CoD. SNC breaks the LLC into two disjointed clusters based on address range, with each cluster bound to a subset of the memory controllers in the system. SNC improves average latency to the LLC and memory. SNC is a replacement for the CoD feature found in previous processor families. For a multisocket system, all SNC clusters are mapped to unique NUMA domains. Integrated memory controller interleaving must be set to the correct value to correspond with the SNC setting."
27,The setting for this BIOS option can be one of the following:
27,●     Disabled: The LLC is treated as one cluster when this option is disabled.
27,●     Enabled: The LLC capacity is used more efficiently and latency is reduced as a result of the core and integrated memory controller proximity. This setting may improve performance on NUMA-aware operating systems.
27,"Note:      If SNC is disabled, integrated memory controller interleaving should be set to Auto. If SNC is enabled, integrated memory controller interleaving should be set to 1-way."
27,Xtended Partition Table prefetch
27,"The XPT prefetcher exists on top of other prefetchers that that can prefetch data in the core DCU, MLC, and LLC. The XPT prefetcher will issue a speculative DRAM read request in parallel to an LLC lookup. This prefetch bypasses the LLC, saving latency. You can specify whether the processor uses the XPT prefetch mechanism to fetch the date into the XPT."
27,The setting can be one of the following:
27,●     Disabled: The processor does not preload any cache data.
27,●     Enabled: The XPT prefetcher preloads the Layer 1 cache with the data it determines to be the most relevant.
27,Intel UltraPath Interconnect prefetch
27,Intel UltraPath Interconnect (UPI) prefetch is a mechanism to get the memory read started early on a Double-Data-Rate (DDR) bus.
27,The setting can be one of the following:
27,●     Disabled: The processor does not preload any cache data.
27,●     Enabled: The UPI prefetcher preloads the Layer 1 cache with the data it determines to be the most relevant.
27,ADDDC Sparing
27,Adaptive Double Device Data Correction (ADDDC) is a memory RAS feature that enables dynamic mapping of failing DRAM by monitoring corrected errors and taking action before uncorrected errors can occur and cause an outage. It is now enabled by default.
27,"After ADDDC sparing remaps a memory region, the system could incur marginal memory latency and bandwidth penalties on memory bandwidth intense workloads that target the impacted region. Cisco recommends scheduling proactive maintenance to replace a failed DIMM after an ADDDC RAS fault is reported."
27,Patrol scrub
27,"You can specify whether the system actively searches for, and corrects, single-bit memory errors even in unused portions of the memory on the server."
27,The setting can be one of the following:
27,●     Disabled: The system checks for memory Error-Correcting Code (ECC) errors only when the CPU reads or writes a memory address.
27,"●     Enabled: The system periodically reads and writes memory searching for ECC errors. If any errors are found, the system attempts to fix them. This option may correct single-bit errors before they become multiple-bit errors, but it may adversely affect performance when the patrol-scrub process is running."
27,Demand scrub
27,Demand scrub occurs when the memory controller reads memory for data or instructions and the demand scrubbing logic detects a correctable error. Correct data is forwarded to the memory controller and written to memory.
27,"With demand scrubbing disabled, the data being read into the memory controller will be corrected by the ECC logic, but no write to main memory occurs. Because the data is not corrected in memory, subsequent read operations to the same data will need to be corrected."
27,Configuring the BIOS for optimized CPU hardware power management
27,"This section summarizes the BIOS settings you can configure to optimize CPU power management. It presents the settings optimized for maximum performance, low latency, and energy efficiency, summarized in Table 4."
27,"Table 4.           BIOS recommendations for maximum performance, low latency, and energy efficiency."
27,BIOS Options
27,BIOS Values (platform-default)
27,Maximum Performance
27,Low-Latency
27,Energy Efficiency
27,Processor Configuration
27,Intel SpeedStep Technology
27,Enabled
27,Platform Default
27,Platform Default
27,Platform Default
27,Intel Hyper-Threading Tech
27,Enabled
27,Platform Default
27,Disabled
27,Platform Default
27,Intel Virtualization Technology (VT)
27,Enabled
27,Disabled
27,Disabled
27,Platform Default
27,Intel VT for Directed I/O
27,Enabled
27,Disabled
27,Disabled
27,Platform Default
27,CPU performance
27,Custom
27,Platform Default
27,Platform Default
27,Platform Default
27,LLC Prefetch
27,Disabled
27,Platform Default
27,Platform Default
27,Platform Default
27,Direct cache access
27,Enabled
27,Platform Default
27,Platform Default
27,Platform Default
27,Advanced Power Management Configuration
27,Power technology
27,Energy-Efficient
27,Custom
27,Custom
27,Custom
27,Intel Turbo Boost
27,Enabled
27,Platform Default
27,Platform Default
27,Disabled
27,P-STATE coordination
27,HW_ALL
27,Platform Default
27,Platform Default
27,Platform Default
27,Energy Performance
27,Balanced Performance
27,Performance
27,Platform Default
27,Platform Default
27,Processor C State
27,Enabled
27,Disabled
27,Disabled
27,Platform Default
27,Processor C1E
27,Enabled
27,Disabled
27,Disabled
27,Platform Default
27,Processor C3
27,Enabled
27,Disabled
27,Disabled
27,Disabled
27,Processor C6
27,Enabled
27,Disabled
27,Disabled
27,Platform Default
27,Processor C7
27,Enabled
27,Disabled
27,Disabled
27,Disabled
27,Package C State limit
27,C0/C1 State
27,Platform Default
27,Platform Default
27,C6 Retention
27,Energy Performance Tuning
27,Platform Default
27,Platform Default
27,Platform Default
27,CPU hardware power mgmt
27,HWPW Native Mode
27,Platform Default
27,Platform Default
27,Platform Default
27,Workload Configuration
27,I/O Sensitive
27,Balanced
27,Balanced
27,Platform Default
27,Autonomous Core C-State
27,Disabled
27,Platform Default
27,Platform Default
27,Enabled
27,Memory & UPI Configuration
27,NUMA Optimized
27,Enabled
27,Platform Default
27,Platform Default
27,Platform Default
27,IMC Interleaving
27,Auto
27,1-way Interleave
27,Platform Default
27,Platform Default
27,XPT Prefetch
27,Auto
27,Platform Default
27,Platform Default
27,Platform Default
27,UPI Prefetch
27,Enabled
27,Platform Default
27,Platform Default
27,Platform Default
27,Sub Numa Clustering
27,Disabled
27,Enabled
27,Platform Default
27,Platform Default
27,Memory RAS configuration
27,ADDDC Sparing
27,Platform Default
27,Platform Default
27,Platform Default
27,ADDDC Sparing
27,Enabled
27,Platform Default
27,Platform Default
27,Platform Default
27,Fan control policy
27,"Fan control policies enable you to control the fan speed to reduce server power consumption and noise levels. Prior to the fan policies, the fan speed increased automatically when the temperature of any server component exceeded the set threshold. To help ensure that the fan speeds were low, the threshold temperatures of components were usually set to high values. Although this behavior suited most server configurations, it did not address the following situations:"
27,"●     Maximum CPU performance: For high performance, certain CPUs must be cooled substantially below the set threshold temperature. This cooling requires very high fan speeds, which results in increased power consumption and noise levels."
27,"●     Low power consumption: To help ensure the lowest power consumption, fans must run very slowly and, in some cases, stop completely on servers that allow this behavior it. But slow fan speeds can cause servers to overheat. To avoid this situation, you need to run fans at a speed that is moderately faster than the lowest possible speed."
27,Following are the fan policies that you can choose:
27,"●     Balanced: This is the default policy. This setting can cool almost any server configuration, but it may not be suitable for servers with PCI Express (PCIe) cards, because these cards overheat easily."
27,●     Low Power: This setting is well suited for minimal-configuration servers that do not contain any PCIe cards.
27,"●     High Power: This setting can be used for server configurations that require fan speeds ranging from 60 to 85 percent. This policy is well suited for servers that contain PCIe cards that easily overheat and have high temperatures. The minimum fan speed set with this policy varies for each server platform, but it is approximately in the range of 60 to 85 percent."
27,"●     Maximum Power: This setting can be used for server configurations that require extremely high fan speeds ranging between 70 and 100 percent. This policy is well suited for servers that contain PCIe cards that easily overheat and have extremely high temperatures. The minimum fan speed set with this policy varies for each server platform, but it is approximately in the range of 70 to 100 percent."
27,Note:      This policy is configurable for standalone Cisco UCS C-Series M5 servers using the Cisco® Integrated Management Controller (IMC) console and the Cisco IMC supervisor. CIMC Web console Ò Compute Ò Power Policies Ò Configured Fan Policy.
27,"For UCS Managed C series servers, it is configurable using Power Control Policies under. Servers Ò Policies Ò root Ò Power control Policies Ò Create Fan Power Control Policy Ò Fan speed Policy."
27,Operating system tuning guidance for best performance
27,You can tune the OS to achieve the best performance.
27,"For Linux, set the following:"
27,●     x86_energy_perf_policy performance
27,"When energy performance tuning is set to OS, the OS controls the Energy Performance Bias (EPB) policy. The EPB features controlled by the policy are Intel Turbo Boost override, memory clock enable (CKE), memory Output Status Register (OSR), Intel QuickPath Interconnect (QPI) L0p, C-state demotion, and I/O bandwidth P-limit. The default OSPM profile is set to Performance, which will not sacrifice performance to save energy."
27,●     cpupower frequency-set -governor performance
27,"The performance governor forces the CPU to use the highest possible clock frequency. This frequency is statically set and will not change. Therefore, this particular governor offers no power-savings benefit. It is suitable only for hours of heavy workload, and even then, only during times in which the CPU is rarely (or never) idle. The default setting is On Demand, which allows the CPU to achieve maximum clock frequency when the system load is high, and the minimum clock frequency when the system is idle. Although this setting allows the system to adjust power consumption according to system load, it does so at the expense of latency from frequency switching."
27,●     Edit /etc/init.d/grub.conf to set intel_pstate=disable
27,Intel_pstate is a part of the CPU performance scaling subsystem in the Linux kernel (CPUFreq). It is a power scaling driver is used automatically on later generations of Intel processors. This driver takes priority over other drivers and is built-in as opposed to being a module. You can force pstate off by appending intel_pstate=disable to the kernel arguments (edit /etc/default/grub)
27,●     tuned-adm profile latency-performance
27,The tuned-adm tool allows users to easily switch among a number of profiles that have been designed to enhance performance for specific use cases.
27,You can apply the tuned-admin server profile for typical latency performance tuning. It disables the tuned and ktune power-saving mechanisms. The CPU speed mode changes to Performance. The I/O elevator is changed to Deadline for each device. The cpu_dma_latency parameter is registered with a value of 0 (the lowest possible latency) for power management QoS to limit latency where possible.
27,Use the following Linux tools to measure maximum turbo frequency and power states:
27,"●     Turbostat: Turbostat is provided in the kernel-tools package. It reports on processor topology, frequency, idle power-state statistics, temperature, and power use on Intel 64 processors. It is useful for identifying servers that are inefficient in terms of power use or idle time. It also helps identify the rate of System Management Interrupts (SMIs) occurring on the system, and it can be used to verify the effects of power management tuning. Use this setting:"
27,turbostat -S
27,"●     Intel PTUmon: The Intel Performance Tuning Utility (PTU) is a cross-platform performance analysis tool set. In addition to such traditional capabilities as tools to identify the hottest modules and functions of the application, track call sequences, and identify performance-critical source code, Intel PTU has new, more powerful data collection, analysis, and visualization capabilities. Intel PTU offers processor hardware event counters for in-depth analysis of the memory system performance, architectural tuning, and other features. Use this setting:"
27,ptumon -l -i 5000
27,Refer the following resources for more information about OS performance tuning:
27,●     Microsoft Windows and Hyper-V tuning is straightforward: set the power policy to High Performance. See:
27,◦    Performance Tuning Guidelines for Microsoft Windows Server 2012 R2
27,◦    Performance Tuning Guidelines for Microsoft Windows Server 2016
27,●     VMware ESXi tuning is straightforward as well: set the power policy to High Performance. See:
27,◦    https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/vmware-perfbest-practices-vsphere6-0-white-paper.pdf
27,"●     To tune Citrix XenServer, set xenpm set-scaling-governor performance. See:"
27,◦    https://support.citrix.com/article/CTX200390
27,"●     To tune Red Hat Enterprise Linux, set CPU power to Performance. See:"
27,◦    https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/pdf/Performance_Tuning_Guide/Red_Hat_Enterprise_Linux-7-Performance_Tuning_Guide-en-US.pdf
27,"●     To tune SUSE Enterprise Linux, set CPU power to Performance. See:"
27,◦    https://www.suse.com/documentation/sles-12/pdfdoc/book_sle_tuning/book_sle_tuning.pdf
27,BIOS recommendations for various workload types
27,This document discusses BIOS settings for the following types of workloads:
27,●     Online transaction processing (OLTP)
27,●     Virtualization
27,●     High-Performance Computing (HPC)
27,●     Java Enterprise Edition (Java EE) application server
27,●     Analytics database Decision-Support System (DSS)
27,Table 5 summarizes the BIOS options and settings available for various workloads.
27,Table 5.           BIOS options for various workloads
27,BIOS Options
27,BIOS Values (platform-default)
27,Online Transaction Processing (OLTP)
27,Virtualization
27,High-Performance Computing (HPC)
27,Java Application Servers
27,Analytic Database Systems (DSS)
27,Processor Configuration
27,Intel SpeedStep Technology
27,Enabled
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Intel Hyper-Threading Tech
27,Enabled
27,Platform Default
27,Platform Default
27,Disabled
27,Platform Default
27,Platform Default
27,Intel Virtualization Technology (VT)
27,Enabled
27,Platform Default
27,Platform Default
27,Disabled
27,Disabled
27,Platform Default
27,Intel VT for Directed I/O
27,Enabled
27,Platform Default
27,Platform Default
27,Disabled
27,Disabled
27,Disabled
27,CPU performance
27,Custom
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,LLC Prefetch
27,Disabled
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Direct cache access
27,Enabled
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Advanced Power Management Configuration
27,Power technology
27,Energy-Efficient
27,Custom
27,Custom
27,Platform Default
27,Custom
27,Custom
27,Intel Turbo Boost
27,Enabled
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,P-STATE coordination
27,HW_ALL
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Energy Performance
27,Balanced Performance
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Processor C State
27,Enabled
27,Disabled
27,Disabled
27,Platform Default
27,Disabled
27,Disabled
27,Processor C1E
27,Enabled
27,Disabled
27,Disabled
27,Platform Default
27,Disabled
27,Disabled
27,Processor C3
27,Enabled
27,Disabled
27,Disabled
27,Platform Default
27,Disabled
27,Disabled
27,Processor C6
27,Enabled
27,Disabled
27,Disabled
27,Platform Default
27,Disabled
27,Disabled
27,Processor C7
27,Enabled
27,Disabled
27,Disabled
27,Platform Default
27,Disabled
27,Disabled
27,Package C State limit
27,C0/C1 State
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Energy Performance Tuning
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,CPU hardware power mgmt
27,HWPW Native Mode
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Workload Configuration
27,I/O Sensitive
27,Platform-default
27,Platform Default
27,Balanced
27,Platform Default
27,Platform-default
27,Autonomous Core C-State
27,Disabled
27,Platform-default
27,Platform-default
27,Platform-default
27,Platform-default
27,Platform-default
27,Memory & UPI Configuration
27,NUMA Optimized
27,Enabled
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,IMC Interleaving
27,Auto
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,XPT Prefetch
27,Auto
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,UPI Prefetch
27,Enabled
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Sub Numa Clustering
27,Disabled
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Memory RAS configuration
27,ADDDC Sparing
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,ADDDC Sparing
27,Enabled
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,Platform Default
27,The following sections describe the BIOS tuning recommendations for all the workloads listed in Table 5.
27,Online transaction processing workloads
27,OLTP systems contain the operational data needed to control and run important transactional business tasks. These systems are characterized by their ability to complete various concurrent database transactions and process real-time data. They are designed to provide optimal data processing speed.
27,OLTP systems are often decentralized to avoid single points of failure. Spreading the work over multiple servers can also support greater transaction processing volume and reduce response time.
27,Processor and memory settings for Cisco UCS managed servers: OLTP
27,Obtaining peak performance requires some system-level tuning. Figure 1 Highlights the BIOS selections that are recommended for optimizing OLTP workloads on Cisco UCS M5 platforms managed by Cisco UCS Manager. Rest of the BIOS settings are configured as “Platform Default”.
27,Figure 1.
27,Processor settings for OLTP workloads
27,"The Intel Turbo Boost and SpeedStep technologies are powerful management features that adjust the CPU voltage and frequency settings to optimize performance and power consumption dynamically. During periods of low CPU consumption, Intel SpeedStep can reduce the CPU frequency by reducing power consumption. Intel Turbo Boost increases the processing speed to accommodate higher demand in situations in which CPU utilization is extremely high. Each core has 20 to 30 percent more processing capability when Intel Turbo Boost is enabled. For example, the Cisco UCS M5 platforms installed with the Intel Xeon Scalable Platinum 8168 CPU operates at a base frequency of 2.7 GHz. If Intel Turbo Boost is enabled, the system can achieve frequencies as high as 3.7 GHz."
27,"When you tune for consistent performance for OLTP applications on a system that does not run at close to 100 percent CPU utilization, you should enable Intel SpeedStep and Turbo Boost and disable C-states. Although this configuration foregoes power savings during idle times, it keeps all CPU cores running at a consistent speed and delivers the most consistent and predictable performance."
27,"Enabling Intel Hyper-Threading Technology helps OLTP systems handle I/O-intensive workloads by allowing the processing of multiple threads per CPU core. OLTP applications typically are multithreaded, with each thread performing a small amount of work that may include I/O operations. A large number of threads results in a considerable amount of context switching, but with Intel Hyper-Threading, the effect of context switching is reduced. When Intel Direct Cache Access is enabled (Figure 2), the I/O controller places data directly into the CPU cache to reduce the cache misses while processing OLTP workloads. This approach results in improved application performance."
27,Figure 2.
27,Intel Directed I/O settings for OLTP workloads
27,"If you are deploying the system in a virtualized environment and the OLTP application uses a directed I/O path, make sure that the VT for Directed IO option is enabled. By default, these options are enabled."
27,Note:      This feature is applicable only if the OLTP system is running in a virtualized environment.
27,Figure 3 shows the recommended settings for optimizing memory for OLTP workloads on servers managed by Cisco UCS Manager.
27,Figure 3.
27,Memory settings for OLTP workloads
27,Processor and memory settings for standalone Cisco UCS C-Series servers: OLTP
27,Figures 4 and 5 show the processor selections that are recommended for OLTP workloads on standalone Cisco UCS C-Series M5 servers.
27,Figure 4.
27,Processor settings for OLTP workloads
27,Figure 5.
27,Power and performance settings for OLTP workloads
27,Figure 6 shows memory settings for OLTP workloads for standalone Cisco UCS C-Series servers.
27,Figure 6.
27,Memory settings for OLTP workloads
27,"OLTP applications have a random memory-access pattern and benefit greatly from larger and faster memory. Therefore, Cisco recommends setting memory RAS features to maximum performance for optimal system performance. In OLTP transactions, if these modes are enabled, I/O operations will be serviced at the highest frequency and will have reduced memory latency."
27,"Note:      If the DIMM pairs in the server have the same type, size, and organization and are populated across the Scalable Memory Interconnect (SMI) channels, you can enable the lockstep mode, an option on the Select Memory RAS menu, to reduce memory-access latency and achieve better performance."
27,Virtualization workloads
27,"Intel Virtualization Technology provides manageability, security, and flexibility in IT environments that use software-based virtualization solutions. With this technology, a single server can be partitioned and can be projected as several independent servers, allowing the server to run different applications on the operating system simultaneously."
27,Processor and memory settings for Cisco UCS managed servers: Virtualization
27,Figure 7 Highlights the BIOS selections that are recommended for virtualized workloads on Cisco UCS M5 platforms managed by Cisco UCS manager. Rest of the BIOS settings are configured as “Platform Default”.
27,Figure 7.
27,Processor settings for virtualized workloads
27,Most of the CPU and memory settings for virtualized workloads are the same as those for OLTP workloads. It is important to enable Intel Virtualization Technology in the BIOS to support virtualization workloads. Make sure that the Intel VT-d option is enabled.
27,"The CPUs that support hardware virtualization allow the processor to run multiple operating systems in the virtual machines. This feature involves some overhead because the performance of a virtual operating system is comparatively slower than that of the native OS. To enhance performance, be sure to enable Intel Turbo Boost and Hyper-Threading for the processors."
27,"The cache prefetching mechanisms (Data-Prefetch-Logic [DPL] prefetch, hardware prefetch, Layer 2 streaming prefetch, and adjacent-cache-line prefetch) usually help increase system performance, especially when memory-access patterns are regular."
27,Intel Directed I/O for virtualized workloads
27,Figure 8 shows the recommended Intel Directed I/O settings for virtualized workloads in Cisco UCS M5 platforms.
27,Figure 8.
27,Intel Directed I/O settings for virtualized workloads
27,"With Cisco Data Center Virtual Machine Fabric Extender VM-FEX technology, virtual machines can now directly write to the virtual network interface cards (vNICs) when the Intel Directed I/O option is enabled at the BIOS level."
27,Memory settings for virtualized workloads
27,Figure 9 shows the recommended memory settings for virtualized workloads in Cisco UCS M5 servers.
27,Figure 9.
27,Memory settings for virtualized workloads
27,"When running applications that access memory randomly, set the Select Memory RAS option to Maximum Performance. This setting helps achieve optimal system performance. In virtualized environments, run the memory at the highest frequency to reduce memory latency."
27,Processor and memory settings for standalone Cisco UCS C-Series servers: Virtualization
27,Figures 10 and 11 show processor and power and performance settings for virtualized workloads in standalone Cisco UCS C-Series M5 servers.
27,Figure 10.
27,Processor settings for virtualized workloads
27,Figure 11.
27,Power and performance settings for virtualized workloads
27,Memory settings for virtualized workloads
27,Figure 12 shows memory settings for virtualized workloads in standalone Cisco UCS C-Series M5 servers.
27,Figure 12.
27,Memory settings for virtualized workloads
27,High-performance computing workloads
27,"HPC refers to cluster-based computing that uses multiple individual nodes that are connected and that work in parallel to reduce the amount of time required to process large data sets that would otherwise take exponentially longer to run on any one system. HPC workloads are computation intensive and typically also network-I/O intensive. HPC workloads require high-quality CPU components and high-speed, low-latency network fabrics for their Message Passing Interface (MPI) connections."
27,"Computing clusters include a head node that provides a single point for administering, deploying, monitoring, and managing the cluster. Clusters also have an internal workload management component, known as the scheduler, that manages all incoming work items (referred to as jobs). Typically, HPC workloads require large numbers of nodes with nonblocking MPI networks so that they can scale. Scalability of nodes is the single most important factor in determining the achieved usable performance of a cluster."
27,Processor and memory settings for Cisco UCS managed servers: HPC
27,Figure 13 Highlights the BIOS selections that are recommended for HPC workloads on Cisco UCS M5 platforms managed by Cisco UCS Manager. Rest of the BIOS settings are configured as “Platform Default”.
27,Figure 13.
27,Processor settings for HPC workloads
27,"You should enable Intel Turbo Boost technology for HPC workloads to increase the computing power. When Intel Turbo Boost is enabled, each core provides higher computing frequency potential, allowing a greater number of parallel requests to be processed efficiently."
27,Intel SpeedStep is enabled because it is required for Intel Turbo Boost to function.
27,"HPC workloads typically do not benefit from Intel Hyper-Threading. Additional threads only serve to create resource contention within the microarchitecture of the CPU. Generally, Intel Hyper-Threading has the greatest impact on workloads in which threads are forced to wait for completion of back-end I/O requests, to reduce thread contention for CPU resources."
27,"Enabling the processor power state C6 helps save power when the CPU is idle. Because HPC is computing intensive, the CPU will likely seldom go into an idle state. However, enabling C-states saves CPU power in the event that there are any inactive requests."
27,"You should set CPU Performance to HPC mode to handle more random, parallel requests by HPC applications. If HPC performs more in-memory processing (for example, for video data), you should enable the prefetcher options so that they can handle multiple parallel requests. This configuration also helps retain some hot data in the Layer 2 cache, and it improves HPC performance (CPU performance)."
27,"HPC requires a high-bandwidth I/O network. When you enable DCA support, network packets go directly into the Layer 3 processor cache instead of the main memory. This approach reduces the number of HPC I/O cycles generated by HPC workloads when certain Ethernet adapters are used, which in turn increases system performance."
27,"You can set the Energy Performance option to Maximum Performance, Balanced Performance, Balanced Power, or Power Saver. Test results demonstrate that most applications run best with the Balanced Performance setting. Applications that are highly I/O sensitive perform best when the Energy Performance option is set to Maximum Performance."
27,Intel Directed I/O for HPC workloads
27,Figure 14 shows the recommended Intel Directed I/O settings for HPC workloads in Cisco UCS M5 platforms.
27,Figure 14.
27,Intel Directed I/O settings for HPC workloads
27,Memory settings for HPC workloads
27,Figure 15 shows the memory settings for HPC workloads on Cisco UCS M5 servers.
27,Figure 15.
27,Memory settings for HPC workloads
27,The NUMA option should be enabled for HPC workloads so that NUMA can determine the memory allocation for each thread run by the HPC applications.
27,"Because HPC workloads perform mostly in-memory processing, you should set DIMMs to run at the highest available frequency to process the data more quickly."
27,Processor and memory settings for standalone Cisco UCS C-Series servers: HPC
27,Figures 16 and 17 show the recommended processor and power and performance settings for HPC workloads in standalone Cisco UCS C-Series M5 servers.
27,Figure 16.
27,Processor settings for HPC workloads
27,Figure 17.
27,Power and performance settings for HPC workloads
27,Figures 18 shows memory settings for HPC workloads for standalone Cisco UCS C-Series M5 servers.
27,Figure 18.
27,Memory settings for HPC workloads
27,Java Enterprise Edition application server workloads
27,"Java EE (previously referred to as the J2EE) defines the core set of APIs and features of Java application servers. Usually, Java EE applications are client-server or server-side applications and require a Java EE application server."
27,Java EE application servers are distinguished by the following characteristics:
27,●     They are fully compliant application servers that implement the full Java EE stack specifications with features such as JBoss Enterprise. Examples of fully compliant application servers are Apache Geronimo and JBoss Application Server.
27,"●     They are web application servers that support only the web tier of Java EE, including the servlet. Examples of fully compliant application servers are Apache Tomcat and Jetty."
27,Processor and memory settings for Cisco UCS managed servers: Java EE
27,Figure 19 Highlights the BIOS selections that are recommended for Java EE application servers on Cisco UCS M5 servers managed by Cisco UCS Manager. Rest of the BIOS settings are configured as “Platform Default”.
27,Figure 19.
27,Processor settings for Java EE application workloads
27,"Intel Turbo Boost Technology enables higher CPU frequency, which helps accelerate processing of application requests. This feature helps reduce end-user response time."
27,"Business scenarios such as batch processes run at a certain time of the day benefit from Intel Turbo Boost. It enables CPU cores to achieve at higher frequency clock speeds, which helps lower batch processing time, thereby helping the business complete and generate business reports more quickly."
27,"You should enable all the C-states. This configuration helps reduce power consumption because only active cores will process requests during nonpeak hours. If the application demands more CPU cores, the inactive cores will become active, which helps increase throughput."
27,"The CPU Performance option should be set to Enterprise. When a web server needs to process a large amount of data in a system, the data-access pattern is predictable (mostly sequential or adjacent lines are accessed). In this situation, it is desirable to enable the prefetchers (MLC and DCU) by setting CPU Performance to Enterprise, to reduce access latency for memory-bound operations."
27,Intel Directed I/O for Java application workloads
27,Figure 20 shows the recommended Intel Directed I/O settings for Java application workloads in Cisco UCS M5 servers.
27,Figure 20.
27,Intel Directed I/O settings for Java application workloads
27,Memory settings for Java EE application server workloads
27,Figure 21 shows the recommended memory settings for Java EE application servers for Cisco UCS M5 servers managed by Cisco UCS Manager.
27,Figure 21.
27,Memory settings for Java EE application workloads
27,"Set the DDR mode to Performance so that the DIMMs work at the highest available frequency for the installed memory and CPU combination. In-memory enterprise applications such as Terracotta Ehcache benefit from the high memory speed. If this mode is enabled in web server workloads, I/O operations will be serviced at the highest frequency, and memory latency will be reduced."
27,Processor and memory settings for standalone Cisco UCS C-Series servers: Java EE
27,Figures 22 and 23 show processor and performance and power settings for Java EE applications in standalone Cisco UCS C-Series M5 servers.
27,Figure 22.
27,Processor settings for Java EE application workloads
27,Figure 23.
27,Power and performance settings for Java EE application workloads
27,Figure 24 shows memory settings for Java EE applications for standalone Cisco UCS C-Series M5 servers.
27,Figure 24.
27,Memory settings for Java EE application workloads
27,Analytics database decision-support system workloads
27,An analytics database is a read-only system that stores historical data for business metrics such as sales performance and inventory levels.
27,"An analytics database is specifically designed to support Business Intelligence (BI) and analytics applications, typically as part of a data warehouse or data mart. This feature differentiates it from operational, transactional, and OLTP databases, which are used for transaction processing: order entry and other “run the business” applications."
27,Processor and memory settings for Cisco UCS managed servers: DSS
27,Figure 25 Highlights the BIOS selections that are recommended for analytics database systems on Cisco UCS M5 servers managed by Cisco UCS Manager. Rest of the BIOS settings are configured as “Platform Default”.
27,Figure 25.
27,Processor settings for analytics database DSS workloads
27,Intel Directed I/O for analytics database DSS workloads
27,Figure 26 shows the recommended Intel Directed I/O settings for analytics database DSS workloads on Cisco UCS M5 servers managed by Cisco UCS Manager.
27,Figure 26.
27,Intel Directed I/O settings for analytics database DSS workloads
27,Memory settings for analytics database DSS workloads
27,Figure 27 show the recommended memory settings for analytics database DSS workloads on Cisco UCS M5 servers managed by Cisco UCS Manager.
27,Figure 27.
27,Memory settings for analytics database DSS workloads
27,Processor and memory settings for standalone Cisco UCS C-Series servers: DSS
27,Figures 28 and 29 show processor and performance and power settings for analytics database DSS workloads on standalone Cisco UCS C-Series M5 servers.
27,Figure 28.
27,Processor settings for analytics database DSS workloads
27,Figure 29.
27,Power and performance settings for analytics database DSS workloads
27,Figure 30 shows memory settings for analytics database DSS workloads in standalone Cisco UCS C-Series M5 servers.
27,Figure 30.
27,Memory settings for analytics database DSS workloads
27,Conclusion
27,"When tuning system BIOS settings for performance, you need to consider a number of processor and memory options. If the best performance is your goal, be sure to choose options that optimize for performance in preference to power savings, and experiment with other options such as CPU prefetchers, CPU power management, and CPU hyperthreading."
27,For more information
27,"For more information about Cisco UCS B-Series and C-Series M5 servers, see:"
27,●     Cisco UCS B200 M5 Blade Server: https://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-b-series-blade-servers/b200m5-specsheet.pdf
27,●     Cisco UCS C220 M5 Rack Server: https://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/c220m5-sff-specsheet.pdf
27,●     Cisco UCS C240 M5 Rack Server: https://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/c240m5-sff-specsheet.pdf
27,●     Cisco UCS B480 M5 Blade Server: https://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-b-series-blade-servers/b480m5-specsheet.pdf
27,●     Cisco UCS C480 M5 Rack Server: https://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/c480-m5-high-performance-specsheet.pdf
27,Our experts recommend
27,Cisco UCS B200 M6 Blade Server At-a-Glance
27,Cisco UCS M6 Servers with 3rd Gen Intel Xeon CPUs FAQ
27,Learn more
27,Follow Us
27,News & EventsNewsroomEventsBlogsCommunity
27,About Us
27,About Us
27,About Cisco
27,Customer stories
27,Investor relations
27,Social responsibility
27,Environmental sustainability
27,The Trust Center
27,Contact Us
27,Contact Us
27,Contact Cisco
27,Meet our partners
27,Find a reseller
27,Work with Us
27,Careers
27,We Are Cisco
27,Partner with Cisco
27,Cisco Sites
27,Meraki
27,All new Webex
27,Cisco Umbrella
27,Contacts
27,Feedback
27,Help
27,Site Map
27,Terms & Conditions
27,Privacy
27,Privacy Statement
27,Cookies
27,Cookies
27,Trademarks
28,SQL Query Optimization — 5 SQL Tuning Tips - Database Management - Blogs - Quest Community
28,Products
28,View all products
28,Free trials
28,Buy online
28,Product lines
28,ApexSQL
28,Change Auditor
28,Enterprise Reporter
28,Foglight Database Monitoring
28,Foglight Evolve
28,KACE
28,Metalogix
28,Migration Manager
28,NetVault Backup
28,One Identity
28,QoreStor
28,Quest On Demand
28,Rapid Recovery
28,Recovery Manager
28,RemoteScan
28,SharePlex
28,Spotlight
28,Stat
28,Toad
28,Featured products
28,Cloud Management
28,Cloud Access Manager
28,"Foglight for Virtualization, Enterprise Edition"
28,Identity Manager
28,On Demand Migration for Email
28,Quest On Demand
28,Rapid Recovery
28,Data Protection
28,Foglight for Virtualization
28,NetVault
28,NetVault for Office 365
28,QorePortal
28,QoreStor
28,Rapid Recovery
28,vRanger
28,Database Management
28,Foglight for Databases
28,Litespeed for SQL Server
28,SharePlex
28,Spotlight SQL Server Enterprise
28,Toad Data Point
28,Toad DevOps Toolkit
28,Toad Edge
28,Toad for Oracle
28,Toad for SQL Server
28,Identity & Access Management
28,Active Roles
28,Defender
28,Identity Manager
28,Identity Manager Data Governance
28,One Identity Safeguard
28,Password Manager
28,Privileged Access Suite for Unix
28,Starling Connect
28,Starling Two-Factor Authentication
28,syslog-ng
28,Microsoft Platform Management
28,Active Administrator
28,Change Auditor
28,Enterprise Reporter
28,GPOADmin
28,InTrust
28,Metalogix
28,Migration Manager
28,On Demand Migration for Email
28,Quest On Demand
28,Recovery Manager
28,Performance Monitoring
28,Foglight Capacity Director
28,Foglight Hybrid Cloud Manager
28,Foglight for Databases
28,Foglight for Operating Systems
28,Foglight for Oracle
28,Foglight for PostgreSQL
28,Foglight for SQL Server
28,Foglight for Storage Management
28,Foglight for Virtualization
28,Spotlight on SQL Server
28,Unified Endpoint Management
28,Desktop Authority Management Suite
28,KACE Cloud Mobile Device Manager
28,KACE Desktop Authority
28,KACE Privilege Manager
28,KACE Systems Deployment Appliance
28,KACE Systems Management Appliance
28,RemoteScan
28,Solutions
28,View all Solutions
28,Industries
28,Education
28,Energy
28,Federal Government
28,Financial Services
28,Healthcare
28,State & Local Government
28,Platforms
28,Active Directory
28,Cisco
28,DB2
28,Exchange
28,Google
28,Hyper-v
28,Lotus Notes
28,OneDrive for Business
28,Office 365
28,Oracle
28,SAP/Sybase
28,SharePoint
28,SQL Server
28,Teams
28,Unix/Linux
28,VMware
28,Windows Server
28,Cloud Management
28,Data Protection
28,Overview
28,Backup & Recovery
28,Cloud Management
28,Deduplication & Compression
28,Disaster Recovery
28,Office 365 Data Protection
28,Virtualization Management
28,Database Management
28,Overview
28,Administration
28,Cloud Migration
28,Data Preparation and Provisioning
28,Development
28,DevOps
28,Performance Monitoring
28,Replication
28,Supported Platforms
28,IBM DB2
28,MySQL
28,Oracle
28,PostgreSQL
28,SAP Solutions
28,SQL Server
28,GDPR Compliance
28,Identity & Access Management
28,Overview
28,Identity Administration
28,Identity Governance
28,Privileged Access Management
28,AD Account Lifecycle Management
28,Access Control
28,Cloud
28,Log Management
28,Microsoft Platform Management
28,Overview
28,Mergers & Acquisitions
28,Migration & Consolidation
28,Office 365 Migration & Management
28,Security & Compliance
28,Windows Backup & Recovery
28,Supported Platforms
28,Active Directory
28,Cisco
28,Exchange
28,Google
28,Groupwise
28,Lotus Notes
28,Office 365
28,OneDrive for Business
28,SharePoint
28,SQL Server
28,Teams
28,Unix/Linux
28,Windows Server
28,Performance Monitoring
28,Overview
28,Database Performance Monitoring
28,Operating System Monitoring
28,Storage Performance & Utilization Management
28,Supported Platforms
28,Active Directory
28,DB2
28,Exchange
28,Java
28,Hyper-V
28,.NET
28,Oracle
28,SAP/Sybase
28,Storage
28,SQL Server
28,VMware
28,Unified Endpoint Management
28,Overview
28,Endpoint Compliance
28,Endpoint Security
28,Endpoint Visibility
28,Industries
28,Education
28,Healthcare
28,Supported Platforms
28,Internet of things
28,Microsoft® Windows
28,MAC
28,UNIX/LinuX
28,Resources
28,Blogs
28,Blogs A to Z
28,Data Protection
28,Database Management
28,Microsoft Platform Management
28,Performance Monitoring
28,Unified Endpoint Management
28,Customer Stories
28,Documents
28,Events
28,Webcasts
28,Technical Documentation
28,Videos
28,Whitepapers
28,Trials
28,Services
28,Consulting Services
28,Overview
28,Microsoft Platform Services
28,Data Protection Services
28,Unified Endpoint Management
28,Performance Monitoring Services
28,Database Management Services
28,Educational Services
28,Support Services
28,Support
28,Support Home
28,By Product
28,All Products
28,AppAssure
28,Archive Manager
28,Change Auditor
28,Desktop Authority
28,DR Series
28,Foglight
28,KACE
28,Migration Manager
28,NetVault
28,Rapid Recovery
28,SharePlex
28,Toad
28,vRanger
28,Contact Support
28,Overview
28,Customer Service
28,Licensing Assistance
28,Renewal Assistance
28,Technical Support
28,Download Software
28,Knowledge Base
28,My Account
28,My Products
28,My Service Requests
28,My Licenses
28,My Groups
28,My Profile
28,Policies & Procedures
28,Consulting Services
28,Microsoft Platform Management
28,Data Protection
28,Unified Endpoint Management
28,Performance Monitoring
28,Database Management
28,Technical Documentation
28,Educational Services
28,User Forums
28,Video Tutorials
28,Partners
28,Overview
28,Partner Circle Log In
28,Become a Partner
28,Find a Partner
28,Partner Community
28,Communities
28,Home
28,Blogs
28,Data Protection
28,Database Management
28,ITNinja
28,Microsoft Platform Management
28,Performance Monitoring
28,Toad World Blog
28,Unified Endpoint Management
28,Forums
28,All Product Forums
28,Active Administrator
28,Desktop Authority
28,Foglight
28,ITNinja
28,Migration Manager for Active Directory
28,NetVault
28,Rapid Recovery
28,Toad World Forum
28,Social Networks
28,Facebook
28,LinkedIn
28,Twitter@Quest
28,Twitter@QuestSupport
28,YouTube
28,製品情報
28,すべての製品情報を見る
28,Change Auditor
28,Foglight
28,KACE
28,Metalogix
28,Migration Manager
28,Migrator for Notes to SharePoint
28,NetVault Backup
28,On Demand Migration for Email
28,QoreStor
28,Rapid Recovery
28,Recovery Manager
28,SharePlex
28,Spotlight
28,Toad
28,ソリューション
28,すべてのプラットフォームを見る
28,クラウド管理
28,GDPRコンプライアンス
28,データ保護
28,クラウド管理
28,ディザスタリカバリ
28,バックアップとリカバリ
28,Office 365 データ保護
28,仮想化管理
28,重複除外と複製
28,データベース管理
28,DevOps
28,データの準備と分析
28,データベースのクラウド移行
28,データベースパフォーマンス監視
28,データベース管理
28,データベース複製ソフトウェアツール
28,統合エンドポイント管理
28,エンドポイントコンプライアンス
28,エンドポイントセキュリティ
28,エンドポイントの可視化
28,Microsoftプラットフォーム管理
28,ハイブリッドActive Directoryのセキュリティとガバナンス
28,Microsoftプラットフォームの移行計画と統合
28,セキュリティとコンプライアンス
28,情報アーカイブおよびストレージ管理ソリューション
28,Windowsのバックアップとリカバリ
28,Microsoft Serverのパフォーマンスと可用性
28,レポート作成機能
28,グループポリシーと権限
28,パフォーマンス監視
28,サービス
28,コンサルティングサービス
28,Microsoftプラットフォーム管理
28,データ保護
28,統合エンドポイント管理
28,パフォーマンス監視
28,データベース管理
28,トレーニングと認定資格
28,サポートサービス
28,サポート
28,サポートホーム
28,製品で検索
28,All Products
28,AppAssure
28,Archive Manager
28,Change Auditor
28,Desktop Authority
28,DR Series
28,Foglight
28,KACE
28,Migration Manager
28,NetVault
28,Rapid Recovery
28,SharePlex
28,Toad
28,vRanger
28,お問い合わせ
28,すべて
28,カスタマサービス
28,ライセンス アシスタンス
28,更新のアシスタンス
28,技術サポート
28,コミュニティフォーラム
28,ソフトウェアのダウンロード
28,ナレッジベース
28,マイアカウント
28,マイ プロダクト
28,Myサービスリクエスト
28,マイ ライセンス
28,マイ グループ
28,マイ プロファイル
28,ポリシーおよび手順
28,コンサルティングサービス
28,Microsoftプラットフォーム管理
28,データ保護
28,統合エンドポイント管理
28,パフォーマンス監視
28,データベース管理
28,リリースノートおよびガイド
28,教育サービス
28,ビデオチュートリアル
28,トライアル
28,パートナー
28,Partner Circleへのログイン
28,パートナーになる
28,Find a Partner
28,パートナーコミュニティ
28,コミュニティ
28,Quest Community
28,Site
28,Search
28,User
28,Site
28,Search
28,User
28,Blogs
28,Database Management
28,SQL Query Optimization — How to Determine When and If It’s Needed
28,Data Protection
28,Database Management
28,Microsoft Platform Management
28,Performance Monitoring
28,Unified Endpoint Management
28,Quest
28,More
28,Cancel
28,New
28,SQL Query Optimization — How to Determine When and If It’s Needed
28,Actions
28,Subscribe by email
28,Posts RSS
28,More
28,Cancel
28,Tags
28,Database Performance
28,SQL Server
28,Janis Griffin
28,"Jun 24, 2020"
28,"It’s easy to start tinkering with the gears of SQL query optimization. You open up SQL Server Management Studio (SSMS), monitor wait time, review the execution plan, gather object information and begin optimizing SQL until you’re running a finely tuned machine."
28,"If you’re good enough at it, you score a quick victory and get back to your regularly scheduled chaos. But if you adjust the wrong thing, or adjust the right thing in the wrong direction, well, there went your Wednesday."
28,SQL query optimization? What makes you think you need it?
28,"Most of the time, it’s a spike in trouble tickets or user complaints. “Why is the system so slow?” your users complain. “It’s taking us forever to run our usual reports this week.”"
28,"That’s a pretty vague description, of course. It would be nice if they could tell you, “Things are slow because you’ve got an implicit conversion in line 62 of CurrentOrderQuery5.sql. The column is varchar and you’re passing in an integer.” But it’s not likely that your users can see that level of detail."
28,"At least trouble tickets and phone calls make for an active metric: easy to spot, easy to measure. When they start rolling in, you can be reasonably sure that it’s time for SQL tuning."
28,"But there are other, passive metrics that make the need less clear. Things like slumping sales, which could be due to any number of factors. Is it because painfully slow queries in your online store are making your customers abandon their shopping carts? Is it because the economy is in bad shape?"
28,Or it could be things like sluggish SQL Server performance. Is it because a poorly written query is sending logical reads through the roof? Is it because the server is low on physical resources like memory and storage?
28,"In both scenarios, SQL query optimization can help with the first option, but not the second."
28,Why apply the right solution to the wrong problem?
28,"Before you go down the path of optimization, make sure that tuning is the right solution to the right problem."
28,"Tuning SQL is a technical process, but every technical step has roots in good business sense. You could spend days trying to shorten execution time by a few milliseconds or reduce the number of logical reads by five percent, but is the reduction worth your time? It’s true that it’s important to meet the requirements of users, but every effort reaches the point of diminishing returns eventually."
28,Consider these SQL query performance problems and the business context around them:
28,"Acceptable performance — A query takes 10 minutes to run and the user wants it to run in one minute; that seems like a reasonable disparity and an achievable goal for optimization. However, if the query takes overnight and the user thinks it should run in one minute, then it may be more than a tuning problem. For one thing, you may have to educate the user about the amount of work the query is actually performing. For another, it may be a problem with the way the database was designed or the way the client application was written."
28,"Utility — Suppose you’re responsible for administering the financial database in a manufacturing company. At the end of every month, users complain about poor performance. You trace the problem to a series of month-end reports run by Accounting that take hours each and go straight into a filing cabinet unexamined by anybody. Instead of tuning, you explain the problem to the business managers and obtain permission to delete the reports."
28,"Time shifting — Or, suppose those same reports are important for governance but not urgent for the business. If they are run once per week or per month, they can be scheduled for off-peak hours by pre-caching the data set and sending the results to a file. That removes the bottleneck on the other business users and frees the Accounting user from having to wait for the reports."
28,"When you factor business context into your decision to optimize, you can set priorities and buy yourself time."
28,"When you do optimize SQL queries, try SQL diagramming"
28,"SSMS and the tools built into SQL Server offer most of what you need for effective SQL query optimization. Combine the tools with a methodical approach around the following steps, as described in the e-book The Fundamental Guide to SQL Query Optimization:"
28,Monitor Wait Time
28,Review the Execution Plan
28,Gather Object Information
28,Find the Driving Table
28,Identify Performance Inhibitors
28,"In step 4, your goal is to drive the query with the table that returns the least data. When you study joins and predicates, and filter earlier in the query rather than later, you reduce the number of logical reads. That’s a big step in SQL query optimization."
28,"SQL diagramming is a graphical technique for mapping the amount of data in the tables and finding which filter will return the fewest records. First, you determine which tables contain the detailed information and which tables are the master or lookup tables. Consider the simple example of this query against a university registration database:"
28,"The detail table is registration. It has two lookup tables, student and class. To diagram these tables, draw an upside-down tree connecting the detail table (at the top) with arrows (or links) to the lookup tables, like this:"
28,"Now, calculate the relative number of records required for the join criteria (that is, the average ratio of rows related between the detail table and lookup tables). Write the numbers at each end of the arrow. In this example, for every student there are about 5 records in the registration table, and for every class there are about 30 records in registration. That means it should never be necessary to JOIN more than 150 (5×30) records to get a result for any single student or any single class."
28,"That exercise is useful if your join columns are not indexed, or if you’re not sure that they’re indexed."
28,"Next, look at the filtering predicates to find which table to drive the query with. This query had two filters: one on registration cancelled = 'N' and the other on signup_date between two dates. To see how selective the filter is, run this query on registration:"
28,select count(1) from registration where cancelled = 'N'
28,AND   r.signup_date BETWEEN :beg_date AND :beg_date +1
28,"It returns 4,344 records out of the 79,800 total records in registration. That is, 5.43 percent of the records will be read with that filter."
28,The other filter is on class:
28,select count(1) from class where name = 'ENGLISH 101'
28,"It returns two records out of 1,000, or 0.2 percent, which represents a much more selective filter. Thus, class is the driving table, and the one on which to focus your SQL tuning first."
28,The voice of the user
28,"If you’re sure you need SQL tuning, The Fundamental Guide to SQL Query Optimization offers further insight. It walks you through five performance tuning tips with copy-and-paste queries and case studies, including the one described above."
28,You’ll probably find that the single most important SQL query optimization tool is the voice of the user. Why? Because that voice lets you know when to start optimizing and it tells you when you have optimized enough. It can ensure that you start tinkering with the gears when you need to and stop while you’re still ahead.
28,"The Fundamental Guide to SQL Query OptimizationSQL query optimization may be the best part of your job as a database professional. You roll up your sleeves, you get analytical, you poke around in code and you measure the impact of your changes."
28,This e-book walks you methodically through five basic steps in the SQL tuning process. Learn how and what to measure both before and after you’ve made changes so you’re not wasting effort on things that don’t matter. Discover how to find and correct common mistakes that eat into SQL query performance.
28,Optimization tips? I’m in!
28,Related Content
28,Company
28,About Us
28,Buy
28,Contact Us
28,Careers
28,News
28,Resources
28,Blogs
28,Customer Stories
28,Documents
28,Events
28,Videos
28,Support
28,Professional Services
28,Renew Support
28,Technical Support
28,Training & Certification
28,Support Services
28,Social Networks
28,Facebook
28,Instagram
28,LinkedIn
28,Twitter
28,YouTube
28,© 2021 Quest Software Inc. ALL RIGHTS RESERVED.
28,Legal
28,Terms of Use
28,Privacy
28,Community Feedback & Support
28,会社名
28,会社情報
28,お問い合わせ
28,採用情報
28,ニュース
28,リソース
28,ブログ
28,お客様の事例
28,ドキュメント
28,イベント
28,ビデオ
28,サポート
28,プロフェッショナルサービス
28,サポートの更新
28,テクニカルサポート
28,トレーニングと認定資格
28,サポートサービス
28,ソーシャルネットワーク
28,Facebook
28,Instagram
28,LinkedIn
28,Twitter
28,YouTube
28,© 2021 Quest Software Inc. ALL RIGHTS RESERVED.
28,「法務」
28,ご利用規約
28,個人情報保護方針
28,コミュニティのフィードバックとサポート
29,OnGres | Boost your User-Defined Functions in PostgreSQL
29,Resources
29,Blog
29,Services
29,Professional Services
29,Consulting
29,Training
29,Products
29,StackGres
29,PostgresqlCO.NF
29,About
29,Contact
29,Post
29,Boost your User-Defined Functions in PostgreSQL
29,Emanuel Calvo
29,Anthony Sotolongo
29,"Feb 5, 2021 ·"
29,9 min read
29,postgresql
29,performance
29,Share this post
29,Emanuel Calvo
29,Database Engineer
29,Anthony Sotolongo
29,Database Engineer
29,Introduction
29,"Using the RDBMS only to store data is restricting the full potential of the database systems, which were designed for server-side processing and provide other options besides being a data container. Some of these options are stored procedures and functions that allow the user to write server-side code, using the principle of bringing computation to data, avoiding large datasets round trips and taking advantage of server resources. PostgreSQL allows programming inside the database since the beginning, with User Defined Functions (UDFs). These functions can be written in several languages like SQL, PL/pgsql, PL/Python, PL/Perl, and others. But the most common are the first two mentioned: SQL and PL/pgsql. However, there may be “anti-patterns” in your code within functions and they can affect performance. This blog will show the reader some simple tips, examples and explanations about increasing performance in server-side processing with User Defined Functions in PostgreSQL. It is also important to clarify that the intention of this post isn’t to discuss whether Business Logic should be placed, but only how you can take advantage of the resources of the database server."
29,Avoid these antipatterns in your PL/pgsql’s code
29,Use PL/pgsql functions for simple SQL statements
29,"SQL functions, in certain conditions, can have their function bodies inlined into the main query directly. This can be a performance advantage because the function code can be analyzed by the planner, which can apply some optimizations. When can I apply this pattern? When you have a query or group of simple queries that do not require intermediate analysis/process before returning the result. On the other hand, writing these simple SQL sentences in PL/pgsql requires overhead for the PL/pgsql compiler."
29,Example:
29,1- CREATE OR REPLACE FUNCTION hemisphere_sql (character varying)
29,RETURNS character varying
29,LANGUAGE sql
29,AS $$
29,SELECT
29,"CASE WHEN $1 IN ('UK', 'Germany', 'Japan', 'US', 'China', 'Canada', 'Russia', 'France') THEN"
29,'North'
29,"WHEN $1 IN ('South Africa', 'Australia', 'Chile') THEN"
29,'South'
29,ELSE
29,'unknown'
29,END
29,$$;
29,2- CREATE OR REPLACE FUNCTION hemisphere_plpgsql (character varying)
29,RETURNS character varying
29,LANGUAGE plpgsql
29,AS $$
29,DECLARE
29,result character varying;
29,BEGIN
29,result:= (
29,SELECT
29,"CASE WHEN $1 IN ('UK', 'Germany', 'Japan', 'US', 'China', 'Canada', 'Russia', 'France') THEN"
29,'North'
29,"WHEN $1 IN ('South Africa', 'Australia', 'Chile') THEN"
29,'South'
29,ELSE
29,'unknown'
29,END);
29,RETURN result;
29,END;
29,$$;
29,"1- EXPLAIN (ANALYZE,VERBOSE ) SELECT hemisphere_sql(country) FROM customers;"
29,Seq Scan on public.customers
29,(cost=0.00..963.00 rows=20000 width=32) (actual time=0.039..29.309 rows=20000 loops=1)
29,"Output: CASE WHEN ((country)::text = ANY ('{UK,Germany,Japan,US,China,Canada,Russia,France}'::text[])) THEN 'North'::text WHEN ((country)::text = ANY ('{""South Africa"",Australia,Chile}'::text[])) THEN 'South'::text ELSE 'unknown'::text END"
29,Planning Time: 0.458 ms
29,Execution Time: 32.306 ms
29,"2-EXPLAIN (ANALYZE,VERBOSE ) SELECT hemisphere_plpgsql(country) FROM customers;"
29,Seq Scan on public.customers
29,(cost=0.00..5688.00 rows=20000 width=32) (actual time=0.654..174.685 rows=20000 loops=1)
29,Output: hemisphere_plpgsql(country)
29,Planning Time: 0.082 ms
29,Execution Time: 175.972 ms
29,"As we can see in the outputs of the explains commands, on first EXPLAIN,"
29,output
29,"tag has the SQL code that belongs to SQL function, which means that this code is inlined into the calling query, rather than call the function, as the second EXPLAIN, this can improve the performance of our queries."
29,Unnecessary usage of SELECT INTO clause
29,"Inside the PL/pgsql function, it is a bit more costly to assign a value using"
29,SELECT INTO than a simple assignment using :=. When can I apply this? When the := operator can replace the INTO clause.
29,Example:
29,1- CREATE OR REPLACE FUNCTION simple ()
29,RETURNS void
29,AS $$
29,DECLARE
29,s int DEFAULT 0;
29,BEGIN
29,FOR i IN 1..10000 LOOP
29,s := s + 1;
29,END LOOP;
29,END;
29,LANGUAGE plpgsql;
29,2- CREATE OR REPLACE FUNCTION using_select ()
29,RETURNS void
29,AS $$
29,DECLARE
29,s int DEFAULT 0;
29,BEGIN
29,FOR i IN 1..10000 LOOP
29,SELECT s + 1 INTO s;
29,END LOOP;
29,END;
29,LANGUAGE plpgsql;
29,1-SELECT simple();
29,Time: 16.980 ms
29,2-SELECT using_select();
29,Time: 86.931 ms
29,Overusing RAISE clause
29,"RAISE clause can be useful to debug and show some information about the code, but it carries or has an extra load within the functions. Only use it if necessary in the production environments. Example:"
29,"1- CREATE OR REPLACE FUNCTION some_sum (val int, cnt int)"
29,RETURNS int
29,AS $$
29,DECLARE
29,i int;
29,result int:= 0;
29,BEGIN
29,FOR i IN 1.. $2 LOOP
29,result:= result + $1;
29,END LOOP;
29,"RAISE notice 'Final value of result: %', result;"
29,RETURN result;
29,END;
29,LANGUAGE plpgsql;
29,"2- CREATE OR REPLACE FUNCTION some_sum_raise (val int, cnt int)"
29,RETURNS int
29,AS $$
29,DECLARE
29,i int;
29,result int:= 0;
29,BEGIN
29,FOR i IN 1.. $2 LOOP
29,result:= result + $1;
29,"RAISE notice 'Temporary value of result: %', result;"
29,END LOOP;
29,"RAISE notice 'Final value of result: %', result;"
29,RETURN result;
29,END;
29,LANGUAGE plpgsql;
29,1- SELECT
29,"some_sum(3,100);"
29,NOTICE:
29,Final value of result: 300
29,Time: 1.843 ms
29,2- SELECT
29,"some_sum_raise(3,100);"
29,NOTICE:
29,Temporary value of result: 3
29,...
29,NOTICE:
29,Temporary value of result: 300
29,NOTICE:
29,Final value of result: 300
29,Time: 8.578 ms
29,Overusing the high-level programming coding style for SQL activities
29,"Even programmers who come from high-level programming are unaware of the benefits of SQL, and ADVANCED SQL a language that can speed up the performance considerably by avoiding unnecessary loops. For example iterating on a FOR LOOP and doing a select within can be replaced by a single query using the LATERAL clause, which essentially"
29,is like a SQL for each loop
29,1- CREATE OR REPLACE
29,FUNCTION oldest_orders_by_customer (int) RETURNS SETOF t_oldest_orders_by_customer
29,AS $$
29,DECLARE
29,c customers;
29,result record;
29,BEGIN
29,FOR c IN SELECT * FROM customers c2 WHERE age > $1
29,loop
29,"SELECT c.firstname,o.orderid, o.orderdate , o.totalamount into result"
29,FROM orders o
29,WHERE o.customerid = c.customerid
29,ORDER BY o.orderdate DESC
29,LIMIT 1;
29,IF result is not null THEN
29,RETURN NEXT result;
29,END IF;
29,END LOOP;
29,RETURN;
29,END;
29,LANGUAGE plpgsql;
29,2- CREATE OR REPLACE
29,FUNCTION oldest_orders_by_customer_lateral (int) RETURNS SETOF t_oldest_orders_by_customer
29,BEGIN
29,RETURN QUERY SELECT customer_sub.firstname
29,", o_sub.*"
29,"FROM (SELECT * FROM customers c2 WHERE age > $1) customer_sub,"
29,"LATERAL (SELECT o.orderid, o.orderdate , o.totalamount"
29,FROM orders o
29,WHERE o.customerid = customer_sub.customerid
29,ORDER BY o.orderdate DESC
29,LIMIT 1) o_sub;
29,END;
29,LANGUAGE plpgsql;
29,1- SELECT * FROM oldest_orders_by_customer(80);
29,Time: 89.296 ms
29,2- SELECT * FROM oldest_orders_by_customer_lateral(80);
29,Time: 45.230 ms
29,Using
29,functions properties
29,The definition of functions has
29,"some properties that can help with function performance, for example:"
29,1: Use PARALLEL SAFE whenever possible
29,"The planner cannot determine automatically if a function is parallel safe, but"
29,"under certain conditions parallel mode can boost performance significantly if you process a large dataset. The number of workers that the planner will use is limited by the parameters max_parallel_workers_per_gather, which in turn are taken from the pool of processes established by max_worker_processes, limited by max_parallel_workers, the maximum number of concurrent queries to execute with parallelism"
29,"is determined by the following formula, as long as max_worker_processes<= server cores:"
29,#Q_concurrent_par = max_worker_processes /max_parallel_workers_per_gather (integer division)
29,"When is it safe to use PARALLEL in a function? As long as your code does not perform the following, you should be ready to use it:"
29,Writes to the database.
29,Access sequences.
29,Change the transaction state.
29,Makes persistent changes to settings.
29,Access temporary tables.
29,Use cursors.
29,Defines prepared statements
29,Example:
29,1- CREATE OR REPLACE FUNCTION pair_div_4 (i int) RETURNS boolean
29,AS $$
29,BEGIN
29,IF $1%2 = 0 AND $1%4 = 0 THEN
29,RETURN TRUE;
29,END IF;
29,RETURN FALSE;
29,END;
29,LANGUAGE plpgsql;
29,2- CREATE OR REPLACE FUNCTION pair_div_4_ps (i int) RETURNS boolean
29,AS $$
29,BEGIN
29,IF $1%2 = 0 AND $1%4 = 0 THEN
29,RETURN TRUE;
29,END IF;
29,RETURN FALSE;
29,END;
29,LANGUAGE plpgsql
29,PARALLEL SAFE;
29,1- EXPLAIN ANALYZE
29,SELECT * from trade where pair_div_4 (id);
29,Seq Scan on trade
29,(cost=0.00..448684.86 rows=563520 width=16) (actual time=0.323..2459.553 rows=422640 loops=1)
29,Filter: pair_div_4(id)
29,Rows Removed by Filter: 1267921
29,Planning Time: 0.070 ms
29,Execution Time: 2471.796 ms
29,2- explain analyze
29,select * from trade where pair_div_4_ps (id);
29,Gather
29,(cost=1000.00..249635.11 rows=563520 width=16) (actual time=0.883..1386.856 rows=422640 loops=1)
29,Workers Planned: 2
29,Workers Launched: 2
29,Parallel Seq Scan on trade
29,(cost=0.00..192283.11 rows=234800 width=16) (actual time=0.868..1301.220 rows=140880 loops=3)
29,Filter: pair_div_4_ps(id)
29,Rows Removed by Filter: 422640
29,Planning Time: 0.138 ms
29,Execution Time: 1405.412 ms
29,"As shown in the outputs of the explains, on the second EXPLAIN,"
29,Workers Launched
29,"tag has value 2, this means that this query used 2 workers to execute, and the first EXPLAIN was executed without parallelisms"
29,2: Use IMMUTABLE when it is possible
29,The IMMUTABLE option informs the query optimizer about the behavior of the function and can apply some optimization. Any call to the function with all-constant arguments can be immediately replaced with the function value. To mark a function as IMMUTABLE you need to comply with the following:
29,You cannot modify the database (state) and always returns the same result for the same argument values;
29,Do not search in the databases or use information that is not directly present in its argument list values.
29,Example:
29,1- CREATE OR REPLACE FUNCTION get_date (date) RETURNS int
29,AS $$
29,DECLARE
29,i int;
29,result int:= 0;
29,BEGIN
29,RETURN extract (day from $1);
29,END;
29,LANGUAGE plpgsql;
29,2- CREATE OR REPLACE FUNCTION get_date_i (date) RETURNS int
29,AS $$
29,DECLARE
29,i int;
29,result int:= 0;
29,BEGIN
29,RETURN extract (day from $1);
29,END;
29,LANGUAGE plpgsql IMMUTABLE;
29,1- SELECT 1
29,from trade where id=get_date(current_date);
29,Time: 2557.521 ms (00:02.558)
29,2- SELECT 1
29,from trade where id=get_date_i(current_date);;
29,Time: 2208.848 ms (00:02.209)
29,Monitoring
29,performance of functions
29,PostgreSQL allows the user to track the
29,"performance of functions in the database. For example, we can see the performance stats using the view pg_stat_user_functions, as long as you configure the parameter named track_functions, that allows tracking function call counts and time spent. To simplify the configuration we can leverage the option that gives us postgresqlcon.nf to share a configuration file,"
29,"download it and apply it to your server. Specifically, to track function performance, select the download format"
29,"alter_system,"
29,"apply the modification to your server,"
29,and reload the configuration using select pg_reload_conf(). This allows us to detect which functions are working as expected or are slow.
29,"For example, to use this view you can write a query like this:"
29,"select schemaname||'.'||funcname func_name, calls, total_time, round((total_time/calls)::numeric,2) as mean_time, self_time"
29,from pg_catalog.pg_stat_user_functions;
29,func_name
29,| calls | total_time | mean_time | self_time
29,---------------------------+-------+------------+-----------+-----------
29,public.f_plpgsql
29,2 |
29,93.908 |
29,46.95 |
29,93.908
29,public.auditoria_clientes |
29,2684 |
29,593.705 |
29,0.22 |
29,593.705
29,public.prc_clientes
29,2 |
29,1.447 |
29,0.72 |
29,0.387
29,public.max_pro_min
29,3 |
29,1.589 |
29,0.53 |
29,1.589
29,public.registro_ddl
29,17 |
29,39.217 |
29,2.31 |
29,39.217
29,public.registro_ddl_drop
29,2 |
29,422.386 |
29,211.19 |
29,422.386
29,calls: Number of times this function has been called
29,total_time: Time(ms) spent in this function and all other functions called by it inside their code
29,mean_time: AVG Time(ms) spent in this function and all other functions called by it inside their code
29,"self_time: Time(ms) spent in this function itself, without including other functions called by it"
29,Conclusions
29,"The tips and examples shown above have shown us that sometimes with minimum changes written in our code in PostgreSQL’s functions we can get some performance benefits. These tips are not exclusive, whenever possible these can be combined to achieve an improvement. e.g.: PARALLEL SAFE and avoid overusing the RAISE clause."
29,"If you know any other tips or examples please feel free to share them with us. Also, we can monitor our function’s performance by issuing a"
29,simple change in PostgreSQL’s configuration.
29,Comments
29,Please enable JavaScript to view the comments powered by Disqus.
29,comments powered by Disqus
29,More Posts
29,You may also likethis related content
29,63-Node EKS Cluster running on a Single Instance with Firecracker
29,Álvaro Hernández
29,"Jan 13, 2021 ·"
29,7 min read
29,63-Node EKS Cluster running on a Single Instance with Firecracker This blog post is a part of a series of posts devoted to Firecracker automation. Currently it consists of the following posts:
29,Read post
29,"Repository, Tuning Guide and API for your postgresql.conf"
29,Álvaro Hernández
29,"Dec 18, 2020 ·"
29,3 min read
29,"Repository, Tuning Guide and API for your postgresql.conf postgresqlco.nf (aka postgresqlCO.NF, or simply &ldquo;CONF&rdquo;) was born a little bit more than two years ago. CONF&rsquo;s main goal was to help Postgres users find more and easier help to understand and tune their postgresql."
29,Read post
29,Free 1GB Postgres Database on AWS CloudShell
29,Álvaro Hernández
29,"Dec 17, 2020 ·"
29,5 min read
29,"Free 1GB Postgres Database on AWS CloudShell TL;DR AWS CloudShell is a CLI embedded in the AWS Web Console. It is meant to make it easier to run the AWS CLI, SDK and other scripts from your web browser, without having to install anything locally or having to deal with local credential and profiles management."
29,Read post
29,About OnGres
29,"We like open source, we develop open source software, and we are very active and well known at Postgres community. We build very innovative projects in Postgres ecosystem and are the founders of Fundación PostgreSQL."
29,Contact us
29,We are currently working on more awesome stuff
29,Subscribe to our newsletter to be up to date!
29,-None-
29,Newsletter
29,Contact Form
29,Careers
29,StackGres
29,I accept the OnGres Privacy Policy and agree to receive news and promotions every now and then
29,Resources
29,Blog
29,Services
29,Professional Services
29,Consulting
29,Training
29,Products
29,StackGres
29,PostgreSQLCO.NF
29,Company
29,Team
29,Careers
29,Contact
29,//Language
29,English
29,© 2021 OnGres Inc.
29,Cookies Policy
29,Privacy Policy
29,"By continuing to browse the site, you agree to our use of cookies"
30,ITOM Practitioner Portal
31,"Distributed Systems Authority - Ideas behind Reliable, Scalable, and Maintenable Systems"
31,Skip to content
31,Distributed Systems Authority
31,"Ideas behind Reliable, Scalable, and Maintenable Systems"
31,Blog
31,Videos
31,Distributed Systems Authority
31,"Ideas behind Reliable, Scalable, and Maintenable Systems"
31,Toggle Navigation
31,Toggle Navigation
31,Blog
31,Videos
31,Search for...
31,"MySQL High Performance Part 3 – EXPLAIN Queriesby Lucian OpreaJanuary 7, 2021January 10, 2021Introduction Once we have a candidate query for optimization, we need to analyze why it is slow, or why it impacts the system soo much.… Read More »MySQL High Performance Part 3 – EXPLAIN Queries"
31,"MySQL High Performance Part 2 – Finding Candidates for Query Optimizationsby Lucian OpreaJanuary 7, 2021January 10, 2021Key Takeaways The Performance Schema is a gold mine for finding querying to optimize becuase it allows us to collecting the necessary data to make… Read More »MySQL High Performance Part 2 – Finding Candidates for Query Optimizations"
31,"MySQL Performance Tuning Part 1 – Architectureby Lucian OpreaJanuary 1, 2021January 10, 2021Key Takeaways We need to understand MySQL design so that we can work with it, and not against it. Everything in InnoDB is an index.… Read More »MySQL Performance Tuning Part 1 – Architecture"
31,"DCA – Describe and demonstrate how to deploy a service on a Docker overlay networkby Lucian OpreaJune 1, 2020July 21, 2020In this article, we’re going to cover 2 main subjects of the networking domain for the Docker Certified Associate DCA certification. Describe and demonstrate how… Read More »DCA – Describe and demonstrate how to deploy a service on a Docker overlay network"
31,"Solr 8 Facet Query – Hands-On Exampleby Lucian OpreaMay 2, 2020May 31, 2020Faceted search has become a critical feature for enhancing user search experience for all types of search applications. This article gives you an introduction to… Read More »Solr 8 Facet Query – Hands-On Example"
31,"Server Tuning Guideline – PostgreSQL 12 High Performance Guide (Part 10/12)by Lucian OpreaDecember 8, 2019December 8, 20192 CommentsKey takeaways The default values in the server configuration file have small memory settings. You should carefully modify primary memory configurations, shared_buffers and work_mem, in… Read More »Server Tuning Guideline – PostgreSQL 12 High Performance Guide (Part 10/12)"
31,"Query Optimizations Tips – PostgreSQL 12 High Performance Guide (Part 9/12)by Lucian OpreaNovember 28, 2019January 18, 2020Key Takeaways First question the semantic correctness of a statement before attacking the performance problem We should avoid SELECT *, ORDER BY and DISTINCT unless… Read More »Query Optimizations Tips – PostgreSQL 12 High Performance Guide (Part 9/12)"
31,"Spotting Query Problems – PostgreSQL 12 High Performance Guide (Part 8/12)by Lucian OpreaNovember 2, 2019November 2, 2019Key takeaways: Queries are executed as a series of nodes that each do a small task, such as fetching data aggregation or sorting. Sequential scans… Read More »Spotting Query Problems – PostgreSQL 12 High Performance Guide (Part 8/12)"
31,"Making Use of Statistics – PostgreSQL 12 High Performance Guide (Part 7/12)by Lucian OpreaOctober 24, 2019January 10, 2020Key takeaways: The database statistics are exposed using views. The fastest way to spot performance issues is by using the pg_stat_statements view Particularly valuable statistics… Read More »Making Use of Statistics – PostgreSQL 12 High Performance Guide (Part 7/12)"
31,"Index Optimization Techniques – PostgreSQL 12 High Performance Guide (Part 6/12)by Lucian OpreaOctober 19, 2019October 23, 2019Key takeaways Adding an index increases overhead every time you add or change rows in a table. Each index needs to satisfy enough queries to… Read More »Index Optimization Techniques – PostgreSQL 12 High Performance Guide (Part 6/12)"
31,Next »
31,PostgreSQL High Performance Tuning Guide Course
31,Use the following coupon
31,to get 66% percent off.
31,"""LEARNING202104"""
31,Check the Full Course
31,Search for:
31,Recent Posts
31,MySQL High Performance Part 3 – EXPLAIN Queries
31,MySQL High Performance Part 2 – Finding Candidates for Query Optimizations
31,MySQL Performance Tuning Part 1 – Architecture
31,DCA – Describe and demonstrate how to deploy a service on a Docker overlay network
31,Solr 8 Facet Query – Hands-On Example
31,Categories
31,Apache Solr
31,Docker Certified Associate
31,MySQL
31,PostgreSQL
31,Reactive Systems
31,Recent CommentsBriannoumb on Server Tuning Guideline – PostgreSQL 12 High Performance Guide (Part 10/12)Briannoumb on Server Tuning Guideline – PostgreSQL 12 High Performance Guide (Part 10/12)
34,Range join optimization | Databricks on AWS
34,Support
34,Feedback
34,Try Databricks
34,Help Center
34,Documentation
34,Knowledge Base
34,Amazon Web Services
34,Microsoft Azure
34,Google Cloud Platform
34,Documentation for Databricks on AWS
34,Getting started with Databricks
34,Databricks SQL Analytics guide
34,Databricks Workspace guide
34,Get started with Databricks Workspace
34,Language roadmaps
34,User guide
34,Data guide
34,Delta Lake and Delta Engine guide
34,Introduction
34,Delta Lake quickstart
34,Introductory notebooks
34,Ingest data into Delta Lake
34,Table batch reads and writes
34,Table streaming reads and writes
34,"Table deletes, updates, and merges"
34,Table utility commands
34,Constraints
34,Table versioning
34,Delta Lake API reference
34,Concurrency control
34,Integrations
34,Migration guide
34,Best practices
34,Frequently asked questions (FAQ)
34,Delta Lake resources
34,Delta Engine
34,Optimize performance with file management
34,Auto Optimize
34,Optimize performance with caching
34,Dynamic file pruning
34,Isolation levels
34,Bloom filter indexes
34,Optimize join performance
34,Range join optimization
34,Skew join optimization
34,Optimized data transformation
34,Machine learning and deep learning guide
34,MLflow guide
34,Genomics guide
34,Administration guide
34,API reference
34,Release notes
34,Resources
34,"Updated Apr 09, 2021"
34,Send us feedback
34,Documentation
34,Databricks Workspace guide
34,Delta Lake and Delta Engine guide
34,Delta Engine
34,Optimize join performance
34,Range join optimization
34,Range join optimization
34,A range join occurs when two relations are joined using a point in interval or interval overlap condition.
34,"The range join optimization support in Databricks Runtime can bring orders of magnitude improvement in query performance, but requires careful manual tuning."
34,Point in interval range join
34,A point in interval range join is a join in which the condition contains predicates specifying that a value from one relation is between two values from the other relation. For example:
34,-- using BETWEEN expressions
34,SELECT *
34,FROM points JOIN ranges ON points.p BETWEEN ranges.start and ranges.end;
34,-- using inequality expressions
34,SELECT *
34,FROM points JOIN ranges ON points.p >= ranges.start AND points.p < ranges.end;
34,-- with fixed length interval
34,SELECT *
34,FROM points JOIN ranges ON points.p >= ranges.start AND points.p < ranges.start + 100;
34,-- join two sets of point values within a fixed distance from each other
34,SELECT *
34,FROM points1 p1 JOIN points2 p2 ON p1.p >= p2.p - 10 AND p1.p <= p2.p + 10;
34,-- a range condition together with other join conditions
34,SELECT *
34,"FROM points, ranges"
34,WHERE points.symbol = ranges.symbol
34,AND points.p >= ranges.start
34,AND points.p < ranges.end;
34,Interval overlap range join
34,An interval overlap range join is a join in which the condition contains predicates specifying an overlap of intervals between two values from each relation. For example:
34,"-- overlap of [r1.start, r1.end] with [r2.start, r2.end]"
34,SELECT *
34,FROM r1 JOIN r2 ON r1.start < r2.end AND r2.start < r1.end;
34,-- overlap of fixed length intervals
34,SELECT *
34,FROM r1 JOIN r2 ON r1.start < r2.start + 100 AND r2.start < r1.start + 100;
34,-- a range condition together with other join conditions
34,SELECT *
34,FROM r1 JOIN r2 ON r1.symbol = r2.symbol
34,AND r1.start <= r2.end
34,AND r1.end >= r2.start;
34,Range join optimization
34,The range join optimization is performed for joins that:
34,Have a condition that can be interpreted as a point in interval or interval overlap range join.
34,"All values involved in the range join condition are of a numeric type (integral, floating point, decimal), DATE, or TIMESTAMP."
34,"All values involved in the range join condition are of the same type. In the case of the decimal type, the values also need to be of the same scale and precision."
34,"It is an INNER JOIN, or in case of point in interval range join, a LEFT OUTER JOIN with point value on the left side, or RIGHT OUTER JOIN with point value on the right side."
34,Have a bin size tuning parameter.
34,Bin size
34,"The bin size is a numeric tuning parameter that splits the values domain of the range condition into multiple bins of equal size. For example, with a bin size of 10, the optimization splits the domain into bins that are intervals of length 10."
34,"If you have a point in range condition of p BETWEEN start AND end, and start is 8 and end is 22, this value interval overlaps with three bins of length 10 – the first bin from 0 to 10, the second bin from 10 to 20, and the third bin from 20 to 30. Only the points that fall within the same three bins need to be considered as possible join matches for that interval. For example, if p is 32, it can be ruled out as falling between start of 8 and end of 22, because it falls in the bin from 30 to 40."
34,Note
34,"For DATE values, the value of the bin size is interpreted as days. For example, a bin size value of 7 represents a week."
34,"For TIMESTAMP values, the value of the bin size is interpreted as seconds. If a sub-second value is required, fractional values can be used. For example, a bin size value of 60 represents a minute, and a bin size value of 0.1 represents 100 milliseconds."
34,You can specify the bin size either by using a range join hint in the query or by setting a session configuration parameter.
34,The range join optimization is applied only if you manually specify the bin size. Section Choose the bin size describes how to choose an optimal bin size.
34,Enable range join using a range join hint
34,"To enable the range join optimization in a SQL query, you can use a range join hint to specify the bin size."
34,The hint must contain the relation name of one of the joined relations and the numeric bin size parameter.
34,"The relation name can be a table, a view, or a subquery."
34,"SELECT /*+ RANGE_JOIN(points, 10) */ *"
34,FROM points JOIN ranges ON points.p >= ranges.start AND points.p < ranges.end;
34,"SELECT /*+ RANGE_JOIN(r1, 0.1) */ *"
34,"FROM (SELECT * FROM ranges WHERE ranges.amount < 100) r1, ranges r2"
34,WHERE r1.start < r2.start + 100 AND r2.start < r1.start + 100;
34,"SELECT /*+ RANGE_JOIN(c, 500) */ *"
34,FROM a
34,JOIN b ON (a.b_key = b.id)
34,JOIN c ON (a.ts BETWEEN c.start_time AND c.end_time)
34,Note
34,"In the third example, you must place the hint on c."
34,"This is because joins are left associative, so the query is interpreted as (a JOIN b) JOIN c,"
34,and the hint on a applies to the join of a with b and not the join with c.
34,"You can also place a range join hint on one of the joined DataFrames. In that case, the hint contains just the numeric bin size parameter."
34,"val df1 = spark.table(""ranges"").as(""left"")"
34,"val df2 = spark.table(""ranges"").as(""right"")"
34,"val joined = df1.hint(""range_join"", 10)"
34,".join(df2, $""left.type"" === $""right.type"" &&"
34,"$""left.end"" > $""right.start"" &&"
34,"$""left.start"" < $""right.end"")"
34,val joined2 = df1
34,".join(df2.hint(""range_join"", 0.5), $""left.type"" === $""right.type"" &&"
34,"$""left.end"" > $""right.start"" &&"
34,"$""left.start"" < $""right.end"")"
34,Enable range join using session configuration
34,"If you don’t want to modify the query, you can specify the bin size as a configuration parameter."
34,SET spark.databricks.optimizer.rangeJoin.binSize=5
34,"This configuration parameter applies to any join with a range condition. However, a different bin size set through a range join hint always overrides the one set through the parameter."
34,Choose the bin size
34,The effectiveness of the range join optimization depends on choosing the appropriate bin size.
34,"A small bin size results in a larger number of bins, which helps in filtering the potential matches."
34,"However, it becomes inefficient if the bin size is significantly smaller than the encountered value intervals, and the value intervals overlap multiple bin intervals. For example, with a condition p BETWEEN start AND end, where start is 1,000,000 and end is 1,999,999, and a bin size of 10, the value interval overlaps with 100,000 bins."
34,"If the length of the interval is fairly uniform and known, we recommend that you set the bin size to the typical expected length of the value interval. However, if the length of the interval is varying and skewed, a balance must be found to set a bin size that filters the short intervals efficiently, while preventing the long intervals from overlapping too many bins. Assuming a table ranges, with intervals that are between columns start and end, you can determine different percentiles of the skewed interval length value with the following query:"
34,"SELECT APPROX_PERCENTILE(CAST(end - start AS DOUBLE), ARRAY(0.5, 0.9, 0.99, 0.999, 0.9999)) FROM ranges"
34,"A recommended setting of bin size would be the maximum of the value at the 90th percentile, or the value at the 99th percentile divided by 10, or the value at the 99.9th percentile divided by 100 and so on. The rationale is:"
34,"If the value at the 90th percentile is the bin size, only 10% of the value interval lengths are longer than the bin interval, so span more than 2 adjacent bin intervals."
34,"If the value at the 99th percentile is the bin size, only 1% of the value interval lengths span more than 11 adjacent bin intervals."
34,"If the value at the 99.9th percentile is the bin size, only 0.1% of the value interval lengths span more than 101 adjacent bin intervals."
34,"The same can be repeated for the values at the 99.99th, the 99.999th percentile, and so on if needed."
34,The described method limits the amount of skewed long value intervals that overlap multiple bin intervals.
34,The bin size value obtained this way is only a starting point for fine tuning; actual results may depend on the specific workload.
34,"© Databricks 2021. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation."
34,Send us feedback
34,| Privacy Policy | Terms of Use
36,Optimizing PostgreSQL for gvmd - Greenbone Source Edition (GSE) - Greenbone Community Portal
36,Optimizing PostgreSQL for gvmd
36,Greenbone Source Edition (GSE)
36,tatooin
36,"September 14, 2020,"
36,2:26pm
36,Hello
36,"Is there any tweaks / hints to optimize / fine-tune PostgreSQL for gvmd ? My installation has plenty of CPU / Memory available; but still gsad is often very slow to answer queries, and very often it fails to receive answers from gvmd due to time out."
36,"Looking at the processes clearly show that postgresql daemons are taking most of the CPU power, but still this remains very low considering the global CPU power of the system. So clearly, it doesn’t seem optimized."
36,Any clue ?
36,Thanks
36,tux
36,"September 15, 2020,"
36,4:56am
36,Which version of PostgreSQL do you use?
36,Lukas
36,"September 15, 2020,"
36,8:46am
36,Are you sure that is a CPU or IO issue ?
36,What does “iotop” and the other process debug features say ?
36,Are you running on bare metal NVMEs ?
36,"Big installations seems to struggle with limited I/O on virtual systems, and should run on very fast NVMEs."
36,bricks
36,"September 15, 2020,"
36,2:30pm
36,It seems that requesting the results is very very slow with Postgres 12 compared to Postgres 11. Postgres 11 is of course recommended because that’s the version in Debian Stable which is our reference system.
36,tatooin
36,"September 16, 2020,"
36,6:42am
36,Hi Everyone and thanks for your interest in the subject.
36,A couple of informations:
36,System is bare metal with OS Linux Mint 19.3 64 bit (Ubuntu 18.04 LTS)
36,Installed version of PostgreSQL is 10.
36,94 Gb RAM and Intel® Xeon® CPU X5680
36,@ 3.33GHz
36,23 cores
36,iostat gives the following:
36,avg-cpu:
36,%user
36,%nice %system %iowait
36,%steal
36,%idle
36,"8,10"
36,"1,19"
36,"9,17"
36,"1,37"
36,"0,00"
36,"80,17"
36,Device
36,tps
36,kB_read/s
36,kB_wrtn/s
36,kB_read
36,kB_wrtn
36,sda
36,"99,82"
36,"11,23"
36,"968,80"
36,40585011 3500199664
36,sdb
36,"0,65"
36,"0,97"
36,"8,32"
36,3505669
36,30047876
36,sdc
36,"0,00"
36,"0,00"
36,"0,00"
36,5480
36,gvm@ov-master-eqi:~$ free -g
36,total
36,used
36,free
36,shared
36,buff/cache
36,available
36,Mem:
36,Swap:
36,"So as you can notice, CPU / Memory isn’t the bottleneck here."
36,Thanks
36,Lukas
36,"September 16, 2020,"
36,7:58am
36,bricks:
36,Postgres 11 is of course recommended
36,"As you use PSQL 10 and not 11 i would upgrade, and then trying to fine tune your database and looking into your system issues. Still you did not answer any questions regarding your storage."
36,"I don´t see any native NVMEs that is what we use for accelerate in our enterprise appliances. They are 10 times faster then spinning rust if you have a huge database, that would kill your"
36,performance due to seek times.
36,"As well your amount of swap is far to high, i would turn swap off with that amount of ram."
36,So classic debugging and performance optimization would be my next steps.
36,tatooin
36,"September 16, 2020,"
36,8:37am
36,"OK, I take the point of PSQL 11. Didn’t know this was the recommended version."
36,"Regarding the storage, sorry I missed it. I’m not using NVMEs drives; currently only ATA."
36,"If you guys believe this would really improve things, then I’ll see how to invest in that."
36,Last point is SWAP; perhaps I’m wrong but I always thought that:
36,disabling SWAP is a bad idea
36,"System will always use memory first, then swap if no more memory is available."
36,Isn’t that correct ?
36,tatooin
36,"September 23, 2020,"
36,5:14pm
36,"Does GVM-11 actually works with PostgreSQL-11, or were you only talking about GVM 20.08 ? On a pre-prod system I have updated my PSQL from 10 to 11, but then I get the following errors when starting gvmd:"
36,main:MESSAGE:2020-09-23 17h09.44 utc:9172:
36,Greenbone Vulnerability Manager version 9.0.0 (DB revision 221)
36,md manage:MESSAGE:2020-09-23 17h09.44 utc:9173: check_db_versions: database version of SCAP database: 16
36,md manage:MESSAGE:2020-09-23 17h09.44 utc:9173: check_db_versions: SCAP database version supported by manager: 15
36,main:CRITICAL:2020-09-23 17h09.44 utc:9173: gvmd: database is wrong version
36,"I don’t really understand this error, as I don’t see how my SCAP database version could be newer than the one supported by the manager, since I havn’t actually changed the SCAP data during the PSQL upgrade."
36,bricks
36,"September 24, 2020,"
36,8:42am
36,For gvmd-20.08 and gvmd-21.04/master PostgreSQL 11 is our reference. The PostgreSQL 12 issue might get fixed but we can’t promise anything at the moment.
36,For other versions below gvmd-20.08 Debian Stretch is the base system and therefore PostgreSQL 9.6 is recommended.
36,"For your current issue with the SCAP db, please try to use gvmd-9.0.1 the latest bugfix release instead of 9.0.0. I can remember either a ticket a GitHub or a topic in this forum where this issue was mentioned to and updating to the bugfix release fixed it."
36,1 Like
36,tatooin
36,"September 24, 2020,"
36,8:45am
36,#10
36,Thanks bricks. That actually answer my question. So there is no point to try mixing gvmd-11 with PSQL-11 if the reference is 9.6.
36,"I’ll focus my effort on upgrading to 20.08 AND PSQL-11 at once. Hopefully, this should solve the huge performance issues I’m facing at the moment."
36,Thanks a lot
36,bricks
36,"September 24, 2020,"
36,8:57am
36,#11
36,Using GVM-11/gvmd-9 with PostgreSQL 11 should work. I’ve used that for a longer period on my personal development computer. But it isn’t tested and there is no guarantee because our corresponding products (GOS 5 and 6) are based on Debian Stretch and therefore PostgreSQL 9.6. We are developing
36,and testing for our products of course. If the community is facing issues with other systems varying from our reference system we sometimes can’t give any promises for fixes.
36,1 Like
36,tatooin
36,"September 25, 2020,"
36,7:52am
36,#12
36,"Using GVM-11/gvmd-9 with PostgreSQL 11 does work, actually. It just that in our production environment, it is pain slow. Perhaps it has nothing to do with the version of PSQL. It’s possible gvmd is just not designed to handle such big database (57GB), that’s why I want to explore all options at this point."
36,Thanks
36,Lukas
36,"September 25, 2020,"
36,9:28pm
36,#13
36,I have seen DBs
36,with more then 10 /8 networks and 16 millions of results.
36,"But they are never at this size, do you store all “log” and “scan-meta” data there ?"
36,How often do you do DB maintenance ?
36,It looks like a non optimal scan configuration and/or missing db-maintenance.
36,Plus our enterprise appliances designed for that type of DBs have a block-acceleration based on special hardware so speed up the DB in hardware.
36,tatooin
36,"September 26, 2020,"
36,8:01am
36,#14
36,I have not tweaked any openvas configuration; as the default values looks good enough:
36,"non_simult_ports = 139, 445, 3389, Services/irc"
36,vendor_version =
36,safe_checks = yes
36,nasl_no_signature_check = yes
36,time_between_request = 0
36,expand_vhosts = yes
36,max_checks = 10
36,optimize_test = yes
36,report_host_details = yes
36,config_file = /opt/gvm/etc/openvas/openvas.conf
36,unscanned_closed_udp = yes
36,include_folders = /opt/gvm/var/lib/openvas/plugins
36,test_empty_vhost = no
36,plugins_timeout = 320
36,cgi_path = /cgi-bin:/scripts
36,checks_read_timeout = 5
36,unscanned_closed = yes
36,auto_enable_dependencies = yes
36,log_whole_attack = no
36,db_address = /var/run/redis/redis.sock
36,drop_privileges = no
36,log_plugins_name_at_load = no
36,scanner_plugins_timeout = 36000
36,timeout_retry = 3
36,max_hosts = 30
36,network_scan = no
36,open_sock_max_attempts = 5
36,plugins_folder = /opt/gvm/var/lib/openvas/plugins
36,Regarding the DB maintenance; I’m doing the following weekly:
36,Freeing some unused storage space in the database
36,/opt/gvm/sbin/gvmd --optimize=vacuum
36,Optimize DB queries
36,/opt/gvm/sbin/gvmd --optimize=analyze
36,"Cleans up references to report formats that have been removed without using the DELETE_REPORT_FORMAT GMP command, for example after a built-in report format has been removed."
36,/opt/gvm/sbin/gvmd --optimize=cleanup-report-formats
36,This cleans up results with missing result_nvt entries which can result in filters and overrides not working properly.
36,/opt/gvm/sbin/gvmd --optimize=cleanup-result-nvts
36,This option removes duplicate preferences from Scan Configs and corrects some broken preference values
36,/opt/gvm/sbin/gvmd --optimize=cleanup-config-prefs
36,This cleans up results with no severity by assigning the default severity set by the user owning the result.
36,/opt/gvm/sbin/gvmd --optimize=cleanup-result-severities
36,This creates the cache containing the unfiltered result counts of all reports that are not cached yet.
36,/opt/gvm/sbin/gvmd --optimize=update-report-cache
36,"I havn’t found any other information to optimize SQL database for GVM usage, so if you have other informations I miss, please share."
36,However you are right on one point; we store log information as well as low/medium/high…
36,Thanks
36,Lukas
36,"September 26, 2020,"
36,8:07am
36,#15
36,tatooin:
36,Freeing some unused storage space in the database
36,/opt/gvm/sbin/gvmd --optimize=vacuum
36,Optimize DB queries
36,/opt/gvm/sbin/gvmd --optimize=analyze
36,"That is not enough with that DB size, as well your default config is not intended for that extra big"
36,enterprise environment. I would run offline optimizations first.
36,I would go with multiple enterprise appliances with that big environment. Otherwise you have to re-invent the wheel based on our OpenSource.
36,tatooin
36,"September 26, 2020,"
36,8:15am
36,#16
36,What do you call offline optimization ? Please share informations if you have it. Also you mention log data; is there a way to stop logging it ? I havn’t found any documentation on this; it only apply to reports.
36,"The appliance is unfortunately not an option in my context; due to prohibitive restrictions to install hardwares in our DCs. And in all case, I understand my DB isn’t optimized since you said you’ve seen DBs with more than 10 /8 networks never reaching that size."
36,"Currently I have no other option but to optimize to the maximum what I have. Again, help is welcome here if you have advices."
36,Thanks
36,Home
36,Categories
36,FAQ/Guidelines
36,Terms of Service
36,Privacy Policy
36,"Powered by Discourse, best viewed with JavaScript enabled"
37,How do I tune Artifactory for heavy loads?
37,Products
37,Solutions
37,Resources
37,Services
37,Pricing
37,Start For FreeProducts
37,JFrog Platform
37,JFrog Artifactory
37,JFrog Xray
37,JFrog Pipelines
37,JFrog Distribution
37,JFrog Mission Control
37,JFrog Container Registry
37,Solutions
37,JFrog for Banking and Financial Services
37,JFrog for The Automotive Industry
37,JFrog for HealthCare
37,JFrog for the Technology and Software Industries
37,Artifact Management
37,JFrog for Security and Compliance
37,JFrog for Continuous Integration and Continuous Delivery (CI/CD)
37,Resources
37,Resource Center
37,Blog
37,User Guides
37,DevOps Tools
37,Integration
37,Academy
37,Customer Zone
37,Knowledge base
37,Upcoming Webinars
37,Services
37,Support
37,Ticket Portal
37,Consulting
37,MyJFrog Customer Portal
37,Certification
37,Pricing
37,Industry
37,Financial Services
37,End-to-End DevOps for Banking and Financial Software Development
37,Learn More
37,Automotive Industry
37,Scalable DevOps for Automotive Companies and OEMs
37,Learn More
37,Healthcare Services
37,Trusted Software Releases for Healthcare Companies
37,Learn More
37,Technology & Software
37,DevOps Automation for Technology and Software Companies
37,Learn More
37,Use Case
37,Artifact Management
37,Scalable DevOps for Software Artifact Management
37,Learn More
37,Security & Compliance
37,DevOps Automation for Security and Compliance Management
37,Learn More
37,CI/CD
37,Software Development Pipeline Automation and Management
37,Learn More
37,ProfessionalServices
37,Consulting
37,Leaping to Enterprise DevOps
37,See More
37,Certification
37,Become a JFrog Artifactory Certified DevOps Engineer
37,See More
37,Support
37,Get Support
37,24/7 R&D Level Support
37,See More
37,Ticket Portal
37,Existing customers? Get direct help from our team
37,Log In
37,Account
37,MyJFrog - Customer Portal
37,Manage your Cloud subscriptions
37,Log In
37,Resources
37,Resource Center
37,"Webinars, articles, white papers, screencasts, use cases,"
37,and more
37,See More
37,User Guides
37,Technical documentation about JFrog products
37,See More
37,DevOps Tools
37,Accelerating software releases
37,Read More
37,Blog
37,"The latest DevOps trends, news on JFrog products, launches and announcements"
37,Read Now
37,JFrog Academy
37,"Self-paced, free training"
37,for JFrog solutions
37,See More
37,Knowledge Base
37,Comprehensive self-service portal
37,See More
37,Upcoming Webinars
37,Join our leading tech experts
37,to enrich your knowledge
37,See More
37,Integrations
37,All of the technologies that integrate with JFrog
37,Read Now
37,Customer Zone
37,All the resources you need to manage and troubleshoot your JFrog products
37,See More
37,Platform
37,The JFrog Platform
37,End-to-end Software Management and Releases
37,Learn More
37,Products
37,JFrog Artifactory
37,Enterprise Universal Artifact Repository
37,Learn More
37,JFrog Pipelines
37,Universal CI/CD DevOps Pipeline for the enterprise
37,Learn More
37,JFrog Mission Control
37,Centralized Global Artifact Management
37,Learn More
37,JFrog Xray
37,Container Security and Universal
37,Artifact Analysis
37,Learn More
37,JFrog Distribution
37,For Trusted Software Releases
37,Learn More
37,JFrog Container Registry
37,"Powerful, Hybrid Docker and Helm Registry."
37,Learn More
37,Find Other Useful Articles:
37,Product
37,All
37,General
37,Artifactory
37,Bintray
37,Mission Control
37,Xray
37,Enterprise Plus
37,Distribution
37,Access
37,JFrog Pipelines
37,Category
37,All
37,Filter
37,Search
37,How do I tune Artifactory for heavy loads?
37,Ariel Kabov 2020-09-03 07:19
37,Relevant Versions: Artifactory 7 and above.A tuning guide for previous versions is available here.
37,"Artifactory comes with a predefined set of default configurations and parameters. The default Artifactory should handle up to ~200 concurrent connections well.If you believe your Artifactory server is under-utilized, or in order to allow it to handle more processes at a given moment, it is possible to tune Artifactory to support a higher load.While it is always possible to scale horizontally by adding additional nodes to your HA cluster, here we will focus on a more vertical scale."
37,"Recommendation: The more crucial Artifactory becomes in your organization, the more crucial will be to have a monitoring system to look over Artifactory.You may read further at Monitoring and Optimizing Artifactory Performance."
37,JVM Memory
37,"By default, Artifactory comes with a predefined JVM memory limit. To modify the JVM memory allocation, please refer to the Product Configuration section that is part of the installation guide. Be advised to follow our hardware recommendations.When increasing the JVM memory allocation, make sure you leave at least 30% of the total RAM to the OS and other services."
37,Database connections
37,We can alter the maximum connections an Artifactory node can open to the DB by modifying the Artifactory System YAML.
37,Default values:artifactory:  database:    maxOpenConnections: 100...    access:  database:    maxOpenConnections: 100...metadata:  database:    maxOpenConnections: 100
37,Tuning example:artifactory:  database:    maxOpenConnections: 300...    access:  database:    maxOpenConnections: 300...metadata:  database:    maxOpenConnections: 300
37,"Important: The Artifactory maxOpenConnections parameter is being used also by the Artifactory Session Management mechanism.This means once the above example is used, the Artifactory node will open up to 1200 DB connections. Therefore we need to make sure the DB can accommodate the total number of connections all Artifactory nodes can open.As a rule of thumb we will require from the DB a number of connections based on:Total # of connections = (number of nodes) * ((artifactory.database.maxOpenConnections * 2) + access.database.maxOpenConnections + metadata.database.maxOpenConnections) + 50;"
37,*The extra 50 connections are to provide extra breathing room in situations where all DB connection pools are exhausted.
37,Tomcat HTTP Connections / Threads
37,"Artifactory runs on top of Apache Tomcat, which manages the incoming HTTP connection pools.This sets the number of concurrent HTTP connections Artifactory can serve.We can override the default thread pool limit by modifying the Artifactory System YAML."
37,Default values:artifactory:  tomcat:    connector:      maxThreads: 200...access:  tomcat:    connector:      maxThreads: 50
37,Tuning example:artifactory:  tomcat:    connector:      maxThreads: 600...access:  tomcat:    connector:      maxThreads: 150
37,"Important: When modifying the Access maxThreads, it is required to update the $JFROG_HOME/artifactory/var/etc/artifactory/artifactory.system.properties file with:artifactory.access.client.max.connections = <VALUE>This is to modify the internal HTTP connection pool Artifactory uses to internally interact with Access."
37,Artifactory async Thread Pool
37,"One of the most important thread pools in Artifactory is the “async” thread pool. This one defines the number of processes that can run in parallel.In addition to configuring the total number of parallel processes, we can also modify the maximum number of processes that can be queued.This is configured in $JFROG_HOME/artifactory/var/etc/artifactory/artifactory.system.properties."
37,Default values:(this means the machines CPU cores times 4)artifactory.async.corePoolSize = (4 * Runtime.getRuntime().availableProcessors()) artifactory.async.poolMaxQueueSize = 10000
37,Tuning example:(Shouldn’t be more than 8x the machine CPU cores)artifactory.async.corePoolSize = 128artifactory.async.poolMaxQueueSize = 100000
37,Garbage Collection
37,"By default, the Artifactory Garbage Collection is configured to run every 4 hours.The GC is a very resource-consuming operation, and if you see correlations between the running period of the GC to slow performance, we would recommend you to alter the Artifactory GC (not related JVM GC) to run at non-peak hours."
37,HTTP Client
37,"Artifactory manages a separate connection pool for outgoing HTTP requests per remote repository.This connection pool is limited by default to 50 concurrent connections, and up to 50 concurrent connections per unique route."
37,This is configured in $JFROG_HOME/artifactory/var/etc/artifactory/artifactory.system.properties.
37,Default values:artifactory.http.client.max.total.connections = 50artifactory.http.client.max.connections.per.route = 50
37,Tuning example:artifactory.http.client.max.total.connections = 150artifactory.http.client.max.connections.per.route = 120
37,Bypassing the Router
37,"The Artifactory 7 System Architecture provides us a flexible way to modify the flow a request will be processed by the Artifactory services.You can benefit from an improved performance by bypassing the Router service for API requests to Artifactory.This can be achieved using a Reverse Proxy such as NGINX or Apache HTTPD.By having a reverse proxy to redirect requests from $JFROG_URL/artifactory directly to $ARTIFACTORY_NODE:8081/artifactory, you will bypass the Router service.When under high load this can help to better distribute the requests in advance."
37,Filestore Configurations
37,"Artifactory supports different backend storage configurations to store the Artifactory filestore.For scenarios where the configured storage is not local, a performance benefit can be using a large Cache-FS provider mounted locally for each node.Cached files will be served quickly, and therefore having a large Cache FS provider will be a performance gain."
37,"Some filestore providers allow tuning and modifications of parameters. For instance: Eventual: ""numberOfThreads"".Eventual-Cluster: ""maxWorkers"".Remote: ""maxConnections""."
37,Read more at the Best Practices for Managing Your Artifactory Filestore white paper.
37,SHARE:
37,Release Fast Or Die
37,ProductsArtifactory
37,Xray
37,Pipelines
37,Distribution
37,Container Registry
37,JFrog Platform
37,ResourcesBlog
37,Events
37,User Guide
37,DevOps Tools
37,Open Source
37,Featured
37,CompanyAbout
37,Management
37,Investor Relations
37,Partners
37,Customers
37,Careers
37,Press
37,Contact Us
37,Brand Guidelines
37,CommunitySolutions
37,Foundations
37,Programs
37,Community Forum
37,© 2021 JFrog Ltd All Rights Reserved
37,Terms of Use
37,Cookies Policy
37,Privacy Policy
37,Accessibility Mode
37,Success
37,Your action was successful
37,Continue
37,Success
37,Your action was successful
37,Get Started
37,Oops...
37,Something went wrong
37,Please try again later
37,Continue
37,Information
37,Modal Message
37,Continue
37,Click Here
37,请点这里
38,Progress® DataDirect®
38,<![endif]-->
38,Progress DataDirect Connect Series for ODBC: Version 7.1.6
38,Panel Progress
38,Table of Contents
38,Index
38,odbc
39,Performance Tuning in Athena - Amazon Athena
39,Performance Tuning in Athena - Amazon Athena
39,AWSDocumentationAmazon AthenaUser Guide
39,Physical LimitsQuery Optimization
39,TechniquesAdditional Resources
39,Performance Tuning in Athena
39,This topic provides general information and specific suggestions for improving the
39,performance of Athena when you have large amounts of data and experience memory usage
39,performance issues.
39,Physical Limits
39,"In general, Athena limits the runtime of each query to 30 minutes. Queries that run"
39,beyond this limit are automatically cancelled without charge. If a query runs out
39,"memory or a node crashes during processing, errors like the following can occur:"
39,INTERNAL_ERROR_QUERY_ENGINE
39,EXCEEDED_MEMORY_LIMIT: Query exceeded local memory limit
39,Query exhausted resources at this scale factor
39,Encountered too many errors talking to a worker node. The node may have crashed or be under too much load.
39,Query Optimization
39,Techniques
39,"For queries that require resources beyond existing limits, you can either optimize"
39,the
39,"query or restructure the data being queried. To optimize your queries, consider the"
39,suggestions in this section.
39,Data Size
39,File Formats
39,"Joins, Grouping, and"
39,Unions
39,Partitioning
39,Window Functions
39,Use More Efficient
39,Functions
39,Data Size
39,Avoid single large files – Single files are
39,"loaded into a single node for processing. If your file size is extremely large, try"
39,to break up the file into smaller files and use partitions to organize them.
39,Read a smaller amount of data at once –
39,Scanning a large amount of data at one time can slow down the query and increase
39,cost. Use partitions or filters to limit the files to be scanned.
39,Avoid having too many columns – The message
39,GENERIC_INTERNAL_ERROR:
39,io.airlift.bytecode.CompilationException can occur when Athena fails
39,to compile the query to bytecode. This exception is usually caused by having too
39,many columns in the query. Reduce the number of the columns in the query or create
39,subqueries and use a JOIN that retrieves a smaller amount of
39,data.
39,Avoid large query outputs – Because query
39,"results are written to Amazon S3 by a single Athena node, a large amount of output"
39,data
39,"can slow performance. To work around this, try using CTAS to create a new table with the"
39,result of the query or INSERT INTO to
39,append new results into an existing table.
39,Avoid CTAS queries with a large output –
39,"Because output data is written by a single node, CTAS queries can also use a large"
39,"amount of memory. If you are outputting large amount of data, try separating the"
39,task into smaller queries.
39,"If possible, avoid having a large number of small"
39,files – Amazon S3 has a limit of 5500
39,requests per second. Athena queries share the same limit. If you need to scan
39,"millions of small objects in a single query, your query can be easily throttled by"
39,"Amazon S3. To avoid excessive scanning, use AWS Glue ETL to periodically compact your"
39,files
39,"or partition the table and add partition key filters. For more information, see"
39,Reading Input Files in Larger Groups in the AWS Glue Developer Guide or
39,How can I
39,configure an AWS Glue ETL job to output larger files? in the AWS
39,Knowledge Center.
39,Avoid scanning an entire table – Use the
39,following techniques to avoid scanning entire tables:
39,"Limit the use of ""*"". Try not to select all columns unless"
39,necessary.
39,Avoid scanning the same table multiple times in the same query
39,Use filters to reduce the amount of data to be scanned.
39,"Whenever possible, add a LIMIT clause."
39,Avoid referring to many views and tables in a single
39,query – Because queries with many views and/or tables must load
39,"a large amount of data to a single node, out of memory errors can occur. If"
39,"possible, avoid referring to an excessive number of views or tables in a single"
39,query.
39,Avoid large JSON strings – If data is stored
39,"in a single JSON string and the size of the JSON data is large, out of memory errors"
39,can occur when the JSON data is processed.
39,File Formats
39,Use an efficient file format such as Parquet or ORC
39,"– To dramatically reduce query running time and costs, use compressed Parquet"
39,or ORC files to store your data. To convert your existing dataset to those formats
39,"in Athena, you can use CTAS. For more information, see Using CTAS and INSERT INTO for ETL and Data"
39,Analysis.
39,Switch between ORC and Parquet formats –
39,Experience shows that the same set of data can have significant differences in
39,processing time depending on whether it is stored in ORC or Parquet format. If you
39,"are experiencing performance issues, try a different format."
39,Hudi queries – Because Hudi queries bypass
39,"the native reader and split generator for files in parquet format, they can be slow."
39,Keep this in mind when querying Hudi datasets.
39,"Joins, Grouping, and"
39,Unions
39,Reduce the usage of memory intensive operations
39,"– Operations like JOIN, GROUP BY, ORDER"
39,"BY, and UNION all require loading large amount of data into"
39,"memory. To speed up your query, find other ways to achieve the same results, or add"
39,a clause like LIMIT to the outer query whenever possible.
39,Consider using UNION ALL – To eliminate
39,"duplicates, UNION builds a hash table, which consumes memory. If your"
39,"query does not require the elimination of duplicates, consider using UNION"
39,ALL for better performance.
39,Use CTAS as an intermediary step to speed up JOIN
39,operations – Instead of loading and processing intermediary data
39,"with every query, use CTAS to persist the intermediary data into Amazon S3. This can"
39,help
39,speed up the performance of operations like JOIN.
39,Partitioning
39,Limit the number of partitions in a table –
39,"When a table has more than 100,000 partitions, queries can be slow because of the"
39,large number of requests sent to AWS Glue to retrieve partition information. To resolve
39,"this issue, try one of the following options:"
39,Use ALTER TABLE DROP PARTITION to remove stale
39,partitions.
39,"If your partition pattern is predictable, use partition projection."
39,Remove old partitions even if they are empty
39,"– Even if a partition is empty, the metadata of the partition is still stored"
39,in AWS Glue. Loading these unneeded partitions can increase query runtimes. To remove
39,"the unneeded partitions, use ALTER TABLE DROP PARTITION."
39,Look up a single partition – When looking up
39,"a single partition, try to provide all partition values so that Athena can locate"
39,the
39,"partition with a single call to AWS Glue. Otherwise, Athena must retrieve all partitions"
39,and filter them. This can be costly and greatly increase the planning time for your
39,"query. If you have a predictable partition pattern, you can use partition"
39,projection to avoid the partition look up calls to AWS Glue.
39,Set reasonable partition projection properties
39,"– When using partition projection, Athena tries to"
39,"create a partition object for every partition name. Because of this, make sure that"
39,the table properties that you define do not create a near infinite amount of
39,possible partitions.
39,"To add new partitions frequently, use ALTER TABLE ADD"
39,PARTITION – If you use MSCK REPAIR TABLE
39,"to add new partitions frequently (for example, on a daily basis) and are"
39,"experiencing query timeouts, consider using ALTER TABLE ADD PARTITION."
39,MSCK REPAIR TABLE is best used when creating a table for the first
39,time or when there is uncertainty about parity between data and partition
39,metadata.
39,Avoid using coalesce()in a WHERE clause with partitioned
39,"columns – Under some circumstances, using the coalesce() or other functions in a WHERE clause against"
39,"partitioned columns might result in reduced performance. If this occurs, try"
39,rewriting your query to provide the same functionality without using
39,coalesce().
39,Window Functions
39,Minimize the use of window functions –
39,Window
39,functions such as rank()
39,"are memory intensive. In general, window functions require an entire dataset to be"
39,"loaded into a single Athena node for processing. With an extremely large dataset,"
39,"this can risk crashing the node. To avoid this, try the following options:"
39,Filter the data and run window functions on a subset of the data.
39,Use the PARTITION BY clause with the window function whenever
39,possible.
39,Find an alternative way to construct the query.
39,Use More Efficient
39,Functions
39,Replace row_number() OVER (...) as rnk ... WHERE rnk =
39,"1 – To speed up a query with a row_number() clause like this, replace this syntax with a combination"
39,"of GROUP BY, ORDER BY, and LIMIT 1."
39,Use regular expressions instead of LIKE on
39,large strings – Queries that include clauses such as LIKE
39,'%string%' on large strings can be very
39,costly. Consider using the regexp_like() function and a regular expression instead.
39,"Use max() instead of element_at(array_sort(), 1)"
39,"– For increased speed, replace the nested functions"
39,"element_at(array_sort(), 1) with max()."
39,Additional Resources
39,"For additional information on performance tuning in Athena, consider the following"
39,resources:
39,Read the AWS Big Data blog post Top 10
39,Performance Tuning Tips for Amazon Athena
39,Read other Athena
39,posts in the AWS Big Data Blog
39,Visit the Amazon Athena Forum
39,Consult the Athena
39,topics in the AWS Knowledge Center
39,"Contact AWS Support (in the AWS console, click Support,"
39,Support Center)
39,Javascript is disabled or is unavailable in your
39,browser.
39,"To use the AWS Documentation, Javascript must be"
39,enabled. Please refer to your browser's Help pages for instructions.
39,Document Conventions
39,Troubleshooting
39,"Code Samples, Service Quotas, and Previous JDBC Driver"
39,Did this page help you? - Yes
39,Thanks for letting us know we're doing a good
39,job!
39,"If you've got a moment, please tell us what we did right"
39,so we can do more of it.
39,Did this page help you? - No
39,Thanks for letting us know this page needs work. We're
39,sorry we let you down.
39,"If you've got a moment, please tell us how we can make"
39,the documentation better.
42,Improve database performance with connection pooling - Stack Overflow Blog
42,Reduce distractions and boost developer productivity with Stack Overflow for Teams. Now available for free
42,What is Teams?
42,"Essays, opinions, and advice on the act of computer programming from Stack Overflow."
42,Search for:
42,Latest
42,Newsletter
42,Podcast
42,Company
42,code-for-a-living
42,"October 14, 2020"
42,Improve database performance with connection pooling
42,"We tend to rely on caching solutions to improve database performance. Caching frequently-accessed queries in memory or via a database can optimize write/read performance and reduce network latency, especially for heavy-workload applications, such as gaming services and Q&A portals. But you can further improve performance by pooling users’ connections to a database. Client users need…"
42,Michael Aboagye
42,"We tend to rely on caching solutions to improve database performance. Caching frequently-accessed queries in memory or via a database can optimize write/read performance and reduce network latency, especially for heavy-workload applications, such as gaming services and Q&A portals. But you can further improve performance by pooling users’ connections to a database."
42,"Client users need to create a connection to a web service before they can perform CRUD operations. Most web services are backed by relational database servers such as Postgres or MySQL. With PostgreSQL, each new connection can take up to 1.3MB in memory. In a production environment where we expect to receive thousands or millions of concurrent connections to the backend service, this can quickly exceed your memory resources (or if you have a scalable cloud, it can get very expensive very quickly)."
42,"Because each time a client attempts to access a backend service, it requires OS resources to create, maintain, and close connections to the datastore. This creates a large amount of overhead causing database performance to deteriorate."
42,"Consumers of your service expect fast response times. If that performance deteriorates, it can lead to poor user experiences, revenue losses, and even unscheduled downtime. If you expose your backend service as an API, repeated slowdowns and failures could cause cascading problems and lose you customers."
42,"Instead of opening and closing connections for every request, connection pooling uses a cache of database connections that can be reused when future requests to the database are required. It lets your database scale effectively as the data stored there and the number of clients accessing it grow. Traffic is never constant, so pooling can better manage traffic peaks without causing outages. Your production database shouldn’t be your bottleneck."
42,"In this article, we will explore how we can use connection pooling middleware like pgpool and pgbouncer to reduce overhead and network latency. For illustration purposes, I will use pgpool-II and pgbouncer to explain concepts of connection pooling and compare which one is more effective in pooling connections because some connection poolers can even affect database performance."
42,We will look at how to use pgbench to benchmark Postgres databases since it is the standard tool provided by PostgreSQL.
42,"Different hardware provides different benchmarking results based on the plan you set. For the  tests below, I’m using these specifications."
42,Specs of my test machine:
42,Linode Server: Ubuntu 16 – 64 bit ( Virtual Machine)  Postgres version 9.5Memory: 2GBDatabase size: 800MBStorage: 2GB
42,Also it is important to isolate the Postgres database server from other frameworks like logstash shipper and other servers for collecting performance metrics because most of these components consume more memory and will affect the test results.
42,Creating a pooled connection
42,"Connecting to a backend service is an expensive operation, as it consists of the following steps:"
42,Open a connection to the database using the database driver.Open a TCP socket for CRUD operations Perform CRUD operations over the socket.  Close the connection.Close the socket.
42,"In a production environment where we expect thousands of concurrent open and close connections from clients, doing the above steps for every single connection can cause the database to perform poorly."
42,"We can resolve this problem by pooling connections from clients. Instead of creating a new connection with every request, connection poolers reuse some existing connections. Thus there is no need to perform multiple expensive full database trips by opening and closing connections to backend service. It prevents the overhead of creating a new connection to the database every time there is a request for a database connection with the same properties (i.e name, database, protocol version)."
42,"Pooling middleware like pgbouncer comes with a pool manager. Usually, the connection pool manager maintains a pool of open database connections. You can not pool connections without a pool manager."
42,A pool contains two types of connections:
42,Active connection: In use by the application.Idle connection:  Available for use by the application.
42,"When a new request to access data from the backend service comes in, the pool manager checks if the pool contains any unused connection and returns one if available. If all the connections in the pool are active, then a new connection is created and added to the pool by the pool manager. When the pool reaches its maximum size, all new connections are queued until a connection in the pool becomes available."
42,"Although most databases do not have an in-built connection pooling system, there are middleware solutions that we can use to pool connections from clients."
42,"For a PostgreSQL database server, both pgbouncer and pgpool-II can serve as a pooling interface between a web service and a Postgres database. Both utilities use the same logic to pool connections from clients."
42,"pgpool-II offers more features beyond connection pooling, such as replication, load balancing, and parallel query features."
42,How do you add connection pooling? Is it as simple as installing the utilities?
42,Two ways to integrate a connection pooler
42,There are two ways of implementing connection pooling for PostgreSQL application:
42,As an external service or middleware such as pgbouncer
42,Connection poolers such as pgbouncer and pgpool-II can be used to pool connections from clients to a PostgreSQL database. The connection pooler sits in between the application and the database server. Pgbouncer or pgpool-II can be configured in a way to relay requests from the application to the database server.
42,Client-side libraries such as c3p0
42,There exist libraries such as c3p0 which extend database driver functionality to include connection pooling support.
42,"However, the best way to implement connection pooling for applications is to make use of an external service or middleware since it is easier to set up and manage. In addition external middleware like pgpool2 provides other features such as load balancing apart from pooling connections."
42,"Now let’s take a deeper look at what happens when a backend service connects to a Postgres database, both with and without pooling."
42,Scaling database performance without connection pooling
42,"We do not need a connection pooler to connect to a backend service. We can connect to a Postgres database directly. To examine how long it takes to execute concurrent connections to a database without a connection pooler, we will use pgbench to benchmark connections to the Postgres database."
42,"Pgbench is based on TPC-B. TPC-B measures throughput in terms of how many transactions per second a system can perform. Pgbench executes five SELECT, INSERT, and UPDATE commands per transaction."
42,"Based on TPC-B-like transactions, pgbench runs the same sequence of SQL commands repeatedly in multiple concurrent database sessions and calculates the average transaction rate."
42,"Before we run pgbench, we need to initialize it with the following command to create the pgbench_history, pgbench_branches, pgbench_tellers, and pgbench_accounts tables. Pgbench uses the following tables to run transactions for benchmarking."
42,pgbench  -i  -s 50  database_name
42,"Afterward, I executed the command below to test the database with 150 clients"
42,pgbench  -c 10  -j 2  -t  10000  database_name
42,"As you see, in our initial baseline test, I instructed pgbench to execute with ten different client sessions. Each client session will execute 10,000 transactions."
42,"From these results, it seems our initial baseline test is 486 transactions per second."
42,"Let’s see how we can make use of connection poolers like pgbouncer and pgpool to increase transaction throughput and avoid a ‘Sorry!, too many clients already’ error."
42,Scaling database performance with pgbouncer
42,Let’s look at how we can use pgbouncer to increase transaction throughput.
42,"Pgbouncer can be installed on almost all Linux distributions. You can check here how to set up pgbouncer. Alternatively, you can install pgbouncer using package managers like apt-get or yum."
42,"If you find it difficult to authenticate clients with pgbouncer, you can check GitHub on how to do so."
42,Pgbouncer comes with three types of pooling:
42,"Session pooling: One of the connections in the pool is assigned to a client until the timeout is reached.  Transaction pooling: Similar to session polling, it gets a connection from the pool. It keeps it until the transaction is done. If the same client wants to run another transaction, it has to wait until it gets another transaction assigned to it. Statement pooling: Connection is returned to the pool as soon as the first query is completed."
42,"We will make use of the transaction pooling mode. Inside the pgbouncer.ini file, I modified the following parameter:"
42,max_client_conn = 100
42,The max_client_conn parameter defines how many client connections to pgbouncer (instead of Postgres) are allowed.
42,default_pool_size = 25
42,The default_pool_size parameter defines how many server connections to allow per user/database pair.
42,reserve_pool_size = 5
42,The reserve_pool_size parameter defines how many additional connections are allowed to the pool.
42,As in the previous test I executed pgbench with ten different client sessions. Each client executes 1000 transactions as shown below.
42,pgbench  -c 10  -p -j 2  -t 1000 database_name
42,"As you see, transaction throughput increased from 486 transactions per second to 566 transactions per second. With the help of pgbouncer, transaction throughput improved by approximately 60%."
42,Now let’s see how we can increase transaction throughput with pgpool-II since it comes with connection pooling features.
42,"Unlike pgbouncer, pgpool-II offers features beyond connection pooling. The documentation provides detailed information about pgpool-II features and how to set it up from source or via a package manager"
42,I changed the following parameters in the pgpool.conf file to make it route clients connections from pgpool2 to Postgres database server.
42,connection_cache  = on
42,listen_addresses  = ‘postgres_database_name’’
42,port  = 5432
42,Setting the connection_cache parameter to on activates pgpool2 pooling capability.
42,"Like the previous test, pgbench executed ten different client sessions. Each client executes 1000 transactions to the Postgres database server. Thus we expect a total of 10,000 transactions from all clients."
42,gbench  -p 9999  -c  10  -C  -t 1000  postgres_database
42,"In the same way we increased transaction throughput with pgbouncer, it seems pgpool2 also increased transaction throughput by 75% as compared to the initial test."
42,Pgbouncer implements connection pooling ‘out of the box’ without the need to fine-tune parameters while pgpool2 allows you to fine-tune parameters to enhance connection pooling.
42,Choosing a connection pooler: pgpool-II or pgbouncer?
42,"There are several factors to consider when choosing a connection pooler to use. Although pgbouncer and pgpool-II are great solutions for connection pooling, each tool has its strengths and weaknesses."
42,Memory/resource consumption
42,"If you are interested in a lightweight connection pooler for your backend service, then pgbouncer is the right tool for you. Unlike pgpool-II, which by default allows 32 child processes to be forked, pgbouncer uses only one process. Thus pgbouncer consumes less memory than pgpool2."
42,Streaming Replication
42,"Apart from pooling connections, you can also manage your Postgres cluster with streaming replication using pgpool-II.  Streaming replication copies data from a primary node to a secondary node. Pgpool-II supports Postgres streaming replication, while pgbouncer does not. It is the best way to achieve high availability and prevent data loss."
42,Centralized password management
42,"In a production environment where you expect many clients/applications to connect to the database through a connection pooler concurrently, it is necessary to use a centralized password management system to manage clients’ credentials."
42,You can make use of auth_query in pgbouncer to load clients’ credentials from the database instead of storing clients’ credentials in a userlist.txt file and comparing credentials from the connection string against the userlist.txt file.
42,Load balancing and high availability
42,"Finally, if you want to add load balancing and high availability to your pooled connections, then pgpool2 is the right tool to use. pgpool2 supports Postgres high availability through the in-built watchdog processes. This pgpool2 sub-process monitors the health of pgpool2 nodes participating in the watchdog cluster as well as coordinating between multiple pgpool2 nodes."
42,Conclusion
42,"Database performance can be improved beyond connection pooling. Replication, load balancing, and in-memory caching can contribute to efficient database performance."
42,"If a web service is designed to make a lot of read and write queries to a database, then you have multiple instances of a Postgres database in place to take care of write queries from clients through a load balancer such as pgpool-II while in-memory caching can be used to optimize read queries."
42,"Despite the pgpool-II ability to function as a loader balancer and connection pooler, pgbouncer is the preferred middleware solution for connection pooling because it is easy to set up, not too difficult to manage, and primarily serves as a connection pooler without any other functions."
42,"Tags: connection pooling, databases, pgbouncer, postgreSQL"
42,"The Stack Overflow Podcast is a weekly conversation about working in software development, learning to code, and the art and culture of computer programming."
42,Related
42,newsletter
42,"October 23, 2020"
42,The Overflow #44: Machine learning in production
42,"Welcome to ISSUE #44 of the Overflow! This newsletter is by developers, for developers, written and curated by the Stack Overflow team and Cassidy Williams at Netlify. This week, get in the fast lane and start pooling your database connections, make a CPU out of electronic components drawn by hand on paper, and learn to toggle multiple property…"
42,Medi Madelen Gwosdz
42,Content Strategist
42,code-for-a-living
42,"January 14, 2021"
42,Have the tables turned on NoSQL?
42,"NoSQL was the next big thing in system architecture in 2011, but overall interest in it has plateaued recently. What is NoSQL, what does it have to do with modern development, and is it worth implementing in your project?"
42,John Biggs and Ryan Donovan
42,code-for-a-living
42,"March 3, 2021"
42,Best practices can slow your application down
42,"In order to get the most performant site possible when building the codebase for our public Stack Overflow site, we didn’t always follow best practices."
42,Roberta Arcoverde and Ryan Donovan
42,code-for-a-living
42,"February 24, 2021"
42,What I wish I had known about single page applications
42,"Single page apps are all the rage today, but they don't always operate the same as traditional web pages."
42,Michael Pratt
42,11 Comments
42,Iwouldliketonotprovidemyname says:
42,14 Oct 20 at 11:20
42,"As you see, transaction throughput increased from 486 transactions per second to 566 transactions per second. With the help of pgbouncer, transaction throughput improved by approximately 60%."
42,That 60% is a bit huge. The increase of 80 TPS is more like 16% of the initial 486 TPS. The 75% increase for pgpool-II is also a bit large.
42,Reply
42,Tien Do says:
42,21 Oct 20 at 6:35
42,"Yeah, how is it 60% and 75%?"
42,Reply
42,CanadianLuke says:
42,22 Oct 20 at 6:55
42,A week with no reply to simple math… Not looking good…
42,Reply
42,Travis says:
42,14 Oct 20 at 3:49
42,"It is certainly an interesting technique, although there are a whole host of unintended consequences associated with this approach that would be well to mention. Pooling transactions to memory prior to pushing them to the database has very serious implications with regards to data integrity, as one main example. This approach also will require a very large amount of server memory to be used; in instances where memory is shared across multiple nodes, this can be problematic if there is a node failure. So, while it may be the case that more transactions per second occur, the risk seems to outweigh any gains."
42,Reply
42,Michael Aboagye says:
42,19 Oct 20 at 7:23
42,"@Travis, please I referred to pooling connection in this article."
42,"But do you know pgpool supports postgres stream replication? In addition, the presence of transaction log ensures data integrity is maintained."
42,Even without in the absence
42,"of pgpool cluster, postgres supports replication concepts such as synchronous and asynchronous replication to prevent data loss."
42,Reply
42,Galletto says:
42,22 Oct 20 at 9:54
42,"Maybe I’m missing something entirely, but I thought connection pooling has been the default in .NET for many years…"
42,Correct?
42,Reply
42,Nilesh says:
42,16 Oct 20 at 11:19
42,@Travis — The article talks about pooling connections but mentions no such thing as pooling transactions in memory .
42,I understand the static memory footprint that postgres has on the server would increase but I can’t see how this would lead to data integrity issues. Am I missing anything ?
42,Reply
42,Michael Aboagye says:
42,19 Oct 20 at 7:24
42,"Thanks for your comments, Nilesh."
42,Reply
42,Jeff Dafoe says:
42,22 Oct 20 at 5:21
42,"One thing to be aware of with connection pooling, particularly under PG, is that a reused pool connection may not be in the same initial state as a brand new connection from the backend. Session variables persist across shared connections, data may not be cleared from temp tables, and some types of errors are not cleared until the backend is recycled. It’s important that the code that is establishing the connection be written with this in mind, it must perform initialization that would not be necessary if the backend were fresh and it should also test the connection to make sure it can actually be queried from."
42,Reply
42,Matthew E says:
42,26 Oct 20 at 3:23
42,"right on, Nilesh and Dafoe."
42,Others:
42,#include
42,Reply
42,Emmanuel Casas says:
42,21 Jan 21 at 5:48
42,"Im concern about the fact PGBENCH is based on http://www.tpc.org/tpcb/ which is obsolete, maybe im missing something here, any thoughts about this guys ? Is there a better way to measure the TPS in postgresql ?"
42,Reply
42,Leave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment Name *
42,Email *
42,Website
42,"Save my name, email, and website in this browser for the next time I comment."
42,This site uses Akismet to reduce spam. Learn how your comment data is processed.
42,© 2021 All Rights Reserved.
42,Proudly powered by Wordpress
42,Stack Overflow
42,Questions
42,Jobs
42,Developer Jobs Directory
42,Salary Calculator
42,Products
42,Teams
42,Talent
42,Advertising
42,Enterprise
42,Company
42,About
42,Press
42,Work Here
42,Legal
42,Privacy Policy
42,Contact Us
42,Channels
42,Podcast
42,Newsletter
42,Facebook
42,Twitter
42,LinkedIn
42,Instagram
44,PostgreSQL: Documentation: 12: 25.3. Continuous Archiving and Point-in-Time Recovery (PITR)
44,Home
44,About
44,Download
44,Documentation
44,Community
44,Developers
44,Support
44,Donate
44,Your account
44,11th February 2021:
44,"PostgreSQL 13.2, 12.6, 11.11, 10.16, 9.6.21, & 9.5.25 Released!"
44,Documentation → PostgreSQL 12
44,Supported Versions:
44,Current
44,(13)
44,9.6
44,Development Versions:
44,devel
44,Unsupported versions:
44,9.5
44,9.4
44,9.3
44,9.2
44,9.1
44,9.0
44,8.4
44,8.3
44,8.2
44,25.3. Continuous Archiving and Point-in-Time Recovery (PITR)
44,Prev
44,Chapter 25. Backup and Restore
44,Home
44,Next
44,25.3. Continuous Archiving and Point-in-Time Recovery (PITR)
44,25.3.1. Setting Up WAL Archiving
44,25.3.2. Making a Base Backup
44,25.3.3. Making a Base Backup Using the Low Level API
44,25.3.4. Recovering Using a Continuous Archive Backup
44,25.3.5. Timelines
44,25.3.6. Tips and Examples
44,25.3.7. Caveats
44,"At all times, PostgreSQL maintains a write ahead log (WAL) in the pg_wal/ subdirectory of the cluster's data directory. The log records every change made to the database's data files. This log exists primarily for crash-safety purposes: if the system crashes, the database can be restored to consistency by “replaying” the log entries made since the last checkpoint. However, the existence of the log makes it possible to use a third strategy for backing up databases: we can combine a file-system-level backup with backup of the WAL files. If recovery is needed, we restore the file system backup and then replay from the backed-up WAL files to bring the system to a current state. This approach is more complex to administer than either of the previous approaches, but it has some significant benefits:"
44,"We do not need a perfectly consistent file system backup as the starting point. Any internal inconsistency in the backup will be corrected by log replay (this is not significantly different from what happens during crash recovery). So we do not need a file system snapshot capability, just tar or a similar archiving tool."
44,"Since we can combine an indefinitely long sequence of WAL files for replay, continuous backup can be achieved simply by continuing to archive the WAL files. This is particularly valuable for large databases, where it might not be convenient to take a full backup frequently."
44,"It is not necessary to replay the WAL entries all the way to the end. We could stop the replay at any point and have a consistent snapshot of the database as it was at that time. Thus, this technique supports point-in-time recovery: it is possible to restore the database to its state at any time since your base backup was taken."
44,"If we continuously feed the series of WAL files to another machine that has been loaded with the same base backup file, we have a warm standby system: at any point we can bring up the second machine and it will have a nearly-current copy of the database."
44,Note
44,pg_dump and pg_dumpall do not produce file-system-level backups and cannot be used as part of a continuous-archiving solution. Such dumps are logical and do not contain enough information to be used by WAL replay.
44,"As with the plain file-system-backup technique, this method can only support restoration of an entire database cluster, not a subset. Also, it requires a lot of archival storage: the base backup might be bulky, and a busy system will generate many megabytes of WAL traffic that have to be archived. Still, it is the preferred backup technique in many situations where high reliability is needed."
44,"To recover successfully using continuous archiving (also called “online backup” by many database vendors), you need a continuous sequence of archived WAL files that extends back at least as far as the start time of your backup. So to get started, you should set up and test your procedure for archiving WAL files before you take your first base backup. Accordingly, we first discuss the mechanics of archiving WAL files."
44,25.3.1. Setting Up WAL Archiving
44,"In an abstract sense, a running PostgreSQL system produces an indefinitely long sequence of WAL records. The system physically divides this sequence into WAL segment files, which are normally 16MB apiece (although the segment size can be altered during initdb). The segment files are given numeric names that reflect their position in the abstract WAL sequence. When not using WAL archiving, the system normally creates just a few segment files and then “recycles” them by renaming no-longer-needed segment files to higher segment numbers. It's assumed that segment files whose contents precede the last checkpoint are no longer of interest and can be recycled."
44,"When archiving WAL data, we need to capture the contents of each segment file once it is filled, and save that data somewhere before the segment file is recycled for reuse. Depending on the application and the available hardware, there could be many different ways of “saving the data somewhere”: we could copy the segment files to an NFS-mounted directory on another machine, write them onto a tape drive (ensuring that you have a way of identifying the original name of each file), or batch them together and burn them onto CDs, or something else entirely. To provide the database administrator with flexibility, PostgreSQL tries not to make any assumptions about how the archiving will be done. Instead, PostgreSQL lets the administrator specify a shell command to be executed to copy a completed segment file to wherever it needs to go. The command could be as simple as a cp, or it could invoke a complex shell script — it's all up to you."
44,"To enable WAL archiving, set the wal_level configuration parameter to replica or higher, archive_mode to on, and specify the shell command to use in the archive_command configuration parameter. In practice these settings will always be placed in the postgresql.conf file. In archive_command, %p is replaced by the path name of the file to archive, while %f is replaced by only the file name. (The path name is relative to the current working directory, i.e., the cluster's data directory.) Use %% if you need to embed an actual % character in the command. The simplest useful command is something like:"
44,archive_command = 'test ! -f /mnt/server/archivedir/%f && cp %p /mnt/server/archivedir/%f'
44,# Unix
44,"archive_command = 'copy ""%p"" ""C:\\server\\archivedir\\%f""'"
44,# Windows
44,"which will copy archivable WAL segments to the directory /mnt/server/archivedir. (This is an example, not a recommendation, and might not work on all platforms.) After the %p and %f parameters have been replaced, the actual command executed might look like this:"
44,test ! -f /mnt/server/archivedir/00000001000000A900000065 && cp pg_wal/00000001000000A900000065 /mnt/server/archivedir/00000001000000A900000065
44,A similar command will be generated for each new file to be archived.
44,"The archive command will be executed under the ownership of the same user that the PostgreSQL server is running as. Since the series of WAL files being archived contains effectively everything in your database, you will want to be sure that the archived data is protected from prying eyes; for example, archive into a directory that does not have group or world read access."
44,"It is important that the archive command return zero exit status if and only if it succeeds. Upon getting a zero result, PostgreSQL will assume that the file has been successfully archived, and will remove or recycle it. However, a nonzero status tells PostgreSQL that the file was not archived; it will try again periodically until it succeeds."
44,The archive command should generally be designed to refuse to overwrite any pre-existing archive file. This is an important safety feature to preserve the integrity of your archive in case of administrator error (such as sending the output of two different servers to the same archive directory).
44,"It is advisable to test your proposed archive command to ensure that it indeed does not overwrite an existing file, and that it returns nonzero status in this case. The example command above for Unix ensures this by including a separate test step. On some Unix platforms, cp has switches such as -i that can be used to do the same thing less verbosely, but you should not rely on these without verifying that the right exit status is returned. (In particular, GNU cp will return status zero when -i is used and the target file already exists, which is not the desired behavior.)"
44,"While designing your archiving setup, consider what will happen if the archive command fails repeatedly because some aspect requires operator intervention or the archive runs out of space. For example, this could occur if you write to tape without an autochanger; when the tape fills, nothing further can be archived until the tape is swapped. You should ensure that any error condition or request to a human operator is reported appropriately so that the situation can be resolved reasonably quickly. The pg_wal/ directory will continue to fill with WAL segment files until the situation is resolved. (If the file system containing pg_wal/ fills up, PostgreSQL will do a PANIC shutdown. No committed transactions will be lost, but the database will remain offline until you free some space.)"
44,"The speed of the archiving command is unimportant as long as it can keep up with the average rate at which your server generates WAL data. Normal operation continues even if the archiving process falls a little behind. If archiving falls significantly behind, this will increase the amount of data that would be lost in the event of a disaster. It will also mean that the pg_wal/ directory will contain large numbers of not-yet-archived segment files, which could eventually exceed available disk space. You are advised to monitor the archiving process to ensure that it is working as you intend."
44,"In writing your archive command, you should assume that the file names to be archived can be up to 64 characters long and can contain any combination of ASCII letters, digits, and dots. It is not necessary to preserve the original relative path (%p) but it is necessary to preserve the file name (%f)."
44,"Note that although WAL archiving will allow you to restore any modifications made to the data in your PostgreSQL database, it will not restore changes made to configuration files (that is, postgresql.conf, pg_hba.conf and pg_ident.conf), since those are edited manually rather than through SQL operations. You might wish to keep the configuration files in a location that will be backed up by your regular file system backup procedures. See Section 19.2 for how to relocate the configuration files."
44,"The archive command is only invoked on completed WAL segments. Hence, if your server generates only little WAL traffic (or has slack periods where it does so), there could be a long delay between the completion of a transaction and its safe recording in archive storage. To put a limit on how old unarchived data can be, you can set archive_timeout to force the server to switch to a new WAL segment file at least that often. Note that archived files that are archived early due to a forced switch are still the same length as completely full files. It is therefore unwise to set a very short archive_timeout — it will bloat your archive storage. archive_timeout settings of a minute or so are usually reasonable."
44,"Also, you can force a segment switch manually with pg_switch_wal if you want to ensure that a just-finished transaction is archived as soon as possible. Other utility functions related to WAL management are listed in Table 9.84."
44,"When wal_level is minimal some SQL commands are optimized to avoid WAL logging, as described in Section 14.4.7. If archiving or streaming replication were turned on during execution of one of these statements, WAL would not contain enough information for archive recovery. (Crash recovery is unaffected.) For this reason, wal_level can only be changed at server start. However, archive_command can be changed with a configuration file reload. If you wish to temporarily stop archiving, one way to do it is to set archive_command to the empty string (''). This will cause WAL files to accumulate in pg_wal/ until a working archive_command is re-established."
44,25.3.2. Making a Base Backup
44,"The easiest way to perform a base backup is to use the pg_basebackup tool. It can create a base backup either as regular files or as a tar archive. If more flexibility than pg_basebackup can provide is required, you can also make a base backup using the low level API (see Section 25.3.3)."
44,"It is not necessary to be concerned about the amount of time it takes to make a base backup. However, if you normally run the server with full_page_writes disabled, you might notice a drop in performance while the backup runs since full_page_writes is effectively forced on during backup mode."
44,"To make use of the backup, you will need to keep all the WAL segment files generated during and after the file system backup. To aid you in doing this, the base backup process creates a backup history file that is immediately stored into the WAL archive area. This file is named after the first WAL segment file that you need for the file system backup. For example, if the starting WAL file is 0000000100001234000055CD the backup history file will be named something like 0000000100001234000055CD.007C9330.backup. (The second part of the file name stands for an exact position within the WAL file, and can ordinarily be ignored.) Once you have safely archived the file system backup and the WAL segment files used during the backup (as specified in the backup history file), all archived WAL segments with names numerically less are no longer needed to recover the file system backup and can be deleted. However, you should consider keeping several backup sets to be absolutely certain that you can recover your data."
44,"The backup history file is just a small text file. It contains the label string you gave to pg_basebackup, as well as the starting and ending times and WAL segments of the backup. If you used the label to identify the associated dump file, then the archived history file is enough to tell you which dump file to restore."
44,"Since you have to keep around all the archived WAL files back to your last base backup, the interval between base backups should usually be chosen based on how much storage you want to expend on archived WAL files. You should also consider how long you are prepared to spend recovering, if recovery should be necessary — the system will have to replay all those WAL segments, and that could take awhile if it has been a long time since the last base backup."
44,25.3.3. Making a Base Backup Using the Low Level API
44,"The procedure for making a base backup using the low level APIs contains a few more steps than the pg_basebackup method, but is relatively simple. It is very important that these steps are executed in sequence, and that the success of a step is verified before proceeding to the next step."
44,Low level base backups can be made in a non-exclusive or an exclusive way. The non-exclusive method is recommended and the exclusive one is deprecated and will eventually be removed.
44,25.3.3.1. Making a Non-Exclusive Low-Level Backup
44,A non-exclusive low level backup is one that allows other concurrent backups to be running (both those started using the same backup API and those started using pg_basebackup).
44,Ensure that WAL archiving is enabled and working.
44,"Connect to the server (it does not matter which database) as a user with rights to run pg_start_backup (superuser, or a user who has been granted EXECUTE on the function) and issue the command:"
44,"SELECT pg_start_backup('label', false, false);"
44,"where label is any string you want to use to uniquely identify this backup operation. The connection calling pg_start_backup must be maintained until the end of the backup, or the backup will be automatically aborted."
44,"By default, pg_start_backup can take a long time to finish. This is because it performs a checkpoint, and the I/O required for the checkpoint will be spread out over a significant period of time, by default half your inter-checkpoint interval (see the configuration parameter checkpoint_completion_target). This is usually what you want, because it minimizes the impact on query processing. If you want to start the backup as soon as possible, change the second parameter to true, which will issue an immediate checkpoint using as much I/O as available."
44,The third parameter being false tells pg_start_backup to initiate a non-exclusive base backup.
44,"Perform the backup, using any convenient file-system-backup tool such as tar or cpio (not pg_dump or pg_dumpall). It is neither necessary nor desirable to stop normal operation of the database while you do this. See Section 25.3.3.3 for things to consider during this backup."
44,"In the same connection as before, issue the command:"
44,"SELECT * FROM pg_stop_backup(false, true);"
44,"This terminates backup mode. On a primary, it also performs an automatic switch to the next WAL segment. On a standby, it is not possible to automatically switch WAL segments, so you may wish to run pg_switch_wal on the primary to perform a manual switch. The reason for the switch is to arrange for the last WAL segment file written during the backup interval to be ready to archive."
44,"The pg_stop_backup will return one row with three values. The second of these fields should be written to a file named backup_label in the root directory of the backup. The third field should be written to a file named tablespace_map unless the field is empty. These files are vital to the backup working, and must be written without modification."
44,"Once the WAL segment files active during the backup are archived, you are done. The file identified by pg_stop_backup's first return value is the last segment that is required to form a complete set of backup files. On a primary, if archive_mode is enabled and the wait_for_archive parameter is true, pg_stop_backup does not return until the last segment has been archived. On a standby, archive_mode must be always in order for pg_stop_backup to wait. Archiving of these files happens automatically since you have already configured archive_command. In most cases this happens quickly, but you are advised to monitor your archive system to ensure there are no delays. If the archive process has fallen behind because of failures of the archive command, it will keep retrying until the archive succeeds and the backup is complete. If you wish to place a time limit on the execution of pg_stop_backup, set an appropriate statement_timeout value, but make note that if pg_stop_backup terminates because of this your backup may not be valid."
44,"If the backup process monitors and ensures that all WAL segment files required for the backup are successfully archived then the wait_for_archive parameter (which defaults to true) can be set to false to have pg_stop_backup return as soon as the stop backup record is written to the WAL. By default, pg_stop_backup will wait until all WAL has been archived, which can take some time. This option must be used with caution: if WAL archiving is not monitored correctly then the backup might not include all of the WAL files and will therefore be incomplete and not able to be restored."
44,25.3.3.2. Making an Exclusive Low-Level Backup
44,Note
44,"The exclusive backup method is deprecated and should be avoided. Prior to PostgreSQL 9.6, this was the only low-level method available, but it is now recommended that all users upgrade their scripts to use non-exclusive backups."
44,"The process for an exclusive backup is mostly the same as for a non-exclusive one, but it differs in a few key steps. This type of backup can only be taken on a primary and does not allow concurrent backups. Moreover, because it creates a backup label file, as described below, it can block automatic restart of the master server after a crash. On the other hand, the erroneous removal of this file from a backup or standby is a common mistake, which can result in serious data corruption. If it is necessary to use this method, the following steps may be used."
44,Ensure that WAL archiving is enabled and working.
44,"Connect to the server (it does not matter which database) as a user with rights to run pg_start_backup (superuser, or a user who has been granted EXECUTE on the function) and issue the command:"
44,SELECT pg_start_backup('label');
44,"where label is any string you want to use to uniquely identify this backup operation. pg_start_backup creates a backup label file, called backup_label, in the cluster directory with information about your backup, including the start time and label string. The function also creates a tablespace map file, called tablespace_map, in the cluster directory with information about tablespace symbolic links in pg_tblspc/ if one or more such link is present. Both files are critical to the integrity of the backup, should you need to restore from it."
44,"By default, pg_start_backup can take a long time to finish. This is because it performs a checkpoint, and the I/O required for the checkpoint will be spread out over a significant period of time, by default half your inter-checkpoint interval (see the configuration parameter checkpoint_completion_target). This is usually what you want, because it minimizes the impact on query processing. If you want to start the backup as soon as possible, use:"
44,"SELECT pg_start_backup('label', true);"
44,This forces the checkpoint to be done as quickly as possible.
44,"Perform the backup, using any convenient file-system-backup tool such as tar or cpio (not pg_dump or pg_dumpall). It is neither necessary nor desirable to stop normal operation of the database while you do this. See Section 25.3.3.3 for things to consider during this backup."
44,"As noted above, if the server crashes during the backup it may not be possible to restart until the backup_label file has been manually deleted from the PGDATA directory. Note that it is very important to never remove the backup_label file when restoring a backup, because this will result in corruption. Confusion about when it is appropriate to remove this file is a common cause of data corruption when using this method; be very certain that you remove the file only on an existing master and never when building a standby or restoring a backup, even if you are building a standby that will subsequently be promoted to a new master."
44,"Again connect to the database as a user with rights to run pg_stop_backup (superuser, or a user who has been granted EXECUTE on the function), and issue the command:"
44,SELECT pg_stop_backup();
44,This function terminates backup mode and performs an automatic switch to the next WAL segment. The reason for the switch is to arrange for the last WAL segment written during the backup interval to be ready to archive.
44,"Once the WAL segment files active during the backup are archived, you are done. The file identified by pg_stop_backup's result is the last segment that is required to form a complete set of backup files. If archive_mode is enabled, pg_stop_backup does not return until the last segment has been archived. Archiving of these files happens automatically since you have already configured archive_command. In most cases this happens quickly, but you are advised to monitor your archive system to ensure there are no delays. If the archive process has fallen behind because of failures of the archive command, it will keep retrying until the archive succeeds and the backup is complete."
44,"When using exclusive backup mode, it is absolutely imperative to ensure that pg_stop_backup completes successfully at the end of the backup. Even if the backup itself fails, for example due to lack of disk space, failure to call pg_stop_backup will leave the server in backup mode indefinitely, causing future backups to fail and increasing the risk of a restart failure during the time that backup_label exists."
44,25.3.3.3. Backing Up the Data Directory
44,"Some file system backup tools emit warnings or errors if the files they are trying to copy change while the copy proceeds. When taking a base backup of an active database, this situation is normal and not an error. However, you need to ensure that you can distinguish complaints of this sort from real errors. For example, some versions of rsync return a separate exit code for “vanished source files”, and you can write a driver script to accept this exit code as a non-error case. Also, some versions of GNU tar return an error code indistinguishable from a fatal error if a file was truncated while tar was copying it. Fortunately, GNU tar versions 1.16 and later exit with 1 if a file was changed during the backup, and 2 for other errors. With GNU tar version 1.23 and later, you can use the warning options --warning=no-file-changed --warning=no-file-removed to hide the related warning messages."
44,"Be certain that your backup includes all of the files under the database cluster directory (e.g., /usr/local/pgsql/data). If you are using tablespaces that do not reside underneath this directory, be careful to include them as well (and be sure that your backup archives symbolic links as links, otherwise the restore will corrupt your tablespaces)."
44,"You should, however, omit from the backup the files within the cluster's pg_wal/ subdirectory. This slight adjustment is worthwhile because it reduces the risk of mistakes when restoring. This is easy to arrange if pg_wal/ is a symbolic link pointing to someplace outside the cluster directory, which is a common setup anyway for performance reasons. You might also want to exclude postmaster.pid and postmaster.opts, which record information about the running postmaster, not about the postmaster which will eventually use this backup. (These files can confuse pg_ctl.)"
44,"It is often a good idea to also omit from the backup the files within the cluster's pg_replslot/ directory, so that replication slots that exist on the master do not become part of the backup. Otherwise, the subsequent use of the backup to create a standby may result in indefinite retention of WAL files on the standby, and possibly bloat on the master if hot standby feedback is enabled, because the clients that are using those replication slots will still be connecting to and updating the slots on the master, not the standby. Even if the backup is only intended for use in creating a new master, copying the replication slots isn't expected to be particularly useful, since the contents of those slots will likely be badly out of date by the time the new master comes on line."
44,"The contents of the directories pg_dynshmem/, pg_notify/, pg_serial/, pg_snapshots/, pg_stat_tmp/, and pg_subtrans/ (but not the directories themselves) can be omitted from the backup as they will be initialized on postmaster startup. If stats_temp_directory is set and is under the data directory then the contents of that directory can also be omitted."
44,Any file or directory beginning with pgsql_tmp can be omitted from the backup. These files are removed on postmaster start and the directories will be recreated as needed.
44,pg_internal.init files can be omitted from the backup whenever a file of that name is found. These files contain relation cache data that is always rebuilt when recovering.
44,"The backup label file includes the label string you gave to pg_start_backup, as well as the time at which pg_start_backup was run, and the name of the starting WAL file. In case of confusion it is therefore possible to look inside a backup file and determine exactly which backup session the dump file came from. The tablespace map file includes the symbolic link names as they exist in the directory pg_tblspc/ and the full path of each symbolic link. These files are not merely for your information; their presence and contents are critical to the proper operation of the system's recovery process."
44,"It is also possible to make a backup while the server is stopped. In this case, you obviously cannot use pg_start_backup or pg_stop_backup, and you will therefore be left to your own devices to keep track of which backup is which and how far back the associated WAL files go. It is generally better to follow the continuous archiving procedure above."
44,25.3.4. Recovering Using a Continuous Archive Backup
44,"Okay, the worst has happened and you need to recover from your backup. Here is the procedure:"
44,"Stop the server, if it's running."
44,"If you have the space to do so, copy the whole cluster data directory and any tablespaces to a temporary location in case you need them later. Note that this precaution will require that you have enough free space on your system to hold two copies of your existing database. If you do not have enough space, you should at least save the contents of the cluster's pg_wal subdirectory, as it might contain logs which were not archived before the system went down."
44,Remove all existing files and subdirectories under the cluster data directory and under the root directories of any tablespaces you are using.
44,"Restore the database files from your file system backup. Be sure that they are restored with the right ownership (the database system user, not root!) and with the right permissions. If you are using tablespaces, you should verify that the symbolic links in pg_tblspc/ were correctly restored."
44,"Remove any files present in pg_wal/; these came from the file system backup and are therefore probably obsolete rather than current. If you didn't archive pg_wal/ at all, then recreate it with proper permissions, being careful to ensure that you re-establish it as a symbolic link if you had it set up that way before."
44,"If you have unarchived WAL segment files that you saved in step 2, copy them into pg_wal/. (It is best to copy them, not move them, so you still have the unmodified files if a problem occurs and you have to start over.)"
44,Set recovery configuration settings in postgresql.conf (see Section 19.5.4) and create a file recovery.signal in the cluster data directory. You might also want to temporarily modify pg_hba.conf to prevent ordinary users from connecting until you are sure the recovery was successful.
44,"Start the server. The server will go into recovery mode and proceed to read through the archived WAL files it needs. Should the recovery be terminated because of an external error, the server can simply be restarted and it will continue recovery. Upon completion of the recovery process, the server will remove recovery.signal (to prevent accidentally re-entering recovery mode later) and then commence normal database operations."
44,"Inspect the contents of the database to ensure you have recovered to the desired state. If not, return to step 1. If all is well, allow your users to connect by restoring pg_hba.conf to normal."
44,"The key part of all this is to set up a recovery configuration that describes how you want to recover and how far the recovery should run. The one thing that you absolutely must specify is the restore_command, which tells PostgreSQL how to retrieve archived WAL file segments. Like the archive_command, this is a shell command string. It can contain %f, which is replaced by the name of the desired log file, and %p, which is replaced by the path name to copy the log file to. (The path name is relative to the current working directory, i.e., the cluster's data directory.) Write %% if you need to embed an actual % character in the command. The simplest useful command is something like:"
44,restore_command = 'cp /mnt/server/archivedir/%f %p'
44,"which will copy previously archived WAL segments from the directory /mnt/server/archivedir. Of course, you can use something much more complicated, perhaps even a shell script that requests the operator to mount an appropriate tape."
44,"It is important that the command return nonzero exit status on failure. The command will be called requesting files that are not present in the archive; it must return nonzero when so asked. This is not an error condition. An exception is that if the command was terminated by a signal (other than SIGTERM, which is used as part of a database server shutdown) or an error by the shell (such as command not found), then recovery will abort and the server will not start up."
44,Not all of the requested files will be WAL segment files; you should also expect requests for files with a suffix of .history. Also be aware that the base name of the %p path will be different from %f; do not expect them to be interchangeable.
44,"WAL segments that cannot be found in the archive will be sought in pg_wal/; this allows use of recent un-archived segments. However, segments that are available from the archive will be used in preference to files in pg_wal/."
44,"Normally, recovery will proceed through all available WAL segments, thereby restoring the database to the current point in time (or as close as possible given the available WAL segments). Therefore, a normal recovery will end with a “file not found” message, the exact text of the error message depending upon your choice of restore_command. You may also see an error message at the start of recovery for a file named something like 00000001.history. This is also normal and does not indicate a problem in simple recovery situations; see Section 25.3.5 for discussion."
44,"If you want to recover to some previous point in time (say, right before the junior DBA dropped your main transaction table), just specify the required stopping point. You can specify the stop point, known as the “recovery target”, either by date/time, named restore point or by completion of a specific transaction ID. As of this writing only the date/time and named restore point options are very usable, since there are no tools to help you identify with any accuracy which transaction ID to use."
44,Note
44,"The stop point must be after the ending time of the base backup, i.e., the end time of pg_stop_backup. You cannot use a base backup to recover to a time when that backup was in progress. (To recover to such a time, you must go back to your previous base backup and roll forward from there.)"
44,"If recovery finds corrupted WAL data, recovery will halt at that point and the server will not start. In such a case the recovery process could be re-run from the beginning, specifying a “recovery target” before the point of corruption so that recovery can complete normally. If recovery fails for an external reason, such as a system crash or if the WAL archive has become inaccessible, then the recovery can simply be restarted and it will restart almost from where it failed. Recovery restart works much like checkpointing in normal operation: the server periodically forces all its state to disk, and then updates the pg_control file to indicate that the already-processed WAL data need not be scanned again."
44,25.3.5. Timelines
44,"The ability to restore the database to a previous point in time creates some complexities that are akin to science-fiction stories about time travel and parallel universes. For example, in the original history of the database, suppose you dropped a critical table at 5:15PM on Tuesday evening, but didn't realize your mistake until Wednesday noon. Unfazed, you get out your backup, restore to the point-in-time 5:14PM Tuesday evening, and are up and running. In this history of the database universe, you never dropped the table. But suppose you later realize this wasn't such a great idea, and would like to return to sometime Wednesday morning in the original history. You won't be able to if, while your database was up-and-running, it overwrote some of the WAL segment files that led up to the time you now wish you could get back to. Thus, to avoid this, you need to distinguish the series of WAL records generated after you've done a point-in-time recovery from those that were generated in the original database history."
44,"To deal with this problem, PostgreSQL has a notion of timelines. Whenever an archive recovery completes, a new timeline is created to identify the series of WAL records generated after that recovery. The timeline ID number is part of WAL segment file names so a new timeline does not overwrite the WAL data generated by previous timelines. It is in fact possible to archive many different timelines. While that might seem like a useless feature, it's often a lifesaver. Consider the situation where you aren't quite sure what point-in-time to recover to, and so have to do several point-in-time recoveries by trial and error until you find the best place to branch off from the old history. Without timelines this process would soon generate an unmanageable mess. With timelines, you can recover to any prior state, including states in timeline branches that you abandoned earlier."
44,"Every time a new timeline is created, PostgreSQL creates a “timeline history” file that shows which timeline it branched off from and when. These history files are necessary to allow the system to pick the right WAL segment files when recovering from an archive that contains multiple timelines. Therefore, they are archived into the WAL archive area just like WAL segment files. The history files are just small text files, so it's cheap and appropriate to keep them around indefinitely (unlike the segment files which are large). You can, if you like, add comments to a history file to record your own notes about how and why this particular timeline was created. Such comments will be especially valuable when you have a thicket of different timelines as a result of experimentation."
44,"The default behavior of recovery is to recover to the latest timeline found in the archive. If you wish to recover to the timeline that was current when the base backup was taken or into a specific child timeline (that is, you want to return to some state that was itself generated after a recovery attempt), you need to specify current or the target timeline ID in recovery_target_timeline. You cannot recover into timelines that branched off earlier than the base backup."
44,25.3.6. Tips and Examples
44,Some tips for configuring continuous archiving are given here.
44,25.3.6.1. Standalone Hot Backups
44,"It is possible to use PostgreSQL's backup facilities to produce standalone hot backups. These are backups that cannot be used for point-in-time recovery, yet are typically much faster to backup and restore than pg_dump dumps. (They are also much larger than pg_dump dumps, so in some cases the speed advantage might be negated.)"
44,"As with base backups, the easiest way to produce a standalone hot backup is to use the pg_basebackup tool. If you include the -X parameter when calling it, all the write-ahead log required to use the backup will be included in the backup automatically, and no special action is required to restore the backup."
44,"If more flexibility in copying the backup files is needed, a lower level process can be used for standalone hot backups as well. To prepare for low level standalone hot backups, make sure wal_level is set to replica or higher, archive_mode to on, and set up an archive_command that performs archiving only when a switch file exists. For example:"
44,archive_command = 'test ! -f /var/lib/pgsql/backup_in_progress || (test ! -f /var/lib/pgsql/archive/%f && cp %p /var/lib/pgsql/archive/%f)'
44,"This command will perform archiving when /var/lib/pgsql/backup_in_progress exists, and otherwise silently return zero exit status (allowing PostgreSQL to recycle the unwanted WAL file)."
44,"With this preparation, a backup can be taken using a script like the following:"
44,touch /var/lib/pgsql/backup_in_progress
44,"psql -c ""select pg_start_backup('hot_backup');"""
44,tar -cf /var/lib/pgsql/backup.tar /var/lib/pgsql/data/
44,"psql -c ""select pg_stop_backup();"""
44,rm /var/lib/pgsql/backup_in_progress
44,tar -rf /var/lib/pgsql/backup.tar /var/lib/pgsql/archive/
44,"The switch file /var/lib/pgsql/backup_in_progress is created first, enabling archiving of completed WAL files to occur. After the backup the switch file is removed. Archived WAL files are then added to the backup so that both base backup and all required WAL files are part of the same tar file. Please remember to add error handling to your backup scripts."
44,25.3.6.2. Compressed Archive Logs
44,"If archive storage size is a concern, you can use gzip to compress the archive files:"
44,archive_command = 'gzip < %p > /var/lib/pgsql/archive/%f'
44,You will then need to use gunzip during recovery:
44,restore_command = 'gunzip < /mnt/server/archivedir/%f > %p'
44,25.3.6.3. archive_command Scripts
44,"Many people choose to use scripts to define their archive_command, so that their postgresql.conf entry looks very simple:"
44,"archive_command = 'local_backup_script.sh ""%p"" ""%f""'"
44,"Using a separate script file is advisable any time you want to use more than a single command in the archiving process. This allows all complexity to be managed within the script, which can be written in a popular scripting language such as bash or perl."
44,Examples of requirements that might be solved within a script include:
44,Copying data to secure off-site data storage
44,"Batching WAL files so that they are transferred every three hours, rather than one at a time"
44,Interfacing with other backup and recovery software
44,Interfacing with monitoring software to report errors
44,Tip
44,"When using an archive_command script, it's desirable to enable logging_collector. Any messages written to stderr from the script will then appear in the database server log, allowing complex configurations to be diagnosed easily if they fail."
44,25.3.7. Caveats
44,"At this writing, there are several limitations of the continuous archiving technique. These will probably be fixed in future releases:"
44,"If a CREATE DATABASE command is executed while a base backup is being taken, and then the template database that the CREATE DATABASE copied is modified while the base backup is still in progress, it is possible that recovery will cause those modifications to be propagated into the created database as well. This is of course undesirable. To avoid this risk, it is best not to modify any template databases while taking a base backup."
44,"CREATE TABLESPACE commands are WAL-logged with the literal absolute path, and will therefore be replayed as tablespace creations with the same absolute path. This might be undesirable if the log is being replayed on a different machine. It can be dangerous even if the log is being replayed on the same machine, but into a new data directory: the replay will still overwrite the contents of the original tablespace. To avoid potential gotchas of this sort, the best practice is to take a new base backup after creating or dropping tablespaces."
44,"It should also be noted that the default WAL format is fairly bulky since it includes many disk page snapshots. These page snapshots are designed to support crash recovery, since we might need to fix partially-written disk pages. Depending on your system hardware and software, the risk of partial writes might be small enough to ignore, in which case you can significantly reduce the total volume of archived logs by turning off page snapshots using the full_page_writes parameter. (Read the notes and warnings in Chapter 29 before you do so.) Turning off page snapshots does not prevent use of the logs for PITR operations. An area for future development is to compress archived WAL data by removing unnecessary page copies even when full_page_writes is on. In the meantime, administrators might wish to reduce the number of page snapshots included in WAL by increasing the checkpoint interval parameters as much as feasible."
44,Prev
44,Next
44,25.2. File System Level Backup
44,Home
44,"Chapter 26. High Availability, Load Balancing, and Replication"
44,Submit correction
44,"If you see anything in the documentation that is not correct, does not match"
44,"your experience with the particular feature or requires further clarification,"
44,please use
44,this form
44,to report a documentation issue.
44,Privacy Policy |
44,Code of Conduct |
44,About PostgreSQL |
44,Contact
44,Copyright © 1996-2021 The PostgreSQL Global Development Group
48,IBM Security Identity Governance and Intelligence v5.2.6 Performance Tuning Guide
48,United States
48,English English
48,IBM®
48,Site map
48,IBM
48,IBM Support
48,Check here to start a new keyword search.
48,Search support or find a product:
48,Search
48,Our apologies
48,No results were found for your search query.
48,"Tips To return expected results, you can:"
48,"Reduce the number of search terms. Each term you use focuses the search further. Check your spelling. A single misspelled or incorrectly typed term can change your result. Try substituting synonyms for your original terms. For example, instead of searching for ""java classes"", try ""java training"" Did you search for an IBM acquired or sold product ? If so, follow the appropriate link below to find the content you need."
48,Our apologies
48,Search results are not available at this time. Please try again later or use one of the other support options on this page. No results were found for your search query.
48,"Tips To return expected results, you can:"
48,"Reduce the number of search terms. Each term you use focuses the search further. Check your spelling. A single misspelled or incorrectly typed term can change your result. Try substituting synonyms for your original terms. For example, instead of searching for ""java classes"", try ""java training"" Did you search for an IBM acquired or sold product ? If so, follow the appropriate link below to find the content you need."
48,Check here to start a new keyword search.
48,Watson Product Search
48,Search
48,"None of the above, continue with my search"
48,IBM Security Identity Governance and Intelligence v5.2.6 Performance Tuning Guide
48,How To
48,Summary
48,"This document provides information on tuning middleware for IBM Security Identity Governance and Intelligence v5.2.6 fp1. It includes tuning for IBM DB2 , PostgreSQL database servers and Oracle."
48,Document Location
48,Worldwide
48,IGIv526FP1PTG_2020-06-08
48,"[{""Business Unit"":{""code"":""BU008"",""label"":""Security""},""Product"":{""code"":""SSGHJR"",""label"":""IBM Security Identity Governance and Intelligence""},""ARM Category"":[{""code"":""a8m0z0000001hj7AAA"",""label"":""Identity Governance & Intelligence->Performance \/ Tuning""}],""ARM Case Number"":"""",""Platform"":[{""code"":""PF025"",""label"":""Platform Independent""}],""Version"":""5.2.6"",""Line of Business"":{""code"":""LOB24"",""label"":""Security Software""}}]"
48,Document Information
48,Modified date:
48,08 June 2020
48,UID
48,ibm16220302
48,Page Feedback
48,Contact and feedback
48,Need support?
48,Submit feedback to IBM Support
48,1-800-IBM-7378 (USA)
48,Directory of worldwide contacts
48,Contact
48,Privacy
48,Terms of use
48,Accessibility
49,Bruce Momjian:
49,Postgres Blog
49,Home
49,Blogs
49,General
49,Postgres
49,Comment
49,Events
49,FAQ
49,Favorites
49,Articles
49,Books
49,Children
49,Documents
49,Hobbies
49,Movies
49,Music
49,Periodicals
49,Quotes
49,Sayings
49,Television
49,Videos
49,Artistic
49,Humorous
49,Serious
49,News
49,Presentations
49,PG Admin.
49,PG Book
49,PG Extended
49,PG Internals
49,Open Source
49,PG Perform.
49,PG Project
49,PG SQL
49,Security
49,General
49,Selecting
49,Press
49,Résumé
49,Search
49,Travel Map
49,Contact
49,Private Area
49,Postgres Blog
49,This blog is about my work on the Postgres open source
49,"database, and is published on Planet PostgreSQL."
49,PgLife allows monitoring of
49,all Postgres community activity.
49,Online status:
49,Unread Postgres emails:
49,Email graphs:
49,"incoming,"
49,"outgoing,"
49,"unread,"
49,commits
49,(details)
49,Category Index
49,2021
49,2020
49,2019
49,2018
49,2017
49,2016
49,2015
49,2014
49,2013
49,2012
49,2011
49,2010
49,2009
49,2008
49,Yearly Chart
49,Thirty Years of Continuous PostgreSQL Development
49,"Wednesday, October 14, 2020"
49,"I did an interview with edb recently, and a"
49,blog post based on that interview was published yesterday.
49,It covers the Postgres 13 feature set and the effects of open source on the software development process.
49,View or Post Comments
49,Community Impact of 2nd Quadrant Purchase
49,"Wednesday, October"
49,"7, 2020"
49,Last week 2nd Quadrant was purchased by edb.
49,"While this is certainly good news for these companies, it can increase risks to the Postgres community."
49,"First, there is an"
49,"unwritten rule that the Postgres core team should not have over half of its members from a single company, and the acquisition causes edb's representation in the"
49,core team to be 60% — the core team is working on a solution for this.
49,"Second, two companies becoming one reduces Postgres user choice for support and services, especially in the North American and western European markets."
49,Reduced vendor options often results in a worse
49,customer service and less innovation.
49,"Since the Postgres community does independent innovation, this might not be an issue for community software, but could be for company-controlled tooling around"
49,Postgres.
49,"Third, there is the risk that an even larger company wanting to hurt Postgres could acquire edb and take it in a direction that is neutral or negative for the Postgres community."
49,Employee
49,"non-compete agreements, and the lack of other Postgres support companies could extend the duration of these effects."
49,There isn't much the community can do to minimize these issues but to be alert for
49,problems.
49,Update: Hacker News thread and tweet 2020-10-07
49,View or Post Comments
49,The Economics of Open Source Contributions
49,"Friday, October"
49,"2, 2020"
49,This long article describes the many challenges of managing open
49,"source projects and the mismatch between resource allocation, e.g., money, and the importance of the software to economic activity."
49,It highlights OpenSSL as an
49,"example where limited funding led to developer burnout and security vulnerabilities, even though so much of the Internet's infrastructure relies on it."
49,"With proprietary software, there is usually a connection between software cost and its economic value, though the linkage varies widely."
49,"(How much of software's cost goes into software development,"
49,"testing, bug fixing, and security analysis has even greater variability.)"
49,"With open source, there is even less linkage."
49,The article explores various methods to increase the linkage.
49,"It is a complex problem, both to get money, and to distribute money in a way that helps and does not harm open source communities."
49,Postgres has been fortunate in this regard.
49,"Funding from Red Hat and Japanese companies (Fujitsu, ntt, sra) helped support critical Postgres activities in the community's early"
49,years.
49,"The special nature of database software has formed an environment where the Postgres community has had healthy infrastructure,"
49,"governance, management, and contributors for over a decade."
49,"On an individual level, we do see these problems."
49,"Some contributors take on tasks that yield them little or no short-term benefit, while incurring huge time and emotional costs."
49,Some users ask the
49,"community for assistance with the minimal amount of information, assuming we will guess their intent, or that we will dig into their bug report with them doing no research."
49,I often just delete such emails
49,from my mailbox.
49,"I sometimes get private emails asking for assistance,"
49,"so I created a stock email reply that starts with, ""Due to time constraints, I do not directly answer general PostgreSQL questions,"""
49,"and information on where to get help, e.g., email lists, irc,"
49,faq.
49,This Slashdot post paints a good picture of how to handle the many demands of open
49,source development.
49,"Open source has been mainstream for at least a decade, so it will be interesting to see if generally-accepted solutions are ever found, or if this will continue be an area of confusion and hand-wringing,"
49,even though open source software use continues to grow.
49,View or Post Comments
49,Three Postgres Adoption Groups in Enterprises
49,"Wednesday, September 30, 2020"
49,"Having watched Postgres grow in popularity over the years, I have seen my share of organizations with competing teams, some promoting Postgres, other dismissing it."
49,I came up with this
49,diagram (slide 23) which shows the three groups typically involved in deciding Postgres adoption.
49,"The groups are Managers, Administrators,"
49,and Developers.
49,"In this diagram, each group has things that motivate them listed below the group name."
49,Postgres adoption goes most smoothly when all three groups see value in Postgres.
49,"When talking to different groups in a company, you should consider what things motivate the group you are speaking to."
49,"You will also find the diagram helpful in identifying which groups are not excited about Postgres, and how to motivate them."
49,View or Post Comments
49,Cloud Vendor Monetization of Open Source
49,"Monday, September 28, 2020"
49,"Since open source became a powerful force in the software world, it has gone through several phases."
49,"The first phase was built around universities and volunteers, with little business involvement."
49,"open source grew, companies like Red Hat were created to simplify deployment of open source software in enterprises."
49,"With the popularity of open source, companies that distributed their software as open"
49,"source, but were company-controlled, started to proliferate, like MySQL."
49,The distinction between openly-developed open source software
49,and company-developed open source software is still not widely understood.
49,"One big distinction is that, for company-developed open source software, a company controls the development process, and bears"
49,"the full cost of development, which leads to greater customer lock-in and potential future costs as the company maximizes its profits."
49,Cloud vendors have upended many of the plans of companies built around open source.
49,"While open source is often valued by vendors and customers for its rapid proliferation and deployment, there was always"
49,a hope that expertise in the open source software would yield sufficient revenue opportunities.
49,"What has upended their plans are cloud vendors, who already have a customer relationship by providing cloud"
49,infrastructure.
49,"They use open source software as an up-sell to cloud customers, bypassing vendors that specialize in open source."
49,This excellent
49,"article exposes much of this shift, particularly with this statement:"
49,"From an economic perspective (which is what all the industry think pieces and analogies are about), the clouds seem to make a better business from open source than the companies built around particular"
49,"projects. If you squint, open source could be seen as a very generous charitable donation to some of the largest and wealthiest corporations on the planet."
49,"Instead of open source vendors providing software to be used by potential customers, it is also being used by actual competitors, often much larger and more visible competitors."
49,Such is the problem of
49,"open source, that you can't easily control how it is used, though some company-developed open source companies, that do control the software license, are"
49,"taking steps to avoid this situation, with predictable reactions from the cloud vendors."
49,"The biggest ""wow"" moment for me in reading this article was the analysis of Red Hat."
49,"Company-developed open source companies were always at risk, since they bore the full cost of software development and"
49,"had a clear monetization strategy — grow and then maximize monetization, but Red Hat was different."
49,It was the poster-child of pure open source (collect from others and bundle) that should have been
49,"more immune to the cloud, but the graphs in the blog clearly show that the purchase of Red Hat by ibm was due to cloud competition on Red Hat."
49,This
49,blog entry from the same person goes into even more gory details.
49,It is not really clear where open source monetization is headed.
49,"If cloud vendors were smart, they would keep open source-dedicated companies alive with enough revenue to continue funding open source"
49,development and innovation.
49,"While open source companies have often yielded high valuations, they have rarely yielded high profits, except perhaps, years ago, for Red Hat."
49,The high-profits world now seems
49,"even more distant for open source companies, as cloud vendors, with existing customer relationships, massive standardized infrastructure and services, and economies of scale, siphon even more profit"
49,from these businesses.
49,View or Post Comments
49,Cloud Vendors as a Barrier
49,"Friday, September 25, 2020"
49,Cloud vendors are barriers like department stores and supermarkets are barriers.
49,Huh?
49,"People associate these entities with providing a huge variety of goods and services, all in one place."
49,How can that be
49,a barrier?
49,"Well, you are looking at it from the consumer perspective."
49,"For the producer, they are a mixed benefit."
49,"These ""super sellers"" allow access to much larger markets for most single-product producers, but"
49,they can have negatives for producers:
49,They become the place consumers associate with your product
49,They have the relationship with the consumer
49,You can easily be replaced if a better product appears
49,They take a markup
49,"As the producer of a physical product, it is up to you to decide if working with department stores and supermarkets is a positive or negative."
49,"However, with open source software, there is no calculus."
49,"Unless your open source license prohibits modified or unmodified hosting of your software on cloud servers, you have no control over whether a cloud vendor is the way consumers interact with your"
49,open source software.
49,"The cloud vendor is the one who downloads the software, configures it, perhaps supports it, and guarantees uptime."
49,The cloud vendor can leverage software revenue opportunities.
49,"avoid cloud usage, some software producers have chosen or created licenses that restrict such usage:"
49,Timescale
49,CockroachDB
49,Elasticsearch
49,MongoDB
49,Redis
49,Agpl license
49,"In response to the license handling of Elasticsearch, aws forked (1,"
49,"2, 3) the Apache"
49,2.0-licensed Elasticsearch source code and started writing code to replace the proprietary features.
49,Sometimes even license changes don't protect open source projects from cloud vendor barriers.
49,This is nothing new.
49,"Commercial companies like Red Hat have supported open source software since their inception, and have generated revenue from that relationship and bundling."
49,Cloud vendors are just
49,"another set of companies that are making open source easier to use, and benefiting from it."
49,"While software-bundle vendors like Red Hat can customize the software for their bundle, cloud vendors can also"
49,optimize the software for their hardware and infrastructure.
49,This is a clear value to customers that is hard for software producers to match.
49,"This article discusses the history of open source business models,"
49,This
49,"article describes cloud vendor behavior as ""strip mining""."
49,There was even a recent
49,conference where open source vendors met to discuss how to deal with cloud vendor competition (sponsored by aws).
49,There is one big distinction in all the open source software products I have listed above — they are all company-controlled open
49,"source, meaning the development is mostly controlled by a single company, which monetizes use of the software, while distributing it as open source."
49,This is drastically different from
49,community-controlled open source projects like Postgres.
49,"There are far fewer challenges to community-controlled open source projects from cloud vendors, mostly because there is no monetization goal."
49,"There is some concern that cloud vendor relationships with users will diminish community contributions, but it is not clear if this true."
49,Cloud vendors clearly increase use of community-controlled
49,"software, and to the extent the cloud vendors reference open source communities in their interaction with customers, it helps these"
49,communities.
49,"Just as bakeries, flower shops, produce markets, meat, cheese, and seafood sellers struggle to survive when supermarkets provide convenient, if somewhat less"
49,"diverse, products, so company-controlled open source will suffer from cloud vendors."
49,"For open source communities, the only real risk is that companies that support its open source developers will"
49,"struggle, and therefore reduce paid developers working on open source projects."
49,"That connection is hard to measure from an open source project perspective, so we will just have to wait and see how things"
49,evolve.
49,View or Post Comments
49,Developers in Front
49,"Wednesday, September 23, 2020"
49,Most companies have marketing and sales people as the visible part of their company.
49,"Technical people, even in technology companies, are often kept in the back, and only brought out for brief periods when"
49,needed.
49,"Open source is different — there are no marketing or sales teams, so software developers are the faces of projects."
49,This gives technical people an opportunity to attain world-wide recognition
49,for their efforts.
49,"There are not many places technical people can truly shine, but open source is one such opportunity."
49,View or Post Comments
49,The Berkeley 39
49,"Monday, September 21, 2020"
49,Postgres turns 34 this year.
49,"Michael Stonebraker, during his 2015 Turing Award speech (which I blogged about"
49,"previously), included the names of the 39 Berkeley students (plus co-leader Larry Rowe) who helped write the original version of Postgres."
49,"I was hoping to add this list to the Postgres release notes, but we recently stopped including back branch release notes in current releases, so now there is no good place to put them."
49,"As a thanks to them,"
49,"and with the help of the community, I am listing them below:"
49,Jeff Alton
49,Paul Aoki
49,James Bell
49,Jennifer Caetta
49,Philip Chang
49,Jolly Chen
49,Ron Choi
49,Matt Dillon
49,Zelaine Fong
49,Adam Glass
49,Jeffrey Goh
49,Steven Grady
49,Serge Granik
49,Marti Hearst
49,Joey Hellerstein
49,Michael Hirohama
49,Chin-Heng Hong
49,Wei Hong
49,Anant Jhingran
49,Greg Kemnitz
49,Marcel Kornacker
49,Case Larsen
49,Boris Livshitz
49,Jeff Meredith
49,Ginger Ogle
49,Mike Olson
49,Nels Olson
49,Lay-Peng Ong
49,Carol Paxson
49,Avi Pfeffer
49,Spyros Potamianos
49,Sunita Surawagi
49,David Muir Sharnoff
49,Mark Sullivan
49,Cimarron Taylor
49,Marc Teitelbaum
49,Yongdong Wang
49,Kristen Wright
49,Andrew Yu
49,I knew some of the names and initials from the C comments.
49,"Just like those of us who have worked on Postgres for decades, I doubt they suspected that their code would be used so many years later."
49,View or Post Comments
49,Postgres and the Artificial Intelligence Landscape
49,"Friday, September 18, 2020"
49,"I recently wrote a presentation, Postgres and the Artificial Intelligence Landscape, which covers the basics of artificial intelligence and"
49,shows how Postgres can be used for this purpose.
49,This week I presented it at the Chicago PostgreSQL Meetup Group so I am now
49,publishing the slides.
49,View or Post Comments
49,Why Not to Choose Postgres
49,"Wednesday, September 16, 2020"
49,"Postgres has made great strides in adding features to match proprietary databases, and it has many complex features that other databases don't have."
49,"However, that doesn't mean it is the best fit for"
49,every organization.
49,There are still reasons not to use Postgres:
49,"Skills in another relational database, and no desire or value to learn Postgres"
49,Custom applications written for another database that you don't want to modify to work with Postgres
49,"Using externally-developed applications, tools, or frameworks that don't support Postgres"
49,Storage of non-transactional or cache data where Postgres's overhead is significant
49,Multi-host workloads with simple queries where NoSQL is a win
49,"Small, single-user systems, where SQLite is best"
49,This email thread had lot of discussion on the topic.
49,What is interesting is that decades-old
49,"complaints about missing features, reliability, and performance are no longer mentioned."
49,View or Post Comments
49,Migrating Away from Hacks
49,"Monday, September 14, 2020"
49,Database applications are initially written using the simplest queries possible.
49,"During testing and in production, some application tasks might have unacceptable performance."
49,This is where
49,"re-architecturing happens, and where simple queries and data schema layouts can get complex."
49,"They might get complex because it is required to accomplish the task, or it might be because of limitations"
49,in how the database software handles certain queries.
49,Database and tooling upgrades can require further complex additions.
49,"When switching to a new database like Postgres, all that complexity comes along for the ride."
49,"Sometimes complexity added to work around deficiencies in other databases work fine in Postgres, but often"
49,that complexity has to be removed to get good performance in Postgres.
49,There also might be cases where complexity has to be added to get good performance in Postgres.
49,"The bottom line is that complexity is bad for applications, so add complexity only when necessary."
49,"Wise application developers remove unnecessary complexity periodically, but it can be hard to know if"
49,database upgrades have made some complexity unnecessary.
49,Porting to a new database is an ideal time to reevaluate if application simplification is possible.
49,View or Post Comments
49,Postgres in the Cloud: The Hard Way
49,"Friday, September 11, 2020"
49,"I recently wrote a presentation, Postgres in the Cloud: The Hard Way, which shows how to create a cloud instance, and install and run"
49,"Postgres, purely from the command line."
49,This helps show how all the pieces fit together.
49,I recently presented this at pgDay Israel so I am now publishing the slides.
49,recording of the presentation is available.
49,View or Post Comments
49,"Changing Cars, Changing Databases"
49,"Thursday, September"
49,"3, 2020"
49,"It would be very easy if I drove the same car regularly, but because of my family size and travels, I don't have that luxury."
49,Some cars I drive have smart
49,"keys, some mechanical keys."
49,"Some have gas tank doors on the driver's side, others from the passenger side."
49,They steer
49,"differently, have different acceleration capabilities, even different service requirements."
49,"I have gotten used to switching cars, but still get confused when I have to fuel the car since I have to"
49,remember which side has the gas tank door.
49,"If I had driven the same car for years, switching to a different car would be even harder — that is the circumstance for people moving from other databases to Postgres."
49,They have often driven the same
49,"car/database for years, perhaps decades."
49,They know the intricacies of how those databases behave that even the average database vendor employee might not know.
49,Switching to another database can be
49,"traumatic, especially if their job relies on databases running reliably."
49,"They probably have all sorts of tricks and procedures for ensuring uptime, and, when switching databases, it isn't always clear if"
49,"those procedures should be modified or abandoned, and what new procedures will be necessary."
49,There are some tasks that are common to all database:
49,"sql, backups, fail over, performance monitoring, but it would be nice if there was a cheat sheet of all the changes necessary in moving from"
49,one database to another.
49,"There are some guides in switching from other databases to Postgres, but they don't cover"
49,"every detail, and many user procedures are based on their workload."
49,It isn't always possible to even know how workloads will behave in Postgres.
49,"It is not clear exactly what recommendation I can give for users switching to Postgres, except to review all existing procedures to determine if they are still necessary or need modification, and if new"
49,procedures are necessary.
49,"One thing is for sure — changes will be needed, and rigidly following procedures used for your previous database is not a wise plan."
49,View or Post Comments
49,The Inner Workings of Oracle Development
49,"Monday, August 31, 2020"
49,"Having worked in open source for decades, where every success and failure is public knowledge, I have always wondered how proprietary"
49,"development is done, particularly for databases."
49,"I have gotten some glimpses into that world from former employees, but this Y combinator thread is"
49,the most extensive view of Oracle development I have ever seen.
49,"It has useful lessons for all developers, as well as comments about the source code of"
49,other relational databases.
49,The first thing that jumped out to me is how overwhelmed the staff appears to be by the source code's complexity.
49,Code cleanup is never fun but it is necessary for efficient coding.
49,"For companies, it is"
49,"hard to justify developer time spent on code cleanup since the benefits of cleanup are often minor for each task but, in aggregate over time, huge."
49,"Code cleanup can rarely be tied to a direct deliverable,"
49,so it is often overlooked.
49,"For open source, clean code encourages developers to spend their free time coding, so there is a more direct incentive to do cleanup, plus deliverables for open source are much"
49,less rigid.
49,There is a mention of Postgres in the thread:
49,I'm sure some of the difference (25M [Oracle] vs. 1.3M [Postgres}) can be attributed to code for Oracle features missing in PostgreSQL. But a significant part of it is due to careful development
49,process mercilessly eliminating duplicate and unnecessary code as part of the regular PostgreSQL development cycle.
49,"It's a bit heartbreaking at first (you spend hours/days/weeks working on something), and then a fellow [Postgres] hacker comes and cuts off the unnecessary pieces, but in the long run I'm grateful we do that."
49,"Second, there is the reliance on test-driven development."
49,"Test-driven development certainly has value, but for Oracle database developers, it appears to"
49,have become a huge drag on development.
49,"The tests took 24+ hours to run, and because the code was so complex, it was hard to know if the changes made would even pass the tests."
49,This
49,video evaluates the pathologies of test-drive development.
49,I think this
49,comment captures it:
49,"Tdd [test-drive development] is like relying on debugger to solve your problem. Is debugger a good tool? yes,it is a great tool. But using it as an excuse to avoid understanding what happens under"
49,the hood is plain wrong.
49,Postgres has succeeded and gained a reputation for reliability without relying on tests (though we have them) but rather by reviewing patches and
49,looking for interactions with other parts of the system.
49,"We are also willing to restructure code where needed, and break backward"
49,compatibility where warranted.
49,"However, our big advantage is a development team that is unmatched in the relational database industry, and an open source development model that efficiently"
49,harnesses their skills.
49,View or Post Comments
49,Development Methods
49,"Wednesday, August 26, 2020"
49,I think there are three types of software development:
49,closed development & closed source distribution
49,closed development & open source distribution
49,open development & open source distribution
49,"#1 is proprietary development, like Oracle and DB2."
49,"#2 is development like MySQL and Mongo, where a single company controls all the development work, but the source is released, often with restrictions on how it is used."
49,Since a company controls the
49,"development, they can focus on specific big customers and adjust usage restrictions in ways that"
49,"encourage payment for certain features, for non-gpl use, or for cloud use."
49,"#3 is Postgres, Linux, etc."
49,The Postgres core team's independence from company control is just one aspect of the open development of Postgres.
49,Understanding the development and distribution categories of software can reliably predict how that software will evolve over time.
49,View or Post Comments
49,Why Database Software Is Unique
49,"Monday, August 24, 2020"
49,"Having worked with databases for over three decades, I have found there are three aspects of database software that make it unique:"
49,Variable workloads
49,Performance requirements
49,Durability
49,Most other software do not have these requirements.
49,"Because of them, databases typically need more tuning, monitoring, and maintenance than other software, like Java applications or even"
49,operating systems.
49,"These requirements also help make companies supporting database software profitable, including open source-based ones."
49,View or Post Comments
49,Standard Deviation
49,"Friday, August 21, 2020"
49,"Postgres has a lot of aggregate functions to make your life, and data analysis, easier."
49,Postgres 12 has 42 aggregate functions.
49,You can
49,see a full list by typing \daS in psql.
49,"While you are probably familiar with common aggregates like count and avg, Postgres supports many more."
49,"There are aggregates for arrays, json, and statistical functions."
49,When
49,"dealing with numeric data, people are usually concerned about the number or total of the entries, but when trying to determine how much values vary, standard deviation is helpful:"
49,"SELECT stddev(x) FROM (VALUES (25), (50), (75)) AS t(x);"
49,stddev
49,---------------------
49,25.0000000000000000
49,"SELECT stddev(x) FROM (VALUES (49), (50), (51)) AS t(x);"
49,stddev
49,------------------------
49,1.00000000000000000000
49,"Both sets of three values total 150, but the first set varies by 25, while the second set varies by only one."
49,"If you are analyzing your bank balance, you might not care how much the numbers vary, but if"
49,"you are looking at web site response times, the variability of the numbers can be significant."
49,View or Post Comments
49,Keyset Pagination in Action
49,"Monday, August 17, 2020"
49,"Having explained keyset pagination, let's look at how it behaves while inserts and deletes are happening in the table."
49,"Using the queries from the previous blog entry, let's see how deletions from previous pages affect displaying of page four from page three:"
49,"SELECT product_id, description"
49,FROM product
49,WHERE product_id > 30
49,ORDER BY product_id
49,LIMIT 11;
49,product_id | description
49,------------+-------------
49,31 | Product 31
49,32 | Product 32
49,33 | Product 33
49,34 | Product 34
49,35 | Product 35
49,36 | Product 36
49,37 | Product 37
49,38 | Product 38
49,39 | Product 39
49,40 | Product 40
49,41 | Product 41
49,DELETE FROM product WHERE product_id <= 5;
49,"SELECT product_id, description"
49,FROM product
49,WHERE product_id > 30
49,ORDER BY product_id
49,LIMIT 11;
49,product_id | description
49,------------+-------------
49,31 | Product 31
49,32 | Product 32
49,33 | Product 33
49,34 | Product 34
49,35 | Product 35
49,36 | Product 36
49,37 | Product 37
49,38 | Product 38
49,39 | Product 39
49,40 | Product 40
49,41 | Product 41
49,"As you can see, the delete had no affect because we were anchored on the last value on the page, rather than, if we had used offset, the first value of the result set."
49,Inserts
49,before page four are also ignored:
49,INSERT INTO product
49,"SELECT id, 'Product ' || id::TEXT"
49,"FROM generate_series(1, 3) AS t(id);"
49,"SELECT product_id, description"
49,FROM product
49,WHERE product_id > 30
49,ORDER BY product_id
49,LIMIT 11;
49,product_id | description
49,------------+-------------
49,31 | Product 31
49,32 | Product 32
49,33 | Product 33
49,34 | Product 34
49,35 | Product 35
49,36 | Product 36
49,37 | Product 37
49,38 | Product 38
49,39 | Product 39
49,40 | Product 40
49,41 | Product 41
49,"Of course, inserts and deletes after the current page would display normally."
49,We can also easily go backward from page four:
49,WITH page AS
49,"SELECT product_id, description"
49,FROM product
49,WHERE product_id < 31
49,ORDER BY product_id DESC
49,LIMIT 11
49,"SELECT product_id, description"
49,FROM page
49,ORDER BY product_id;
49,product_id | description
49,------------+-------------
49,20 | Product 20
49,21 | Product 21
49,22 | Product 22
49,23 | Product 23
49,24 | Product 24
49,25 | Product 25
49,26 | Product 26
49,27 | Product 27
49,28 | Product 28
49,29 | Product 29
49,30 | Product 30
49,Going from page three to page two:
49,"(Remember, when paging backward, the first row (e.g., 20) is used as a marker to indicate if another previous page exists.)"
49,WITH page AS
49,"SELECT product_id, description"
49,FROM product
49,WHERE product_id < 21
49,LIMIT 11
49,"SELECT product_id, description"
49,FROM page
49,ORDER BY product_id;
49,product_id | description
49,------------+-------------
49,10 | Product 10
49,11 | Product 11
49,12 | Product 12
49,13 | Product 13
49,14 | Product 14
49,15 | Product 15
49,16 | Product 16
49,17 | Product 17
49,18 | Product 18
49,19 | Product 19
49,20 | Product 20
49,Trying to view page one causes the problem outlined in the previous blog post because only eight rows are returned:
49,WITH page AS
49,"SELECT product_id, description"
49,FROM product
49,WHERE product_id < 11
49,ORDER BY product_id DESC
49,LIMIT 11
49,"SELECT product_id, description"
49,FROM page
49,ORDER BY product_id;
49,product_id | description
49,------------+-------------
49,1 | Product 1
49,2 | Product 2
49,3 | Product 3
49,6 | Product 6
49,7 | Product 7
49,8 | Product 8
49,9 | Product 9
49,10 | Product 10
49,"This is where it is suggested that the first page be regenerated in its entirety, and the user informed:"
49,"SELECT product_id, description"
49,FROM product
49,ORDER BY product_id
49,LIMIT 11;
49,product_id | description
49,------------+-------------
49,1 | Product 1
49,2 | Product 2
49,3 | Product 3
49,6 | Product 6
49,7 | Product 7
49,8 | Product 8
49,9 | Product 9
49,10 | Product 10
49,11 | Product 11
49,12 | Product 12
49,13 | Product 13
49,Page two would be generated using the new rows from page one:
49,"SELECT product_id, description"
49,FROM product
49,WHERE product_id > 12
49,ORDER BY product_id
49,LIMIT 11;
49,product_id | description
49,------------+-------------
49,13 | Product 13
49,14 | Product 14
49,15 | Product 15
49,16 | Product 16
49,17 | Product 17
49,18 | Product 18
49,19 | Product 19
49,20 | Product 20
49,21 | Product 21
49,22 | Product 22
49,23 | Product 23
49,Non-sort key updates are uninteresting.
49,"Primary key updates can easily cause rows from earlier pages to appear on later pages, or rows from later pages to appear on earlier previous pages, if"
49,the primary key is updated across the current page view result set.
49,Let's show the page previous to page two before and after two updates:
49,WITH page AS
49,"SELECT product_id, description"
49,FROM product
49,WHERE product_id < 13
49,ORDER BY product_id DESC
49,LIMIT 11
49,"SELECT product_id, description"
49,FROM page
49,ORDER BY product_id;
49,product_id | description
49,------------+-------------
49,1 | Product 1
49,2 | Product 2
49,3 | Product 3
49,6 | Product 6
49,7 | Product 7
49,8 | Product 8
49,9 | Product 9
49,10 | Product 10
49,11 | Product 11
49,12 | Product 12
49,UPDATE product SET product_id = 4 WHERE product_id = 14;
49,UPDATE product SET product_id = 5 WHERE product_id = 15;
49,WITH page AS
49,"SELECT product_id, description"
49,FROM product
49,WHERE product_id < 13
49,ORDER BY product_id DESC
49,LIMIT 11
49,"SELECT product_id, description"
49,FROM page
49,ORDER BY product_id;
49,product_id | description
49,------------+-------------
49,2 | Product 2
49,3 | Product 3
49,4 | Product 14
49,5 | Product 15
49,6 | Product 6
49,7 | Product 7
49,8 | Product 8
49,9 | Product 9
49,10 | Product 10
49,11 | Product 11
49,12 | Product 12
49,"This causes the problem, also outlined in the previous blog post, where a query of the first page returns 11 rows, meaning the first page results should be recreated and the user informed:"
49,"SELECT product_id, description"
49,FROM product
49,ORDER BY product_id
49,LIMIT 11;
49,product_id | description
49,------------+-------------
49,1 | Product 1
49,2 | Product 2
49,3 | Product 3
49,4 | Product 14
49,5 | Product 15
49,6 | Product 6
49,7 | Product 7
49,8 | Product 8
49,9 | Product 9
49,10 | Product 10
49,11 | Product 11
49,Hopefully this blog post gives you an idea of how keyset pagination behaves in an environment where the result set is changing between page requests.
49,View or Post Comments
49,Keyset Pagination
49,"Wednesday, August 12, 2020"
49,"Having covered pagination in a previous blog post, I want to explore method #5, limit/offset and where, also called"
49,keyset or seek pagination.
49,"This method avoids many of the performance pitfalls and inconsistent results of the other methods, but it comes with its own complexities."
49,"First, let's set expectations."
49,"Let's assume ten results per page, so each page should show ten results, except for the last page, which can contain 1–10 results."
49,The last page should ideally
49,not show zero results.
49,"Let's also suppose we are displaying products, and the page results are ordered by a unique product id."
49,Here is a sample table:
49,"CREATE TABLE product (product_id SERIAL PRIMARY KEY, description TEXT);"
49,INSERT INTO product
49,"SELECT id, 'Product ' || id::TEXT"
49,"FROM generate_series(1, 100) AS t(id);"
49,Generating the first page is quite simple using limit.
49,Paging forward is simple too.
49,"The user is not requesting to see previous pages, so any additions or removals in those pages are not"
49,significant.
49,"They are also not asking to see an updated display of the current page's values, so that can be ignored too."
49,"Using limit and where, you can see the next ten values in order"
49,relative to the currently viewed page.
49,"For example, you are on page 3 (result set rows 21–30), and product 30 is the last product on the page — this query will display the next page:"
49,"SELECT product_id, description"
49,FROM product
49,WHERE product_id > 30
49,ORDER BY product_id
49,LIMIT 11;
49,product_id | description
49,------------+-------------
49,31 | Product 31
49,32 | Product 32
49,33 | Product 33
49,34 | Product 34
49,35 | Product 35
49,36 | Product 36
49,37 | Product 37
49,38 | Product 38
49,39 | Product 39
49,40 | Product 40
49,41 | Product 41
49,"I have used limit 11 above so if less than eleven rows are returned, the ""Next Page"" option can be disabled, since there are no more results right now, though there might be later."
49,"Obviously,"
49,only the first ten rows should be displayed.
49,An index can often be used for this query.
49,Offset would be used to jump forward or backward multiple pages.
49,Paging backward gets tricky.
49,"As stated above, only the last page can display less then ten results, but rows might be added and removed from previous pages, so just because we are on page 3 doesn't mean"
49,there are twenty previous rows.
49,can't just do LIMIT 11 OFFSET 10 to display the previous page without a repeatable read or serializable snapshot — we might get more or less than ten rows.
49,"To do this properly, assuming the first displayed row on page 3 has a product_id of 21, it is necessary to reverse the order by clause, use limit, and then use a common table expression"
49,to re-order the results for display:
49,WITH page AS
49,"SELECT product_id, description"
49,FROM product
49,WHERE product_id < 21
49,ORDER BY product_id DESC
49,LIMIT 11
49,"SELECT product_id, description"
49,FROM page
49,ORDER BY product_id;
49,product_id | description
49,------------+-------------
49,10 | Product 10
49,11 | Product 11
49,12 | Product 12
49,13 | Product 13
49,14 | Product 14
49,15 | Product 15
49,16 | Product 16
49,17 | Product 17
49,18 | Product 18
49,19 | Product 19
49,20 | Product 20
49,"This gets the 11 result set rows before page 3, and in this case the first row is used to indicated if there is a valid ""Previous Page""."
49,However there are more complexities:
49,"If 11 rows are returned and your user interface suggests you should be on the first page, you should probably repopulate the first page and inform the user."
49,"If only ten rows are returned, you can assume you are on the first page, even if that was not true based on previous queries."
49,"If you get less then ten rows, you will need to re-issue the initial query to populate the first page with ten results, and perhaps inform the user since result rows might be duplicated from previous"
49,pages.
49,"As stated earlier, the keyset pagination method has many advantages in terms of performance and consistency, but it can be complex to implement because you are effectively getting a new snapshot of the"
49,"data each time you ask for another page, and making that line up with pagination requirements can be tricky."
49,View or Post Comments
49,Pagination Tips
49,"Monday, August 10, 2020"
49,"We have all used applications that allow searches which return results in sections or ""pages"", like for products or flights."
49,"For example, you might search for a product, and there might be 1,500 matches,"
49,"but you don't see 1,500 results — you only see ten results (1–10), and you have the option of seeing the next ten (11–20), and the next ten (21–30), etc."
49,Pagination is done
49,for several reasons:
49,The user probably doesn't want to see all the results at once
49,The user probably doesn't want to ever see all of the results
49,It might be inefficient to transfer and display all results
49,There are several ways to enable pagination using a client/server architecture like databases.
49,"One approach is to transfer all results to the client, and let the client do the pagination."
49,That handles
49,"the first issue, but it doesn't handle cases where the transfer and storage of many results is inefficient."
49,"While this might work for 1,500 results, it is going to perform badly for 150 million results."
49,"Therefore, some kind of pagination support on the server side is required if large result sets are possible."
49,There are four common approaches:
49,Cursors:
49,Open a non-with hold cursor in a transaction block and fetch rows in
49,pages.
49,The downside of this is that the entire result set often has be computed before returning any result.
49,"Also, the transaction must be kept open while users are paging through the results, leading to"
49,"potentially long-running transactions, and therefore decreased cleanup efficiency."
49,Using
49,idle_in_transaction_session_timeout prevents sessions from keeping transactions open too long.
49,"Using with hold cursors avoids keeping a transaction open during page fetches, but it does require the entire result set to be stored in server memory."
49,limit/offset:
49,These keywords allow select to return partial
49,"results, ideally suited to pagination."
49,Limit/offset also allow the optimizer (slide 52) to efficiently access limited result
49,sets by often using indexes which are only efficient for small result sets.
49,This also avoids creation of the entire result in memory.
49,Limit/offset with repeatable read: Limit/offset alone has a problem displaying accurate
49,data if the underlying results change during page views.
49,"For example, if I am viewing results 21–30, and a row is added which should appear in range 11–20, if I request the next page,"
49,what was item 30 will appear again as item 31.
49,"If an earlier item is deleted, what was item 31 will now be item 30, and will not appear in range 31–40."
49,"Effectively, paging back and forward will"
49,"cause items to not appear, or appear twice."
49,The fix for this is to use limit/offset in a repeatable read or serializable
49,"transaction, so that all page requests use the same snapshot."
49,This does have the same problem that non-with hold
49,"cursors had with long-running transactions, so the use of idle_in_transaction_session_timeout is recommended."
49,Limit/offset and where: It is possible to use limit/offset with a where restriction that only returns rows after the previous last row returned based on the result
49,"set ordering, or before the previous first row returned for previous page result sets."
49,This avoids the duplicate/missing row problem of #4 without requiring a transaction to remain open.
49,These blog
49,"entries (1, 2) have more details about this method."
49,"One optimization to reduce query requests is to fetch a large page range in the application, display smaller page ranges to the user as requested, and request more page ranges as needed."
49,One other thing people expect from pagination is an estimate of the number of matches.
49,While you can run count(&ast) to find
49,"the number of matches, this can be very inefficient."
49,"A more creative approach is to use the optimizer's statistics as an estimate, since it"
49,already maintains statistics to generate efficient plans.
49,"For example, if you create this function:"
49,CREATE OR REPLACE FUNCTION row_count_estimate(query TEXT) RETURNS TEXT
49,AS $$
49,DECLARE str TEXT;
49,row_count INTEGER;
49,BEGIN
49,-- get just the first row
49,EXECUTE $a$EXPLAIN $a$ || query INTO str;
49,-- extract the row value
49,SELECT substring(substring(str FROM ' rows=[0-9]+') FROM 7) INTO row_count;
49,RETURN row_count;
49,END;
49,LANGUAGE plpgsql;
49,you can use it to estimate the number of rows in a result set. (Don't specify limit/offset since we want the total count.)
49,SELECT row_count_estimate('SELECT * FROM product WHERE name LIKE ''a%''');
49,row_count_estimate
49,--------------------
49,"To improve the accuracy of the estimates, you can increase the frequency of statistics updates by modifying autovacuum's analyze settings (autovacuum_analyze_) either at the"
49,cluster or per-table level. You can
49,also run analyze manually.
49,"You can also increase the granularity of statistics, which can improve estimates."
49,This can also be set a the
49,cluster or per-table level.
49,A creative option is
49,"to get an exact row count using a count(&ast) query only if explain reports a low row count, meaning count(&ast) would be inexpensive."
49,View or Post Comments
49,Invalid Times
49,"Wednesday, August"
49,"5, 2020"
49,Have you ever wondered how invalid or ambiguous times are handled by Postgres?
49,"For example, during a daylight saving time transition in the"
49,"usa, time switches either from 2am to 3am, or from 2am back to 1am."
49,"On a fall-forward day (the former), how would 2:30am be represented?"
49,Is 1:30am
49,during a fall-back day represented as the first or second 1:30am of that day?
49,This email thread explains
49,"the problem, and this patch documents the behavior."
49,"(November 4, 2018 was a"
49,"""fall back"" date in the usa.)"
49,"The original poster, Michael Davidson, was showing the first query, and Tom Lane was saying you would need to use one of the two later queries to qualify the 1am time:"
49,SHOW TIME ZONE;
49,TimeZone
49,------------------
49,America/New_York
49,SELECT '2018-11-04 01:00:00'::timestamp WITH TIME ZONE;
49,timestamptz
49,------------------------
49,2018-11-04 01:00:00-05
49,SELECT '2018-11-04 01:00:00 EST'::timestamp WITH TIME ZONE;
49,timestamptz
49,------------------------
49,2018-11-04 01:00:00-05
49,SELECT '2018-11-04 01:00:00 EDT'::timestamp WITH TIME ZONE;
49,timestamptz
49,------------------------
49,2018-11-04 01:00:00-04
49,"I am not sure how to specify this for time zones that don't have daylight/non-daylight saving time abbreviations — I suppose utc offsets would need to be used, e.g.:"
49,SELECT '2018-11-04 01:00:00-05'::timestamp WITH TIME ZONE;
49,timestamptz
49,------------------------
49,2018-11-04 01:00:00-05
49,"Interestingly, in this case the time zone abbreviation contains more information than the more generic time zone string 'America/New_York'."
49,View or Post Comments
49,Differences Between Dates
49,"Monday, August"
49,"3, 2020"
49,What is the difference between two dates?
49,"You would think there was one answer, but there isn't."
49,"You can give an answer in calendar terms (years/months/days), the number of days, or the number of"
49,seconds.
49,Postgres offers all of these options:
49,"SELECT age('2019-12-25', '2018-06-01');"
49,age
49,-----------------------
49,1 year 6 mons 24 days
49,SELECT '2019-12-25'::timestamp - '2018-06-01'::timestamp;
49,?column?
49,----------
49,572 days
49,SELECT '2019-12-25'::timestamptz - '2018-06-01'::timestamptz;
49,?column?
49,-------------------
49,572 days 01:00:00
49,SELECT '2019-12-25'::date - '2018-06-01'::date;
49,?column?
49,----------
49,572
49,SELECT EXTRACT(EPOCH FROM '2019-12-25'::timestamptz) - EXTRACT(EPOCH FROM '2018-06-01'::timestamptz);
49,?column?
49,----------
49,49424400
49,Unless you are Mr. Spock
49,you probably can see the value in all of these methods.
49,Age() provides the
49,simplest option for human consumption.
49,"Timestamp subtraction (no tz) returns an interval, which can calculate day and second differences, but doesn't handle"
49,daylight saving time changes.
49,"Timestamptz subtraction also returns interval, but handles daylight saving time changes."
49,Subtraction of date
49,"values returns an integer number of days and is good for cases where the smallest unit of calculation is a day, like bank loan interest or hotel nights."
49,Extract with epoch is ideal when
49,"exact precision is required, like electricity consumption or astronomical measurements."
49,You can also think of it as the time difference calculations they ignore.
49,"For example, age() compares months/days/seconds units to find the simplest difference, and ignores daylight saving time changes,"
49,and sometimes even ignores leap days. Date and timestamp subtraction ignores daylight saving time changes.
49,Subtraction using timestamptz and
49,extract with epoch honor both.
49,"(Unfortunately, Postgres ignores leap"
49,seconds).
49,"Here is an example of age() ignoring leap days when the day of the month is the same, but reflecting leap days if the the days of the month are different:"
49,"-- leap year, same day of the month"
49,"SELECT age('2020-03-01', '2020-02-01');"
49,age
49,-------
49,1 mon
49,"-- non-leap year, same day of the month"
49,"SELECT age('2021-03-01', '2021-02-01');"
49,age
49,-------
49,1 mon
49,"-- leap year, different day of the month"
49,"SELECT age('2020-03-01', '2020-02-02');"
49,age
49,---------
49,28 days
49,"-- non-leap year, different day of the month"
49,"SELECT age('2021-03-01', '2021-02-02');"
49,age
49,---------
49,27 days
49,This is because age() tries to find the simplest difference.
49,Here is an example of daylight saving time changes using age() and subtraction:
49,"-- Daylight saving time started in America/New_York on March 8, 2020 02:00:00"
49,SHOW timezone;
49,TimeZone
49,------------------
49,America/New_York
49,"SELECT age('2020-03-09', '2020-03-08');"
49,age
49,-------
49,1 day
49,"SELECT age('2020-03-09 00:00:00'::timestamptz, '2020-03-08 00:00:00'::timestamptz);"
49,age
49,-------
49,1 day
49,SELECT '2020-03-09'::date - '2020-03-08'::date;
49,?column?
49,----------
49,SELECT '2020-03-09'::timestamp - '2020-03-08'::timestamp;
49,?column?
49,----------
49,1 day
49,SELECT '2020-03-09'::timestamptz - '2020-03-08'::timestamptz;
49,?column?
49,----------
49,23:00:00
49,-- a 23-hour day
49,SELECT EXTRACT(EPOCH FROM '2020-03-09'::timestamptz) - EXTRACT(EPOCH FROM '2020-03-08'::timestamptz);
49,?column?
49,----------
49,82800
49,-- a 24-hour day
49,SELECT EXTRACT(EPOCH FROM '2020-03-10'::timestamptz) - EXTRACT(EPOCH FROM '2020-03-09'::timestamptz);
49,?column?
49,----------
49,86400
49,"If you are doing a later calculation using a computed date difference, consider how accurate you want the result to be:"
49,Do you want it to be the same day based on the calendar?
49,Use age()
49,Do you want the number of days to be exactly the same?
49,Use date or timestamp subtraction
49,Do you want the number of seconds to be exactly the same?
49,Use timestamptz subtraction or extract with epoch
49,I have already written about the complexities of using and computing
49,interval values.
49,View or Post Comments
49,Computing Interval Values
49,"Friday, July 31, 2020"
49,"The interval data type stores time duration as months, days, and seconds."
49,Years are represented as a fixed
49,"number of months, and hours and minutes as a fixed number of seconds."
49,Using interval values makes time computation very simple:
49,"-- Daylight saving time started in America/New_York on March 8, 2020 02:00:00"
49,SHOW timezone;
49,TimeZone
49,------------------
49,America/New_York
49,SELECT '2020-03-07 00:00:00'::timestamptz + '2 days';
49,?column?
49,------------------------
49,2020-03-09 00:00:00-04
49,SELECT '2020-03-07 00:00:00'::timestamptz + '48 hours';
49,?column?
49,------------------------
49,2020-03-09 01:00:00-04
49,This computation spans a usa daylight saving time change.
49,The interval data type allows the user to specify whether they want days or
49,"hours added, with different results because of the daylight saving time change."
49,This is a great use of the interval data type.
49,Using queries that output interval values is more nuanced.
49,"While you can specify units on input, e.g. days, hours, seconds, you can't specify whether you care about months, days, or seconds in"
49,interval output.
49,"Therefore, different operations are required to produce different outputs, e.g.:"
49,SELECT '2020-03-09 00:00:00'::timestamptz - '2020-03-07 00:00:00'::timestamptz;
49,?column?
49,----------------
49,1 day 23:00:00
49,"SELECT age('2020-03-09 00:00:00'::timestamptz, '2020-03-07 00:00:00'::timestamptz);"
49,age
49,--------
49,2 days
49,"The first query uses subtraction, and computes based on seconds."
49,The second query internally calls
49,timestamp_age().
49,The comment in the source code is illustrative:
49,Calculate time difference while retaining year/month fields.
49,Note that this does not result in an accurate absolute time span since year and month are out of context once the arithmetic is done.
49,"The key part is the ""out of context"" mention."
49,"What it is saying is that when age() returns 2 days, that could have spanned 47, 48, or 49 hours, based on usa time zones, and that"
49,detail is lost in the output.
49,Look at this:
49,SELECT '2020-03-07 00:00:00'::timestamptz + '1 day 23 hours';
49,?column?
49,------------------------
49,2020-03-09 00:00:00-04
49,SELECT '2020-03-07 00:00:00'::timestamptz + '47 hours';
49,?column?
49,------------------------
49,2020-03-09 00:00:00-04
49,SELECT '2020-03-07 00:00:00'::timestamptz + '2 days';
49,?column?
49,------------------------
49,2020-03-09 00:00:00-04
49,SELECT '2020-03-07 00:00:00'::timestamptz + '2 days -1 hours';
49,?column?
49,------------------------
49,2020-03-08 23:00:00-04
49,"The first three return the same result, though the last does not because days is computed first, then hours."
49,"However, when the first and third queries are run in a different context, i.e., for a"
49,"different date, they generate different outputs:"
49,SELECT '2020-03-12 00:00:00'::timestamptz + '1 day 23 hours';
49,?column?
49,------------------------
49,2020-03-13 23:00:00-04
49,SELECT '2020-03-12 00:00:00'::timestamptz + '2 days';
49,?column?
49,------------------------
49,2020-03-14 00:00:00-04
49,"When you are inputting interval values, be conscious of the month, day, and second values you specify, and when calling a function that produces interval output, consider how it computes"
49,its output.
49,View or Post Comments
49,pgFormatter
49,"Wednesday, July 29, 2020"
49,"In my years with Postgres, I have seen some amazingly complex queries posted to the email lists."
49,"I have never understood how people can read complex queries with no formatting, e.g., no keyword"
49,"capitalization, no indenting, no line breaks for new clauses:"
49,"select n.nspname as ""Schema"", p.proname as ""Name"","
49,"pg_catalog.format_type(p.prorettype, null) as ""Result data type"", case"
49,when p.pronargs = 0 then cast('*' as pg_catalog.text) else
49,"pg_catalog.pg_get_function_arguments(p.oid) end as ""Argument data"
49,"types"", pg_catalog.obj_description(p.oid, 'pg_proc') as ""Description"""
49,from pg_catalog.pg_proc p left join pg_catalog.pg_namespace n on n.oid =
49,p.pronamespace where p.prokind = 'a' and n.nspname <> 'pg_catalog' and
49,n.nspname <> 'information_schema' and
49,"pg_catalog.pg_function_is_visible(p.oid) order by 1, 2, 4;"
49,"Obviously, some people can read such queries, but I never can."
49,I rely on clean formatting to conceptually understand queries and detect errors.
49,"Some people have no trouble understanding, but I find"
49,this much clearer:
49,"SELECT n.nspname AS ""Schema"", p.proname AS ""Name"","
49,"pg_catalog.format_type(p.prorettype, NULL) AS ""Result data type"", CASE WHEN"
49,p.pronargs = 0 THEN
49,CAST('*' AS pg_catalog.text)
49,ELSE
49,pg_catalog.pg_get_function_arguments(p.oid)
49,"END AS ""Argument data types"", pg_catalog.obj_description(p.oid, 'pg_proc')"
49,"AS ""Description"""
49,FROM pg_catalog.pg_proc p
49,LEFT JOIN pg_catalog.pg_namespace n ON n.oid = p.pronamespace
49,WHERE p.prokind = 'a'
49,AND n.nspname <> 'pg_catalog'
49,AND n.nspname <> 'information_schema'
49,AND pg_catalog.pg_function_is_visible(p.oid)
49,"ORDER BY 1, 2, 4;"
49,"In the past, I used to clean up queries using a tool designed to reformat Informix 4GL programs, and I would use sed to"
49,fix some things the tool missed.
49,"That was obviously far from ideal, so last year I tried Gilles Darold's pgFormatter."
49,My first test was to run the
49,sql files I use for my presentations through the formatter.
49,I found a few problems that Gilles fixed quickly.
49,I then ran the Postgres
49,"regression test queries through the formatter, which turned up more problems."
49,"Again, Gilles fixed them, and I could then run all 78k lines of"
49,sql queries from the regression tests with satisfactory results.
49,"Now, when I am presented with a query that looks like the first version, I run to pgFormatter, and with my favorite flags, I get a query that looks like I typed it."
49,View or Post Comments
49,Writing Style
49,"Monday, July 27, 2020"
49,There seem to be as many methods of writing sql queries as ways of writing essays.
49,"While spacing, capitalization, and naming are all personal preferences, there are also logical arguments for why"
49,certain styles are better than others.
49,"This web page outlines one set of styles, based on Joe Celko's"
49,SQL Programming Style.
49,"While I don't agree with all the style decisions, I feel it is a good way to think about"
49,your own style decisions and increase style consistency.
49,View or Post Comments
49,Encryption at Rest
49,"Friday, July 24, 2020"
49,There are many security guides that require encryption at rest.
49,"However, it is unclear exactly what ""at rest"" means."
49,Encrypted at rest can potentially mean encrypted when the:
49,Storage is powered off
49,File system is unmounted
49,Database is not running
49,Data is in the kernel's file system cache
49,Write-ahead log is in archive storage
49,Data is in backups
49,Data is in the database process's memory
49,"Odds are some of these items are required to implement a guide's ""encryption at rest,"" but maybe not all of them."
49,"The last one is particularly difficult, especially since the unlock key probably also has"
49,to be somewhere in process memory.
49,"When implementing encryption at rest, it is good to be clear exactly what encryption levels are required, and what risks they are designed to minimize."
49,View or Post Comments
49,Passwords in Log files
49,"Wednesday, July 22, 2020"
49,No one likes user passwords appearing in log files (except the bad guys/gals).
49,"Because Postgres uses sql queries to manage user accounts, including password assignment, it is possible for"
49,"passwords to appear in the server logs if, for example, log_statement is enabled or an error is generated,"
49,depending on the setting of log_min_error_statement.
49,"One solution is to encrypt the password manually before issuing the query, but that can be error-prone."
49,A simpler solution is to use psql to
49,"automatically encrypt passwords before sending them to the server, e.g.:"
49,SET client_min_messages = 'log';
49,SET log_statement = 'all';
49,\password postgres
49,Enter new password:
49,Enter it again:
49,LOG:
49,statement: show password_encryption
49,LOG:
49,statement: ALTER USER postgres PASSWORD 'md567429efea5606f58dff8f67e3e2ad490'
49,Notice psql runs show password_encryption to determine if md5 or scram-sha-256 should be used.
49,psql then hashes the supplied password and issues an alter user command.
49,While it
49,"is not ideal that hashed passwords can appear in the server logs, it is better than having user-typed passwords in the logs."
49,"(It might be wise to set log_min_error_statement to ""panic"" to prevent"
49,logging of error queries.)
49,Another secure option is to use syslog to send the logs to a secure server.
49,View or Post Comments
49,Force Password Changes
49,"Monday, July 20, 2020"
49,I have written about the limitations of passwords before.
49,Postgres has limited ability to control passwords — mostly how passwords
49,are transferred and how long passwords remain valid.
49,For more complex
49,"password requirements, the Postgres project recommends using an external authentication system that manages passwords like ldap or"
49,gssapi. Cert authentication has many
49,advantages because it does not require a password.
49,"Last year, one big change in password policy was the us nist"
49,announcement that periodic password changes are no longer
49,recommended.
49,This article explains why this is a logical approach to
49,security.
49,The new recommendations also suggests the limited value of password complexity requirements.
49,Users occasionally ask for for more complex password management features to be built into Postgres.
49,These new
49,guidelines make it less likely these features will ever be added to core Postgres.
49,View or Post Comments
49,Credential Rotation Using Certificates
49,"Friday, July 17, 2020"
49,The traditional method of authenticating is local password.
49,Authentication using external password managers is also very popular.
49,"However, passwords have known limitations, particularly password selection."
49,Managing password changes can also be complex.
49,"If users change their own passwords, the process is simple."
49,"However, if someone else needs to change the password, and there is a delay between the user"
49,"requesting the change and the change being made, it can be difficult for the user to know when to switch to using the new password."
49,For applications that connect to the database by supplying passwords
49,"automatically, password changes can be even more complex."
49,Programming applications to try both old and new passwords is awkward.
49,"Instead of using something you know, e.g., passwords, another authentication approach is to use something you have. One ideal method of"
49,"""something you have"" is certificate authentication."
49,It allows for more effective credential rotation because the old and new
49,certificates can be valid at the same time.
49,The process of replacing ssl certificates is:
49,"Create a new certificate, signed by a certificate authority in the same certificate chain as the server certificate"
49,Replace the old client certificate with the new one
49,Add the old client certificate to the certificate revocation list (crl)
49,"After step one, administrators don't need to rush step two because the old and new certificates are both valid."
49,"Once step two is complete, step three can be done."
49,Step three is only useful if the server
49,has been set up to honor certificate revocation lists.
49,You also must set an expiration date for certificates to limit their lifetimes.
49,There is no similar way to do delayed authentication rotation using password-based methods.
49,View or Post Comments
49,Grouping Sets and Null Values
49,"Wednesday, July 15, 2020"
49,"You might be aware that grouping sets, along with rollup and cube, allow queries to"
49,return additional group by rows.
49,"However, because these additional rows are added by Postgres, it is unclear what value to use for summary fields, so the sql standard specifies"
49,null for those fields.
49,Let's start with a simple example using a simple group by:
49,"CREATE TABLE test (x TEXT, y INTEGER);"
49,"INSERT INTO test VALUES ('a', 0), ('b', 0), (NULL, 0);"
49,"SELECT x, COUNT(*) FROM test GROUP BY x ORDER BY x;"
49,x | count
49,---+-------
49,a |
49,b |
49,Each row represents a value in test and each row has a count of one since there is one matching row for each value.
49,"The last row is the number of nulls in the table, also one. Here is the same"
49,query using rollup:
49,"SELECT x, COUNT(*) FROM test GROUP BY ROLLUP(x) ORDER BY x;"
49,x | count
49,---+-------
49,a |
49,b |
49,Notice there are two rows where the first column is null.
49,Let's verify these are nulls and not something else using psql's \pset:
49,\pset null '(null)'
49,"SELECT x, COUNT(*) FROM test GROUP BY ROLLUP(x) ORDER BY x;"
49,| count
49,--------+-------
49,(null) |
49,(null) |
49,-- reset the psql NULL display
49,\pset null ''
49,"Yes, they are both null."
49,"The first null is the one generated by rollup, and the last row is the number of null values."
49,This is quite confusing.
49,"If we expect a column involved in a grouping set-type operation to perhaps contain nulls, how can we distinguish between those values and values added by grouping"
49,set-type operations?
49,"Well, your first inclination might be to use coalesce:"
49,"SELECT COALESCE(x, '(null)'), COUNT(*) FROM test GROUP BY ROLLUP(x) ORDER BY x;"
49,coalesce | count
49,----------+-------
49,(null)
49,(null)
49,"(Remember we turned off psql's special displaying of nulls, so (null) is coming from coalesce.)"
49,That didn't help.
49,The reason is that the select target list is executed
49,"last, as covered in a previous blog post."
49,The right way to do it is to force coalesce to run before the grouping
49,set-type operation using a common table expression:
49,WITH t AS (
49,"SELECT COALESCE(x, '(null)') AS x, y FROM test"
49,"SELECT x, COUNT(*) FROM t GROUP BY ROLLUP(x) ORDER BY x;"
49,| count
49,--------+-------
49,(null) |
49,"That's what we wanted — (null) for the inserted null, and a real null for the grouping set-type added row."
49,"Another option is to use grouping, which returns a non-zero value if the row is generated by a grouping"
49,"set, e.g.:"
49,"SELECT x, COUNT(*), GROUPING(x) FROM test GROUP BY ROLLUP(x) ORDER BY x;"
49,x | count | grouping
49,---+-------+----------
49,a |
49,1 |
49,b |
49,1 |
49,3 |
49,1 |
49,This can be combined with case to control the label of the grouping set null column:
49,SELECT CASE
49,WHEN GROUPING(x) != 0 THEN x
49,WHEN x IS NOT NULL THEN x
49,ELSE '(null)'
49,"END AS y,"
49,COUNT(*)
49,FROM test
49,GROUP BY ROLLUP(x)
49,ORDER BY y;
49,| count
49,--------+-------
49,(null) |
49,View or Post Comments
49,JSONB:
49,A Container of Types
49,"Monday, July 13, 2020"
49,The popular json and jsonb data types are more than just a collection of single-data-type values like
49,arrays.
49,They can contain multiple data types:
49,CREATE TABLE test(x JSONB);
49,"INSERT INTO test VALUES ('""abc""'), ('5'), ('true'), ('null'), (NULL);"
49,\pset null (null)
49,"SELECT x, jsonb_typeof(x), pg_typeof(x) FROM test;"
49,| jsonb_typeof | pg_typeof
49,--------+--------------+-----------
49,"""abc"""
49,| string
49,| jsonb
49,| number
49,| jsonb
49,true
49,| boolean
49,| jsonb
49,null
49,| null
49,| jsonb
49,(null) | (null)
49,| jsonb
49,"You can see I stored a string, number, boolean, json null, and an sql null in a single jsonb column, and Postgres knew the type of each value in the"
49,"jsonb column, as shown by jsonb_typeof()."
49,"However, pg_typeof() still sees all these as jsonb values, and that is the type exposed to anything referencing the json column."
49,Here is another example that uses #>> to convert all json values to text:
49,"SELECT x, x::TEXT, x#>>'{}', pg_typeof(x#>>'{}') FROM test;"
49,| ?column? | pg_typeof
49,--------+--------+----------+-----------
49,"""abc"""
49,"| ""abc"""
49,| abc
49,| text
49,| 5
49,| 5
49,| text
49,true
49,| true
49,| true
49,| text
49,null
49,| null
49,| (null)
49,| text
49,(null) | (null) | (null)
49,| text
49,(I am using #>> rather than the more popular ->> because it lets me access json values that are not associated with keys.)
49,Notice that using
49,":: to cast to text retains the double-quotes, while #>> removes them."
49,(->> does also.)
49,"However, the return value for #>>"
49,did not change based on the json contents — it always returned text or sql null.
49,"Also notice that :: casting returns the string ""null"" for the jsonb"
49,"null value, while #>> returns an sql null."
49,(I used psql's \pset null above.)
49,There is a fundamental problem with the interaction between jsonb and sql data types.
49,Postgres knows the data type of each json value inside the jsonb field (as shown
49,"by jsonb_typeof), but each field can have multiple json values inside, and each row can be different as well."
49,"Therefore, you only have two choices for interfacing jsonb to sql"
49,data types:
49,Use ->> or #>> to map all values to text
49,Cast values to an sql data type
49,"For number one, all data types can be converted to text, so it is a simple solution, though it doesn't work well if you need to process the values in a non-textual way, e.g., a numeric comparison."
49,You can cast jsonb values to any sql data type (number two) as long as all jsonb values can be cast successfully.
49,For example:
49,DELETE FROM test;
49,-- all inserted values must cast to a jsonb type
49,"INSERT INTO test VALUES ('""1""'), ('2'), ('3e1'),"
49,"('4f1'),"
49,"('true'), ('null'), (NULL);"
49,ERROR:
49,invalid input syntax for type json
49,"LINE 1: ...NSERT INTO test VALUES ('""1""'), ('2'), ('3e1'),"
49,"('4f1'),"
49,(...
49,DETAIL:
49,"Token ""4f1"" is invalid."
49,CONTEXT:
49,"JSON data, line 1: 4f1"
49,-- 4e1 uses exponential notation
49,"INSERT INTO test VALUES ('""1""'), ('2'), ('3e1'),"
49,"('4e1'),"
49,"('true'), ('null'), (NULL);"
49,"SELECT x, x::TEXT, x#>>'{}', jsonb_typeof(x), pg_typeof(x#>>'{}') FROM test;"
49,| ?column? | jsonb_typeof | pg_typeof
49,--------+--------+----------+--------------+-----------
49,"""1"""
49,"| ""1"""
49,| 1
49,| string
49,| text
49,| 2
49,| 2
49,| number
49,| text
49,| 30
49,| 30
49,| number
49,| text
49,| 40
49,| 40
49,| number
49,| text
49,true
49,| true
49,| true
49,| boolean
49,| text
49,null
49,| null
49,| (null)
49,| null
49,| text
49,(null) | (null) | (null)
49,| (null)
49,| text
49,-- all values can't be cast to numeric
49,"SELECT x, x::TEXT, x#>>'{}', x::NUMERIC * 5, jsonb_typeof(x)"
49,FROM test;
49,ERROR:
49,cannot cast jsonb string to type numeric
49,-- all values of jsonb type 'number' can be cast to numeric
49,"SELECT x, x::TEXT, x#>>'{}', x::NUMERIC * 5, jsonb_typeof(x)"
49,FROM test
49,WHERE jsonb_typeof(x) = 'number';
49,| x
49,| ?column? | ?column? | jsonb_typeof
49,----+----+----------+----------+--------------
49,| 2
49,| 2
49,10 | number
49,30 | 30 | 30
49,150 | number
49,40 | 40 | 40
49,200 | number
49,-- Use #>> to remove double-quotes from the jsonb string
49,"SELECT x, x::TEXT, x#>>'{}', (x#>>'{}')::NUMERIC * 5, jsonb_typeof(x)"
49,FROM test
49,WHERE jsonb_typeof(x) = 'number' OR
49,jsonb_typeof(x) = 'string';
49,| ?column? | ?column? | jsonb_typeof
49,-----+-----+----------+----------+--------------
49,"""1"" | ""1"" | 1"
49,5 | string
49,| 2
49,| 2
49,10 | number
49,| 30
49,| 30
49,150 | number
49,| 40
49,| 40
49,200 | number
49,"The first insert fails because 4f1 isn't double-quoted and can't be cast to a json numeric, but 4e1 can because it represents exponential notation."
49,In trying to cast all values to numeric in the
49,"select, the ""1"" has double-quotes, so it fails."
49,Using #>> removes the double quotes and allows the string value to be cast to numeric too.
49,This
49,discussion exposes the confusion of using json without casting.
49,"I have been using single jsonb values, but the same behavior happens with jsonb documents:"
49,DELETE FROM test;
49,-- create document with keys 'a' and 'b'
49,"INSERT INTO test VALUES ('{""a"": ""xyz"", ""b"": 5}');"
49,-- access key 'a'
49,"SELECT x->'a', jsonb_typeof(x->'a'), pg_typeof(x->'a'), (x->'a')::TEXT, x->>'a', pg_typeof(x->>'a') FROM test;"
49,?column? | jsonb_typeof | pg_typeof | text
49,| ?column? | pg_typeof
49,----------+--------------+-----------+-------+----------+-----------
49,"""xyz"""
49,| string
49,| jsonb
49,"| ""xyz"" | xyz"
49,| text
49,-- access key 'b'
49,"SELECT x->'b', jsonb_typeof(x->'b'), pg_typeof(x->'b'), (x->'b')::TEXT, x->>'b', pg_typeof(x->>'b') FROM test;"
49,?column? | jsonb_typeof | pg_typeof | text | ?column? | pg_typeof
49,----------+--------------+-----------+------+----------+-----------
49,| number
49,| jsonb
49,| 5
49,| 5
49,| text
49,"So, in summary:"
49,"If you want to use json values as text, use ->> or #>>"
49,"If you want to cast to another sql data type, use :: or cast to cast, but be sure all values can be cast to the new sql data type"
49,View or Post Comments
49,Two Interviews
49,"Friday, July 10, 2020"
49,I have done two interviews in the past month.
49,The first one was done by the Linux Inlaws and was published on Hacker Public
49,Radio.
49,"This interview discusses the history and open source aspects of the Postgres project, and its health and future direction."
49,"The second interview is more personal, discussing how I got involved in computers and my early experiences with Postgres."
49,It also discusses technology
49,"disruption, and its challenge to time management."
49,The final quarter covers religious topics.
49,View or Post Comments
49,Postgres Marketing
49,"Wednesday, July"
49,"8, 2020"
49,"Postgres is mostly a technology-driven community, so marketing often suffers."
49,"However, one great thing about the community is that it is distributed, so anyone can get involved and help."
49,Here are
49,some examples of very successful community-driven marketing ideas:
49,PostgreSQL Funds Group release 12 coin
49,Pg-us Postgres community founding coin
49,Russian Postgres/MySQL coin
49,Postgres Pro PostgreSQL for Beginners booklet
49,Here are front and rear images of these objects.
49,"Of course, I have also accumulated many Postgres"
49,pins over the years.
49,I am inspired by these marketing efforts and hope they continue.
49,View or Post Comments
49,Boolean Indexes
49,"Friday, July"
49,"3, 2020"
49,"For btree and hash indexes to be used for lookups, values being requested must be very"
49,"restrictive, roughly 3-5% of a table's rows."
49,Let's consider a
49,boolean column — it can contain only three values:
49,"true, false, and null."
49,"By definition, at least one of those three"
49,"values will be in more than 5% of the table's rows, so why index them?"
49,"A better solution, particularly for boolean fields, is to create a"
49,partial index so only rare values are indexed.
49,Partial indexes can also be used to index non-boolean fields when there are a large number
49,of common values that aren't worth indexing.
49,View or Post Comments
49,Global Indexes
49,"Wednesday, July"
49,"1, 2020"
49,Postgres indexes can only be defined on single tables.
49,"Why would you want to have indexes that reference multiple tables, i.e., global indexes?"
49,This email covers some of the reasons why global indexes can be useful. One big use-case is
49,"the ability to create indexes on partitioned tables that index all its child tables, rather than requiring a separate index on each child"
49,table.
49,This would allow references to partitioned tables as foreign keys without requiring the partition key to be part of the foreign key
49,reference;
49,Postgres 12 allows such foreign keys if they match partition keys.
49,A second use-case for global indexes is the ability to add a uniqueness constraint to a partitioned table where the unique columns are not part of the partition key.
49,A third use-case is the ability to
49,"index values that only appear in a few partitions, and are not part of the partition key."
49,A global index would avoid having to check each partition table's index for the desired value.
49,It is still unclear if these use-cases justify the architectural changes needed to enable global indexes.
49,Some of these features can be simulated using triggers and user lookup tables.
49,A large global
49,index might also reintroduce problems that prompted the creation of partitioning in the first place.
49,Update: summary email 2021-01-08
49,View or Post Comments
49,Hardware Acceleration for Databases
49,"Monday, June 29, 2020"
49,"There is a long history of hardware acceleration, i.e., hardware modules helping the cpu."
49,"There was the 80287 math coprocessor, sound cards, and"
49,video cards.
49,"The computer industry is constantly moving things from the cpu to the motherboard and external cards, and back again."
49,Movement is mostly determined by whether the cpu is
49,"able to efficiently perform the task, the transfer bandwidth needed to perform the task, and the flexibility of replaceable external cards."
49,"This year, the big questions for database software is if and how to make use of graphics processing unit (gpu) and"
49,field-programmable gate arrays (fpga).
49,This
49,"article does a good job of explaining the history of hardware acceleration, and mentions"
49,Netezza's (no longer used) use of fpgas as hardware acceleration for databases.
49,The same historic hardware acceleration questions apply to database acceleration today:
49,"Are they better suited than cpus to do some database processing tasks, and how common are those tasks?"
49,Is there sufficient transfer bandwidth to gpus and fpgas to justify their use?
49,Is hardware acceleration worth the deployment complexity?
49,"PgOpenCL, HeteroDB, and PG-Strom (from HeteroDB) are projects that are"
49,experimenting with the value of gpus and fpgas in Postgres.
49,"As cloud providers increase the availability of gpus and fpgas, we might start see their usage increase."
49,View or Post Comments
49,Can Case Comparison Be Controlled?
49,"Friday, June 26, 2020"
49,Computer tasks are one of the most precise activities we do on a daily basis.
49,"Driving, cooking, walking, and reading are fairly imprecise compared to computer interaction."
49,"Computers represent symbols like ""a"" and ""A"" precisely and require external facilities to define relationships between them."
49,This
49,"email thread makes a convincing argument that you usually want case-preserving, but"
49,less-precise case-insensitive behavior.
49,"Let's go over some Postgres case-precision behaviors like the handling of character strings, identifiers, and keywords."
49,"For example, these queries do the same thing:"
49,SELECT COUNT() FROM pg_class;
49,count
49,-------
49,386
49,SELECT COUNT() FROM PG_CLASS;
49,count
49,-------
49,386
49,Select Count(*) From Pg_Class;
49,count
49,-------
49,386
49,"This is because Postgres, and the sql standard, ignore the case of keywords, e.g., select."
49,They also ignore the case of
49,"identifiers, e.g., pg_class, when not double-quoted. Double-quoting adds case precision to identifiers:"
49,"SELECT ""count""() FROM ""pg_class"";"
49,count
49,-------
49,386
49,"SELECT ""COUNT""() FROM ""pg_class"";"
49,ERROR:
49,function COUNT() does not exist
49,"LINE 1: SELECT ""COUNT""() FROM ""pg_class"";"
49,HINT:
49,No function matches the given name and argument types. You might need to add explicit type casts.
49,"SELECT ""Count""() FROM ""Pg_Class"";"
49,ERROR:
49,"relation ""Pg_Class"" does not exist"
49,"LINE 1: SELECT ""Count""() FROM ""Pg_Class"";"
49,"SELECT ""COUNT""() FROM ""PG_CLASS"";"
49,ERROR:
49,"relation ""PG_CLASS"" does not exist"
49,"LINE 1: SELECT ""COUNT""(*) FROM ""PG_CLASS"";"
49,There is no ability to add case precision to keywords:
49,"""SELECT"" COUNT() FROM pg_class;"
49,ERROR:
49,"syntax error at or near """"SELECT"""""
49,"LINE 1: ""SELECT"" COUNT() FROM pg_class;"
49,"When comparing values, Postgres is precise by default:"
49,SELECT 'a' = 'A';
49,?column?
49,----------
49,SELECT '-' = '_';
49,?column?
49,----------
49,"SELECT '.' = ',';"
49,?column?
49,----------
49,"For certain symbols, is it sometimes visually hard to see the difference."
49,"As shown above, double-quotes adds precision to identifiers."
49,"For value comparisons, you have to be explicit to remove precision:"
49,SELECT upper('a') = upper('A');
49,?column?
49,----------
49,SELECT lower('a') = lower('A');
49,?column?
49,----------
49,Most people aren't comparing constants in sql but compare column values:
49,SELECT oid FROM pg_class WHERE relname = 'pg_class';
49,oid
49,------
49,1259
49,SELECT oid FROM PG_CLASS WHERE relname = 'pg_class';
49,oid
49,------
49,1259
49,SELECT oid FROM pg_class WHERE relname = 'Pg_Class';
49,oid
49,-----
49,SELECT oid FROM pg_class WHERE relname = 'PG_CLASS';
49,oid
49,-----
49,Notice that these queries use pg_class as an identifier (without single quotes) and as a value (with single quotes). The identifier usage is case insensitive;
49,the value usage is case sensitive.
49,You
49,can explicitly reduce comparison precision using function calls:
49,SELECT oid FROM pg_class WHERE lower(relname) = lower('pg_class');
49,oid
49,------
49,1259
49,SELECT oid FROM pg_class WHERE lower(relname) = lower('Pg_Class');
49,oid
49,------
49,1259
49,SELECT oid FROM pg_class WHERE lower(relname) = lower('PG_CLASS'^);
49,oid
49,------
49,1259
49,These convert the column values and constants to lower case before comparison.
49,(Upper case could also have been used.)
49,"Indexes are also case sensitive by default, and it obviously would be inefficient"
49,"to lower-case every index entry for comparison, so function calls on columns cannot use an ordinary column index:"
49,EXPLAIN SELECT oid FROM pg_class WHERE relname = 'pg_class';
49,QUERY PLAN
49,-------------------------------------------------------------------------------------------
49,Index Scan using pg_class_relname_nsp_index on pg_class
49,(cost=0.27..8.29 rows=1 width=4)
49,Index Cond: (relname = 'pg_class'::name)
49,EXPLAIN SELECT oid FROM pg_class WHERE lower(relname) = lower('pg_class');
49,QUERY PLAN
49,---------------------------------------------------------
49,Seq Scan on pg_class
49,(cost=0.00..19.76 rows=2 width=4)
49,Filter: (lower((relname)::text) = 'pg_class'::text)
49,They can use expression indexes that are created to match function calls:
49,-- create a user table because users can't create indexes on system tables
49,CREATE TABLE my_pg_class AS SELECT * FROM pg_class;
49,-- create non-expression index
49,CREATE INDEX i_my_pg_class_relname ON my_pg_class (relname);
49,-- create expression index
49,CREATE INDEX i_my_pg_class_relname_lower ON my_pg_class ((lower(relname)));
49,-- The optimizer needs statistics
49,-- Doing the analyze after the expression index creation allows creation of statistics on the expression.
49,-- see https://momjian.us/main/blogs/pgblog/2017.html#February_20_2017
49,-- Autovacuum would have eventually done this automatically.
49,ANALYZE my_pg_class;
49,-- use non-expression index
49,EXPLAIN SELECT oid FROM my_pg_class WHERE relname = 'pg_class';
49,QUERY PLAN
49,-----------------------------------------------------------------------------------------
49,Index Scan using i_my_pg_class_relname on my_pg_class
49,(cost=0.27..8.29 rows=1 width=4)
49,Index Cond: (relname = 'pg_class'::name)
49,-- use expression index
49,EXPLAIN SELECT oid FROM my_pg_class WHERE lower(relname) = lower('pg_class');
49,QUERY PLAN
49,-----------------------------------------------------------------------------------------------
49,Index Scan using i_my_pg_class_relname_lower on my_pg_class
49,(cost=0.27..8.29 rows=1 width=4)
49,Index Cond: (lower((relname)::text) = 'pg_class'::text)
49,The citext extension allows the creation of columns whose values are automatically compared in a case-insensitive manner:
49,CREATE EXTENSION citext;
49,-- 'x' column added so the row has a typical length
49,"CREATE TABLE my_pg_class2 AS SELECT oid, relname::citext, repeat('x', 256) FROM pg_class;"
49,CREATE INDEX i_my_pg_class_relname2 ON my_pg_class2 (relname);
49,ANALYZE my_pg_class2;
49,\d my_pg_class2
49,"Table ""public.my_pg_class2"""
49,Column
49,Type
49,| Collation | Nullable | Default
49,---------+--------+-----------+----------+---------
49,oid
49,| oid
49,relname | citext | C
49,repeat
49,| text
49,Indexes:
49,"""i_my_pg_class_relname2"" btree (relname)"
49,SELECT oid FROM my_pg_class2 WHERE relname = 'pg_class';
49,oid
49,------
49,1259
49,SELECT oid FROM my_pg_class2 WHERE relname = 'PG_CLASS';
49,oid
49,------
49,1259
49,EXPLAIN SELECT oid FROM my_pg_class2 WHERE relname = 'pg_class';
49,QUERY PLAN
49,-------------------------------------------------------------------------------------------
49,Index Scan using i_my_pg_class_relname2 on my_pg_class2
49,(cost=0.27..8.29 rows=1 width=4)
49,Index Cond: (relname = 'pg_class'::citext)
49,EXPLAIN SELECT oid FROM my_pg_class2 WHERE relname = 'PG_CLASS';
49,QUERY PLAN
49,-------------------------------------------------------------------------------------------
49,Index Scan using i_my_pg_class_relname2 on my_pg_class2
49,(cost=0.27..8.29 rows=1 width=4)
49,Index Cond: (relname = 'PG_CLASS'::citext)
49,There are more facilities available to further reduce precision:
49,unaccent
49,fuzzystrmatch
49,pg_trgm with similarity
49,full text search with stemming
49,"If Postgres 12 or later is compiled with the icu library support (view the system table column pg_collation.collprovider to check), you can use"
49,nondeterministic collations that are
49,case and accent-insensitive.
49,View or Post Comments
49,Force One Row
49,"Monday, June 22, 2020"
49,How can you force a table to have at most one row?
49,"It is actually very easy by creating a unique expression index on a constant, with no"
49,column name references:
49,"CREATE TABLE onerow (a INTEGER, b TEXT);"
49,CREATE UNIQUE INDEX ON onerow ((1));
49,-- this adds a single row
49,"INSERT INTO onerow VALUES (1, 'foo')"
49,"ON CONFLICT ((1)) DO UPDATE SET a = excluded.a, b = excluded.b;"
49,SELECT * FROM onerow;
49,a |
49,---+-----
49,1 | foo
49,-- this updates the single row
49,"INSERT INTO onerow VALUES (2, 'bar')"
49,"ON CONFLICT ((1)) DO UPDATE SET a = excluded.a, b = excluded.b;"
49,SELECT * FROM onerow;
49,a |
49,---+-----
49,2 | bar
49,-- this also updates the single row
49,"INSERT INTO onerow VALUES (3, 'baz')"
49,"ON CONFLICT ((1)) DO UPDATE SET a = excluded.a, b = excluded.b;"
49,SELECT * FROM onerow;
49,a |
49,---+-----
49,3 | baz
49,-- try INSERT without ON CONFLICT
49,"INSERT INTO onerow VALUES (4, 'foo2');"
49,ERROR:
49,"duplicate key value violates unique constraint ""onerow_expr_idx"""
49,DETAIL:
49,Key ((1))=(1) already exists.
49,"By using on conflict, it is possible to add a row, but if a row already exists, to replace it with a new value."
49,View or Post Comments
49,Storing Binary Data in the Database
49,"Friday, June 19, 2020"
49,There are some very good responses in an email thread about whether to store binary data in
49,Postgres or externally.
49,The binary storage options discussed were:
49,In the database (toast helps with performance)
49,"In another database, like SQLite"
49,In a local or network file system
49,Using cloud storage.
49,"This email reply had many good insights, and this"
49,wiki page has even more.
49,I have covered data storage outside of databases
49,before.
49,View or Post Comments
49,Dinner Q&A
49,"Wednesday, June 17, 2020"
49,"My employer, EnterpriseDB, has been organizing events where potential customers and interested people can ask me questions while enjoying a meal."
49,I thought the idea was
49,"strange, but I have done it ten times, and they have gone very well."
49,The Q&A portion usually lasts one hour and forty-five minutes.
49,"During a November, 2019 event in"
49,"Utrecht, the Netherlands, a sketch artist was present."
49,The artist illustrated my ideas as I spoke and created this
49,"diagram, which I found quite interesting."
49,View or Post Comments
49,Controlling Server Variables at Connection Time
49,"Monday, June 15, 2020"
49,I have recently covered the importance of libpq environment variables and
49,connection specification options.
49,"While most libpq options control how to connect to the Postgres server, there is one special option that"
49,"can change variables on the server you connect to, e.g.:"
49,$ psql 'options=-cwork_mem=100MB dbname=test'
49,psql (13devel)
49,"Type ""help"" for help."
49,test=> SHOW work_mem;
49,work_mem
49,----------
49,100MB
49,"This can also be done using environment variables, with all the benefits of environment variables:"
49,"$ PGOPTIONS=""-c work_mem=100MB"" psql test"
49,"These settings can also be set at the user, database, and"
49,"cluster level on the database side too, but having control on the client side is often useful."
49,View or Post Comments
49,Connect Parameter Specification Options
49,"Friday, June 12, 2020"
49,I have previously covered the importance of libpq and environment
49,variables.
49,"While you can specify discrete connection command-line parameters and environment variables, there is a catch-all setting that allows connection options to be specified in a single string,"
49,e.g.:
49,$ psql -d test
49,psql (13devel)
49,"Type ""help"" for help."
49,test=> \q
49,$ psql --dbname test
49,psql (13devel)
49,"Type ""help"" for help."
49,test=> \q
49,$ psql 'dbname=test'
49,psql (13devel)
49,"Type ""help"" for help."
49,test=> \q
49,The first psql command uses a single-letter command-line option.
49,The second one uses a long-format option.
49,The third uses a
49,parameter key word.
49,Multiple key words can be used to specify multiple connection options:
49,$ psql 'host=myhost.com port=5433 dbname=test'
49,You can also use a uri syntax to specify the same parameters as above:
49,$ psql postgresql://myhost.com:5433/test
49,View or Post Comments
49,Controlling Connection Parameters Using Environment Variables
49,"Wednesday, June 10, 2020"
49,Libpq is the Postgres connection library used by almost every non-jdbc application.
49,"It allows many connection parameters,"
49,which can be specified on the command line or embedded in applications:
49,$ psql -h myhost.com -d mydb
49,"In the above case, the psql host name and database name are specified on the command-line and interpreted by libpq."
49,"However, it is also possible"
49,"to specify parameters using environment variables, which are also interpreted by libpq:"
49,$ PGHOST=myhost.com PGDATABASE=mydb psql
49,"There is obviously no value in specifying libpq parameters using environment variables in this example, but there are use cases."
49,"For example, if you want to perform multiple operations on the same host"
49,"name and database, you can do:"
49,$ export PGHOST=myhost.com
49,$ export PGDATABASE=mydb
49,$ vacuumdb
49,$ reindexdb
49,"This avoids specifying the host and database names multiple times, though with a loss of clarity."
49,Sometimes environment variables are best used as defaults when connection options are not specified:
49,$ export PGHOST=myhost.com
49,$ export PGDATABASE=mydb
49,$ reindexdb
49,$ reindexdb --dbname mydb2
49,$ reindexdb --host myhost3.com --dbname mydb3
49,"This reindexes databases mydb and mydb2 on host myhost.com, and database mydb3 on myhost3.com."
49,"Another use-case for environment variables is to set parameters for users, without having to pass them as parameters to commands:"
49,$ # must use 'sh' so the redirect happens as root
49,"$ sudo sh -c ""echo PGHOST='example.com' >> ~bruce/.profile"""
49,"By appending this to bruce's .profile file, all applications that use libpq without a specific host name will connect to example.com automatically."
49,"Once bruce logs out and back in again, all his"
49,applications will start using the new .profile pghost setting.
49,"Finally, environment variables make it possible to set default connection values for all users."
49,"For example, on Debian, to default all tcp"
49,(non-Unix Domain socket) connections to fully verify
49,"ssl certificates, you can do:"
49,"$ sudo sh -c ""echo PGSSLMODE='verify-full' >> /etc/profile.d/pgdefaults.sh"""
49,"Of course, you can change environment variables set at login and applications can override connection parameters set by environment variables."
49,View or Post Comments
49,Safety Systems Can Reduce Safety
49,"Monday, June"
49,"8, 2020"
49,What is the purpose of safety systems?
49,To make things safer?
49,To make them appear safer?
49,To satisfy some external requirement?
49,The purpose of safety
49,"systems is not always clear, but even for safety systems whose sole purpose is to increase safety — do they always succeed in increasing safety?"
49,"The simple answer is ""no""."
49,Here are three examples:
49,Crash of ValuJet 592 caused by transporting safety equipment
49,Three Mile Island nuclear reactor meltdown caused by the failure of complex safety
49,systems
49,Chernobyl nuclear reactor explosion caused by a safety test
49,"In all three cases, safety systems did not prevent disasters — they caused them."
49,"Safety systems are often very useful, and the modern world could not operate with them."
49,"However, they also add"
49,"complexity, and that added complexity can introduce failure modes that did not exist without the safety systems."
49,"So, where does that leave us?"
49,"Safety systems are useful, but too many of them are bad?"
49,How many is too many?
49,"These are hard questions, but there are some guidelines:"
49,How serious is the failure that the safety system is trying to prevent?
49,How likely is the failure that the safety system is trying to prevent?
49,How likely is the safety system to fail?
49,What impact will failure of the safety system have on the overall system?
49,"This gets into a very complex calculus where you are computing the likelihood and seriousness of the failure that the safety system is trying to prevent, and the likelihood and seriousness of safety system"
49,failure.
49,"The big point is that while you are computing the likelihood and seriousness of failures and adding safety systems to compensate for them, you have to be aware of the cost of adding those safety"
49,"systems, in both the likelihood and impact of their failure."
49,What does this have to do with databases?
49,"Well, you might have a database on an airplane or at a nuclear reactor site, in which case the database is part of a complex system."
49,"However, databases are also"
49,"complex systems, and we regularly add safety systems to increase their reliability."
49,"How often to do we consider the cost of those safety systems, in terms of the likelihood and seriousness of failures?"
49,Let's look at some typical database safety systems:
49,Backups
49,Error reporting
49,Replication
49,Connection pooling
49,Let's walk through the calculus for backups:
49,How serious is the failure that the safety system is trying to prevent?
49,High
49,How likely is the failure that the safety system is trying to prevent? High
49,How reliable is the safety system?
49,Medium
49,What impact will failure of the safety system have on the overall system?
49,Low
49,"The last question is not considering the seriousness of a failed backup on performing it safety goal, but rather how likely is a backup to cause a failure on its own?"
49,It could fill up the disk with backup
49,"files, or cause too much load on the system, but those risks are low."
49,"Similarly, error reporting has minimal risk on destabilizing the system, except for consuming I/O and storage space."
49,Replication and connection poolers are in a different class of risk.
49,Replication's goal is to allow for controlled switchover or failover
49,"in case of primary server failure, but what is its reliability and the impact if it fails?"
49,If synchronous replication is
49,"used, failure to replicate will cause the system to stop accepting writes."
49,"This can be caused by a network failure to the replicas, a replica outage, or even misconfiguration."
49,Even failure of
49,"asynchronous replication can cause the write-ahead log directory to fill up storage, causing an outage."
49,"Similarly, the failure of a connection pooler can cause a total outage."
49,"You can use multiple poolers, but what if the failure of one pooler prevents the other from working, or if they both work and"
49,conflict with each other.
49,Multiple poolers can also add complexity to the system which makes debugging other problems harder.
49,"This is a great example where, to avoid the problems of safety system"
49,"failure, you create two safety systems, but the two safety systems can interact in ways that make two safety systems less reliable than one safety system."
49,Multi-master replication can have similar downsides.
49,"Even auto-failover has risks, and auto-failback, even more."
49,"The bottom line is that safety systems can be useful, but they can also add complexity which makes systems more fragile and harder to control;"
49,consider how safety systems interact and implement them only
49,where their value is clear.
49,View or Post Comments
49,When Does a Commit Happen?
49,"Wednesday, June"
49,"3, 2020"
49,"Most people who deal with relational databases think of transaction commits as binary operations — the query is running and not yet committed, then it is completed and committed."
49,"However, internally,"
49,there are many stages to a commit:
49,Write commit record to the write-ahead log
49,Flush the write-ahead log to durable storage
49,Update the pg_xact (slide 57)
49,Transfer to replicas
49,Mark the commit as visible to other sessions (ProcArrayEndTransaction() updates
49,PGPROC)
49,Communicate commit to the client
49,These steps are implemented in RecordTransactionCommit().
49,"What is interesting is that this process can be interrupted at anytime, by perhaps a server crash or network failure."
49,For example:
49,The commit could be flushed to disk but not yet visible to other clients
49,The commit could be replicated (and visible to queries on replicas) but not visible to queries on the primary
49,Queries could appear committed to other sessions before the client issuing the query receives notification
49,This email thread explains the problem that commit with standbys is not always an atomic
49,operation.
49,Postgres has the function txid_status() which allows clients to
49,"check if a transaction, perhaps from a previous disconnected"
49,"session, was committed."
49,View or Post Comments
49,Lessons from the Online Conference Trenches
49,"Monday, June"
49,"1, 2020"
49,"Having presented at two online conferences in the past two weeks, presenting at two this coming week, and presenting at many"
49,"edb-sponsored webinars, I have learned a few things about online presentations that might be helpful for Postgres organizers, speakers, and attendees:"
49,For Organizers:
49,"With no need for speakers and attendees to travel to online conferences, there are many more potential speakers available, and many more potential attendees than for in-person"
49,conferences.
49,"However, the technical challenges of hosting an online conference are significant because producing and consuming content can require multiple platforms that must be integrated seamlessly for"
49,a positive user experience.
49,"The content production platform, e.g. Zoom, is often different than the consumption platform, e.g. YouTube, Facebook Live."
49,"If the user experience is poor, people will leave"
49,because they are not bound to a physical location like an in-person event.
49,Adjusting to the time zones of speakers and attendees can be complex — doing a 24-hour conference like
49,Precona Live (organizer tips) solves many of the time
49,"zone problems, but requires moderators from many time zones."
49,"The moderator's job is much more extensive for online conferences since they control access, deal with technical problems, and manage the"
49,all-important chat channel.
49,"For online conferences, chat is the best way to promote attendee engagement."
49,"If chat is done well, user engagement during presentations can be even better than in-person"
49,conferences.
49,For Speakers:
49,"Just like for organizers, speakers have more technical challenges than in-person conferences — it is harder to engage the audience, more things can go wrong, and attendees can more"
49,easily leave.
49,"As a speaker, I have a checklist that I always references before each presentation:"
49,"Mute phone, chat, email, and upgrade notifications"
49,"Use a laptop on AC power with wired Ethernet, for reliability"
49,Have a count-up clock to keep track of the talk duration
49,Use a headset so you don't lean toward a microphone
49,Use a presentation remote so you don't lean forward to change slides
49,Turn on your video camera to increase audience engagement
49,Make sure the lighting is good and the background is uncluttered
49,For attendees:
49,"Many online conferences are a mix of different technologies, not always seamlessly integrated, so anticipate that it will take time to get initially connected."
49,Consider watching the
49,"conference on a large television, or from a tablet you can carry around."
49,Use chat to engage with the speaker and other attendees.
49,Feel free to switch to a more interesting presentation without guilt.
49,"When you get frustrated, consider how much time you are saving by not having to travel."
49,View or Post Comments
49,Visualizing Collations
49,"Friday, May 29, 2020"
49,"There is still significant confusion about characters sets, encodings, and collations."
49,"This is because in the real, non-digital world, we"
49,"usually treat languages, their characters, and ordering as unified, but in the digital world, they are treated separately, and their distinction can be hard to visualize."
49,"These two posted queries illustrate collation in a very creative way. The first query, SELECT"
49,"chr(i) FROM generate_series(1, 50000) i ORDER BY chr(i) COLLATE ""C"", outputs characters in their ""C"" binary order, with ascii as the first 128 characters, successive groups of languages"
49,"following, and ending with pictographic languages."
49,"The second query, SELECT chr(i) FROM generate_series(1, 50000) i ORDER BY chr(i) COLLATE ""en_US.utf8"", outputs the same 50,000 characters in ""United States English"" utf8 order."
49,The
49,"output starts with pictographic languages, not ascii."
49,"The Latin alphabet appears, but not until line 19068."
49,"What is interesting is that there are 118 symbols grouped together that look like 'a',"
49,"'a' with diacritics, or have 'a' as part of their symbol."
49,"Then 'b' appears with a group of 38 symbols that look like or use 'b', and so on through the Latin"
49,alphabet.
49,"(If you highlight a character and paste it into a search box, Google will tell you about that Unicode symbol.)"
49,"I found it interesting that it groups letters that look like Latin letters, even if the they are not from Latin alphabets and don't sound like Latin letters."
49,Cyrillic is grouped in a section after the Latin alphabet section.
49,These sql queries are the clearest example I have seen of
49,collation ordering.
49,"If I had used a different collation, instead of ""United States English"", there would have been a different ordering."
49,"This is why index storage is sensitive to collations, i.e., indexes with different"
49,collations store the same stings in a different order.
49,"Collation affects other things like upper/lower case processing, the ordering of query output, and certain optimizations."
49,View or Post Comments
49,What is an Lsn?
49,"Wednesday, May 27, 2020"
49,You might have seen that there is a pg_lsn data type:
49,test=> \dTS pg_lsn
49,List of data types
49,Schema
49,Name
49,Description
49,------------+--------+-------------------------
49,pg_catalog | pg_lsn | PostgreSQL LSN datatype
49,Client programs pg_receivewal and pg_recvlogical have options that take
49,"lsn values, but what is an lsn?"
49,"It stands for ""Log Sequence Number"" — it is a 64-bit value that represents a position in the write-ahead"
49,log.
49,"It is usually displayed as two 32-bit hex values, separated by a slash."
49,"For example, pg_controldata displays lsn values:"
49,$ pg_controldata
49,Latest checkpoint location:
49,0/15AE1B8
49,Latest checkpoint's REDO location:
49,0/15AE180
49,"So, the next time you view or need to specify a write-ahead log location, you are using an lsn."
49,View or Post Comments
49,Taking Snapshots of Clusters Which Use Tablespaces
49,"Monday, May 25, 2020"
49,Postgres already documents the ability to backup the database cluster using file
49,system snapshots.
49,"Unfortunately, database clusters that use tablespaces often cannot use this method if the storage system doesn't"
49,support simultaneous snapshots across file systems.
49,"However, simultaneous snapshots across file systems might not be a hard requirement for Postgres snapshot backups."
49,It might be possible for snapshots to be non-simultaneous as long as the
49,"write-ahead log that spans the time frame between snapshots is included in the backup, and"
49,checkpoints do not happen during that time frame.
49,"Internally, starting Postgres from a snapshot backup replays write-ahead log records to make a single file system snapshot consistent."
49,"Potentially, it could do the same for non-simultaneous snapshots of"
49,multiple file systems.
49,"However, documenting this, giving users a reliable list of steps to perform, and making sure it always works is probably too complex to justify."
49,View or Post Comments
49,"Moving Tables, Indexes, and Tablespaces Between Clusters"
49,"Friday, May 22, 2020"
49,"Currently, it is impossible to move tables, indexes, and entire tablespaces from one cluster to another — that is because each table and index file is bound to the cluster's infrastructure because of:"
49,Table and index definitions
49,pg_xact (commit/abort/in-progress transaction status records)
49,pg_multixact (used for multi-session row locking)
49,"Fyi, you can easily move tablespaces to new directories as long as it remains in the same cluster, and move tables and indexes"
49,between tablespaces.
49,"So, how could it be made possible?"
49,"Freeze can remove references to pg_xact and pg_multixact, assuming there are no active transactions"
49,during the freeze operation.
49,"Table and index definitions can be more complex, but it certainly seems possible."
49,This requires more research.
49,View or Post Comments
49,Why Pgdata Should Not Be at the Top of a Mount Point
49,"Wednesday, May 20, 2020"
49,This email thread is illustrative of why it is unwise to place the Postgres data
49,directory (pgdata) at the top of a mount point.
49,"Instead, create a subdirectory under the mount point and put pgdata there."
49,This has the advantage of avoiding possible data corruption if
49,"mounting fails, and allows more efficient use of pg_upgrade."
49,View or Post Comments
49,Percona Live Online
49,"Monday, May 18, 2020"
49,"I am planning to virtually attend and present at the Percona Live Online conference tomorrow, May 19."
49,"It starts at 10am, Eastern us time, and"
49,"spans 24 hours, so it covers every time zone."
49,"I am giving my Will Postgres Live Forever? presentation at noon, Eastern us"
49,time.
49,"Attendance is free, so you might want to check it out."
49,I saw some interesting topics on the program.
49,"I am also curious to experience a 24-hour virtual conference, though I am unlikely to remain awake"
49,that long.
49,View or Post Comments
49,Using Non-Login Roles
49,"Monday, May 18, 2020"
49,"When we talk about database roles, most people immediately think of login roles, which allow people to log in."
49,"However, another user management"
49,"feature is the ability to create non-login roles, formerly called groups."
49,"Non-login roles can also be assigned permissions, e.g., via"
49,"grant, and can have login roles as members."
49,Non-login roles can be even be
49,members of other non-login roles.
49,What is the value of using non-login roles?
49,"They allow a group of people to be assigned as members of a non-login role, and that role can be used to abstract permission assignment."
49,"For example, if you"
49,"have shop foremen, you can configure the login roles of all foremen to be members of a non-login foreman role."
49,As people are added and removed from that
49,"staff position, they can be added/removed from the non-login role without the need to change permissions for the foreman role."
49,"A further advantage of non-login roles, as explained in a recent email, is that Postgres"
49,can start to suffer performance problems if more than a few dozen roles are granted permission on an object.
49,A much simpler and more manageable solution is to add users to a non-login role and assign
49,object permissions to that non-login role.
49,View or Post Comments
49,Draft of Postgres 13 Release Notes
49,"Friday, May 15, 2020"
49,"I have completed the draft version of the Postgres 13 release notes, containing 181 items."
49,The release notes will be continually updated until
49,"the final release, which is expected to be in September or October of this year."
49,Beta testing will start in the next few weeks.
49,View or Post Comments
49,"Avoiding Cache Wipe, Synchronized Scans"
49,"Wednesday, May 13, 2020"
49,"Whenever you are dealing with a lot of data, it helps to cache it."
49,Postgres does this using
49,shared_buffers.
49,"However, one risk of caching data is that a large query that accesses a lot of data"
49,might remove frequently-accessed data from the cache;
49,this is called cache wipe.
49,"To avoid this, Postgres limits the number of shared buffers used by data operations that are expected to access a lot"
49,of data.
49,"Looking at C function GetAccessStrategy(), you can see there are four shared buffer access strategies."
49,"The first one,"
49,"bas_normal, is used for normal scans;"
49,the rest are to avoiding cache wiping:
49,bas_bulkread is enabled for large reads and uses 256 kilobytes of shared buffers (typically 32 8kB shared buffers)
49,bas_bulkwrite: is enabled for large writes and uses 16 megabytes of shared buffers
49,bas_vacuum is for vacuum operations and uses 256 kilobytes of shared buffers
49,"For example, function initscan() uses bas_bulkread if the scan is expected to access more than one-quarter of"
49,shared buffers.
49,"Similarly, table rewrites, create table as, and copy from use bas_bulkwrite. Bas_bulkwrite is larger because we can't discard written buffers from"
49,"the cache until they are written to storage, unlike unmodified buffers which can be discarded anytime."
49,"You might wonder, with operations using so few shared buffers, what happens if another session needs to scan the same data?"
49,Doesn't performance suffer?
49,"Well, another Postgres facility, that was developed"
49,"independently, helps with this: synchronized scans."
49,The top of syncscan.c explains it well:
49,"When multiple backends run a sequential scan on the same table, we try to keep them synchronized to reduce the overall I/O needed."
49,"The goal is to read each page into shared buffer cache only once,"
49,and let all backends that take part in the shared scan process the page before it falls out of the cache.
49,"Both synchronized scans and shared buffer access strategies work automatically, so most Postgres users don't even know they exist, but they do improve Postgres performance."
49,View or Post Comments
49,Why Do We Freeze?
49,"Monday, May 11, 2020"
49,"You might have seen autovacuum running, and noticed that it sometimes performs freeze operations on transaction ids (32 bits)"
49,and multi-xacts (used for multi-session row locking).
49,The frequency of freeze operations is controlled by
49,autovacuum_freeze_max_age and autovacuum_multixact_freeze_max_age.
49,You can
49,reduce the frequency of freezing if you are sure the freeze operation will complete
49,before transaction wraparound is reached.
49,"There is regular discussion about how freezing could be avoided, and this email does the best"
49,job of explaining the options.
49,"We could expand transaction ids to 64 bits, either on each row or perhaps with a page-level default, but"
49,pg_xact (commit/abort/in-progress transaction status records) still need cleanup.
49,This area probably needs more
49,thought.
49,View or Post Comments
49,Postgres Internals Website
49,"Friday, May"
49,"8, 2020"
49,"I am often asked how someone can learn more about Postgres, particularly the internals."
49,"There is the Postgres developer page, which has links to many resources,"
49,and the developer's faq item about learning about the internals.
49,One link on
49,that page I was not aware of is Hironobu Suzuki's very detailed website about Postgres internals.
49,"It has a lot of details I have never seen written before, so I suggest"
49,those interested should check it out.
49,View or Post Comments
49,Portability's Surprising Win
49,"Wednesday, May"
49,"6, 2020"
49,"When writing software, it is necessary to decide whether to use external facilities available in command-line tools, libraries, frameworks, and the operating system, or write the facilities yourself."
49,Why
49,would you write them yourself?
49,You might be worried about adding reliance on an external facility or a facility might not have sufficient flexibility or performance.
49,The Postgres development team has had to make similar decisions.
49,"Fortunately, we have tended to favor reliance on common operating system interfaces, tools, and libraries, e.g.,"
49,"OpenSSL, bison."
49,We have avoided reliance on external facilities that are uncommon or not well maintained.
49,"Postgres has reproduced facilities that were commonly available in the operating system, tools, or libraries only when there was a clear benefit."
49,"Reproducing such facilities for a small benefit, like a"
49,"little more control or a little more performance, is rarely wise."
49,While relying on external facilities often makes Postgres less flexible and perhaps less
49,"performant, there are long-term benefits:"
49,"As external facilities improve their feature-set and performance, Postgres benefits from these improvements with little effort"
49,"As new demands are required of these external facilities, Postgres again benefits effortlessly"
49,Let's be specific.
49,"In the early days of Postgres, file systems were not optimized for database storage."
49,"Fortunately, Postgres never implemented file systems on"
49,raw devices.
49,"Now that modern file systems, like ext4, give good database performance, Postgres"
49,benefits from file system improvements with almost no effort.
49,Even when ssds started being
49,"used, the only change needed in Postgres was the ability to set"
49,random_page_cost at the tablespace
49,level to handle databases where some tablespaces are on ssds and some are on magnetic storage.
49,"When virtualization, cloud, containers, and container orchestration (e.g., Kubernetes) became popular, Postgres had to do almost nothing to run well on these"
49,platforms.
49,(It is true that some of the Postgres enterprise deployment tools required repackaging and re-engineering.)
49,"Because of this philosophy, Postgres has remained relatively light-weight compared to other relational database systems, and this has benefited Postgres in environments where nimble deployments are"
49,favored.
49,View or Post Comments
49,With ... Materialized and Optimizer Control
49,"Monday, May"
49,"4, 2020"
49,"Before Postgres 12, queries specified as common table expressions (with clauses) always behaved as optimization barriers, meaning that"
49,"common table expression queries were executed independently, and were not moved to later parts of the query."
49,"Starting in Postgres 12, if a common table expression is referenced only once, and the keyword materialized is not used, it can be moved to a place later in the query where it can be better"
49,optimized;
49,"this improves optimization possibilities. However, if the movement of common table expression queries increases the from clause table count above the"
49,"geqo_threshold, Postgres will decide it can't efficiently optimize such a high table count query and will use"
49,the genetic query optimizer.
49,"So, while the new Postgres 12 behavior of in-lining common table expressions usually increases the quality of optimized plans, in some cases it can decrease them by enabling the genetic query optimizer."
49,"In a way, in pre-Postgres 12 or with the use of materialized, the query author is doing the optimization by creating common table expressions, while in other cases, the optimizer has greater"
49,"control, though even the optimizer can determine the query is too complex and fall back to less-precise genetic query optimization."
49,View or Post Comments
49,Background Writes
49,"Friday, May"
49,"1, 2020"
49,Postgres must guarantee durability and good performance.
49,"To meet these objectives, Postgres does writes to the file system and storage in the"
49,background as much as possible.
49,"In fact, there are only two major cases where writes happen in the foreground:"
49,Write-ahead log writes happen before commits are acknowledged to the client
49,A needed shared_buffer is dirty and must be written to storage so it can be replaced
49,Write-ahead log writes (#1) can be controlled using various settings.
49,Dirty shared buffer writes (#2) that happen in the foreground are
49,minimized if the background writer is operating efficiently.
49,You can monitor such writes by
49,viewing the probe buffer-write-dirty-start and buffer-write-dirty-done.
49,View or Post Comments
49,Optimal Use of Ssds
49,"Wednesday, April 29, 2020"
49,"Ssds have different performance characteristics than magnetic disks, and using them optimally isn't always clear."
49,Ssds have several performance benefits:
49,Very fast fsyncs
49,Much faster random reads and writes
49,Faster sequential reads and writes
49,"So, if all your data is stored on ssds, you will certainly improve performance."
49,"If you are mixing ssds and magnetic disks, ideally you should use ssds in ways that give the"
49,greatest benefit.
49,"Starting with number one, putting the write-ahead log on ssds is a great way to improve fsync performance."
49,"For number two, moving indexes to tablespaces using ssd storage can greatly improve performance because index access is usually random."
49,"When using ssds, the default value for"
49,"random_page_cost should be lowered, perhaps to 1.1."
49,This can be set at the
49,tablespace level if there is a mix of tablespaces on ssds and magnetic disks.
49,"For number three, it is also possible to create tablespaces on ssds for current data, and place archive data on tablespaces that use magnetic disks."
49,By using
49,"table partitioning, a partitioned table can transparently span ssds and magnetic disk tablespaces."
49,View or Post Comments
49,Does Postgres Support Compression?
49,"Monday, April 27, 2020"
49,"I am often asked if Postgres supports compression, and my answer is always a complicated dance around what ""compression"" level they are asking about."
49,There are six possible levels of database compression:
49,single field
49,across rows in a single page
49,across rows in a single column
49,across all columns and rows in a table
49,across tables in a database
49,across databases
49,Number one (single field) is currently done by toast.
49,Number two (across rows in a single page) is a practical optimization
49,where a compression routine blindly looks for repeating values in a page without understanding its structure.
49,The difficulty of implementing this happens when a page is stored using its compressed length
49,"(rather than the uncompressed 8k), the page contents change, and the new contents compress less well than the previous contents."
49,"In this case, the compressed page contents would be larger and it would be"
49,very complex to fit the page into the existing space in the file.
49,"A different file layout is really required for this, so pages can be placed anywhere in the file, without affecting index access."
49,A team
49,is working on adding this feature using Postgres's
49,table access method interface.
49,Number three (across rows in a single column) is the classic definition of a columnar database.
49,A team is also
49,working on that.
49,"Just like number two, this requires using a different storage layout than Postgres's default, and the table access method"
49,interface makes this possible.
49,Number four can be done using file system compression.
49,"Numbers five and six would be nice, but it unclear how this could be done efficiently without adding unacceptable complexity to the database."
49,View or Post Comments
49,Multi-Host Technologies
49,"Friday, April 24, 2020"
49,"There are so many multi-host technologies and it is hard to remember the benefits of each one, so I decided to create a list:"
49,"High availability: Streaming replication is the simplest way to have multiple copies of your database, ready for fail over"
49,Read scaling: Pgpool allows replicas (slide 17) to handle a database's read-only
49,workload
49,Write scaling: Sharding allows for write scaling
49,Partial replication: Logical replication allows partial replication
49,"Reduce latency: Multi-master replication allows servers to be located close to users, reducing transmission latency"
49,Hopefully this is helpful to people.
49,View or Post Comments
49,Performance Goalposts
49,"Wednesday, April 22, 2020"
49,"In talking to EnterpriseDB customers, I am often asked about the performance limits of Postgres:"
49,How many connections can it handle?
49,How many tps?
49,"Well, those are good questions, but it is hard to give accurate answers since so much depends on the hardware and workload."
49,"Eventually, testing of actual workloads on intended hardware has to be done, but"
49,not giving ball-park answers is unhelpful.
49,What answer can I give?
49,"Well, I came up with this chart:"
49,#SimultaneousLimitSolution
49,1Connections< 250direct connect
49,2Queries< 250pgbouncer
49,3Write queries< 250Pgpool with read-only replicas (slide 17)
49,4Write queries>= 250sharding
49,"Earlier items use simpler architectures than later items, and are therefore preferred."
49,"For under 250 simultaneous connections(#1), it isn't generally necessary to use any complex architecture, though using a pooler is recommended for workloads with many short-lived sessions to"
49,reduce connection startup time.
49,"Even when under this limit, performance can be limited by the number of cpus."
49,Oltp databases typically cannot process more than 5 x cpu cores without having to time-slice among cpus.
49,"Olap uses 2 x cpu cores, or less if parallelism is used."
49,"For over 250 simultaneous connections but under 250 simultaneous queries(#2), a connection pooler can be beneficial because it reduces"
49,the overhead of managing many open connections.
49,"For over 250 simultaneous queries but under 250 simultaneous write queries(#3), a combination of Pgpool with read queries routed to streaming replicas"
49,can be very efficient.
49,"For over 250 simultaneous write queries(#4), a sharding solution should be considered."
49,"These numbers can vary greatly based on workload and hardware, but I think they are good starting points to consider."
49,View or Post Comments
49,Fast Enough?
49,"Monday, April 20, 2020"
49,Everyone one wants their software to perform as fast as possible.
49,"Some people think that unless the program ends before it begins, it's too slow."
49,"However, realistically, making something as fast as possible is not a universal good, meaning that increasing performance beyond a certain point can cause problems that far exceed the value of the"
49,improved performance.
49,Let's give some examples:
49,Most people use high-level languages that are compiled into
49,cpu instructions or interpreted while being run.
49,"In an ideal world, every program would be written"
49,in assembly language.
49,(Postgres does use some assembly language code for
49,locking.)
49,"However, writing something like a database in assembly language, though it might give slightly better performance if developers knew the behavior of every cpu, would be a huge challenge"
49,"for even the most expert developers, and code maintenance and feature additions might be impossible."
49,"In general, there just isn't enough benefit to using assembly language for anything but the most"
49,"discrete, performance-critical functions."
49,"Java isn't often chosen for its performance, but rather its ability to allow development teams to produce complex software efficiently."
49,"Lighter-weight languages might give better performance, but they"
49,don't offer the same development efficiency as Java.
49,"Sql is a heavy-weight way to request data, but it is very efficient for developers since they can express their requests in a"
49,declarative way.
49,"You could probably write a custom data storage program to run faster than sql, but the effort involved to"
49,create and maintain it would be nearly insurmountable.
49,"You can even layout data in sql in more efficient ways, and sometimes it is worth it, but data maintainability, access flexibility, and storage efficiency can suffer."
49,"The bottom line is that performance is rarely a universal good — it has to be balanced against development time, solution flexibility, and maintainability."
49,Sometimes people suggest that Postgres should
49,"offer some ""super fancy"" optimization, and sometimes we can implement it, but we always have to balance development time, solution"
49,"flexibility, and maintainability with improved performance."
49,View or Post Comments
49,No Travel
49,"Friday, April 17, 2020"
49,"With the Coronavirus outbreak, almost all Postgres events through June have been either cancelled, rescheduled, or moved online."
49,This has given me time
49,to consider my past travel.
49,"I have been blessed to visit so many places, not as a tourist, but rather a guest."
49,I see countries more as a native than as a
49,"tourist, and I have many event organizers and hosts to thank for this."
49,"It is hard to know when on-site events will resume, but I can remember what it was like to travel roughly 90 days a year."
49,Going to social outings at
49,"home often felt riding a train through my home town, waving from behind a train window to my friends on the platform."
49,I would sit at home and wonder how long I would be there until I had to leave again.
49,"Now, sitting at home, the images of our family travels appearing on our kitchen slideshow show places that seem farther away than ever, and I wonder if I will ever see these places again."
49,"I am sure many others have similar feelings, and I have hope that, someday, we will all return to the road to spend time together again."
49,View or Post Comments
49,Database Interoperability at Risk
49,"Monday, March 16, 2020"
49,This article parallels Oracle's copying of the sql syntax
49,from ibm in the late 1970's with Google's copying of the Java api.
49,It also explains the possible impact of the current case
49,soon to be decided by the US Supreme Court.
49,One thing it does not fully cover is the impact on sql-level interoperability between databases.
49,"If Oracle can claim the Java api as copyrightable, the sql language could be"
49,"considered copyrightable, allowing ibm to sue all relational database vendors and users for copyright infringement."
49,It might also allow database vendors to sue competitors when their
49,"sql-level features are copied, requiring either huge payments or the removal of interoperability syntax."
49,"The Postgres copyright is open, so any database vendor copying Postgres syntax is fine, but Postgres copying the syntax of other databases could be"
49,problematic.
49,"Relational database interoperability has been of huge benefit to data management, and this single case could call that into question."
49,Update: The case was decided in Google's favor. 2021-04-08
49,View or Post Comments
49,"Databases, Containers, and the Cloud"
49,"Saturday, March"
49,"7, 2020"
49,"A few months ago, I wrote a high-level presentation about the deployment benefits of using containers and cloud infrastructure for databases."
49,I am now ready to
49,share this presentation.
49,I have also added QR codes to the first and last slides of all
49,"my presentations, linking to my website."
49,View or Post Comments
49,2021
49,2020
49,2019
49,2018
49,2017
49,2016
49,2015
49,2014
49,2013
49,2012
49,2011
49,2010
49,2009
49,2008
49,Yearly Chart
49,RSS
50,Chapter 3. PostGIS AdministrationChapter 3. PostGIS AdministrationPrev   NextChapter 3. PostGIS AdministrationTable of Contents3.1. Performance Tuning3.1.1. Startup3.1.2. Runtime3.2. Configuring raster support3.3. Creating spatial databases3.3.1. Spatially enable database using EXTENSION3.3.2. Spatially enable database without using EXTENSION (discouraged)3.3.3. Create a spatially-enabled database from a template3.4. Upgrading spatial databases3.4.1. Soft upgrade3.4.2. Hard upgrade3.1. Performance Tuning3.1.1. Startup3.1.2. RuntimeTuning for PostGIS performance is much like tuning for any PostgreSQL workload.
50,"The only additional consideration is that geometries and rasters are usually large,"
50,"so memory-related optimizations generally have more of an impact on PostGIS than other types of PostgreSQL queries.For general details about optimizing PostgreSQL, refer to Tuning your PostgreSQL Server.For PostgreSQL 9.4+ configuration can be set at the server level without touching postgresql.conf or postgresql.auto.conf"
50,by using the ALTER SYSTEM command.ALTER SYSTEM SET work_mem = '256MB';
50,-- this forces non-startup configs to take effect for new connections
50,SELECT pg_reload_conf();
50,-- show current setting value
50,-- use SHOW ALL to see all settings
50,"SHOW work_mem;In addition to the Postgres settings, PostGIS has some custom settings which are listed in Section 5.23, “Grand Unified Custom Variables (GUCs)”.3.1.1. Startup"
50,These settings are configured in postgresql.conf:
50,constraint_exclusion
50,Default: partition
50,"This is generally used for table partitioning. The default for this is set to ""partition"" which is ideal for PostgreSQL 8.4 and above since"
50,it will force the planner to only analyze tables for constraint consideration if they are in an inherited hierarchy
50,and not pay the planner penalty otherwise.
50,shared_buffers
50,Default: ~128MB in PostgreSQL 9.6
50,Set to about 25% to 40% of available RAM.
50,On windows you may not be able to set as high.
50,max_worker_processes
50,This setting is only available for PostgreSQL 9.4+.
50,For PostgreSQL 9.6+ this setting has additional importance in that it controls the
50,max number of processes you can have for parallel queries.
50,Default: 8
50,Sets the maximum number of background processes that
50,the system can support. This parameter can only be set at
50,server start.
50,3.1.2. Runtime
50,work_mem
50,- sets the size of memory used for sort operations and complex queries
50,Default: 1-4MB
50,"Adjust up for large dbs, complex queries, lots of RAM"
50,Adjust down for many concurrent users or low RAM.
50,If you have lots of RAM and few developers:
50,SET work_mem TO '256MB';
50,maintenance_work_mem
50,"- the memory size used for VACUUM, CREATE INDEX, etc."
50,Default: 16-64MB
50,"Generally too low - ties up I/O, locks objects while swapping memory"
50,"Recommend 32MB to 1GB on production servers w/lots of RAM, but depends"
50,on the # of concurrent users.
50,If you have lots of RAM and few developers:
50,SET maintenance_work_mem TO '1GB';
50,max_parallel_workers_per_gather
50,"This setting is only available for PostgreSQL 9.6+ and will only affect PostGIS 2.3+, since only PostGIS 2.3+ supports parallel queries."
50,"If set to higher than 0, then some queries such as those involving relation functions like ST_Intersects can use multiple processes and can run"
50,more than twice as fast when doing so.
50,"If you have a lot of processors to spare, you should change the value of this to as many processors as you have."
50,Also make sure to bump up max_worker_processes to at least as high as this number.
50,Default: 0
50,Sets the maximum number of workers that can be started
50,by a single Gather node.
50,Parallel workers are taken from the pool of processes
50,established by max_worker_processes.
50,Note that the requested number of workers may not
50,"actually be available at run time. If this occurs, the"
50,"plan will run with fewer workers than expected, which may"
50,"be inefficient. Setting this value to 0, which is the"
50,"default, disables parallel query execution."
50,3.2. Configuring raster support
50,If you enabled raster support you may want to read
50,below how to properly configure it.
50,"As of PostGIS 2.1.3, out-of-db rasters and all raster drivers are disabled by default. In order to re-enable these, you need to set the following environment variables"
50,"POSTGIS_GDAL_ENABLED_DRIVERS and POSTGIS_ENABLE_OUTDB_RASTERS in the server environment. For PostGIS 2.2, you can use the more cross-platform approach of setting the corresponding Section 5.23, “Grand Unified Custom Variables (GUCs)”.If you want to enable offline raster:POSTGIS_ENABLE_OUTDB_RASTERS=1Any other setting or no setting at all will disable out of db rasters.In order to enable all GDAL drivers available in your GDAL install, set this environment variable as followsPOSTGIS_GDAL_ENABLED_DRIVERS=ENABLE_ALLIf you want to only enable specific drivers, set your environment variable as follows:POSTGIS_GDAL_ENABLED_DRIVERS=""GTiff PNG JPEG GIF XYZ""If you are on windows, do not quote the driver listSetting environment variables varies depending on OS."
50,"For PostgreSQL installed on Ubuntu or Debian via apt-postgresql, the preferred way is to"
50,"edit /etc/postgresql/10/main/environment where 10 refers to version of PostgreSQL and main refers to the cluster.On windows, if you are running as a service, you can set via System variables which for Windows 7 you can get to by right-clicking on Computer->Properties Advanced System Settings or in explorer navigating to Control Panel\All Control Panel Items\System."
50,"Then clicking Advanced System Settings ->Advanced->Environment Variables and adding new system variables.After you set the environment variables, you'll need to restart your PostgreSQL service for the changes to take effect.3.3. Creating spatial databases3.3.1. Spatially enable database using EXTENSION3.3.2. Spatially enable database without using EXTENSION (discouraged)3.3.3. Create a spatially-enabled database from a template3.3.1. Spatially enable database using EXTENSION"
50,"If you are using PostgreSQL 9.1+ and have compiled and installed the extensions/postgis modules, you"
50,can turn a database into a spatial one using the EXTENSION mechanism.
50,"Core postgis extension includes geometry, geography,"
50,spatial_ref_sys and all the functions and comments.
50,Raster and topology are packaged as a separate extension.
50,Run the following SQL snippet in the database you want to enable spatially:
50,CREATE EXTENSION IF NOT EXISTS plpgsql;
50,CREATE EXTENSION postgis;
50,CREATE EXTENSION postgis_raster; -- OPTIONAL
50,CREATE EXTENSION postgis_topology; -- OPTIONAL
50,3.3.2. Spatially enable database without using EXTENSION (discouraged)This is generally only needed if you cannot or don't
50,want to get PostGIS installed in the PostgreSQL extension directory
50,"(for example during testing, development or in a restricted"
50,environment).
50,Adding PostGIS objects and function definitions into your
50,database is done by loading the various sql files located in
50,[prefix]/share/contrib as specified during
50,the build phase.
50,"The core PostGIS objects (geometry and geography types, and their"
50,support functions) are in the postgis.sql
50,script.
50,Raster objects are in the rtpostgis.sql script.
50,Topology objects are in the topology.sql script.
50,"For a complete set of EPSG coordinate system definition identifiers, you"
50,can also load the spatial_ref_sys.sql definitions
50,file and populate the spatial_ref_sys table. This will
50,permit you to perform ST_Transform() operations on geometries.
50,"If you wish to add comments to the PostGIS functions, you can find"
50,them in the postgis_comments.sql script.
50,Comments can be viewed by simply typing \dd
50,[function_name] from a psql terminal window.
50,Run the following Shell commands in your terminal:
50,DB=[yourdatabase]
50,SCRIPTSDIR=`pg_config --sharedir`/contrib/postgis-3.1/
50,# Core objects
50,psql -d ${DB} -f ${SCRIPTSDIR}/postgis.sql
50,psql -d ${DB} -f ${SCRIPTSDIR}/spatial_ref_sys.sql
50,psql -d ${DB} -f ${SCRIPTSDIR}/postgis_comments.sql # OPTIONAL
50,# Raster support (OPTIONAL)
50,psql -d ${DB} -f ${SCRIPTSDIR}/rtpostgis.sql
50,psql -d ${DB} -f ${SCRIPTSDIR}/raster_comments.sql # OPTIONAL
50,# Topology support (OPTIONAL)
50,psql -d ${DB} -f ${SCRIPTSDIR}/topology.sql
50,psql -d ${DB} -f ${SCRIPTSDIR}/topology_comments.sql # OPTIONAL
50,3.3.3. Create a spatially-enabled database from a template
50,Some packaged distributions of PostGIS (in particular the Win32 installers
50,for PostGIS >= 1.1.5) load the PostGIS functions into a template
50,database called template_postgis. If the
50,template_postgis database exists in your PostgreSQL
50,installation then it is possible for users and/or applications to create
50,spatially-enabled databases using a single command. Note that in both
50,"cases, the database user must have been granted the privilege to create"
50,new databases.
50,From the shell:
50,# createdb -T template_postgis my_spatial_db
50,From SQL:
50,postgres=# CREATE DATABASE my_spatial_db TEMPLATE=template_postgis3.4. Upgrading spatial databases3.4.1. Soft upgrade3.4.2. Hard upgrade
50,Upgrading existing spatial databases can be tricky as it requires
50,replacement or introduction of new PostGIS object definitions.
50,Unfortunately not all definitions can be easily replaced in a live
50,"database, so sometimes your best bet is a dump/reload process."
50,"PostGIS provides a SOFT UPGRADE procedure for minor or bugfix releases,"
50,and a HARD UPGRADE procedure for major releases.
50,"Before attempting to upgrade PostGIS, it is always worth to backup your"
50,data. If you use the -Fc flag to pg_dump you will always be able to
50,restore the dump with a HARD UPGRADE.
50,"3.4.1. Soft upgrade3.4.1.1. Soft Upgrade Pre 9.1+ or without extensions3.4.1.2. Soft Upgrade 9.1+ using extensionsIf you installed your database using extensions, you'll need to upgrade using the extension model as well."
50,"If you installed using the old sql script way,"
50,then you should upgrade using the sql script way. Please refer to the appropriate.3.4.1.1. Soft Upgrade Pre 9.1+ or without extensionsThis section applies only to those who installed PostGIS
50,not using extensions.
50,If you have extensions and try to
50,upgrade with this approach you'll get messages like:can't drop ... because postgis extension depends on itNOTE: if you are moving from PostGIS 1.* to PostGIS 2.* or from
50,"PostGIS 2.* prior to r7409, you cannot use this procedure but"
50,would rather need to do a
50,HARD UPGRADE.
50,After compiling and installing (make install) you should
50,find a set of	*_upgrade.sql
50,files in the installation folders. You can list
50,them all with:
50,ls `pg_config --sharedir`/contrib/postgis-3.1.2dev/*_upgrade.sql
50,"Load them all in turn, starting from postgis_upgrade.sql."
50,psql -f postgis_upgrade.sql -d your_spatial_database
50,"The same procedure applies to raster,"
50,"topology and sfcgal extensions, with upgrade files named"
50,"rtpostgis_upgrade.sql,"
50,topology_upgrade.sql and
50,sfcgal_upgrade.sql respectively.
50,If you need them:
50,psql -f rtpostgis_upgrade.sql -d your_spatial_databasepsql -f topology_upgrade.sql -d your_spatial_databasepsql -f sfcgal_upgrade.sql -d your_spatial_database
50,If you can't find the
50,postgis_upgrade.sql specific for
50,upgrading your version you are using a version too
50,early for a soft upgrade and need to do a
50,HARD UPGRADE.
50,The PostGIS_Full_Version function
50,should inform you about the need to run this kind of
50,"upgrade using a ""procs need upgrade"" message."
50,"3.4.1.2. Soft Upgrade 9.1+ using extensionsIf you originally installed PostGIS with extensions, then you need to upgrade using extensions as well."
50,"Doing a minor upgrade with extensions, is fairly painless.ALTER EXTENSION postgis UPDATE TO ""3.1.2dev"";"
50,"ALTER EXTENSION postgis_topology UPDATE TO ""3.1.2dev"";If you get an error notice something like:No migration path defined for ... to 3.1.2devThen you'll need to backup your database, create a fresh one as described in Section 3.3.1, “Spatially enable database using EXTENSION” and then restore your backup ontop of this new database.If you get a notice message like:Version ""3.1.2dev"" of extension ""postgis"" is already installed"
50,Then everything is already up to date and you can safely ignore it. UNLESS
50,you're attempting to upgrade from an development version to the next (which
50,"doesn't get a new version number); in that case you can append ""next"" to the version"
50,"string, and next time you'll need to drop the ""next"" suffix again:"
50,"ALTER EXTENSION postgis UPDATE TO ""3.1.2devnext"";"
50,"ALTER EXTENSION postgis_topology UPDATE TO ""3.1.2devnext"";If you installed PostGIS originally without a version specified, you can often skip the reinstallation of postgis extension before restoring since the backup just has CREATE EXTENSION postgis and thus"
50,picks up the newest latest version during restore.
50,If you are upgrading PostGIS extension from a version prior to 3.0.0
50,you'll end up with an unpackaged PostGIS Raster support. You can
50,repackage the raster support using:
50,CREATE EXTENSION postgis_raster FROM unpackaged;
50,"And then, if you don't need it, drop it with:"
50,DROP EXTENSION postgis_raster;
50,3.4.2. Hard upgrade
50,By HARD UPGRADE we mean full dump/reload of postgis-enabled databases.
50,You need a HARD UPGRADE when PostGIS objects' internal storage changes
50,or when SOFT UPGRADE is not possible. The
50,Release Notes
50,appendix reports for each version whether you need a dump/reload (HARD
50,UPGRADE) to upgrade.
50,The dump/reload process is assisted by the postgis_restore.pl
50,script which takes care of skipping from the dump all
50,"definitions which belong to PostGIS (including old ones),"
50,allowing you to restore your schemas and data into a
50,database with PostGIS installed without getting duplicate
50,symbol errors or bringing forward deprecated objects.
50,Supplementary instructions for windows users are available at
50,Windows Hard upgrade.
50,The Procedure is as follows:
50,"Create a ""custom-format"" dump of the database you want"
50,to upgrade (let's call it olddb)
50,include binary blobs (-b) and verbose (-v) output.
50,"The user can be the owner of the db, need not be postgres"
50,super account.
50,"pg_dump -h localhost -p 5432 -U postgres -Fc -b -v -f ""/somepath/olddb.backup"" olddb"
50,Do a fresh install of PostGIS in a new database -- we'll
50,refer to this database as newdb.
50,"Please refer to Section 3.3.2, “Spatially enable database without using EXTENSION (discouraged)” and Section 3.3.1, “Spatially enable database using EXTENSION” for"
50,instructions on how to do this.
50,The spatial_ref_sys entries found in your dump will be
50,"restored, but they will not override existing ones in"
50,spatial_ref_sys.
50,This is to ensure that fixes in the
50,official set will be properly propagated to restored
50,databases. If for any reason you really want your own
50,overrides of standard entries just don't load the
50,spatial_ref_sys.sql file when creating the new db.
50,If your database is really old or you know you've
50,been using long deprecated functions in your
50,"views and functions, you might need to load"
50,legacy.sql for all your functions
50,and views etc. to properly come back.
50,Only do this if _really_ needed. Consider upgrading your
50,"views and functions before dumping instead, if possible."
50,The deprecated functions can be later removed by loading
50,uninstall_legacy.sql.
50,Restore your backup into your fresh
50,newdb database using
50,postgis_restore.pl.
50,"Unexpected errors, if any, will be printed to the standard"
50,error stream by psql. Keep a log of those.
50,"perl utils/postgis_restore.pl ""/somepath/olddb.backup"" | psql -h localhost -p 5432 -U postgres newdb 2> errors.txt"
50,Errors may arise in the following cases:
50,Some of your views or functions make use of deprecated PostGIS objects.
50,In order to fix this you may try loading legacy.sql
50,script prior to restore or you'll have to restore to a
50,version of PostGIS which still contains those objects
50,and try a migration again after porting your code.
50,"If the legacy.sql way works for you, don't forget"
50,to fix your code to stop using deprecated functions and drop them
50,loading uninstall_legacy.sql.
50,Some custom records of spatial_ref_sys in dump file have
50,an invalid SRID value. Valid SRID values are bigger than 0
50,and smaller than 999000. Values in the 999000.999999 range
50,are reserved for internal use while values > 999999 can't
50,be used at all.
50,"All your custom records with invalid SRIDs will be retained,"
50,"with those > 999999 moved into the reserved range, but the"
50,spatial_ref_sys table would lose a check constraint guarding
50,for that invariant to hold and possibly also its primary key
50,( when multiple invalid SRIDS get converted to the same reserved
50,SRID value ).
50,In order to fix this you should copy your custom SRS to
50,a SRID with a valid value (maybe in the 910000..910999
50,"range), convert all your tables to the new srid (see"
50,"UpdateGeometrySRID), delete the invalid"
50,entry from spatial_ref_sys and re-construct the check(s) with:
50,ALTER TABLE spatial_ref_sys ADD CONSTRAINT spatial_ref_sys_srid_check check (srid > 0 AND srid < 999000 );
50,ALTER TABLE spatial_ref_sys ADD PRIMARY KEY(srid));
50,If you are upgrading an old database containing french
50,IGN
50,"cartography, you will have probably SRIDs out"
50,"of range and you will see, when importing your database, issues like this :"
50,WARNING: SRID 310642222 converted to 999175 (in reserved zone)
50,"In this case, you can try following steps : first throw"
50,out completely the IGN from the sql which is resulting
50,"from postgis_restore.pl. So, after having run :"
50,"perl utils/postgis_restore.pl ""/somepath/olddb.backup"" > olddb.sql"
50,run this command :
50,grep -v IGNF olddb.sql > olddb-without-IGN.sql
50,"Create then your newdb, activate the required Postgis extensions,"
50,and insert properly the french system IGN with :
50,this script
50,"After these operations, import your data :"
50,psql -h localhost -p 5432 -U postgres -d newdb -f olddb-without-IGN.sql
50,2> errors.txt
50,Prev   NextChapter 2. PostGIS Installation Home Chapter 4. PostGIS Usage
51,"Performance | GORM - The fantastic ORM library for Golang, aims to be developer friendly."
51,GORM
51,DocsCommunityAPIContribute
51,English
51,English
51,简体中文
51,Deutsch
51,Español
51,bahasa Indonesia
51,Italiano
51,日本語
51,Русский
51,French
51,한국어
51,Performance
51,"GORM optimizes many things to improve the performance, the default performance should good for most applications, but there are still some tips for how to improve it for your application."
51,"Disable Default TransactionGORM perform write (create/update/delete) operations run inside a transaction to ensure data consistency, which is bad for performance, you can disable it during initialization"
51,"db, err := gorm.Open(sqlite.Open(""gorm.db""), &gorm.Config{"
51,"SkipDefaultTransaction: true,})"
51,Caches Prepared StatementCreates a prepared statement when executing any SQL and caches them to speed up future calls
51,"// Globally modedb, err := gorm.Open(sqlite.Open(""gorm.db""), &gorm.Config{"
51,"PrepareStmt: true,})// Session modetx := db.Session(&Session{PrepareStmt: true})tx.First(&user, 1)tx.Find(&users)tx.Model(&user).Update(""Age"", 18)"
51,NOTE Also refer how to enable interpolateparams for MySQL to reduce roundtrip https://github.com/go-sql-driver/mysql#interpolateparams
51,"SQL Builder with PreparedStmtPrepared Statement works with RAW SQL also, for example:"
51,"db, err := gorm.Open(sqlite.Open(""gorm.db""), &gorm.Config{"
51,"PrepareStmt: true,})db.Raw(""select sum(age) from users where role = ?"", ""admin"").Scan(&age)"
51,"You can also use GORM API to prepare SQL with DryRun Mode, and execute it with prepared statement later, checkout Session Mode for details"
51,"Select FieldsBy default GORM select all fields when querying, you can use Select to specify fields you want"
51,"db.Select(""Name"", ""Age"").Find(&Users{})"
51,Or define a smaller API struct to use the smart select fields feature
51,type User struct {
51,uint
51,Name
51,string
51,Age
51,int
51,Gender string
51,// hundreds of fields}type APIUser struct {
51,uint
51,"Name string}// Select `id`, `name` automatically when querydb.Model(&User{}).Limit(10).Find(&APIUser{})// SELECT `id`, `name` FROM `users` LIMIT 10"
51,Iteration / FindInBatchesQuery and process records with iteration or in batches
51,"Index HintsIndex is used to speed up data search and SQL query performance. Index Hints gives the optimizer information about how to choose indexes during query processing, which gives the flexibility to choose a more efficient execution plan than the optimizer"
51,"import ""gorm.io/hints""db.Clauses(hints.UseIndex(""idx_user_name"")).Find(&User{})// SELECT * FROM `users` USE INDEX (`idx_user_name`)db.Clauses(hints.ForceIndex(""idx_user_name"", ""idx_user_id"").ForJoin()).Find(&User{})// SELECT * FROM `users` FORCE INDEX FOR JOIN (`idx_user_name`,`idx_user_id`)""db.Clauses("
51,"hints.ForceIndex(""idx_user_name"", ""idx_user_id"").ForOrderBy(),"
51,"hints.IgnoreIndex(""idx_user_name"").ForGroupBy(),).Find(&User{})// SELECT * FROM `users` FORCE INDEX FOR ORDER BY (`idx_user_name`,`idx_user_id`) IGNORE INDEX FOR GROUP BY (`idx_user_name`)"""
51,"Read/Write SplittingIncrease data throughput through read/write splitting, check out Database Resolver"
51,Last updated: 2021-04-09
51,PrevNext
51,Platinum Sponsors
51,Become a Sponsor!
51,Platinum Sponsors
51,Become a Sponsor!
51,OpenCollective Sponsors
51,Contents
51,Disable Default TransactionCaches Prepared StatementSQL Builder with PreparedStmtSelect FieldsIteration / FindInBatchesIndex HintsRead/Write Splitting
51,Improve this page
51,Back to Top
51,Getting StartedOverviewDeclaring ModelsConnecting to DatabaseCRUD InterfaceCreateQueryAdvanced QueryUpdateDeleteRaw SQL & SQL BuilderAssociationsBelongs ToHas OneHas ManyMany To ManyAssociation ModePreloading (Eager Loading)TutorialsContextError HandlingMethod ChainingSessionHooksTransactionsMigrationLoggerGeneric Database InterfacePerformanceCustomize Data TypesScopesConventionsSettingsAdvanced TopicsDatabase ResolverPrometheusHintsIndexesConstraintsComposite Primary KeySecurityGORM ConfigWrite PluginsWrite DriverChangeLogCommunityContributeTranslate current site
51,© 2013~2021 Jinzhu
51,Documentation licensed under CC BY 4.0.
51,感谢 七牛云 对 CDN 的赞助，无闻 对域名 gorm.cn 的捐赠
51,浙ICP备2020033190号-1
51,Home
51,DocsCommunityAPIContribute
51,Getting StartedOverviewDeclaring ModelsConnecting to DatabaseCRUD InterfaceCreateQueryAdvanced QueryUpdateDeleteRaw SQL & SQL BuilderAssociationsBelongs ToHas OneHas ManyMany To ManyAssociation ModePreloading (Eager Loading)TutorialsContextError HandlingMethod ChainingSessionHooksTransactionsMigrationLoggerGeneric Database InterfacePerformanceCustomize Data TypesScopesConventionsSettingsAdvanced TopicsDatabase ResolverPrometheusHintsIndexesConstraintsComposite Primary KeySecurityGORM ConfigWrite PluginsWrite DriverChangeLogCommunityContributeTranslate current site
51,English
51,English
51,简体中文
51,Deutsch
51,Español
51,bahasa Indonesia
51,Italiano
51,日本語
51,Русский
51,French
51,한국어
52,sql - Performance Tuning in ODI 12c - Stack Overflow
52,Stack Overflow
52,About
52,Products
52,For Teams
52,Stack Overflow
52,Public questions & answers
52,Stack Overflow for Teams
52,Where developers & technologists share private knowledge with coworkers
52,Jobs
52,Programming & related technical career opportunities
52,Talent
52,Recruit tech talent & build your employer brand
52,Advertising
52,Reach developers & technologists worldwide
52,About the company
52,Loading…
52,Log in
52,Sign up
52,current community
52,Stack Overflow
52,help
52,chat
52,Meta Stack Overflow
52,your communities
52,Sign up or log in to customize your list.
52,more stack exchange communities
52,company blog
52,"Join Stack Overflow to learn, share knowledge, and build your career."
52,Sign up with email
52,Sign up
52,Sign up with Google
52,Sign up with GitHub
52,Sign up with Facebook
52,Home
52,Public
52,Questions
52,Tags
52,Users
52,Find a Job
52,Jobs
52,Companies
52,Teams
52,Stack Overflow for Teams
52,– Collaborate and share knowledge with a private group.
52,Create a free Team
52,What is Teams?
52,Teams
52,What’s this?
52,Create free Team
52,Teams
52,Q&A for work
52,Connect and share knowledge within a single location that is structured and easy to search.
52,Learn more
52,Performance Tuning in ODI 12c
52,Ask Question
52,Asked
52,8 months ago
52,Active
52,7 months ago
52,Viewed
52,346 times
52,I want to migrate more than 10 million of data from source (oracle) to target (oracle) in shorter time in Oracle Data Integrator 12c. I have tried to create multiple scenarios and assign each scenario a million records and run the package of 10 scenarios. Time was reduced but is there any other way so that i can increase the performance of my ODI mapping having more than 10 million records?
52,I expect a less time for the execution of the mapping for better performance.
52,sql oracle oracle-data-integrator
52,Share
52,Improve this question
52,Follow
52,edited Aug 6 '20 at 8:41
52,James Z
52,11.8k1010 gold badges2424 silver badges4141 bronze badges
52,asked Aug 6 '20 at 8:35
52,Arif AliArif Ali
52,5388 bronze badges
52,Please don't measure things using Indian words
52,– James Z
52,Aug 6 '20 at 8:41
52,use this link kpipartners.com/blog/bid/149359/…
52,– Roberto Hernandez
52,Aug 6 '20 at 12:25
52,"We need more details about your mappings. Which knowledge modules (LKM, IKM and potentially CKM) are you using? What is the network architecture between your source and your target? Which steps takes the longest in the operator? Do you have indexes? Do you have partitioning? Are the table statistics correctly gathered? Are you using hints?"
52,– JeromeFr
52,Sep 30 '20 at 9:17
52,"For the loading stage we are using LKM Oracle to Oracle , IKM oracle incremental update, CKM oracle. The steps which takes longer is Update existing rows , we dont have indexes, partitioning, using hints on target."
52,– Arif Ali
52,Oct 5 '20 at 9:27
52,Add a comment
52,1 Answer
52,Active
52,Oldest
52,Votes
52,"Further to add, on the ODI 12c you have option to place the source side indxes, in case the query is performing bad on the source side under the joins option you that option in the physical tab, else you can also deploy the hints in the physical tab. please let me know if these work."
52,Share
52,Improve this answer
52,Follow
52,answered Sep 4 '20 at 21:22
52,Rohith AmazRohith Amaz
52,Add a comment
52,Your Answer
52,"Thanks for contributing an answer to Stack Overflow!Please be sure to answer the question. Provide details and share your research!But avoid …Asking for help, clarification, or responding to other answers.Making statements based on opinion; back them up with references or personal experience.To learn more, see our tips on writing great answers."
52,Draft saved
52,Draft discarded
52,Sign up or log in
52,Sign up using Google
52,Sign up using Facebook
52,Sign up using Email and Password
52,Submit
52,Post as a guest
52,Name
52,Email
52,"Required, but never shown"
52,Post as a guest
52,Name
52,Email
52,"Required, but never shown"
52,Post Your Answer
52,Discard
52,"By clicking “Post Your Answer”, you agree to our terms of service, privacy policy and cookie policy"
52,Not the answer you're looking for? Browse other questions tagged sql oracle oracle-data-integrator
52,or ask your own question.
52,The Overflow Blog
52,What international tech recruitment looks like post-COVID-19
52,"Podcast 328: For Twilio’s CIO, every internal developer is a customer"
52,Featured on Meta
52,"Stack Overflow for Teams is now free for up to 50 users, forever"
52,Outdated Answers: results from use-case survey
52,Related
52,ODI 12c LKM in mapping without IKM
52,how to create a non-existing odi target mapping datasource from source datasource?
52,How to add new datatype in ODI using groovy script
52,How can we migrate the data directly from source to target table in ODI
52,"Is thera a way to find forced execution context on datastore objects, somewhere in ODI metadata database?"
52,Using Oracle Data Integrator 12c with Snowflake
52,"Does cube of ODI support OLAP operations like dicing, drilling, pivoting"
52,Hot Network Questions
52,Ambiguity of regular expressions
52,Security on a Nexus 6P purchased on Ebay
52,How can I compress my .PDF (two pages) to less than 100 kB?
52,How to extract unique values from .pbf file with GDAL
52,Can I have utility location done well in advance of a fence project?
52,Promised authorship was not given; am I supposed to get an automatic acknowledgement?
52,Spelling a word with a double letter in spoken French
52,What is the age of the old man?
52,Cooperative Counting
52,"Is Ezekiel 20 talking about works-based salvation when it says ""my rules, by which, if a person does them, he shall live""?"
52,Representations of products of symmetric groups
52,Did Lorentz remain an ether advocate till his death?
52,Regarding the writing of non-canon works in your own universe
52,Does biometric authentication in Android transfer any biometric data to the app?
52,Is there any (security) reason in Windows for this very weird behavior?
52,"What's the best way to actually ""type"" special UTF-8 chars?"
52,The Genie warlock patron's Expanded Spell List adds the Wish spell as a 9th-level Mystic Arcanum option. Do they suffer stress from using Wish?
52,Does an extra old internal hard disk drive affect my new PC system’s performance?
52,"If the Roman Empire in the Year 90 AD Found a Living Tyrannosaurus Rex, What Creature in Their Mythology Would They Most Likely Mistake It For?"
52,Making Suitable Clothing For Fire Mages
52,"If I short crypto, why don't I get the money immediately?"
52,Why do images not appear inverted when looking directly through a pinhole camera?
52,An Old Number theory IMO question
52,Why are 2x4 bricks hard to get in certain colors?
52,more hot questions
52,Question feed
52,Subscribe to RSS
52,Question feed
52,"To subscribe to this RSS feed, copy and paste this URL into your RSS reader."
52,lang-sql
52,Stack Overflow
52,Questions
52,Jobs
52,Developer Jobs Directory
52,Salary Calculator
52,Help
52,Mobile
52,Products
52,Teams
52,Talent
52,Advertising
52,Enterprise
52,Company
52,About
52,Press
52,Work Here
52,Legal
52,Privacy Policy
52,Terms of Service
52,Contact Us
52,Cookie Settings
52,Cookie Policy
52,Stack Exchange Network
52,Technology
52,Life / Arts
52,Culture / Recreation
52,Science
52,Other
52,Stack Overflow
52,Server Fault
52,Super User
52,Web Applications
52,Ask Ubuntu
52,Webmasters
52,Game Development
52,TeX - LaTeX
52,Software Engineering
52,Unix & Linux
52,Ask Different (Apple)
52,WordPress Development
52,Geographic Information Systems
52,Electrical Engineering
52,Android Enthusiasts
52,Information Security
52,Database Administrators
52,Drupal Answers
52,SharePoint
52,User Experience
52,Mathematica
52,Salesforce
52,ExpressionEngine® Answers
52,Stack Overflow em Português
52,Blender
52,Network Engineering
52,Cryptography
52,Code Review
52,Magento
52,Software Recommendations
52,Signal Processing
52,Emacs
52,Raspberry Pi
52,Stack Overflow на русском
52,Code Golf
52,Stack Overflow en español
52,Ethereum
52,Data Science
52,Arduino
52,Bitcoin
52,Software Quality Assurance & Testing
52,Sound Design
52,Windows Phone
52,more (28)
52,Photography
52,Science Fiction & Fantasy
52,Graphic Design
52,Movies & TV
52,Music: Practice & Theory
52,Worldbuilding
52,Video Production
52,Seasoned Advice (cooking)
52,Home Improvement
52,Personal Finance & Money
52,Academia
52,Law
52,Physical Fitness
52,Gardening & Landscaping
52,Parenting
52,more (10)
52,English Language & Usage
52,Skeptics
52,Mi Yodeya (Judaism)
52,Travel
52,Christianity
52,English Language Learners
52,Japanese Language
52,Chinese Language
52,French Language
52,German Language
52,Biblical Hermeneutics
52,History
52,Spanish Language
52,Islam
52,Русский язык
52,Russian Language
52,Arqade (gaming)
52,Bicycles
52,Role-playing Games
52,Anime & Manga
52,Puzzling
52,Motor Vehicle Maintenance & Repair
52,Board & Card Games
52,Bricks
52,Homebrewing
52,Martial Arts
52,The Great Outdoors
52,Poker
52,Chess
52,Sports
52,more (16)
52,MathOverflow
52,Mathematics
52,Cross Validated (stats)
52,Theoretical Computer Science
52,Physics
52,Chemistry
52,Biology
52,Computer Science
52,Philosophy
52,Linguistics
52,Psychology & Neuroscience
52,Computational Science
52,more (10)
52,Meta Stack Exchange
52,Stack Apps
52,API
52,Data
52,Blog
52,Facebook
52,Twitter
52,LinkedIn
52,Instagram
52,site design / logo © 2021 Stack Exchange Inc; user contributions licensed under cc by-sa.
52,rev 2021.4.9.39043
52,Stack Overflow works best with JavaScript enabled
52,Your privacy
52,"By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy."
52,Accept all cookies
52,Customize settings
56,Teaching new Presto performance tricks to the Old-School DBA - Engineering Blog
56,Skip to content
56,Engineering Blog
56,Menu
56,Home
56,Front-End
56,Performance
56,Mobile
56,Conferences
56,Jobs
56,Teaching new Presto performance tricks to the Old-School DBA
56,"Posted on June 29, 2020 by Ed Presz"
56,Stories You will love
56,MySQL High Availability at Eventbrite
56,Leveraging AWS “spot” instances to drive down costs
56,"Open Data: The what, why and how to get started"
56,Boosting Big Data workloads with Presto Auto Scaling
56,"I’ve spent much of my career working with relational databases such as Oracle and MySQL, and SQL performance has always been an area of focus for me. I’ve spent countless hours reviewing EXPLAIN plans, rewriting subqueries, adding new indexes, and chasing down table-scans. I’ve been trained to make performance improvements such as:  only choose columns in a SELECT that are absolutely necessary, stay away from LIKE clauses, review the cardinality of columns before adding indexes, and always JOIN on indexed columns."
56,It’s been an instinctual part of my life as a Database Administrator who supports OLTP databases that have sold in excess of 20K tickets per minute to your favorite events. I remember a specific situation where a missing index caused our production databases to get flooded with table-scans that brought a world-wide on-sale to an immediate halt. I had a lot of explaining to do that day as the missing index made it to QA and Stage but not Production!
56,"In recent years, I’ve transitioned to Data Engineering and began supporting Big Data environments.  Specifically, I’m supporting Eventbrite’s Data Warehouse which leverages Presto and Apache Hive using the Presto/Hive connector. The data files can be of different formats, but we’re using HDFS and S3.  The Hive metadata describes how data stored in HDFS/S3 maps to schemas, tables, and columns to be queried via SQL. We persist this metadata information in Amazon Aurora and access it through the Presto/Hive connector via the Hive Metastore Service (HMS)."
56,"The stakes have changed and so have the skill-sets required. I’ve needed to retrain myself in how to write optimal SQL for Presto. Some of the best practices for Presto are the same as relational databases and others are brand new to me. This blog post summarizes some of the similarities and some of the differences with writing efficient SQL on MySQL vs Presto/Hive. Along the way I’ve had to learn new terms such as “federated queries”, “broadcast joins”, “reshuffling”, “join reordering”, and “predicate pushdown”."
56,Let’s start with the basics:
56,"What is MySQL? The world’s most popular open source database. The MySQL software delivers a fast, multi-threaded, multi-user, and robust SQL (Structured Query Language) database server. MySQL is intended for mission-critical, heavy-load production database usage."
56,"What is Presto? Presto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes. Presto doesn’t use the map reduce framework for its execution. Instead, Presto directly accesses the data through a specialized distributed query engine that is very similar to those found in commercial parallel relational databases."
56,"Presto uses ANSI SQL syntax/semantics to build its queries. The advantage of this is that analysts with experience with relational databases will find it very easy and straightforward to write Presto queries! That said, the best practices for developing efficient SQL via Presto/Hive are different from those used to query standard RDBMS databases."
56,Let’s transition to Presto performance tuning tips and how they compare to standard best practices with MySQL.
56,1. Only specify the columns you need
56,Restricting columns for SELECTs can improve your query performance significantly. Specify only needed columns instead of using a wildcard (*). This applies to Presto as well as MySQL!
56,2. Consider the cardinality within GROUP BY
56,"When using GROUP BY, order the columns by the highest cardinality (that is, most number of unique values) to the lowest."
56,"The GROUP BY operator distributes rows based on the order of the columns to the worker nodes, which hold the GROUP BY values in memory. As rows are being ingested, the GROUP BY columns are looked up in memory and the values are compared. If the GROUP BY columns match, the values are then aggregated together."
56,3. Use LIMIT with ORDER BY
56,"The ORDER BY clause returns the results of a query in sort order. To  process the sort, Presto must send all rows of data to a single worker and then sort them. This sort can be a very memory-intensive operation for large datasets that will put strain on the Presto workers. The end result will be long execution times and/or memory errors."
56,"If you are using the ORDER BY clause to look at the top N values, then use a LIMIT clause to reduce the cost of the sort significantly by pushing the sorting/limiting to individual workers, rather than the sorting being done by a single worker."
56,I highly recommend you use the LIMIT clause not just for SQL with ORDER BY but in any situation when you’re validating new SQL. This is good practice for MySQL as well as Presto!
56,4. Using approximate aggregate functions
56,When exploring large datasets often an approximation (with standard deviation of 2.3%) is more than good enough! Presto has approximate aggregation functions that give you significant performance improvements. Using the approx_distinct(x) function on large data sets vs COUNT(DISTINCT x) will result in performance gains.
56,"When an exact number may not be required―for instance, if you are looking for a rough estimate of the number of New Years events in the Greater New York area then consider using approx_distinct(). This function minimizes the memory usage by counting unique hashes of values instead of entire strings. The drawback is that there is a small standard deviation."
56,5. Aggregating a series of LIKE clauses in one single regexp_like clause
56,The LIKE operation is well known to be slow especially when not anchored to the left (i.e. the search text is surrounded by ‘%’ on both sides) or when used with a series of OR conditions. So it is no surprise that Presto’s query optimizer is unable to improve queries that contain many LIKE clauses.
56,"We’ve found improved  LIKE performance on Presto by  substituting the LIKE/OR  combination with a single REGEXP_LIKE clause, which is Presto native.  Not only is it easier to read but it’s also more performant. Based on some quick performance tests, we see ~30% increase in run-times with REGEXP_LIKE vs comparable LIKE/OR combination."
56,For example:
56,SELECT  ...FROM zoo
56,WHERE method LIKE '%monkey%' OR
56,method LIKE '%hippo%' OR
56,method LIKE '%tiger%' OR
56,method LIKE '%elephant%'
56,can be optimized by replacing the four LIKE clauses with a single REGEXP_LIKE clause:
56,SELECT  ...FROM zoo
56,"WHERE REGEXP_LIKE(method, 'monkey|hippo|tiger|elephant')"
56,6. Specifying large tables first in join clause
56,"When joining tables, specify the largest table first in the join. The default join algorithm of Presto is broadcast join, which partitions the left-hand side table of a join and sends (broadcasts) a copy of the entire right-hand side table to all of the worker nodes that have the partitions. If the right-hand side table is “small” then it can be replicated to all the join workers which will save CPU and network costs.  This type of join will be most efficient when the right-hand side table is small enough to fit within one node."
56,"If you receive an ‘Exceeded max memory’ error, then the right-hand side table is too large. Presto does not perform automatic join-reordering, so make sure your largest table is the first table in your sequence of joins."
56,"This was an interesting performance tip for me. As we know, SQL is a declarative language and the ordering of tables used in joins in MySQL, for example,  is *NOT* particularly important. The MySQL optimizer will re-order to choose the most efficient path. With Presto, the join order matters. You’ve been WARNED! Presto does not perform automatic join-reordering unless using the Cost Based Optimizer!"
56,7. Turning on the distributed hash join
56,"If you’re battling with memory errors then try a distributed hash join. This algorithm partitions both the left and right tables using the hash values of the join keys. So the distributed join works even if the right-hand side table is large, but the performance might be slower because the join increases the number of network data transfers."
56,At Eventbrite we have the distributed_join variable set to ‘true’. (SHOW SESSION). Also it can be enabled by setting a session property (set session distributed_join = ‘true’).
56,8. Partition your data
56,"Partitioning divides your table into parts and keeps the related data together based on column values such as date or country.  You define partitions at table creation, and they help reduce the amount of data scanned per query, thereby improving performance."
56,Here are some hints on partitioning:
56,Columns that are used as WHERE filters are good candidates for partitioning.
56,"Partitioning has a cost. As the number of partitions in your table increases, the higher the overhead of retrieving and processing the partition metadata, and the smaller your files. Use caution when partitioning and make sure you don’t partition too finely."
56,"If your data is heavily skewed to one partition value, and most queries use that value, then the overhead may wipe out the initial benefit."
56,A key partition column at Eventbrite is transaction date (txn_date).
56,CREATE TABLE IF NOT EXISTS fact_ticket_purchase
56,"ticket_id STRING,"
56,....
56,"create_date STRING,"
56,update_date STRING
56,PARTITIONED BY (trx_date STRING)
56,STORED AS PARQUET
56,TBLPROPERTIES ('parquet.compression'='SNAPPY')
56,9. Optimize columnar data store generation
56,"Apache Parquet and Apache ORC are popular columnar data stores. They provide features that store data efficiently by using column-wise compression based on data type, special encoding, and predicate pushdown. At Eventbrite, we define Hive tables as PARQUET using compression equal to SNAPPY…."
56,CREATE TABLE IF NOT EXISTS dim_event
56,"dim_event_id STRING,"
56,....
56,"create_date STRING,"
56,"update_date STRING,"
56,STORED AS PARQUET
56,TBLPROPERTIES ('parquet.compression'='SNAPPY')
56,"Apache Parquet is an open-source, column-oriented data storage format. Snappy is designed for speed and will not overload your CPU cores. The downside of course is that it does not compress as well as gzip or bzip2."
56,10. Presto’s Cost-Based Optimizer/Join Reordering
56,"We’re not currently using Presto’s Cost-Based Optimizer (CBO)! Eventbrite data engineering released Presto 330 in March 2020, but we haven’t tested CBO yet."
56,"CBO inherently requires the table stats be up-to-date which we only calculate for a small subset of tables! Using the CBO, Presto will be able to intelligently decide the best sequence based on the statistics stored in the Hive Metastore."
56,"As mentioned above, the order in which joins are executed in a query can have a big impact on performance. If we collect table statistics then the CBO can automatically pick the join order with the lowest computed costs. This is governed by the join_reordering_strategy (=AUTOMATIC) session property and I’m really excited to see this feature in action."
56,"Another interesting join optimization is dynamic filtering. It relies on the stats estimates of the CBO to correctly convert the join distribution type to “broadcast” join. By using dynamic filtering via run-time predicate pushdown, we can squeeze out more performance gains for highly-selective inner-joins.  We look forward to using this feature in the near future!"
56,11. Using WITH Clause
56,"The WITH clause is used to define an inline view within a single query.  It allows for flattening nested subqueries. I find it hugely helpful for simplifying SQL, and making it more readable and easier to support."
56,12. Use Presto Web Interface
56,Presto provides a web interface for monitoring queries (https://prestodb.io/docs/current/admin/web-interface.html).
56,"The main page has a list of queries along with information like unique query ID, query text, query state, percentage completed, username and source from which this query originated. If Presto cluster is having any performance-related issues, this web interface is a good place to go to identify and capture slow running SQL!"
56,13. Explain plan with Presto/Hive (Sample)
56,EXPLAIN is an invaluable tool for showing the logical or distributed execution plan of a statement and to validate the SQL statements.
56,— Logical Plan with Presto
56,"explain select SUBSTRING(last_modified,1,4) ,count(*)  from hive.df_machine_learning.event_text where lower(name) like ‘%wilbraham%’ or (REGEXP_LIKE(lower(name), ‘.*wilbraham.*’)) group by 1 order by 1;"
56,14. Explain plan with MySQL (Sample)
56,In this particular case you can see that the primary key is used on the ‘ejp_events’ table and the non-primary key on the “ejp_orders’ table. This query is going to be fast!
56,Conclusion
56,"Presto is the “SQL-on-Anything” solution that powers Eventbrite’s data warehouse. It’s been very rewarding for me as the “Old School DBA” to learn new SQL tricks related to a distributed query engine such as Presto. In most cases, my SQL training on MySQL/Oracle has served me well but there are some interesting differences which I’ve attempted to call-out above. Thanks for reading and making it to the end. I appreciate it!"
56,We look forward to giving Presto’s Cost-Based Optimizer a test drive and kicking the tires on new features such as dynamic filtering & partition pruning!
56,"All comments are welcome, or you can message me at ed@eventbrite.com. You can learn more about Eventbrite’s use of Presto by checking out my previous post at Boosting Big Data workloads with Presto Auto Scaling."
56,"Special thanks to Eventbrite’s Data Foundry team (Jeremy Bakker,  Alex Meyer, Jasper Groot, Rainu Ittycheriah, Gray Pickney, and Beck Cronin-Dixon) for the world-class Presto support, and Steven Fast for reviewing this blog post. Eventbrite’s Data Teams rock!"
56,"CategoriesAnalytics, Architecture, Data Stores"
56,Leave a Reply Cancel reply
56,Your email address will not be published. Required fields are marked *Comment Name *
56,Email *
56,Website
56,Notify me of follow-up comments by email. Notify me of new posts by email.
56,Post navigation
56,Previous PostPrevious
56,Create Meaningful (and Fun!) Remote CommunityNext PostNext How are you building/maintaining team cohesion?
56,"Our Writing TeamAndrew Smelser8 Simple Tips for better Communication with Customer-Facing TeamsRethinking quality and the engineers who protect itBartek OgryczakPackaging and Releasing Private Python Code (Pt.2)Packaging and Releasing Private Python Code (Pt.1)Beck Cronin-DixonEventbrite and SEO: How does Google find our pages?Eventbrite and SEO: The BasicsBen IlegboduWhy Would Webpack Stop Re-compiling? (The Quest for Micro-Apps)The Quest for React Micro-Apps: Single App ModeThe Quest for React Micro-Apps: The BeginningLearning ES6: Generators as IteratorsLearning ES6: Iterators & iterablesBryan MayesSoftware Developers to Nashville, “Stop calling us IT”Daniel CarterCreating Flexible and Reusable React File UploadersDelaine WendlingBriteBytes: Maddie CousensBriteBytes: Nam-Chi VanEd PreszMySQL High Availability at EventbriteBuilding a Protest Map: A Behind the Scenes Look!Teaching new Presto performance tricks to the Old-School DBALeveraging AWS “spot” instances to drive down costsBoosting Big Data workloads with Presto Auto ScalingElizabeth Viera & Loretta StokesWhat the Top Minds in Tech Communicated at Hopperx1 SeattleeventbriteIsomorphic React Sans NodeReact + ES.next = ❤Engineering + Accounting for Marketplace BusinessesEscapándome de las Software FactoryEventbrite’s Search-Based Approach to RecommendationsEyal ReuveniReplayable Pub/Sub Queues with Cassandra and ZooKeeperSmarter Unit Testing with nose-knowsWatching Metadata Changes in a Distributed Application Using ZooKeeperGagoThe Realistic Code Reviewer, Part IIThe Realistic Code Reviewer, Part ICode Review: The art of writing code for othersDiego Girotti8 Reasons Why Manual Testing is Still ImportantHanahCreate Meaningful (and Fun!) Remote CommunityHow to be a Successful Junior EngineerJay ChanMulti-Index Locality Sensitive Hashing for Fun and ProfitJessica KatzThe Truth about Boundaries, Curiosity, and Requests (Part 2 of 2)The Truth about Boundaries, Curiosity, and Requests (Part 1 of 2)The Lies We Tell OurselvesJiangyue ZhuA Story of a React Re-Rendering BugJohn BerrymanThe Fundamental Problem of SearchCowboys and Consultants Don’t Need Unit TestsSearch Precision and Recall By ExampleBuilding a Marketplace — Search and Recommendation at EventbriteLoretta StokesGrace Hopper 2018: Five Unforgettable ExperiencesMalina WiesenMother May I?The Lifecycle of an Eventbrite WebhookMarcos IglesiasDiscover “Pro D3.js”, a new book to improve your JavaScript data visualizationsSimple and Easy Mentorship with a Mentoring AgreementBritecharts v2.0 ReleasedIntroducing Britecharts: Eventbrite’s Reusable Charting Library Based on D3Leveling Up D3: Events and RefactoringsMartin BrambatiHow are you building/maintaining team cohesion?Matthew HimelsteinThe 63-point Plan for Helping Your Remote Team SucceedHow to Make Your Next Event App Remarkable with these 4 Mobile Navigation GesturesMaria EguiluzDesign System Wednesday: A Supportive Professional CommunityHow to Make Swift Product Changes Using a Design SystemMelisa PiccinettiBe the changeMiguel HernandezEventbrite Engineering at PyConESGetting started with Unit TestsNatalia CorteseOpen Data: The what, why and how to get startedPat PoelsThe “Aha” Moments of Becoming an Engineering ManagerRandall KannaHow Your Company Can Support Junior EngineersRashad Russell6 Unsuspecting Problems in HTML, CSS, and JavaScript – Part 26 Unsuspecting Problems in HTML, CSS, and JavaScript – Part 16 Unsuspecting Problems in HTML, CSS, and JavaScript – Part 3Santiago Hollmann5 Good Practices I Follow When I Code Using GitSahar BalaHow to fix the ugly focus ring and not break accessibility in ReactWhat is the best way to hire QA Engineers?How To Move From Customer Support to Engineering in 5 StepsStephanie PiGetting the most out of React AlicanteSteve FrenchHeavy Hitters in RedisTamara ChuBriteBytes: Diego “Kartones” MuñozStyleguide-Driven Development at Eventbrite: IntroductionTom InsamSetting the title of AirDrop shares under iOS 7Toph BurnsHow to Craft a Successful Engineering InterviewVictoria ZhangThe Elevator Pitch from a Data StrategistVincent BudrovichVarnish and A-B Testing: How to Play Nice"
56,Proudly powered by WordPress
57,"Top Tools to Manage Postgres in an Enterprise: Administration, Performance, High Availability, and Migration | Ashnik"
57,About us
57,Our Team
57,Partners and Associates
57,Careers
57,Technologies
57,EDB Postgres
57,Docker Enterprise
57,Redis Labs
57,MongoDB
57,NGINX
57,HashiCorp
57,Confluent
57,Sysdig
57,Pentaho
57,Services
57,Technical Services
57,Managed Services
57,Consulting Services
57,Training
57,PostgreSQL
57,Docker
57,MongoDB
57,Resources
57,Surveys
57,Webinars
57,Newsletters
57,Monthly Newsletter
57,Partner Newsletter
57,Blogs
57,Success Stories
57,White papers
57,EDB Postgres
57,NGINX
57,Docker
57,Sysdig
57,Pentaho
57,MongoDB
57,HashiCorp
57,Ashnik Whitepapers
57,Videos
57,Events
57,Ashnik News
57,Gallery
57,Open Source Quiz
57,Contact Us
57,Menu
57,About us
57,Our Team
57,Partners and Associates
57,Careers
57,Technologies
57,EDB Postgres
57,Docker Enterprise
57,Redis Labs
57,MongoDB
57,NGINX
57,HashiCorp
57,Confluent
57,Sysdig
57,Pentaho
57,Services
57,Technical Services
57,Managed Services
57,Consulting Services
57,Training
57,PostgreSQL
57,Docker
57,MongoDB
57,Resources
57,Surveys
57,Webinars
57,Newsletters
57,Monthly Newsletter
57,Partner Newsletter
57,Blogs
57,Success Stories
57,White papers
57,EDB Postgres
57,NGINX
57,Docker
57,Sysdig
57,Pentaho
57,MongoDB
57,HashiCorp
57,Ashnik Whitepapers
57,Videos
57,Events
57,Ashnik News
57,Gallery
57,Open Source Quiz
57,Contact Us
57,Search
57,"Top Tools to Manage Postgres in an Enterprise: Administration, Performance, High Availability, and Migration"
57,"Home Top Tools to Manage Postgres in an Enterprise: Administration, Performance, High Availability, and Migration"
57,No Comments
57,"Top Tools to Manage Postgres in an Enterprise: Administration, Performance, High Availability, and Migration"
57,"EDB PostgresSingapore, 23 Apr 2020"
57,"by , ,"
57,No Comments
57,23-Apr-2020
57,"This blog post lists tools and recommendations for managing Postgres in the enterprise. While this is a big topic, there are common denominators that we have seen in many Postgres projects and that we recommend for Postgres practitioners."
57,"Our list is not exhaustive, but it provides a starting point."
57,Tools
57,"We list tools for Development and Administration, Performance Tuning and Monitoring, High Availability and Disaster Recovery, Connection Pooling and Query Routing, and Migration."
57,Not all the tools listed below are part of the EDB support scope. Many of them are open source tools with varying degrees of support and maintenance. (**) identifies tools that are part of EDB’s support scope.
57,Development and Administration
57,Tool
57,Description
57,License
57,psql (**)
57,Postgres core client
57,https://www.postgresql.org/docs/current/app-psql.html
57,PostgreSQL
57,pgAdmin (**)
57,"Desktop or web-based; browse and modify a database schema, run queries, debug stored procedures and much more."
57,https://www.pgadmin.org
57,PostgreSQL
57,Toad Edge for Postgres
57,A new version of the classic DBA tool for Oracle
57,https://www.quest.com/products/toad-edge/toad-edge-postgres.aspx
57,Quest proprietary license
57,sqlSmith
57,A fuzz testing tool for developers. sqlSmith generates random queries
57,https://github.com/anse1/sqlsmith
57,GPL V3
57,Performance Tuning and Monitoring
57,Tool
57,Description
57,License
57,Postgres Enterprise Manager (**)
57,"All the functionality of pgAdmin, plus 24×7 monitoring and alerting and various enterprise management tools. Includes Tuning Wizard, Wait State Analyser for EDBAS, a SQL Profiler, and monitoring dashboards."
57,https://www.enterprisedb.com/edb-docs/p/edb-postgres-enterprise-manager
57,EnterpriseDB proprietary license
57,Pg_stat_statements (**)
57,A Postgres extension that tracks execution statistics of all SQL statements executed by a server.
57,https://www.postgresql.org/docs/current/pgstatstatements.html
57,PostgreSQL
57,Auto_explain (**)
57,A Postgres extension that logs execution plans of slow queries automatically.
57,PostgreSQL
57,pgBadger
57,"Very popular Postgres log file analyzer with a graphical output. Help find long-running queries, demanding workloads, etc."
57,https://github.com/darold/pgbadger
57,PostgreSQL
57,Disaster Recovery and High Availability
57,Tool
57,Description
57,License
57,EDB Postgres Failover Manager (**)
57,"Cluster-aware database availability monitoring, automatic failover and support of manual switchover in support of maintenance activities."
57,https://www.enterprisedb.com/edb-docs/p/edb-postgres-failover-manager
57,EnterpriseDB proprietary license
57,EDB Backup And Recovery Tool (**)
57,Block-level incremental backup for Postgres.
57,https://www.enterprisedb.com/edb-docs/p/edb-backup-and-recovery-tool
57,EnterpriseDB proprietary license
57,RepMgr
57,Replication Manager for Postgres streaming replication and failover
57,https://repmgr.org
57,GPL V3
57,pgBackRest
57,Advanced backup and recovery tool for Postgres
57,https://pgbackrest.org
57,MIT License
57,Barman
57,Backup and recovery tool for Postgres
57,https://www.pgbarman.org
57,GPL V3
57,Patroni
57,"High availability tool for Postgres, mostly used in containerized deployments"
57,https://github.com/zalando/patroni
57,MIT License
57,Connection Management/Connection Pooling
57,Tool
57,Description
57,License
57,pgPool-II (**)
57,Connection pooler and query router for Postgres
57,https://www.pgpool.net
57,BSD License
57,pgBouncer (**)
57,Lightweight connection pooler for Postgres
57,https://www.pgbouncer.org
57,BSD License
57,Migration from Commercial Databases
57,Tool
57,Description
57,License
57,EDB Migration Portal (**)
57,"Web-based Oracle to Postgres migration tool. Maps DDL, DML, packages, stored procedures, and other proprietary extensions to the SQL standard from Oracle to Postgres."
57,https://Migration.EnterpriseDB.com
57,EnterpriseDB proprietary license
57,EDB Migration Toolkit (**)
57,"Command-line tool for migration of DDL, DML, and data from Oracle, SQL Server, Sybase and MySQL to Postgres"
57,https://www.enterprisedb.com/edb-docs/p/edb-postgres-migration-toolkit
57,EnterpriseDB proprietary license
57,ora2PG
57,Maps Oracle schemas and data types to Postgres; provides some DML transformationhttp://ora2pg.darold.net
57,GPL V3
57,Cybertech migrator
57,"Maps Oracle schemas and data types to Postgres, provide DML transformation, provide some PL/SQL transformation and an interface to debug it"
57,https://www.cybertec-postgresql.com/en/products/cybertec-migrator/
57,Cybertech proprietary license
57,Recommendations
57,"One of the most important lessons learned is: “Don’t go it alone!” Postgres is mature, and in many regards, it behaves like Oracle and SQL Server, but there are significant differences in tooling, management practices, and performance behavior."
57,"The ROI of Postgres projects is very high. Project delays, partial implementations, and substandard performance are the enemies of a timely move to production."
57,Supported Postgres
57,We highly recommend that users work with a well-established Postgres company that has a staffed 24X7 support team with commercial-grade support SLAs. That support team must be backed by software engineers who are involved in the development of the Postgres database (check the list of committers and contributors at PostgreSQL.org — don’t work with ‘Postgres Companies’ that don’t have several full-time team members who are on that list. They will not be able to help you in a timely manner on the rare occasion where Postgres has issues).
57,"EnterpriseDB’s Postgres Advanced Server is a managed fork of Postgres. This allows EnterpriseDB to provide features for customers (Oracle compatibility, Resource Management, Query Hints, PCI-compliant password management, etc.) that may otherwise not be possible in the community process."
57,Technical Account Management
57,"Technical Account Managers (TAMs) are a staple of the commercial software world, especially for mission-critical infrastructure software. TAMs provide an intimate and knowledgeable link between the customer’s needs and the software provider’s engineering team. TAMs greatly improve communication and help make projects successful. They also act like the customer’s advocate and influence the product roadmap."
57,DBA Services
57,"Specialized Postgres DBAs monitor and manage Postgres databases in the customer’s data center, in cloud IaaS deployments, or in cloud-based DBaaS projects. Postgres is close to Oracle and SQL Server, but some key management processes, such as bloat and vacuum, or backup/HA, are different. Query tuning and performance management practices also differ significantly – especially when considering the requirements of Tier 1 99.99%+ SLA solutions."
57,Many customers use DBA services as ‘training wheels’ for their first project; other customers focus their in-house DBAs on innovation and data management and use EDB’s DBAs to keep the lights on.
57,Consulting and Architecture Advice
57,"Highly performing and reliable Postgres architectures are foundational building blocks if an enterprise wants to transition a significant part of their database estate from commercial databases to open source based ones. Understanding how to use Postgres and how to take advantage of its unparalleled innovations, its flexible data model, extensions, GIS and document capabilities, is important in order to achieve the ROI and get value from Open Source."
57,The Need for an Integrated Platform
57,"The open-source community has provided a plethora of tools and capabilities around Postgres – almost everything from HA to DR and log analyzers, are available as ‘free’ building blocks. However, none of the blocks fit together seamlessly, none of them are on the same release schedule, and they are not covered by the same support SLA. None of that matters if an enterprise can invest enough into a large number of in-house Postgres knowledgeable resources, has plenty of time to mature its Postgres projects, and does not plan to use Postgres for Tier 1 mission-critical applications."
57,"However, that is not acceptable for enterprises that want to take advantage of open source-based technology to reduce cost, drive innovation, and support digital transformation. Those users require:"
57,"An integrated platform that includes management, monitoring, tuning, HA, and DR tools that fit together seamlessly and create a robust management platform"
57,One integrated release schedule to make sure that the tools work together
57,An integrated SLA and support structure for the tools and the database. A whole is only as strong as its weakest part.
57,Conclusions
57,"The enterprise demands imposed on an open-source based platform, such as Postgres, are in no way different from the demands imposed on closed source software. Increasing use of Postgres for mission critical apps means that the tooling and the best practices align increasingly with traditional soft practices. Integrated platforms, single vendor support solutions, and the need for an agile roadmap are obvious requirements when enterprises start betting on Postgres."
57,"EDB Postgres is an open source Database Platform enabling digital transformation. It delivers a premium open source-based, multi-model data platform for new applications, cloud re-platforming, application modernization, and legacy database migration."
57,More From EDB Postgres :
57,PostgreSQL and Machine Learning
57,3 Keys to Finding Digital Transformation Success Using Open Source
57,"Your Questions, Answered: Common Questions about EDB and Reference Architectures"
57,23-Apr-2020
57,YOU MAY ALSO LIKE:
57,DIGITAL TRANSFORMATION (DX) RESOURCES
57,Top Asian Banks chart their DX Journey with Open Source
57,The Millennials and Gen Z - and their 'Digital' habits!
57,"Power of Digital Transformation for young and mature organizations, globally!"
57,BLOGS
57,The Hybrid Approach to Cloud-Based Database Environments
57,"- Ken Rugg | Chief Product and Strategy Officer, EDB"
57,NGINX plus Directives and their roles in configuring basic
57,Caching
57,"- Sandeep Khuperkar I CTO and Director, Ashnik"
57,Using ELK Stack for Monitoring JVM at scale
57,"- Nikhil Shetty | Database Consultant, Ashnik"
57,VIDEOS
57,Junk the linear thinking.. Get ready for Exponential innovation!
57,WHITEPAPER
57,Making the transition to DevOps
57,Submit your details to download the document.
57,Country*SingaporeIndonesiaMalaysiaIndiaPhilippinesThailandVietnamMongoliaBangladeshSri LankaMyanmarCambodiaLaosBruneiTimor LesteChinaHong KongUSAEuropeJapanKoreaANZAfricaSouth America
57,Where did you find us?*Google SearchSocial MediaNewsletterEventEmailFriend/ColleagueOthers
57,* Mandatory Fields.
57,Source
57,Form Name
57,Date
57,facebook
57,twitter
57,linkedln
57,youtube
57,instagram
57,slideshare
57,email
57,Back to top
57,Latest Blogs
57,Benefits of adopting cloud operating model at the infrastructure provisioning layer
57,"16 March, 2021"
57,How to configure High available Elastic search?
57,"16 March, 2021"
57,Latest Tweets
57,"Here's your quick guide to configuring highly available Elasticsearch: #ELK"" title=""https://t.co/dUH1XUGKkF"
57,"#ELK"""
57,"target=""_blank"""
57,>https://t.co/dUH1XUGKkF
57,"#ELK #Search… https://t.co/jDXenp9m155 hours agoKubernetes is deprecating Docker! What can you do next? Find out: #docker"" title=""https://t.co/EGgcZyJLMM"
57,"#docker"""
57,"target=""_blank"""
57,>https://t.co/EGgcZyJLMM
57,"#docker #kubernetes… https://t.co/bAZ3rkyNpU6 hours agoIn the time of such crisis, we as humankind may have got some valuable lessons, and we will hopefully adapt to chan… https://t.co/EiyOoHDKdX6 hours agoTake a pause and see if you are also making some of these 6 common PostgreSQL mistakes. And learn how to avoid them… https://t.co/00BHkItt9TyesterdayLearn what it takes to achieve 5,000 Queries per second and how techies at Ashnik did it! https://t.co/Z7JnSL4ALa… https://t.co/ygLZuE6hf4yesterdayWhat is the best approach for multi-cloud infrastructure provisioning? Join us to find out: https://t.co/mV6RcnHcCC… https://t.co/Iechpuji6G2 days agoUsed by some of the busiest sites in the world, NGINX needs no introduction. Now learn what NGINX Controller can do… https://t.co/jrXyXLd0Ba3 days agoMissed our webinar on the challenges of multi-cloud and how to tackle them with Redis Labs? Check out the complete… https://t.co/qKMlLVV2qu3 days agoDatabase downtime can cost your business big dollars and stress. Learn how EDB Postgres helps achieve business cont… https://t.co/ElfHCOMlnW3 days agoLet's go back to the basics of container technology, the difference between containers and VMs, the significance of… https://t.co/mILoHAbZ944 days ago"
57,News
57,Benefits of adopting cloud operating model at the infrastructure provisioning layer
57,How to configure High available Elastic search?
57,The top 5 trends for shaping your workforce in 2021
57,Contact Us
57,Locate us
57,success@ashnik.com
57,SG: +65 64383504
57,IN: 022 25771219
57,IN: 022 25792714
57,IN: +91 9987536436
57,Home |
57,About Us |
57,Our Team |
57,Partners and Associates |
57,Careers |
57,Technologies |
57,EnterpriseDB |
57,Nginx |
57,MongoDB |
57,Pentaho |
57,Docker |
57,Elastic Stack |
57,Solutions |
57,Open Source Database Solutions |
57,Backup and Replication Services |
57,Database Migration and Upgrade |
57,Monitoring and Health Check |
57,Remote and Annual Maintenance |
57,Consulting & Training |
57,EDB Certified PostgreSQL Training |
57,PostgreSQL Training – India |
57,Oracle Migration workshop |
57,Book a Seat |
57,Oracle to PostgreSQL Migration solution |
57,Big Data Platform Solutions |
57,Big Data Talk – Transformation Series |
57,Resources |
57,Newsletter |
57,Partner Tech Newsletter |
57,Success Stories |
57,Blogs |
57,Ashnik News |
57,Videos |
57,Events |
57,Press Releases |
57,Downloads Whitepapers |
57,Download EnterpriseDB Whitepapers |
57,Download Pentaho Whitepapers |
57,Download Nginx Whitepapers |
57,Download MongoDB Whitepapers |
57,Download Docker Whitepapers |
57,Ashnik Whitepapers |
57,Contact Us |
57,Ask Ashnik |
57,Newsletter Subscription |
57,open source singapore |
57,Postgresql singapore |
57,open source Philippines |
57,PostgreSQL Philippines |
57,open source Malaysia |
57,Postgresql Malaysia |
57,open source Indonesia |
57,Postgresql Indonesia |
57,open source Thailand |
57,Postgresql Thailand
57,Terms & Conditions
57,Privacy Policy
57,E-Waste Management Policy
57,Back to top
57,Copyright © 2020 ashnik.com All rights reserved.
57,About us▼Our TeamPartners and AssociatesCareersTechnologies▼EDB PostgresDocker EnterpriseRedis LabsMongoDBNGINXHashiCorpConfluentSysdigPentahoServices▼Technical ServicesManaged ServicesConsulting ServicesTraining▼PostgreSQLDockerMongoDBResources▼SurveysWebinarsNewsletters▼Monthly NewsletterPartner NewsletterBlogsSuccess StoriesWhite papers▼EDB PostgresNGINXDockerSysdigPentahoMongoDBHashiCorpAshnik WhitepapersVideosEventsAshnik NewsGalleryOpen Source QuizContact Us
58,Tuning: Better Backup Performance and Bottlenecks Treatment - Bacula Latin America & Brazil
58,Skip to content
58,Menu
58,+1 909 655-8971
58,contato@bacula.com.br
58,BRAZIL AND LATIN AMERICA
58,Enterprise
58,The Enterprise Bacula
58,Slides
58,Video Presentation
58,Cases
58,BWeb Online Demo
58,Download Trial
58,Enterprise Contact Us
58,Quick Guides
58,Bacula Enterprise Automated Install Script for Centos 7
58,Bacula Client Installation
58,Plugins
58,Drivers
58,Community
58,Installation
58,Bacula.org Packages Installation
58,Compilation Install
58,Baculum Installation
58,Bacula Client Installation
58,Aligned Dedup
58,Cloud S3 Driver
58,Cloud with Rclone
58,The Bacula Book
58,Support Contract
58,In-company Training
58,Video Classes
58,Bacula Certification
58,Bacula Dell Appliance
58,Manual
58,Demos
58,Services
58,Support Contract
58,In-Company Training & Deploy
58,Bacula Certification
58,Posts
58,Introduction
58,Articles
58,Hints
58,Quick Guides
58,Specific Applications
58,Videos
58,Pictures
58,Bacula Enterprise
58,About
58,Contact Us
58,About
58,Bacula Latin America
58,Bacula Brazil
58,Support
58,Menu
58,Enterprise
58,The Enterprise Bacula
58,Slides
58,Video Presentation
58,Cases
58,BWeb Online Demo
58,Download Trial
58,Enterprise Contact Us
58,Quick Guides
58,Bacula Enterprise Automated Install Script for Centos 7
58,Bacula Client Installation
58,Plugins
58,Drivers
58,Community
58,Installation
58,Bacula.org Packages Installation
58,Compilation Install
58,Baculum Installation
58,Bacula Client Installation
58,Aligned Dedup
58,Cloud S3 Driver
58,Cloud with Rclone
58,The Bacula Book
58,Support Contract
58,In-company Training
58,Video Classes
58,Bacula Certification
58,Bacula Dell Appliance
58,Manual
58,Demos
58,Services
58,Support Contract
58,In-Company Training & Deploy
58,Bacula Certification
58,Posts
58,Introduction
58,Articles
58,Hints
58,Quick Guides
58,Specific Applications
58,Videos
58,Pictures
58,Bacula Enterprise
58,About
58,Contact Us
58,About
58,Bacula Latin America
58,Bacula Brazil
58,Support
58,Search
58,Close
58,Request
58,OFFICIAL DISTRIBUTOR
58,Menu
58,BRASIL Y AMÉRICA LATINA
58,Enterprise
58,The Enterprise Bacula
58,Slides
58,Video Presentation
58,Cases
58,BWeb Online Demo
58,Download Trial
58,Enterprise Contact Us
58,Quick Guides
58,Bacula Enterprise Automated Install Script for Centos 7
58,Bacula Client Installation
58,Plugins
58,Drivers
58,Community
58,Installation
58,Bacula.org Packages Installation
58,Compilation Install
58,Baculum Installation
58,Bacula Client Installation
58,Aligned Dedup
58,Cloud S3 Driver
58,Cloud with Rclone
58,The Bacula Book
58,Support Contract
58,In-company Training
58,Video Classes
58,Bacula Certification
58,Bacula Dell Appliance
58,Manual
58,Demos
58,Services
58,Support Contract
58,In-Company Training & Deploy
58,Bacula Certification
58,Posts
58,Introduction
58,Articles
58,Hints
58,Quick Guides
58,Specific Applications
58,Videos
58,Pictures
58,Bacula Enterprise
58,About
58,Contact Us
58,About
58,Bacula Latin America
58,Bacula Brazil
58,Support
58,Menu
58,Enterprise
58,The Enterprise Bacula
58,Slides
58,Video Presentation
58,Cases
58,BWeb Online Demo
58,Download Trial
58,Enterprise Contact Us
58,Quick Guides
58,Bacula Enterprise Automated Install Script for Centos 7
58,Bacula Client Installation
58,Plugins
58,Drivers
58,Community
58,Installation
58,Bacula.org Packages Installation
58,Compilation Install
58,Baculum Installation
58,Bacula Client Installation
58,Aligned Dedup
58,Cloud S3 Driver
58,Cloud with Rclone
58,The Bacula Book
58,Support Contract
58,In-company Training
58,Video Classes
58,Bacula Certification
58,Bacula Dell Appliance
58,Manual
58,Demos
58,Services
58,Support Contract
58,In-Company Training & Deploy
58,Bacula Certification
58,Posts
58,Introduction
58,Articles
58,Hints
58,Quick Guides
58,Specific Applications
58,Videos
58,Pictures
58,Bacula Enterprise
58,About
58,Contact Us
58,About
58,Bacula Latin America
58,Bacula Brazil
58,Support
58,Search
58,Close
58,Tuning: Better Backup Performance and Bottlenecks Treatment
58,Home→Backup→Tuning: Better Backup Performance and Bottlenecks Treatment
58,Post published:2 de September de 2020
58,Post category:atualização 2a edição / atualização 3a edição / Backup / bacula / book update
58,Post comments:1 Comment
58,Bacula
58,Some companies use Antivirus for Windows and Even for Linux. Put Bacula’s Daemons as exceptions.
58,Split FileSet in two when backing up more than 20 million files.
58,"Windows file systems especially do not handle volumes with gigantic amounts of small files well. In this case, the ideal is to create multiple FileSets and Jobs (ex .: one for each Volume or some directories), in order to parallelize the copy operations. For example, a server with C: and E: volumes."
58,"Job1, FileSet1. Include, File = “C:/”"
58,"Job2, FileSet2. Include, Plugin = “alldrives: exclude=C”"
58,"Job3, FileSet3. Include, Plugin = “vss:/@SYSTEMSTATE/”"
58,"Using alldrives is important for backing up all other drives except C:, which is already backed up by Job1. If someone creates a new Volume on this server, Job2 will automatically back up."
58,"Job3 would be for exclusive backup of Windows System State (if you want to separate too). Vss: is an exclusive plugin for Enterprise, but it is also possible to use scripts to generate Windows System State in a separate volume."
58,"In some graphical interfaces (such as Bweb), it is possible to group Jobs from the same client for better management and statistics."
58,Decrease the GZIP compression level (if enabled – always less than 6) or use LZO. Do not use compression via Bacula software for tapes.
58,Run multiple simultaneous backup jobs (Maximum Concurrent Jobs).
58,Be sure to enable competition in the 4 places:
58,a) Director resource in bacula-dir.conf
58,b) Storage feature in bacula-dir.conf
58,c) Storage feature in bacula-sd.conf
58,d) Device stanza in bacula-sd.conf resource
58,"Back up to multiple disks, tapes or different storages daemons simultaneously."
58,Tapes: Enable SSD / NVME Disk Spooling. Traditional HD discs can be slower than tapes.
58,"Tapes: Increase the Minimum (eg 256K) and Maximum Block Size to 256K to 512K (* for LTO4. 1M too large and can cause problems. Specified in: bacula-sd.conf, Device feature). It is necessary to recreate all volumes with the new maximum block size, otherwise Bacula will not be able to read the previous ones."
58,"Tapes: Increase the Maximum File Size to 10GB to 20GB (Specified in: bacula-sd.conf, Device feature)."
58,Disable AutoPrunning for Clients and Jobs (Pruning volumes once a day through an Admin Job).
58,Turn on Attribute Spooling for all Jobs (Default for version 7.0 onwards).
58,"Use batch insert in the database (it is usually standard, defined in the compilation and needs to be supported by the database)."
58,Catalog (database)
58,a) PostgreSQL
58,Avoid creating additional indexes.
58,Use special settings for Postgresql (postgresql.conf):
58,wal_buffers = 64kB
58,shared_buffers = 1GB # up to 8GB
58,work_mem = 64MB
58,effective_cache_size = 2GB
58,checkpoint_segments = 64
58,checkpoint_timeout = 20min
58,checkpoint_completion_target = 0.9
58,maintenance_work_mem = 256MB
58,synchronous_commit = on
58,"Performing a periodic vacuumdb in the database (postgreSQL), with the passage of time the major change of records ends up making insertion in the database more time consuming. [1]"
58,[1] Tip from Edmar Araújo. References: http://www.postgresql.org/docs/9.0/static/app-vacuumdb.html | Carlos Eduardo Smanioto -> Otimização – Uma Ferramenta Chamada Vacuum: http://www.devmedia.com.br/otimizacao-uma-ferramenta-chamada-vacuum/1710
58,b) MySQL
58,Use special configurations for MySQL:
58,sort_buffer_size = 2MB
58,innodb_buffer_pool_size = 128MB
58,innodb_flush_log_at_trx_commit = 0
58,innodb_flush_method = O_DIRECT
58,"By default, innodb_flush_log_at_trx_commit would be 1, meaning that the transaction log is stored on disk at each commit in the bank and transactions would not be lost in the event of an operating system crash. Since Bacula uses many small transactions, you can reduce log I/O and increase backup performance exponentially by setting it to 0, meaning that there will be no log storage for each transaction. As in case of job interruption it would be necessary to restart the backup job in any way, so it is a very interesting option."
58,Run mysqltuner (apt-get install mysql tuner) and implement the suggested changes.
58,Network (SD and FD)
58,Add more interfaces (bonding / NIC Teaming) and faster switches (you can use the Bacula status network command or the ethtool application to check the speed of your ethernet connection).
58,"Set the Maximum Network Buffer Size = bytes, which specifies the initial size of the network buffer. This size is adjusted downward if the operating OS does not accept it, at the cost of many system calls (unwanted). The default value is 32,768 bytes. The standard was chosen to be wide enough for transmission over the internet, but on a local network it can be increased to improve performance. Some users have noticed a 10-fold improvement in data transfer using 65,536 bytes in this value."
58,Avoid traffic through firewalls and routers.
58,Use Jumbo Frames.
58,Customize the Kernel (Ref .: https://fasterdata.es.net/host-tuning/linux/). Example:
58,"echo """
58,# allow testing with buffers up to 128MB
58,net.core.rmem_max = 134217728
58,net.core.wmem_max = 134217728
58,# increase Linux autotuning TCP buffer limit to 64MB
58,net.ipv4.tcp_rmem = 4096 87380 67108864
58,net.ipv4.tcp_wmem = 4096 65536 67108864
58,# recommended default congestion control is htcp
58,net.ipv4.tcp_congestion_control=htcp
58,# recommended for hosts with jumbo frames enabled
58,# net.ipv4.tcp_mtu_probing=1
58,# recommended for CentOS7+/Debian8+ hosts
58,"net.core.default_qdisc = fq"" >> /etc/sysctl.conf"
58,reboot
58,Operating System
58,RAM (> 8GB)
58,vm.dirty_ratio = 2
58,vm.dirty_background_ratio = 1
58,vm.swappiness = 10
58,vm.zone_reclaim_node = 0
58,Disk Access
58,"Use the XFS file system as it excels in performing parallel input / output (I/O) operations due to its design, which is based on allocation groups (a type of subdivision of the physical volumes on which XFS is used, shortened for AGs). Because of this, XFS allows for extreme scalability of I/O threads, bandwidth of the file system and size of the files and the file system itself, while spanning multiple physical storage devices."
58,Use the “deadline disk scheduler”.
58,Use RAID with a good battery controller (eg ARECA).
58,Disponível em: Português (Portuguese (Brazil))EnglishEspañol (Spanish)
58,Read more articles
58,Previous PostDeletion of Bacula Client’s Finished BackupsNext PostBacula Training in the Brazilian Navy
58,You Might Also Like
58,Happy Holidays!
58,27 de December de 2019
58,[KB] Error: lib/bsockcore.c:285 gethostbyname() for host “xxx” failed: ERR=No such host is known.
58,26 de January de 2021
58,Oracle Cloud and Oracle DB Plugins
58,17 de December de 2019
58,This Post Has One Comment
58,Pingback: １１：リストア速度（Building directory treeの速度）が劇的に早くなった | www.kinryo.netwww.kinryo.net
58,Leave a Reply Cancel replyCommentEnter your name or username to comment
58,Enter your email address to comment
58,Enter your website URL (optional)
58,Pesquisar
58,Search for:
58,Postagens recentes
58,Locaweb: the ISP leader in Brazil and Latin America uses Bacula
58,Increased Protection Against Ransomware in Bacula Enterprise Version 12.6
58,Bacula Community 11.0.0 (beta)
58,Advanced Backed Up File Names and Directories Search in Bacula bconsole – sqlquery
58,[KB] Error: lib/bsockcore.c:285 gethostbyname() for host “xxx” failed: ERR=No such host is known.
58,"Physical, Virtual and Cloud Disaster Recovery for the Modern Data Center™"
58,Facebook
58,Twitter
58,Linkedin
58,Contact
58,Our analysts are experts in reducing license costs and backing up data centers.
58,+55 61 99271-3350
58,+55 61 98268-4220
58,Telegram
58,contato@bacula.lat
58,OFFICIAL PARTNER
58,Privacy Policy
58,Copyright 2021 – BaculaCriação de Sites por Flow Digital
61,Performance Guidelines | GitLab
61,GitLab Docs
61,GitLab.com (13.11-pre)
61,GitLab.com (13.11-pre)
61,13.10
61,13.9
61,13.8
61,12.10
61,11.11
61,Archives
61,Get free trial
61,GitLab Docs
61,Subscriptions
61,GitLab.com subscriptions
61,Storage usage quota
61,Self-managed subscriptions
61,Activate Enterprise Edition
61,Features available to Starter and Bronze subscribers
61,Installation
61,Requirements
61,Omnibus packages
61,Kubernetes
61,Docker
61,From source
61,Azure
61,Google Cloud Platform (GCP)
61,Amazon Web Services (AWS)
61,Administration
61,Reference Architectures
61,"Up to 1,000 users"
61,"Up to 2,000 users"
61,"Up to 3,000 users"
61,"Up to 5,000 users"
61,"Up to 10,000 users"
61,"Up to 25,000 users"
61,"Up to 50,000 users"
61,Troubleshooting
61,Authentication and authorization
61,Atlassian Crowd
61,Atlassian
61,Auth0
61,Authentiq
61,AWS Cognito
61,Azure
61,Bitbucket Cloud
61,CAS
61,Facebook
61,Generic OAuth2
61,GitHub
61,GitLab.com
61,Google
61,JWT
61,Kerberos
61,LDAP
61,LDAP (Google Secure)
61,LDAP Troubleshooting
61,OAuth service provider
61,Okta
61,OmniAuth
61,OpenID Connect OmniAuth
61,OpenID Connect identity
61,Salesforce
61,SAML
61,Smartcard
61,Twitter
61,Vault
61,Configuration and Admin Area
61,Admin Area settings
61,Appearance
61,Authentication
61,CI/CD
61,Custom instance-level project templates
61,Diff limits
61,Email
61,External pipeline validation
61,Feature flags
61,Geo nodes
61,Git LFS administration
61,GitLab Pages
61,Health Check
61,Incoming email
61,Job artifacts
61,Job logs
61,Labels
61,Log system
61,Maintenance Mode
61,Rate limits on issue creation
61,Rate limits on note creation
61,Reply by email
61,Repository checks
61,Repository size
61,Sign-in restrictions
61,Sign-up restrictions
61,System Hooks
61,Timezone
61,Uploads
61,User Cohorts
61,Security
61,Password storage
61,Custom password length limits
61,Generated passwords and integrated authentication
61,Credentials inventory
61,Limits on SSH keys
61,Rate limits
61,Webhooks
61,Information exclusivity
61,Reset user password
61,Unlock a user
61,"Unknown sign-ins, email notification"
61,Users pending approval
61,User file uploads
61,Manage the CRIME vulnerability
61,Enforce two-factor authentication (2FA)
61,User email confirmation
61,Security of running jobs
61,Proxying assets
61,CI/CD variables
61,Token overview
61,Abuse reports
61,Activate and deactivate users
61,Create users
61,Analytics
61,Audit events
61,Audit reports
61,Block and unblock users
61,Broadcast messages
61,Consul
61,Compliance features
61,Email from GitLab
61,Global user settings
61,Instance Review
61,Invalidate Markdown cache
61,Issue closing pattern
61,PostgreSQL
61,PgBouncer
61,Replication and failover
61,External database service
61,Load balancer
61,NFS
61,Redis
61,Sidekiq
61,Kubernetes Agent Server
61,Repository storage
61,Repository storage types
61,Gitaly and Gitaly Cluster
61,Configure Gitaly
61,Configure Gitaly Cluster
61,Gitaly reference
61,Gitaly timeouts
61,Metrics
61,Configure GitLab
61,Configure Grafana
61,GitHub imports
61,GitLab exporter
61,GitLab Prometheus metrics
61,GitLab self monitoring project
61,IP allowlist endpoints
61,Node exporter
61,PGBouncer exporter
61,PostgreSQL server exporter
61,Prometheus
61,Performance bar
61,Performance monitoring
61,Redis exporter
61,Registry exporter
61,Request profiling
61,Usage statistics
61,Object storage
61,Operations
61,Clean up Redis sessions
61,Fast SSH key lookup
61,Filesystem benchmarking
61,Move repositories
61,Multiple Sidekiq processes
61,Rails console
61,Sidekiq MemoryKiller
61,Switch to Puma
61,Unicorn
61,Use SSH certificates
61,Packages
61,Container Registry
61,Dependency Proxy
61,Geo
61,Setting up Geo
61,Database replication
61,External PostgreSQL instances
61,Configuration
61,Using a Geo site
61,Updating Geo nodes
61,Using object storage
61,Using Docker Registry
61,Geo for multiple servers
61,Geo security review
61,Location-aware Git remote URLs
61,Tuning Geo
61,Disable Geo
61,Removing a Geo site
61,Supported data types
61,Frequently asked questions
61,Troubleshooting
61,Validation tests
61,Geo Glossary
61,Disaster recovery (Geo)
61,Planned failover
61,Bring primary back
61,Automatic background verification
61,Rake tasks
61,Backup and restore
61,Clean up
61,Enable namespaces
61,General maintenance
61,Geo tasks
61,GitHub import
61,Import repositories
61,Integrity check
61,LDAP maintenance
61,List repositories
61,Praefect tasks
61,Project import and export
61,Repository storage
61,Sample Prometheus data
61,Uploads migration
61,Uploads sanitization
61,User management
61,Webhooks administration
61,X509 signatures
61,Server hooks
61,Static objects external storage
61,Terraform state
61,Update
61,Releases and maintenance
61,Analytics
61,Instance-level
61,DevOps Report
61,Usage Trends
61,Group-level
61,Contribution
61,DevOps Adoption
61,Insights
61,Issue
61,Productivity
61,Repository
61,Value Stream
61,Project-level
61,CI/CD
61,Code Review
61,Insights
61,Issue
61,Merge Request
61,Repository
61,Value Stream
61,Projects
61,Working with projects
61,Badges
61,Bulk editing (project)
61,Code intelligence
61,Code owners
61,Compliance
61,License Compliance
61,Compliance Dashboard
61,Description templates
61,Deploy keys
61,Deploy tokens
61,File finder
61,GitLab Pages
61,Getting started
61,"Default domains, URLs, and baseurls"
61,CI/CD for GitLab Pages
61,Custom domains and SSL/TLS certificates
61,Let's Encrypt integration
61,Access control
61,Redirects
61,Exploring GitLab Pages
61,Insights
61,Members
61,Migrating projects
61,Bitbucket Cloud
61,Bitbucket Server
61,ClearCase
61,CVS
61,FogBugz
61,Gemnasium
61,GitHub
61,GitLab.com
61,Gitea
61,Jira
61,Perforce Helix
61,Phabricator
61,Repo by manifest file
61,Repo by URL
61,SVN
61,TFVC
61,Push options
61,Releases
61,Repositories
61,Branches
61,Default branch
61,Git attributes
61,Git LFS
61,Locked files
61,File Blame
61,File History
61,Mirroring
61,Protected branches
61,Protected tags
61,Push rules
61,Reduce repository size
61,Signed Commits
61,Syntax highlighting
61,Web Editor
61,Web IDE
61,Requirements
61,Settings
61,Project Import/Export
61,Project/Group Import/Export rate limits
61,Project access tokens
61,Share projects
61,Snippets
61,Static Site Editor
61,Wikis
61,Groups
61,Bulk editing (group)
61,Contribution analytics
61,Custom group-level project templates
61,Epics
61,Epic boards
61,Manage epics
61,Group Import/Export
61,Insights
61,Issue analytics
61,Iterations
61,Migrating groups
61,Public access
61,Roadmaps
61,Repositories Analytics
61,SAML SSO for GitLab.com groups
61,Group Managed Accounts (Closed Beta)
61,SCIM provisioning
61,Troubleshooting Group SAML and SCIM
61,Subgroups
61,User
61,Abuse reports
61,Account
61,Active sessions
61,Delete account
61,Permissions
61,Personal access tokens
61,Profile preferences
61,Two-factor authentication
61,Discussions
61,Git
61,Partial clone
61,Troubleshooting Git
61,Useful commands
61,GitLab.com settings
61,Keyboard shortcuts
61,Markdown
61,AsciiDoc
61,Notification emails
61,Quick actions
61,Autocomplete characters
61,Reserved project and group names
61,Search
61,Advanced Search
61,SSH
61,Time tracking
61,To-Do lists
61,Issues
61,Award emoji
61,Confidential issues
61,Crosslinking issues
61,CSV export
61,CSV import
61,Design management
61,Due dates
61,Issue Boards
61,Issue data and actions
61,Labels
61,Managing issues
61,Milestones
61,Burndown and burnup charts
61,Multiple assignees
61,Linked issues
61,Service Desk
61,Sorting and ordering issue lists
61,Weight
61,Zoom meetings in issues
61,Merge requests
61,Allow collaboration
61,Approvals
61,Creating merge requests
61,Cherry pick changes
61,Drafts
61,Export Merge Requests to CSV
61,Getting started
61,Merge request dependencies
61,Fast forward
61,Merge when pipeline succeeds
61,Resolve conflicts
61,Reverting changes
61,Reviewing and managing merge requests
61,Squash and merge
61,Versions
61,Workflows
61,Draft merge requests
61,Operations
61,Metrics dashboard
61,Set up alerts for metrics
61,Embedding metrics in Markdown
61,Embedding metrics in Grafana
61,Custom dashboards
61,GitLab-defined metrics dashboards
61,Dashboard YAML properties
61,Dashboard settings
61,Panel types for dashboards
61,Using variables
61,Templates for custom dashboards
61,Templating variables for dashboards
61,Metrics library
61,Monitoring AWS resources
61,HAProxy
61,Kubernetes
61,NGINX
61,NGINX Ingress
61,NGINX Ingress VTS
61,Error Tracking
61,Tracing
61,Incident management
61,Alerts
61,Paging and notifications
61,Incidents
61,Integrations
61,Status page
61,On-call schedules
61,Feature Flags
61,Product analytics
61,CI/CD
61,Get started
61,CI/CD concepts
61,Migrate from CircleCI
61,Migrate from Jenkins
61,Enable or disable CI/CD
61,Pipelines
61,Schedule a pipeline
61,Trigger a pipeline
61,Pipeline settings
61,Pipeline architectures
61,Pipeline efficiency
61,Directed acyclic graph (DAG)
61,Multi-project pipelines
61,Parent-child pipelines
61,Pipelines for merge requests
61,Pipelines for merged results
61,Merge trains
61,Jobs
61,Access a terminal for a running job
61,Format scripts and job logs
61,Git submodules
61,Variables
61,Predefined variables
61,Where variables can be used
61,Environments and deployments
61,Environments Dashboard
61,Protected environments
61,Deployment safety
61,Roll out an application incrementally
61,Deploy to AWS
61,Review Apps
61,Runners
61,Best practices for large repositories
61,Cache and artifacts
61,Job artifacts
61,Pipeline artifacts
61,.gitlab-ci.yml
61,.gitlab-ci.yml reference
61,Validate syntax
61,Pipeline Editor
61,Include examples
61,Docker
61,Run CI/CD jobs in Docker containers
61,Use Docker to build Docker images
61,Use kaniko to build Docker images
61,Services
61,MySQL Service
61,PostgreSQL Service
61,Redis Service
61,Auto DevOps
61,Get started
61,Requirements
61,Stages
61,Customize
61,Upgrade PostgreSQL
61,Upgrade Auto Deploy dependencies
61,Troubleshooting
61,Testing
61,Accessibility testing
61,Browser performance testing
61,Code quality
61,Load performance testing
61,Metrics reports
61,Test cases
61,External integrations
61,SSH keys
61,Bitbucket Cloud
61,GitHub
61,Slack
61,CI/CD examples
61,Deployment with Dpl
61,End-to-end testing
61,NPM with semantic-release
61,PHP with PHPunit and atoum
61,PHP with NPM and SCP
61,PHP with Laravel and Envoy
61,Troubleshooting CI/CD
61,Application security
61,Security Configuration
61,Container Scanning
61,Threat Monitoring
61,Dependency Scanning
61,Dependency Scanning Analyzers
61,Dependency List
61,Static Application Security Testing
61,SAST Analyzers
61,Secret Detection
61,Dynamic Application Security Testing (DAST)
61,DAST Troubleshooting
61,API Fuzzing
61,Coverage Fuzzing
61,Security Dashboard
61,Offline Environments
61,Vulnerability Reports
61,Vulnerability Pages
61,Scan Policies
61,Security scanner integration
61,Secure and Protect Terminology
61,Packages & Registries
61,Package Registry
61,Composer
61,Conan
61,Go Proxy
61,Maven
61,npm
61,NuGet
61,PyPI
61,Ruby gems
61,Generic
61,Store all packages in one project
61,Container Registry
61,Dependency Proxy
61,Infrastructure
61,Adding and removing clusters
61,Add EKS clusters
61,Add GKE clusters
61,Group-level clusters
61,Instance-level clusters
61,Canary deployments
61,Cluster environments
61,Cluster cost management
61,Deploy boards
61,GitLab Managed Apps
61,Configuring Crossplane
61,Infrastructure as code
61,GitLab managed Terraform state
61,Terraform integration in merge requests
61,Kubernetes Agent
61,Agent configuration repository
61,Management project
61,Pod logs
61,Runbooks
61,Serverless
61,Deploying AWS Lambda functions
61,Securing your deployed applications
61,Web Application Firewall
61,Container Network Security
61,Container Host Security
61,Integrations
61,Akismet
61,Elasticsearch
61,Gitpod
61,Jira integrations
61,Jira
61,GitLab for Jira app
61,Jira DVCS connector
61,Jira Development Panel
61,Create Jira Server user
61,Create Jira Cloud API token
61,Kroki diagrams
61,PlantUML
61,Project integration management
61,Project integrations
61,Overview
61,Asana
61,Bamboo CI
61,Discord
61,Emails on push
61,GitHub
61,Hangouts Chat
61,Irker
61,Jenkins
61,Mattermost notifications
61,Mattermost slash commands
61,Microsoft Teams
61,Mock CI
61,Prometheus
61,Service templates
61,Slack notifications
61,Slack slash commands
61,Slack application
61,Unify Circuit
61,Webex Teams
61,Webhooks
61,External issue tracker
61,Bugzilla
61,Custom issue tracker
61,IBM Engineering Workflow Management
61,Redmine
61,YouTrack
61,Gmail actions buttons
61,reCAPTCHA
61,Security partners
61,Sourcegraph
61,Trello
61,API
61,Resources
61,.gitignore (templates)
61,.gitlab-ci.yml (templates)
61,Access requests
61,Appearance (application)
61,Applications
61,Audit events
61,Avatar
61,Award emoji
61,Badges (project)
61,Badges (group)
61,Branches
61,Broadcast messages
61,Clusters (project)
61,Clusters (group)
61,Clusters (instance)
61,Commits
61,Composer
61,Conan
61,Container Registry
61,Custom attributes
61,Dashboard annotations
61,Dependencies
61,Dependency Proxy
61,Deploy keys
61,Deploy tokens
61,Deployments
61,Discussions
61,Dockerfile (templates)
61,DORA4 metrics
61,DORA4 project analytics
61,DORA4 group analytics
61,Environments
61,Epics
61,Error tracking
61,Events
61,Experiments
61,Features flags
61,Feature flag user lists
61,Freeze periods
61,Geo nodes
61,GitLab Pages
61,Group activity analytics
61,Group Import/Export
61,Group repository storage moves
61,Group wikis
61,Groups
61,Import
61,Instance-level CI/CD variables
61,Invitations
61,Issue boards (project)
61,Issue boards (group)
61,Issues
61,Issues (epic)
61,Issues statistics
61,Iterations (project)
61,Iterations (group)
61,Jobs
61,Job artifacts
61,Keys
61,Labels (project)
61,Labels (group)
61,License
61,Licenses (templates)
61,Links (issue)
61,Links (epic)
61,Managed licenses
61,Markdown
61,Maven
61,Members
61,Merge request approvals
61,Merge request context commits
61,Merge requests
61,Merge trains
61,Milestones (project)
61,Milestones (group)
61,Namespaces
61,Notes (comments)
61,Notification settings
61,NuGet
61,Packages
61,Pages domains
61,Personal access tokens
61,Pipelines schedules
61,Pipeline triggers
61,Pipelines
61,Plan limits
61,Project access tokens
61,Project aliases
61,Project import/export
61,Project remote mirrors
61,Project repository storage moves
61,Project statistics
61,Project templates
61,Project vulnerabilities
61,Projects
61,Protected branches
61,Protected environments
61,Protected tags
61,PyPI
61,Releases
61,Release links
61,Repositories
61,Repository files
61,Repository submodules
61,Resource iteration events
61,Resource label events
61,Resource milestone events
61,Resource state events
61,Resource weight events
61,Ruby gems
61,Runners
61,SCIM
61,Search
61,Services
61,Settings (application)
61,Sidekiq metrics
61,Sidekiq queues
61,Snippet repository storage moves
61,Snippets
61,Snippets (project)
61,Statistics (application)
61,Suggestions
61,System hooks
61,Tags
61,To-Do lists
61,Users
61,User-starred metrics dashboards
61,Variables (project)
61,Variables (group)
61,Version
61,Visual Review discussions
61,Vulnerabilities
61,Vulnerability export
61,Vulnerability Findings
61,Wikis
61,GraphQL
61,Getting started
61,GraphQL reference
61,Create audit report (example)
61,Identify issue boards (example)
61,Removed items
61,v3 to v4
61,Lint .gitlab-ci.yml
61,GitLab as an OAuth2 provider
61,Omnibus GitLab
61,Architecture
61,Omnibus packages and images
61,Package information
61,Package defaults
61,Package licensing
61,Package signatures
61,Installation
61,Container Registry
61,Deprecation policy
61,Deprecated OSes
61,Docker images
61,Manual installation
61,Install JiHu Edition
61,Configure
61,Custom environment variables
61,Backups
61,Database
61,GitLab Mattermost
61,Grafana
61,High availability roles
61,LDAP
61,Logs
61,NGINX
61,Gitaly Cluster
61,Prometheus
61,Puma
61,Raspberry Pi
61,Redis
61,SMTP
61,SSL
61,DNS
61,Unicorn
61,Image scaling
61,Memory-constrained environments
61,Release process
61,OpenShift release process
61,Update
61,Convert to Omnibus
61,Package signatures
61,GitLab 13 changes
61,GitLab 12 changes
61,GitLab 11 changes
61,GitLab 10 changes
61,Maintain
61,Troubleshoot
61,GitLab Runner
61,Install
61,Docker
61,FreeBSD
61,Kubernetes (Helm Chart)
61,Kubernetes (Agent)
61,Linux
61,macOS
61,OpenShift
61,Windows
61,Bleeding edge releases
61,Official Linux packages
61,Old GitLab Runner URLs
61,Configure
61,Advanced config
61,Autoscale config
61,Autoscale on AWS EC2
61,Autoscale on AWS Fargate
61,Commands
61,Feature flags
61,OpenShift
61,Running behind a proxy
61,Rate limited requests
61,Self-signed certificates
61,System services
61,Speed up job execution
61,Register
61,Examples
61,Executors
61,Custom
61,libvirt
61,LXD
61,Docker
61,Docker Machine
61,Kubernetes
61,Shell
61,SSH
61,Parallels
61,Virtual Box
61,Monitor
61,Security
61,Shells
61,Troubleshoot
61,Best Practices
61,GitLab Helm Charts
61,Install
61,Required tools
61,Cloud cluster preparation
61,AKS
61,EKS
61,GKE
61,OpenShift
61,Deploy
61,Upgrade
61,Backup and Restore
61,Backup
61,Restore
61,Migrate from Omnibus
61,Version mappings
61,Configure
61,Globals
61,GitLab sub-charts
61,Gitaly chart
61,GitLab Exporter chart
61,GitLab Grafana chart
61,GitLab Pages chart
61,GitLab Runner chart
61,GitLab Shell chart
61,KAS chart
61,Migrations chart
61,Praefect chart
61,Sidekiq chart
61,Task Runner chart
61,Webservice chart
61,Minio chart
61,Nginx chart
61,Redis chart
61,Redis HA chart
61,Registry chart
61,Advanced
61,External database
61,External Gitaly
61,External Mattermost
61,External Nginx
61,External object storage
61,External Redis
61,Persistent volumes
61,Troubleshoot
61,Contribute to GitLab
61,Get started
61,Changelog entries
61,Community roles
61,Design and UI
61,GitLab Development Kit
61,Issues workflow
61,Merge request workflow
61,Code review guidelines
61,Style guides
61,Architecture
61,CI/CD development
61,CI/CD template development
61,Auto DevOps development
61,Code intelligence
61,Danger bot
61,Database development
61,Case study - filtering by label
61,Case study - namespaces storage statistics
61,Database review guidelines
61,Migrations style guide
61,SQL guidelines
61,Understanding EXPLAIN plans
61,Avoiding downtime in migrations
61,Developer guide to logging
61,Development Rake tasks
61,Mass insert Rails models
61,Documentation
61,Style guide
61,GraphQL style guide
61,RESTful API style guide
61,Topic types
61,Process
61,Testing
61,Site architecture
61,Global navigation
61,Deployment process
61,Release process
61,Distributed tracing
61,Experiments
61,Experimentation module
61,GLEX
61,Feature flags for GitLab development
61,Controlling feature flags
61,Documenting feature flags
61,Framework - DeclarativePolicy
61,Frontend development
61,Accessibility
61,Architecture
61,Axios
61,Dark mode
61,Design patterns
61,Development process
61,Droplab
61,Emojis
61,Filter
61,Frontend FAQ
61,GraphQL
61,Icons and SVG illustrations
61,InputSetter
61,Performance
61,Principles
61,Security
61,Tooling
61,Vuex
61,Vue
61,Widgets
61,GitLab Pages development
61,Geo development
61,Geo framework
61,Gitaly development
61,GitLab Design System
61,GitLab development style guides
61,API style guide
61,Go standards and style guidelines
61,GraphQL API style guide
61,Guidelines for shell commands
61,HTML style guide
61,JavaScript style guide
61,Newlines style guide
61,Python development guidelines
61,SCSS style guide
61,Shell scripting standards and style guidelines
61,Sidekiq debugging
61,Sidekiq style guide
61,Vue style guide
61,GitLab group migration
61,GitLab project pipelines
61,GitLab Runner
61,Review GitLab Runner
61,Add new Windows version support for Docker executor
61,GraphQL development
61,GraphQL BatchLoader
61,GraphQL pagination
61,GraphQL Pro
61,Helm Charts
61,Architecture of Cloud native GitLab Helm charts
61,Backup and Restore
61,Goals
61,Architecture
61,Design Decisions
61,Resource Usage
61,Environment setup
61,Style guide
61,Versioning and release
61,Import/Export
61,Instrumenting Ruby code
61,Issuable-like Rails models utilities
61,Issue types
61,Kubernetes Agent
61,Routing kas requests
61,Repository overview
61,Identity and authentication
61,User stories
61,GitOps with the Kubernetes Agent
61,Running locally
61,Kubernetes integration
61,Omnibus GitLab
61,Build locally
61,Build Omnibus GitLab package
61,Build all-in-one Docker image
61,Information for GitLab team members
61,Set up a development environment
61,Config options
61,Changing YAML config options
61,Adding deprecation messages
61,Adding new gitlab-ctl commands
61,Adding new services
61,Adding new software definitions
61,Creating patches
61,Installing OpenShift
61,Managing PostgreSQL versions
61,Working with public_attributes.json
61,Package development
61,Permissions guide
61,Testing standards and styles
61,Flaky tests
61,Frontend testing standards and style guidelines
61,GitLab tests in CI context
61,Review apps
61,Smoke tests
61,Testing best practices
61,Testing levels
61,Testing Rails migrations
61,Testing Rake tasks
61,Testing (end to end)
61,Beginner's guide to writing end-to-end tests
61,Best practices when writing end-to-end tests
61,Dynamic element validation
61,Flows in GitLab QA
61,Page objects in GitLab QA
61,Resource class in GitLab QA
61,Style guide for writing end-to-end tests
61,Testing with feature flags
61,Translate GitLab
61,Externalization
61,Translation
61,Proofreading
61,Merging
61,Snowplow guide
61,Usage Ping guide
61,Metrics dictionary guide
61,Metrics dictionary
61,Value Stream Analytics
61,Working with Prometheus metrics
61,GitLab Docs
61,Contributor and Development Docs
61,Performance Guidelines
61,Workflow
61,Tooling
61,Benchmarks
61,Profiling
61,Development
61,Production
61,RSpec profiling
61,Memory optimization
61,Memory allocations
61,Checking memory pressure of own code
61,Different types of allocations
61,Using Memory Profiler
61,Rbtrace
61,Importance of Changes
61,Slow Operations & Sidekiq
61,Git Operations
61,Caching
61,String Freezing
61,Banzai pipelines and filters
61,Reading from files and other data sources
61,Recommendations
61,Anti-Patterns
61,Moving Allocations to Constants
61,How to seed a database with millions of rows
61,Examples
61,Performance Guidelines
61,This document describes various guidelines to follow to ensure good and
61,consistent performance of GitLab.
61,Workflow
61,The process of solving performance problems is roughly as follows:
61,"Make sure there’s an issue open somewhere (for example, on the GitLab CE issue"
61,"tracker), and create one if there is not. See #15607 for an example."
61,Measure the performance of the code in a production environment such as
61,GitLab.com (see the Tooling section below). Performance should be
61,measured over a period of at least 24 hours.
61,"Add your findings based on the measurement period (screenshots of graphs,"
61,"timings, etc) to the issue mentioned in step 1."
61,Solve the problem.
61,"Create a merge request, assign the “Performance” label and follow the performance review process."
61,Once a change has been deployed make sure to again measure for at least 24
61,hours to see if your changes have any impact on the production environment.
61,Repeat until you’re done.
61,When providing timings make sure to provide:
61,The 95th percentile
61,The 99th percentile
61,The mean
61,"When providing screenshots of graphs, make sure that both the X and Y axes and"
61,the legend are clearly visible. If you happen to have access to GitLab.com’s own
61,monitoring tools you should also provide a link to any relevant
61,graphs/dashboards.
61,Tooling
61,GitLab provides built-in tools to help improve performance and availability:
61,Profiling.
61,Distributed Tracing
61,GitLab Performance Monitoring.
61,Request Profiling.
61,QueryRecoder for preventing N+1 regressions.
61,Chaos endpoints for testing failure scenarios. Intended mainly for testing availability.
61,Service measurement for measuring and logging service execution.
61,GitLab team members can use GitLab.com’s performance monitoring systems located at
61,"dashboards.gitlab.net, this requires you to log in using your"
61,@gitlab.com email address. Non-GitLab team-members are advised to set up their
61,own Prometheus and Grafana stack.
61,Benchmarks
61,Benchmarks are almost always useless. Benchmarks usually only test small bits of
61,"code in isolation and often only measure the best case scenario. On top of that,"
61,benchmarks for libraries (such as a Gem) tend to be biased in favour of the
61,library. After all there’s little benefit to an author publishing a benchmark
61,that shows they perform worse than their competitors.
61,Benchmarks are only really useful when you need a rough (emphasis on “rough”)
61,"understanding of the impact of your changes. For example, if a certain method is"
61,slow a benchmark can be used to see if the changes you’re making have any impact
61,"on the method’s performance. However, even when a benchmark shows your changes"
61,improve performance there’s no guarantee the performance also improves in a
61,production environment.
61,When writing benchmarks you should almost always use
61,benchmark-ips. Ruby’s Benchmark
61,module that comes with the standard library is rarely useful as it runs either a
61,single iteration (when using Benchmark.bm) or two iterations (when using
61,"Benchmark.bmbm). Running this few iterations means external factors, such as a"
61,"video streaming in the background, can very easily skew the benchmark"
61,statistics.
61,"Another problem with the Benchmark module is that it displays timings, not"
61,iterations. This means that if a piece of code completes in a very short period
61,of time it can be very difficult to compare the timings before and after a
61,certain change. This in turn leads to patterns such as the following:
61,Benchmark.bmbm(10) do |bench|
61,bench.report 'do something' do
61,100.times do
61,... work here ...
61,end
61,end
61,end
61,This however leads to the question: how many iterations should we run to get
61,meaningful statistics?
61,"The benchmark-ips Gem basically takes care of all this and much more, and as a"
61,result of this should be used instead of the Benchmark module.
61,In short:
61,Don’t trust benchmarks you find on the internet.
61,"Never make claims based on just benchmarks, always measure in production to"
61,confirm your findings.
61,X being N times faster than Y is meaningless if you don’t know what impact it
61,has on your production environment.
61,A production environment is the only benchmark that always tells the truth
61,(unless your performance monitoring systems are not set up correctly).
61,If you must write a benchmark use the benchmark-ips Gem instead of Ruby’s
61,Benchmark module.
61,Profiling
61,"By collecting snapshots of process state at regular intervals, profiling allows"
61,you to see where time is spent in a process. The
61,"Stackprof gem is included in GitLab,"
61,allowing you to profile which code is running on CPU in detail.
61,It’s important to note that profiling an application alters its performance.
61,Different profiling strategies have different overheads. Stackprof is a sampling
61,profiler. It samples stack traces from running threads at a configurable
61,"frequency (e.g. 100hz, that is 100 stacks per second). This type of profiling"
61,has quite a low (albeit non-zero) overhead and is generally considered to be
61,safe for production.
61,Development
61,"A profiler can be a very useful tool during development, even if it does run in"
61,"an unrepresentative environment. In particular, a method is not necessarily"
61,"troublesome just because it’s executed many times, or takes a long time to"
61,execute. Profiles are tools you can use to better understand what is happening
61,in an application - using that information wisely is up to you!
61,"Keeping that in mind, to create a profile, identify (or create) a spec that"
61,"exercises the troublesome code path, then run it using the bin/rspec-stackprof"
61,"helper, for example:"
61,$ LIMIT=10 bin/rspec-stackprof spec/policies/project_policy_spec.rb
61,8/8 |====== 100 ======>| Time: 00:00:18
61,Finished in 18.19 seconds (files took 4.8 seconds to load)
61,"8 examples, 0 failures"
61,==================================
61,Mode: wall(1000)
61,Samples: 17033 (5.59% miss rate)
61,GC: 1901 (11.16%)
61,==================================
61,TOTAL
61,(pct)
61,SAMPLES
61,(pct)
61,FRAME
61,6000
61,(35.2%)
61,2566
61,(15.1%)
61,Sprockets::Cache::FileStore#get
61,2018
61,(11.8%)
61,888
61,(5.2%)
61,ActiveRecord::ConnectionAdapters::PostgreSQLAdapter#exec_no_cache
61,1338
61,(7.9%)
61,640
61,(3.8%)
61,ActiveRecord::ConnectionAdapters::PostgreSQL::DatabaseStatements#execute
61,3125
61,(18.3%)
61,394
61,(2.3%)
61,Sprockets::Cache::FileStore#safe_open
61,913
61,(5.4%)
61,301
61,(1.8%)
61,ActiveRecord::ConnectionAdapters::PostgreSQLAdapter#exec_cache
61,288
61,(1.7%)
61,288
61,(1.7%)
61,ActiveRecord::Attribute#initialize
61,246
61,(1.4%)
61,246
61,(1.4%)
61,Sprockets::Cache::FileStore#safe_stat
61,295
61,(1.7%)
61,193
61,(1.1%)
61,block (2 levels) in class_attribute
61,187
61,(1.1%)
61,187
61,(1.1%)
61,block (4 levels) in class_attribute
61,You can limit the specs that are run by passing any arguments rspec would
61,normally take.
61,The output is sorted by the Samples column by default. This is the number of
61,samples taken where the method is the one currently being executed. The Total
61,"column shows the number of samples taken where the method, or any of the methods"
61,"it calls, were being executed."
61,To create a graphical view of the call stack:
61,stackprof tmp/project_policy_spec.rb.dump --graphviz > project_policy_spec.dot
61,dot -Tsvg project_policy_spec.dot > project_policy_spec.svg
61,To load the profile in KCachegrind:
61,stackprof tmp/project_policy_spec.rb.dump --callgrind > project_policy_spec.callgrind
61,kcachegrind project_policy_spec.callgrind # Linux
61,qcachegrind project_policy_spec.callgrind # Mac
61,"For flame graphs, enable raw collection first. Note that raw"
61,"collection can generate a very large file, so increase the INTERVAL, or"
61,run on a smaller number of specs for smaller file size:
61,RAW=true bin/rspec-stackprof spec/policies/group_member_policy_spec.rb
61,"You can then generate, and view the resultant flame graph. It might take a"
61,while to generate based on the output file size:
61,# Generate
61,stackprof --flamegraph tmp/group_member_policy_spec.rb.dump > group_member_policy_spec.flame
61,# View
61,stackprof --flamegraph-viewer=group_member_policy_spec.flame
61,"It may be useful to zoom in on a specific method, for example:"
61,$ stackprof tmp/project_policy_spec.rb.dump --method warm_asset_cache
61,TestEnv#warm_asset_cache (/Users/lupine/dev/gitlab.com/gitlab-org/gitlab-development-kit/gitlab/spec/support/test_env.rb:164)
61,samples:
61,0 self (0.0%)
61,6288 total (36.9%)
61,callers:
61,6288
61,100.0%)
61,block (2 levels) in <top (required)>
61,callees (6288 total):
61,6288
61,100.0%)
61,Capybara::RackTest::Driver#visit
61,code:
61,164
61,def warm_asset_cache
61,165
61,return if warm_asset_cache?
61,166
61,return unless defined?(Capybara)
61,167
61,6288
61,(36.9%)
61,168
61,Capybara.current_session.driver.visit '/'
61,169
61,end
61,$ stackprof tmp/project_policy_spec.rb.dump --method BasePolicy#abilities
61,BasePolicy#abilities (/Users/lupine/dev/gitlab.com/gitlab-org/gitlab-development-kit/gitlab/app/policies/base_policy.rb:79)
61,samples:
61,0 self (0.0%)
61,50 total (0.3%)
61,callers:
61,50.0%)
61,BasePolicy.abilities
61,50.0%)
61,BasePolicy#collect_rules
61,callees (50 total):
61,50.0%)
61,ProjectPolicy#rules
61,50.0%)
61,BasePolicy#collect_rules
61,code:
61,def abilities
61,return RuleSet.empty if @user && @user.blocked?
61,return anonymous_abilities if @user.nil?
61,(0.3%)
61,collect_rules { rules }
61,end
61,Since the profile includes the work done by the test suite as well as the
61,"application code, these profiles can be used to investigate slow tests as well."
61,"However, for smaller runs (like this example), this means that the cost of"
61,setting up the test suite tends to dominate.
61,Production
61,Stackprof can also be used to profile production workloads.
61,"In order to enable production profiling for Ruby processes, you can set the STACKPROF_ENABLED environment variable to true."
61,The following configuration options can be configured:
61,STACKPROF_ENABLED: Enables Stackprof signal handler on SIGUSR2 signal.
61,Defaults to false.
61,STACKPROF_MODE: See sampling modes.
61,Defaults to cpu.
61,STACKPROF_INTERVAL: Sampling interval. Unit semantics depend on STACKPROF_MODE.
61,For object mode this is a per-event interval (every nth event is sampled)
61,and defaults to 1000.
61,For other modes such as cpu this is a frequency and defaults to 10000 μs (100hz).
61,STACKPROF_FILE_PREFIX: File path prefix where profiles are stored. Defaults
61,to $TMPDIR (often corresponds to /tmp).
61,STACKPROF_TIMEOUT_S: Profiling timeout in seconds. Profiling will
61,automatically stop after this time has elapsed. Defaults to 30.
61,STACKPROF_RAW: Whether to collect raw samples or only aggregates. Raw
61,"samples are needed to generate flame graphs, but they do have a higher memory"
61,and disk overhead. Defaults to true.
61,"Once enabled, profiling can be triggered by sending a SIGUSR2 signal to the"
61,Ruby process. The process begins sampling stacks. Profiling can be stopped
61,"by sending another SIGUSR2. Alternatively, it stops automatically after"
61,the timeout.
61,"Once profiling stops, the profile is written out to disk at"
61,$STACKPROF_FILE_PREFIX/stackprof.$PID.$RAND.profile. It can then be inspected
61,"further via the stackprof command line tool, as described in the previous"
61,section.
61,Currently supported profiling targets are:
61,Puma worker
61,Sidekiq
61,noteThe Puma master process is not supported. Neither is Unicorn.
61,"Sending SIGUSR2 to either of those triggers restarts. In the case of Puma,"
61,take care to only send the signal to Puma workers.
61,This can be done via pkill -USR2 puma:. The : distinguishes between puma
61,4.3.3.gitlab.2 ... (the master process) from puma: cluster worker 0: ... (the
61,"worker processes), selecting the latter."
61,"For Sidekiq, the signal can be sent to the sidekiq-cluster process via pkill"
61,"-USR2 bin/sidekiq-cluster, which forwards the signal to all Sidekiq"
61,"children. Alternatively, you can also select a specific PID of interest."
61,Production profiles can be especially noisy. It can be helpful to visualize them
61,as a flame graph. This can be done
61,via:
61,bundle exec stackprof --stackcollapse /tmp/stackprof.55769.c6c3906452.profile | flamegraph.pl > flamegraph.svg
61,RSpec profiling
61,The GitLab development environment also includes the
61,"rspec_profiling gem, which is used"
61,to collect data on spec execution times. This is useful for analyzing the
61,"performance of the test suite itself, or seeing how the performance of a spec"
61,may have changed over time.
61,"To activate profiling in your local environment, run the following:"
61,export RSPEC_PROFILING=yes
61,rake rspec_profiling:install
61,"This creates an SQLite3 database in tmp/rspec_profiling, into which statistics"
61,are saved every time you run specs with the RSPEC_PROFILING environment
61,variable set.
61,Ad-hoc investigation of the collected results can be performed in an interactive
61,shell:
61,$ rake rspec_profiling:console
61,irb(main):001:0> results.count
61,=> 231
61,irb(main):002:0> results.last.attributes.keys
61,"=> [""id"", ""commit"", ""date"", ""file"", ""line_number"", ""description"", ""time"", ""status"", ""exception"", ""query_count"", ""query_time"", ""request_count"", ""request_time"", ""created_at"", ""updated_at""]"
61,"irb(main):003:0> results.where(status: ""passed"").average(:time).to_s"
61,"=> ""0.211340155844156"""
61,These results can also be placed into a PostgreSQL database by setting the
61,RSPEC_PROFILING_POSTGRES_URL variable. This is used to profile the test suite
61,when running in the CI environment.
61,We store these results also when running nightly scheduled CI jobs on the
61,default branch on gitlab.com. Statistics of these profiling data are
61,available online. For
61,"example, you can find which tests take longest to run or which execute the most"
61,queries. This can be handy for optimizing our tests or identifying performance
61,issues in our code.
61,Memory optimization
61,"We can use a set of different techniques, often in combination, to track down memory issues:"
61,Leaving the code intact and wrapping a profiler around it.
61,Use memory allocation counters for requests and services.
61,Monitor memory usage of the process while disabling/enabling different parts of the code we suspect could be problematic.
61,Memory allocations
61,Ruby shipped with GitLab includes a special patch to allow tracing memory allocations.
61,This patch is available by default for
61,"Omnibus,"
61,"CNG,"
61,"GitLab CI,"
61,GCK
61,and can additionally be enabled for GDK.
61,This patch provides a set of 3 metrics that makes it easier to understand efficiency of memory usage for a given codepath:
61,mem_objects: the number of objects allocated.
61,mem_bytes: the number of bytes allocated by malloc.
61,mem_mallocs: the number of malloc allocations.
61,The number of objects and bytes allocated impact how often GC cycles happen.
61,Fewer objects allocations result in a significantly more responsive application.
61,It is advised that web server requests do not allocate more than 100k mem_objects
61,and 100M mem_bytes. You can view the current usage on GitLab.com.
61,Checking memory pressure of own code
61,There are two ways of measuring your own code:
61,"Review api_json.log, development_json.log, sidekiq.log that includes memory allocation counters."
61,Use Gitlab::Memory::Instrumentation.with_memory_allocations for a given codeblock and log it.
61,Use Measuring module
61,"{""time"":""2021-02-15T11:20:40.821Z"",""severity"":""INFO"",""duration_s"":0.27412,""db_duration_s"":0.05755,""view_duration_s"":0.21657,""status"":201,""method"":""POST"",""path"":""/api/v4/projects/user/1"",""mem_objects"":86705,""mem_bytes"":4277179,""mem_mallocs"":22693,""correlation_id"":""...}"
61,Different types of allocations
61,The mem_* values represent different aspects of how objects and memory are allocated in Ruby:
61,The following example will create around of 1000 of mem_objects since strings
61,"can be frozen, and while the underlying string object remains the same, we still need to allocate 1000 references to this string:"
61,Gitlab::Memory::Instrumentation.with_memory_allocations do
61,1_000.times { '0123456789' }
61,end
61,"=> {:mem_objects=>1001, :mem_bytes=>0, :mem_mallocs=>0}"
61,"The following example will create around of 1000 of mem_objects, as strings are created dynamically."
61,"Each of them will not allocate additional memory, as they fit into Ruby slot of 40 bytes:"
61,Gitlab::Memory::Instrumentation.with_memory_allocations do
61,s = '0'
61,1_000.times { s * 23 }
61,end
61,"=> {:mem_objects=>1002, :mem_bytes=>0, :mem_mallocs=>0}"
61,"The following example will create around of 1000 of mem_objects, as strings are created dynamically."
61,Each of them will allocate additional memory as strings are larger than Ruby slot of 40 bytes:
61,Gitlab::Memory::Instrumentation.with_memory_allocations do
61,s = '0'
61,1_000.times { s * 24 }
61,end
61,"=> {:mem_objects=>1002, :mem_bytes=>32000, :mem_mallocs=>1000}"
61,"The following example will allocate over 40kB of data, and perform only a single memory allocation."
61,The existing object will be reallocated/resized on subsequent iterations:
61,Gitlab::Memory::Instrumentation.with_memory_allocations do
61,str = ''
61,append = '0123456789012345678901234567890123456789' # 40 bytes
61,1_000.times { str.concat(append) }
61,end
61,"=> {:mem_objects=>3, :mem_bytes=>49152, :mem_mallocs=>1}"
61,"The following example will create over 1k of objects, perform over 1k of allocations, each time mutating the object."
61,This does result in copying a lot of data and perform a lot of memory allocations
61,(as represented by mem_bytes counter) indicating very inefficient method of appending string:
61,Gitlab::Memory::Instrumentation.with_memory_allocations do
61,str = ''
61,append = '0123456789012345678901234567890123456789' # 40 bytes
61,1_000.times { str += append }
61,end
61,"=> {:mem_objects=>1003, :mem_bytes=>21968752, :mem_mallocs=>1000}"
61,Using Memory Profiler
61,We can use memory_profiler for profiling.
61,"The memory_profiler gem is already present in the GitLab Gemfile,"
61,you just need to require it:
61,require 'sidekiq/testing'
61,report = MemoryProfiler.report do
61,# Code you want to profile
61,end
61,"output = File.open('/tmp/profile.txt','w')"
61,report.pretty_print(output)
61,The report breaks down 2 key concepts:
61,Retained: long lived memory use and object count retained due to the execution of the code block.
61,Allocated: all object allocation and memory allocation during code block.
61,"As a general rule, retained is always smaller than or equal to allocated."
61,The actual RSS cost is always slightly higher as MRI heaps are not squashed to size and memory fragments.
61,Rbtrace
61,One of the reasons of the increased memory footprint could be Ruby memory fragmentation.
61,"To diagnose it, you can visualize Ruby heap as described in this post by Aaron Patterson."
61,"To start, you want to dump the heap of the process you’re investigating to a JSON file."
61,"You need to run the command inside the process you’re exploring, you may do that with rbtrace."
61,"rbtrace is already present in GitLab Gemfile, you just need to require it."
61,It could be achieved running webserver or Sidekiq with the environment variable set to ENABLE_RBTRACE=1.
61,To get the heap dump:
61,"bundle exec rbtrace -p <PID> -e 'File.open(""heap.json"", ""wb"") { |t| ObjectSpace.dump_all(output: t) }'"
61,"Having the JSON, you finally could render a picture using the script provided by Aaron or similar:"
61,ruby heapviz.rb heap.json
61,Fragmented Ruby heap snapshot could look like this:
61,"Memory fragmentation could be reduced by tuning GC parameters as described in this post. This should be considered as a tradeoff, as it may affect overall performance of memory allocation and GC cycles."
61,Importance of Changes
61,"When working on performance improvements, it’s important to always ask yourself"
61,the question “How important is it to improve the performance of this piece of
61,code?”. Not every piece of code is equally important and it would be a waste to
61,spend a week trying to improve something that only impacts a tiny fraction of
61,"our users. For example, spending a week trying to squeeze 10 milliseconds out of"
61,a method is a waste of time when you could have spent a week squeezing out 10
61,seconds elsewhere.
61,There is no clear set of steps that you can follow to determine if a certain
61,piece of code is worth optimizing. The only two things you can do are:
61,"Think about what the code does, how it’s used, how many times it’s called and"
61,"how much time is spent in it relative to the total execution time (for example, the"
61,total time spent in a web request).
61,Ask others (preferably in the form of an issue).
61,Some examples of changes that are not really important/worth the effort:
61,Replacing double quotes with single quotes.
61,Replacing usage of Array with Set when the list of values is very small.
61,Replacing library A with library B when both only take up 0.1% of the total
61,execution time.
61,Calling freeze on every string (see String Freezing).
61,Slow Operations & Sidekiq
61,"Slow operations, like merging branches, or operations that are prone to errors"
61,(using external APIs) should be performed in a Sidekiq worker instead of
61,directly in a web request as much as possible. This has numerous benefits such
61,as:
61,An error doesn’t prevent the request from completing.
61,The process being slow doesn’t affect the loading time of a page.
61,In case of a failure you can retry the process (Sidekiq takes care of
61,this automatically).
61,By isolating the code from a web request it should be easier to test
61,and maintain.
61,It’s especially important to use Sidekiq as much as possible when dealing with
61,Git operations as these operations can take quite some time to complete
61,depending on the performance of the underlying storage system.
61,Git Operations
61,"Care should be taken to not run unnecessary Git operations. For example,"
61,retrieving the list of branch names using Repository#branch_names can be done
61,"without an explicit check if a repository exists or not. In other words, instead"
61,of this:
61,if repository.exists?
61,repository.branch_names.each do |name|
61,...
61,end
61,end
61,You can just write:
61,repository.branch_names.each do |name|
61,...
61,end
61,Caching
61,"Operations that often return the same result should be cached using Redis,"
61,"in particular Git operations. When caching data in Redis, make sure the cache is"
61,"flushed whenever needed. For example, a cache for the list of tags should be"
61,flushed whenever a new tag is pushed or a tag is removed.
61,"When adding cache expiration code for repositories, this code should be placed"
61,"in one of the before/after hooks residing in the Repository class. For example,"
61,if a cache should be flushed after importing a repository this code should be
61,added to Repository#after_import. This ensures the cache logic stays within
61,the Repository class instead of leaking into other classes.
61,"When caching data, make sure to also memoize the result in an instance variable."
61,"While retrieving data from Redis is much faster than raw Git operations, it still"
61,"has overhead. By caching the result in an instance variable, repeated calls to"
61,the same method don’t retrieve data from Redis upon every call. When
61,"memoizing cached data in an instance variable, make sure to also reset the"
61,instance variable when flushing the cache. An example:
61,def first_branch
61,@first_branch ||= cache.fetch(:first_branch) { branches.first }
61,end
61,def expire_first_branch_cache
61,cache.expire(:first_branch)
61,@first_branch = nil
61,end
61,String Freezing
61,In recent Ruby versions calling freeze on a String leads to it being allocated
61,"only once and re-used. For example, on Ruby 2.3 or later this only allocates the"
61,“foo” String once:
61,10.times do
61,'foo'.freeze
61,end
61,Depending on the size of the String and how frequently it would be allocated
61,"(before the .freeze call was added), this may make things faster, but"
61,this isn’t guaranteed.
61,Strings are frozen by default in Ruby 3.0. To prepare our codebase for
61,"this eventuality, we are adding the following header to all Ruby files:"
61,# frozen_string_literal: true
61,This may cause test failures in the code that expects to be able to manipulate
61,"strings. Instead of using dup, use the unary plus to get an unfrozen string:"
61,"test = +""hello"""
61,"test += "" world"""
61,"When adding new Ruby files, please check that you can add the above header,"
61,as omitting it may lead to style check failures.
61,Banzai pipelines and filters
61,"When writing or updating Banzai filters and pipelines,"
61,"it can be difficult to understand what the performance of the filter is, and what effect it might"
61,have on the overall pipeline performance.
61,To perform benchmarks run:
61,bin/rake benchmark:banzai
61,This command generates output like this:
61,"--> Benchmarking Full, Wiki, and Plain pipelines"
61,Calculating -------------------------------------
61,Full pipeline
61,1.000
61,i/100ms
61,Wiki pipeline
61,1.000
61,i/100ms
61,Plain pipeline
61,1.000
61,i/100ms
61,-------------------------------------------------
61,Full pipeline
61,3.357
61,(±29.8%) i/s -
61,31.000
61,Wiki pipeline
61,2.893
61,(±34.6%) i/s -
61,25.000
61,10.677014s
61,Plain pipeline
61,15.447
61,(±32.4%) i/s -
61,119.000
61,Comparison:
61,Plain pipeline:
61,15.4 i/s
61,Full pipeline:
61,3.4 i/s - 4.60x slower
61,Wiki pipeline:
61,2.9 i/s - 5.34x slower
61,--> Benchmarking FullPipeline filters
61,Calculating -------------------------------------
61,Markdown
61,24.000
61,i/100ms
61,Plantuml
61,8.000
61,i/100ms
61,SpacedLink
61,22.000
61,i/100ms
61,...
61,TaskList
61,49.000
61,i/100ms
61,InlineDiff
61,9.000
61,i/100ms
61,SetDirection
61,369.000
61,i/100ms
61,-------------------------------------------------
61,Markdown
61,237.796
61,(±16.4%) i/s -
61,2.304k
61,Plantuml
61,80.415
61,(±36.1%) i/s -
61,520.000
61,SpacedLink
61,168.188
61,(±10.1%) i/s -
61,1.672k
61,...
61,TaskList
61,101.145
61,(± 6.9%) i/s -
61,1.029k
61,InlineDiff
61,52.925
61,(±15.1%) i/s -
61,522.000
61,SetDirection
61,3.728k (±17.2%) i/s -
61,34.317k in
61,10.617882s
61,Comparison:
61,Suggestion:
61,739616.9 i/s
61,Kroki:
61,306449.0 i/s - 2.41x slower
61,InlineGrafanaMetrics:
61,156535.6 i/s - 4.72x slower
61,SetDirection:
61,3728.3 i/s - 198.38x slower
61,...
61,UserReference:
61,2.1 i/s - 360365.80x slower
61,ExternalLink:
61,1.6 i/s - 470400.67x slower
61,ProjectReference:
61,0.7 i/s - 1128756.09x slower
61,--> Benchmarking PlainMarkdownPipeline filters
61,Calculating -------------------------------------
61,Markdown
61,19.000
61,i/100ms
61,-------------------------------------------------
61,Markdown
61,241.476
61,(±15.3%) i/s -
61,2.356k
61,"This can give you an idea how various filters perform, and which ones might be performing the slowest."
61,The test data has a lot to do with how well a filter performs. If there is nothing in the test data
61,"that specifically triggers the filter, it might look like it’s running incredibly fast."
61,Make sure that you have relevant test data for your filter in the
61,spec/fixtures/markdown.md.erb
61,file.
61,Reading from files and other data sources
61,Ruby offers several convenience functions that deal with file contents specifically
61,or I/O streams in general. Functions such as IO.read and IO.readlines make
61,"it easy to read data into memory, but they can be inefficient when the"
61,data grows large. Because these functions read the entire contents of a data
61,"source into memory, memory use grows by at least the size of the data source."
61,"In the case of readlines, it grows even further, due to extra bookkeeping"
61,the Ruby VM has to perform to represent each line.
61,"Consider the following program, which reads a text file that is 750MB on disk:"
61,File.readlines('large_file.txt').each do |line|
61,puts line
61,end
61,"Here is a process memory reading from while the program was running, showing"
61,how we indeed kept the entire file in memory (RSS reported in kilobytes):
61,$ ps -o rss -p <pid>
61,RSS
61,783436
61,And here is an excerpt of what the garbage collector was doing:
61,pp GC.stat
61,":heap_live_slots=>2346848,"
61,":malloc_increase_bytes=>30895288,"
61,...
61,"We can see that heap_live_slots (the number of reachable objects) jumped to ~2.3M,"
61,which is roughly two orders of magnitude more compared to reading the file line by
61,"line instead. It was not just the raw memory usage that increased, but also how the garbage collector (GC)"
61,responded to this change in anticipation of future memory use. We can see that malloc_increase_bytes jumped
61,"to ~30MB, which compares to just ~4kB for a “fresh” Ruby program. This figure specifies how"
61,much additional heap space the Ruby GC claims from the operating system next time it runs out of memory.
61,"Not only did we occupy more memory, we also changed the behavior of the application"
61,to increase memory use at a faster rate.
61,"The IO.read function exhibits similar behavior, with the difference that no extra memory is"
61,allocated for each line object.
61,Recommendations
61,"Instead of reading data sources into memory in full, it is better to read them line by line"
61,"instead. This is not always an option, for instance when you need to convert a YAML file"
61,"into a Ruby Hash, but whenever you have data where each row represents some entity that"
61,"can be processed and then discarded, you can use the following approaches."
61,"First, replace calls to readlines.each with either each or each_line."
61,The each_line and each functions read the data source line by line without keeping
61,already visited lines in memory:
61,File.new('file').each { |line| puts line }
61,"Alternatively, you can read individual lines explicitly using IO.readline or IO.gets functions:"
61,while line = file.readline
61,# process line
61,end
61,"This might be preferable if there is a condition that allows exiting the loop early, saving not"
61,just memory but also unnecessary time spent in CPU and I/O for processing lines you’re not interested in.
61,Anti-Patterns
61,This is a collection of anti-patterns that should be avoided
61,"unless these changes have a measurable, significant, and positive impact on"
61,production environments.
61,Moving Allocations to Constants
61,Storing an object as a constant so you only allocate it once may improve
61,"performance, but this is not guaranteed. Looking up constants has an"
61,"impact on runtime performance, and as such, using a constant instead of"
61,referencing an object directly may even slow code down. For example:
61,SOME_CONSTANT = 'foo'.freeze
61,9000.times do
61,SOME_CONSTANT
61,end
61,The only reason you should be doing this is to prevent somebody from mutating
61,"the global String. However, since you can just re-assign constants in Ruby"
61,there’s nothing stopping somebody from doing this elsewhere in the code:
61,SOME_CONSTANT = 'bar'
61,How to seed a database with millions of rows
61,"You might want millions of project rows in your local database, for example,"
61,"in order to compare relative query performance, or to reproduce a bug. You could"
61,do this by hand with SQL commands or using Mass Inserting Rails
61,Models functionality.
61,"Assuming you are working with ActiveRecord models, you might also find these links helpful:"
61,Insert records in batches
61,BulkInsert gem
61,ActiveRecord::PgGenerateSeries gem
61,Examples
61,You may find some useful examples in this snippet.
61,Help & feedback
61,Docs
61,Edit this page
61,to fix an error or add an improvement in a merge request.
61,Create an issue
61,to suggest an improvement to this page.
61,Show and post comments
61,to review and give feedback about this page.
61,Product
61,Create an issue
61,if there's something you don't like about this feature.
61,Propose functionality
61,by submitting a feature request.
61,Join First Look
61,to help shape new features.
61,Feature availability and product trials
61,View pricing
61,"to see all GitLab tiers and features, or to upgrade."
61,Try GitLab for free
61,with access to all features for 30 days.
61,Get Help
61,"If you didn't find what you were looking for,"
61,search the docs.
61,"If you want help with something specific and could use community support,"
61,post on the GitLab forum.
61,For problems setting up or using this feature (depending on your GitLab
61,subscription).
61,Request support
61,Please enable JavaScript to view the
61,comments powered by Disqus.
61,Twitter
61,Facebook
61,YouTube
61,LinkedIn
61,Docs Repo
61,About GitLab
61,Terms
61,Privacy Policy
61,Cookies Policy
61,Contact
61,View page source -
61,Edit in Web IDE
61,Twitter
61,Facebook
61,YouTube
61,LinkedIn
62,Some SQL Tricks of an Application DBA | Haki Benita
62,Haki Benita
62,About
62,Subscribe
62,27 July 2020
62,"SQL,"
62,Performance
62,Some SQL Tricks of an Application DBA
62,Non-trivial tips for database development
62,"When I started my career in development, my first job was a DBA. Back then, before AWS RDS, Azure, Google Cloud and the rest of them cloud services, there were two types of DBAs:"
62,"The Infrastructure DBA was in charge of setting up the database, configuring the storage and taking care of backups and replication. After setting up the database, the infrastructure DBA would pop up from time to time and do some ""instance tuning"", things like sizing caches."
62,"The Application DBA got a clean database from the infrastructure DBA, and was in charge of schema design: creating tables, indexes, constraints, and tuning SQL. The application DBA was also the one who implemented ETL processes and data migrations. In teams that used stored procedures, the application DBA would maintain those as well."
62,"Application DBAs were usually part of the development team. They would possess deep domain knowledge so normally they would work on just one or two projects. Infrastructure DBAs would usually be part of some IT team, and would work on many projects simultaneously."
62,I'm an Application DBA
62,"I never had any desire to fiddle with backups or tune storage (I'm sure it's fascinating!). Until this day I like to say I'm a DBA that knows how to develop applications, and not a developer that knows his way around the database."
62,In this article I share some non-trivial tips about database development I gathered along the way.
62,Be that guy...Image by CommitStrip
62,Table of Contents
62,Update Only What Needs Updating
62,Disable Constraints and Indexes During Bulk Loads
62,Use UNLOGGED Tables for Intermediate Data
62,Implement Complete Processes Using WITH and RETURNING
62,Avoid Indexes on Columns With Low Selectivity
62,Use Partial Indexes
62,Always Load Sorted Data
62,Index Columns With High Correlation Using BRIN
62,"Make Indexes ""Invisible"""
62,Don't Schedule Long Running Processes at Round Hours
62,Conclusion
62,Update Only What Needs Updating
62,UPDATE is a relatively expensive operation. To speed up an UPDATE command it's best to make sure you only update what needs updating.
62,Take this query for example that normalizes an email column:
62,db=# UPDATE users SET email = lower(email);
62,UPDATE 1010000
62,Time: 1583.935 ms (00:01.584)
62,"Looks innocent, right? the query updated emails of 1,010,000 users. But, did all rows really needed to update?"
62,db=# UPDATE users SET email = lower(email)
62,db-# WHERE email != lower(email);
62,UPDATE 10000
62,Time: 299.470 ms
62,"Only 10,000 rows needed to update. By reducing the amount of affected rows, the execution time went down from 1.5 seconds to just less than 300ms. Updating fewer rows also saves the database maintenance later on."
62,Update Only What Needs Updating
62,"This type of large updates are very common in data migration scripts. So the next time you write a migration script, make sure to only update what needs updating."
62,Disable Constraints and Indexes During Bulk Loads
62,"Constraints are an important part of relational databases: they keep the data consistent and reliable. Their benefits come at a cost though, and it's most noticeable when loading or updating a lot of rows."
62,"To demonstrate, set up a small schema for a store:"
62,DROP TABLE IF EXISTS product CASCADE;
62,CREATE TABLE product (
62,"id serial PRIMARY KEY,"
62,"name TEXT NOT NULL,"
62,price INT NOT NULL
62,"INSERT INTO product (name, price)"
62,"SELECT random()::text, (random() * 1000)::int"
62,"FROM generate_series(0, 10000);"
62,DROP TABLE IF EXISTS customer CASCADE;
62,CREATE TABLE customer (
62,"id serial PRIMARY KEY,"
62,name TEXT NOT NULL
62,INSERT INTO customer (name)
62,SELECT random()::text
62,"FROM generate_series(0, 100000);"
62,DROP TABLE IF EXISTS sale;
62,CREATE TABLE sale (
62,"id serial PRIMARY KEY,"
62,"created timestamptz NOT NULL,"
62,"product_id int NOT NULL,"
62,customer_id int NOT NULL
62,"The schema defines different types of constraints such as ""not null"" and unique constraints."
62,"To set a baseline, start by adding foreign keys to the sale table, and then load some data into it:"
62,db=# ALTER TABLE sale ADD CONSTRAINT sale_product_fk
62,db-# FOREIGN KEY (product_id) REFERENCES product(id);
62,ALTER TABLE
62,Time: 18.413 ms
62,db=# ALTER TABLE sale ADD CONSTRAINT sale_customer_fk
62,db-# FOREIGN KEY (customer_id) REFERENCES customer(id);
62,ALTER TABLE
62,Time: 5.464 ms
62,db=# CREATE INDEX sale_created_ix ON sale(created);
62,CREATE INDEX
62,Time: 12.605 ms
62,"db=# INSERT INTO SALE (created, product_id, customer_id)"
62,db-# SELECT
62,db-#
62,"now() - interval '1 hour' * random() * 1000,"
62,db-#
62,"(random() * 10000)::int + 1,"
62,db-#
62,(random() * 100000)::int + 1
62,"db-# FROM generate_series(1, 1000000);"
62,INSERT 0 1000000
62,Time: 15410.234 ms (00:15.410)
62,"After defining constraints and indexes, loading a million rows to the table took ~15.4s."
62,"Next, try to load the data into the table first, and only then add constraints and indexes:"
62,"db=# INSERT INTO SALE (created, product_id, customer_id)"
62,db-# SELECT
62,db-#
62,"now() - interval '1 hour' * random() * 1000,"
62,db-#
62,"(random() * 10000)::int + 1,"
62,db-#
62,(random() * 100000)::int + 1
62,"db-# FROM generate_series(1, 1000000);"
62,INSERT 0 1000000
62,Time: 2277.824 ms (00:02.278)
62,db=# ALTER TABLE sale ADD CONSTRAINT sale_product_fk
62,db-# FOREIGN KEY (product_id) REFERENCES product(id);
62,ALTER TABLE
62,Time: 169.193 ms
62,db=# ALTER TABLE sale ADD CONSTRAINT sale_customer_fk
62,db-# FOREIGN KEY (customer_id) REFERENCES customer(id);
62,ALTER TABLE
62,Time: 185.633 ms
62,db=# CREATE INDEX sale_created_ix ON sale(created);
62,CREATE INDEX
62,Time: 484.244 ms
62,"Loading data into a table without indexes and constraints was much faster, 2.27s compared to 15.4s before. Creating the indexes and constraints after the data was loaded into the table took a bit longer, but overall the entire process was much faster, 3.1s compared to 15.4s."
62,"Unfortunately, for indexes PostgreSQL does not provide an easy way of doing this other than dropping and re-creating the indexes. In other databases such as Oracle, you can disable and enable indexes without having to re-create them."
62,Use UNLOGGED Tables for Intermediate Data
62,"When you modify data in PostgreSQL, the changes are written to the write ahead log (WAL). The WAL is used to maintain integrity, to fast forward the database during recovery and to maintain replication."
62,"Writing to the WAL is often needed, but there are some circumstances where you might be willing to give up some of its uses to make things a bit faster. One example is intermediate tables."
62,"Intermediate tables are disposable tables that stores temporary data used to implement some process. For example, a very common pattern in ETL processes is to load data from a CSV file to an intermediate table, clean the data, and then load it to the target table. In this use-case, the intermediate table is disposable and there is no use for it in backups or replicas."
62,UNLOGGED table
62,"Intermediate tables that don't need to be restored in case of disaster, and are not needed in replicas, can be set as UNLOGGED:"
62,CREATE UNLOGGED TABLE staging_table ( /* table definition */ );
62,BEWARE: Before using UNLOGGED make sure you understand its full implications.
62,Implement Complete Processes Using WITH and RETURNING
62,"Say you have a users table, and you find that you have some duplicates in the table:"
62,Table setup
62,CREATE TABLE users (
62,"id SERIAL PRIMARY KEY,"
62,email TEXT UNIQUE
62,CREATE TABLE orders (
62,"id SERIAL PRIMARY KEY,"
62,"user_id INT,"
62,CONSTRAINT orders_user_fk
62,FOREIGN KEY (user_id)
62,REFERENCES USERS(id)
62,INSERT INTO users (email) VALUES
62,"('foo@bar.baz'),"
62,"('me@hakibenita.com'),"
62,('ME@hakibenita.com');
62,INSERT INTO orders (user_id) VALUES
62,"(1),"
62,"(1),"
62,"(2),"
62,"(3),"
62,(3);
62,"db=# SELECT u.id, u.email, o.id as order_id"
62,FROM orders o JOIN users u ON o.user_id = u.id;
62,id |
62,email
62,| order_id
62,----+-------------------+----------
62,1 | foo@bar.baz
62,1 | foo@bar.baz
62,2 | me@hakibenita.com |
62,3 | ME@hakibenita.com |
62,3 | ME@hakibenita.com |
62,"The user haki benita registered twice, once with the email ME@hakibenita.com and again with me@hakibenita.com. Because we didn't normalize the emails when we inserted them into the table, we now have to deal with duplication."
62,"To consolidate the duplicate users, we want to:"
62,Identify duplicate users by lower case email
62,Update orders to reference one of the duplicate users
62,Remove the duplicate users from the users table
62,One way to consolidate duplicate users is to use an intermediate table:
62,db=# CREATE UNLOGGED TABLE duplicate_users AS
62,db-#
62,SELECT
62,db-#
62,"lower(email) AS normalized_email,"
62,db-#
62,"min(id) AS convert_to_user,"
62,db-#
62,"array_remove(ARRAY_AGG(id), min(id)) as convert_from_users"
62,db-#
62,FROM
62,db-#
62,users
62,db-#
62,GROUP BY
62,db-#
62,normalized_email
62,db-#
62,HAVING
62,db-#
62,count(*) > 1;
62,CREATE TABLE
62,db=# SELECT * FROM duplicate_users;
62,normalized_email
62,| convert_to_user | convert_from_users
62,-------------------+-----------------+--------------------
62,me@hakibenita.com |
62,2 | {3}
62,"The intermediate table holds a mapping of duplicate users. For each user that appears more than once with the same normalized email address, we define the user with the min ID as the user we convert all duplicates to. The other users are kept in an array column, and all the references to these users will be updated."
62,"Using the intermediate table, we update references of duplicate users in the orders table:"
62,db=# UPDATE
62,db-#
62,orders o
62,db-# SET
62,db-#
62,user_id = du.convert_to_user
62,db-# FROM
62,db-#
62,duplicate_users du
62,db-# WHERE
62,db-#
62,o.user_id = ANY(du.convert_from_users);
62,UPDATE 2
62,"Now that there are no more references, we can safely delete the duplicate users from the users table:"
62,db=# DELETE FROM
62,db-#
62,users
62,db-# WHERE
62,db-#
62,id IN (
62,db(#
62,SELECT unnest(convert_from_users)
62,db(#
62,FROM duplicate_users
62,db(#
62,DELETE 1
62,"Notice that we used the function unnest to ""transpose"" the array, that is, turn each array element into a row."
62,This is the result:
62,"db=# SELECT u.id, u.email, o.id as order_id"
62,db-# FROM orders o JOIN users u ON o.user_id = u.id;
62,id |
62,email
62,| order_id
62,----+-------------------+----------
62,1 | foo@bar.baz
62,1 | foo@bar.baz
62,2 | me@hakibenita.com |
62,2 | me@hakibenita.com |
62,2 | me@hakibenita.com |
62,"Nice, all occurrences of user 3 (ME@hakibenita.com) are converted to user 2 (me@hakibenita.com)."
62,We can also verify that the duplicate users were deleted from the users table:
62,db=# SELECT * FROM users;
62,id |
62,email
62,----+-------------------
62,1 | foo@bar.baz
62,2 | me@hakibenita.com
62,Now we can get rid of the intermediate table:
62,db=# DROP TABLE duplicate_users;
62,DROP TABLE
62,"This is fine, but very long and needs cleaning up! Is there a better way?"
62,Using Common Table Expressions (CTE)
62,"Using Common Table Expressions, also known as the WITH clause, we can perform the entire process with just one SQL statement:"
62,WITH duplicate_users AS (
62,SELECT
62,"min(id) AS convert_to_user,"
62,"array_remove(ARRAY_AGG(id), min(id)) as convert_from_users"
62,FROM
62,users
62,GROUP BY
62,lower(email)
62,HAVING
62,count(*) > 1
62,update_orders_of_duplicate_users AS (
62,UPDATE
62,orders o
62,SET
62,user_id = du.convert_to_user
62,FROM
62,duplicate_users du
62,WHERE
62,o.user_id = ANY(du.convert_from_users)
62,DELETE FROM
62,users
62,WHERE
62,id IN (
62,SELECT
62,unnest(convert_from_users)
62,FROM
62,duplicate_users
62,"Instead of creating the intermediate table, we create a common table expression and reuse it multiple times."
62,Returning Results From CTE
62,"A nice feature of executing DML inside a WITH clause, is that you can return data from it using the RETURNING keyword. For example, let's report the number of updated and deleted rows:"
62,WITH duplicate_users AS (
62,SELECT
62,"min(id) AS convert_to_user,"
62,"array_remove(ARRAY_AGG(id), min(id)) as convert_from_users"
62,FROM
62,users
62,GROUP BY
62,lower(email)
62,HAVING
62,count(*) > 1
62,update_orders_of_duplicate_users AS (
62,UPDATE
62,orders o
62,SET
62,user_id = du.convert_to_user
62,FROM
62,duplicate_users du
62,WHERE
62,o.user_id = ANY(du.convert_from_users)
62,RETURNING o.id
62,delete_duplicate_user AS (
62,DELETE FROM
62,users
62,WHERE
62,id IN (
62,SELECT unnest(convert_from_users)
62,FROM duplicate_users
62,RETURNING id
62,SELECT
62,"(SELECT count(*) FROM update_orders_of_duplicate_users) AS orders_updated,"
62,(SELECT count(*) FROM delete_duplicate_user) AS users_deleted
62,This is the result:
62,orders_updated | users_deleted
62,----------------+---------------
62,2 |
62,"The main appeal of this approach is that the entire process is executed in a single command, so no need to manage a transaction or worry about cleaning up the intermediate table if the process fails."
62,CAUTION: A reader on Reddit pointed me to a possibly unpredictable behavior of executing DML in common table expressions:
62,"The sub-statements in WITH are executed concurrently with each other and with the main query. Therefore, when using data-modifying statements in WITH, the order in which the specified updates actually happen is unpredictable"
62,"This means you cannot rely on the order in which independent sub-statements are executed. It seems that when there is a dependency between sub-statements, like in the example above, you can rely on a dependent sub-statement to execute before it is being used."
62,Avoid Indexes on Columns With Low Selectivity
62,"Say you have a registration process where users sign up with an email address. To activate the account, they have to verify their email. Your table can look like this:"
62,db=# CREATE TABLE users (
62,db-#
62,"id serial,"
62,db-#
62,"username text,"
62,db-#
62,activated boolean
62,db-#);
62,CREATE TABLE
62,"Most of your users are good citizens, they sign up with a valid email and immediately activate the account. Let's populate the table with user data, where roughly 90% of the users are activated:"
62,"db=# INSERT INTO users (username, activated)"
62,db-# SELECT
62,db-#
62,"md5(random()::text) AS username,"
62,db-#
62,random() < 0.9 AS activated
62,db-# FROM
62,db-#
62,"generate_series(1, 1000000);"
62,INSERT 0 1000000
62,"db=# SELECT activated, count(*) FROM users GROUP BY activated;"
62,activated | count
62,-----------+--------
62,| 102567
62,| 897433
62,db=# VACUUM ANALYZE users;
62,VACUUM
62,"To query for activated and unactivated users, you might be tempted to create an index on the column activated:"
62,db=# CREATE INDEX users_activated_ix ON users(activated);
62,CREATE INDEX
62,"When you try to query unactivated users, the database is using the index:"
62,db=# EXPLAIN SELECT * FROM users WHERE NOT activated;
62,QUERY PLAN
62,--------------------------------------------------------------------------------------
62,Bitmap Heap Scan on users
62,(cost=1923.32..11282.99 rows=102567 width=38)
62,Filter: (NOT activated)
62,Bitmap Index Scan on users_activated_ix
62,(cost=0.00..1897.68 rows=102567 width=0)
62,Index Cond: (activated = false)
62,"The database estimated that the filter will result in 102,567 which are roughly 10% of the table. This is consistent with the data we populated, so the database has a good sense of the data."
62,"However, when you try to query for activated users you find that the database decided not to use the index:"
62,db=# EXPLAIN SELECT * FROM users WHERE activated;
62,QUERY PLAN
62,---------------------------------------------------------------
62,Seq Scan on users
62,(cost=0.00..18334.00 rows=897433 width=38)
62,Filter: activated
62,"Many developers are often baffled when they the database is not using an index. One way of explaining why an index is not always the best choice is this: if you had to read the entire table, would you use the index?"
62,"The answer is probably no, because why would you? Reading from disk is expensive and you want to read as little as possible. If for example, a table is 10MB and the index is 1MB, to read the entire table you would have to read 10MB from disk. To read the table using the index you would have to read 11MB from disk. This is wasteful."
62,"With this understanding, let's have a look at the statistics PostgreSQL gather on the table:"
62,"db=# SELECT attname, n_distinct, most_common_vals, most_common_freqs"
62,db-# FROM pg_stats
62,db-# WHERE tablename = 'users' AND attname='activated';
62,------------------+------------------------
62,attname
62,| activated
62,n_distinct
62,| 2
62,most_common_vals
62,"| {t,f}"
62,"most_common_freqs | {0.89743334,0.10256667}"
62,"When PostgreSQL analyzed the table it found that the column activated has two distinct values. The value t in the most_common_vals column corresponds to the frequency 0.89743334 in the column most_common_freqs, and the value f corresponds to the frequency 0.10256667. This means that after analyzing the table, the database estimates that 89.74% of the table are activated users, and the rest 10.26% are unactivated users."
62,"With these stats, PostgreSQL decided it's best to scan the entire table if it expects 90% of the rows to satisfy the condition. The threshold after which the database may decide to use or not to use the index depends on many factors, and there is no rule of thumb you can use."
62,Index for a column with low selectivity vs. high selectivity
62,Use Partial Indexes
62,"In the previous section we created an index on a boolean column where ~90% of the of the values were true (activated user). When we tried to query for active users, the database did not use the index. However, when we queried unactivated users the database did use the index."
62,"This brings us to the next question.... if the database is not going to use the index to filter active users, why should we index them in the first place?"
62,Before we answer this question let's look at how much the full index on the activated column weighs:
62,db=# \di+ users_activated_ix
62,Schema |
62,Name
62,| Type
62,| Owner | Table | Size
62,--------+--------------------+-------+-------+-------+------
62,public | users_activated_ix | index | haki
62,| users | 21 MB
62,"The index is 21MB. Just for reference, the users table is 65MB. This means the index weighs ~32% the size of the table. We also know that ~90% of the index is likely not going to be used."
62,"In PostgreSQL, there is a way to create an index on only a part of the table, using whats called a partial index:"
62,db=# CREATE INDEX users_unactivated_partial_ix ON users(id)
62,db-# WHERE not activated;
62,CREATE INDEX
62,"Using a WHERE clause, we restrict the rows indexed by the index. Let's first make sure it works:"
62,db=# EXPLAIN SELECT * FROM users WHERE not activated;
62,QUERY PLAN
62,------------------------------------------------------------------------------------------------
62,Index Scan using users_unactivated_partial_ix on users
62,(cost=0.29..3493.60 rows=102567 width=38)
62,"Amazing, the database was smart enough to understand that the predicate we used in the query can be satisfied by the partial index."
62,There is another benefit to using partial indexes:
62,db=# \di+ users_unactivated_partial_ix
62,List of relations
62,Schema |
62,Name
62,| Type
62,| Owner | Table |
62,Size
62,--------+------------------------------+-------+-------+-------+---------
62,public | users_unactivated_partial_ix | index | haki
62,| users | 2216 kB
62,"The partial index weighs only 2.2MB. The full index on the column weighed 21MB. The partial index is exactly 10% the size of the full index, which matches the ratio of inactive users in the table."
62,Always Load Sorted Data
62,This is one of the things I comment most about in code reviews. It's not as intuitive as the other tips and it can have a huge impact on performance.
62,Say you have a large sales fact table:
62,"db=# CREATE TABLE sale_fact (id serial, username text, sold_at date);"
62,CREATE TABLE
62,"Every night, during some ETL process, you load data into the table:"
62,"db=# INSERT INTO sale_fact (username, sold_at)"
62,db-# SELECT
62,db-#
62,"md5(random()::text) AS username,"
62,db-#
62,'2020-01-01'::date + (interval '1 day') * round(random() * 365 * 2) AS sold_at
62,db-# FROM
62,db-#
62,"generate_series(1, 100000);"
62,INSERT 0 100000
62,db=# VACUUM ANALYZE sale_fact;
62,VACUUM
62,"To fake a loading process we used random data. We inserted 100K rows with random username, and sale dates from 2020-01-01 to two years forward."
62,The table is used mostly to produce summary sales reports. Most reports filter by date to get the sales at a specific period. To speed up range scans you create an index on sold_at:
62,db=# CREATE INDEX sale_fact_sold_at_ix ON sale_fact(sold_at);
62,CREATE INDEX
62,Let's look at the execution plan of a query to fetch all sales made in June 2020:
62,db=# EXPLAIN (ANALYZE)
62,db-# SELECT *
62,db-# FROM sale_fact
62,db-# WHERE sold_at BETWEEN '2020-07-01' AND '2020-07-31';
62,QUERY PLAN
62,-----------------------------------------------------------------------------------------------
62,Bitmap Heap Scan on sale_fact
62,(cost=108.30..1107.69 rows=4293 width=41)
62,Recheck Cond: ((sold_at >= '2020-07-01'::date) AND (sold_at <= '2020-07-31'::date))
62,Heap Blocks: exact=927
62,Bitmap Index Scan on sale_fact_sold_at_ix
62,(cost=0.00..107.22 rows=4293 width=0)
62,Index Cond: ((sold_at >= '2020-07-01'::date) AND (sold_at <= '2020-07-31'::date))
62,Planning Time: 0.191 ms
62,Execution Time: 5.906 ms
62,"After executing the query several times to warm up the cache, the timing settled at ~6ms."
62,Bitmap Scan
62,"Looking at the execution plan, we can see that the database used a bitmap scan. A bitmap scan works in two stages:"
62,Bitmap Index Scan: Go through the entire index sale_fact_sold_at_ix and map all the table pages that contain relevant rows.
62,"Bitmap Heap Scan: Read the pages that contain relevant rows, and find the rows inside these pages that satisfy the condition."
62,"Pages can contain multiple rows. The first step uses the index to find pages. The second step check for rows inside these pages, hence the ""Recheck Cond"" operation in the execution plan."
62,"At this point many DBAs and developers will call it a day and move on to the next query. BUT, there's a way to make this query better."
62,Index Scan
62,"To make things better, we'll make a small change in how we load the data."
62,db=# TRUNCATE sale_fact;
62,TRUNCATE TABLE
62,"db=# INSERT INTO sale_fact (username, sold_at)"
62,db-# SELECT
62,db-#
62,"md5(random()::text) AS username,"
62,db-#
62,'2020-01-01'::date + (interval '1 day') * round(random() * 365 * 2) AS sold_at
62,db-# FROM
62,db-#
62,"generate_series(1, 100000)"
62,db-# ORDER BY sold_at;
62,INSERT 0 100000
62,db=# VACUUM ANALYZE sale_fact;
62,VACUUM
62,"This time, we loaded the data sorted by the sold_at."
62,Let's see what the execution plan for the exact same query looks like now:
62,db=# EXPLAIN (ANALYZE)
62,db-# SELECT *
62,db-# FROM sale_fact
62,db-# WHERE sold_at BETWEEN '2020-07-01' AND '2020-07-31';
62,QUERY PLAN
62,---------------------------------------------------------------------------------------------
62,Index Scan using sale_fact_sold_at_ix on sale_fact (cost=0.29..184.73 rows=4272 width=41)
62,Index Cond: ((sold_at >= '2020-07-01'::date) AND (sold_at <= '2020-07-31'::date))
62,Planning Time: 0.145 ms
62,Execution Time: 2.294 ms
62,"After running the query several times we get a stable timing at round 2.3ms. Compared to the previous query that took ~6ms, we get a consistent saving of ~60%."
62,"Another thing we can see right away, is that the database did not use a bitmap scan this time, but a ""regular"" index scan. Why is that?"
62,Correlation
62,When the database is analyzing a table it collects all sort of statistics. One of those statistics is correlation:
62,"Statistical correlation between physical row ordering and logical ordering of the column values. This ranges from -1 to +1. When the value is near -1 or +1, an index scan on the column will be estimated to be cheaper than when it is near zero, due to reduction of random access to the disk."
62,"As the official documentation explains, the correlation measures how ""sorted"" values of a specific column are on disk."
62,correlation = 1
62,"When the correlation is 1, or close to 1, it means the pages in the tables are stored on disk in roughly the same order as the rows in the table. This is actually very common. For example, auto incrementing ID's will usually have a correlation close to 1. Date and timestamp columns that keeps track of when rows were created will also usually have a correlation close to 1."
62,"When the correlation is -1, the pages of the table are sorted in reverse order relative to the column."
62,correlation ~ 0
62,"When the correlation is close to 0, it mean the values in the column have no or very little correlation to how the pages of the table are stored."
62,"Going back to our sale_fact table, when we loaded the data into the table without sorting it first, these were the correlations:"
62,"db=# SELECT tablename, attname, correlation"
62,db-# FROM pg_stats
62,db=# WHERE tablename = 'sale_fact';
62,tablename | attname
62,| correlation
62,-----------+----------+--------------
62,sale
62,| id
62,sale
62,| username | -0.005344716
62,sale
62,| sold_at
62,| -0.011389783
62,The auto generated column id has a correlation of 1. The sold_at column has a very low correlation: consecutive values are scattered across the entire table.
62,"When we loaded sorted data into the table, these were the correlations calculated by the database:"
62,tablename | attname
62,correlation
62,-----------+----------+----------------
62,sale_fact | id
62,sale_fact | username | -0.00041992788
62,sale_fact | sold_at
62,The correlation for sold_at is now 1.
62,"So why did the database use a bitmap scan when the correlation was low, and an index scan when the correlation was close to 1?"
62,"When the correlation was 1, the database estimated that rows in the requested range are likely to be in consecutive pages. In this case, an index scan is likely to read very few pages."
62,"When the correlation was close to 0, the database estimated that rows in the requested range are likely to be scattered across the entire table. In this case, it makes sense to use a bitmap scan to map the table pages in which rows exist, and only then fetch them and apply the condition."
62,"The next time you load data into a table, think about how the data is going to be queried, and make sure you sort it in a way that indexes used for range scan can benefit from."
62,CLUSTER Command
62,"Another way of ""sorting a table on disk"" by a specific index is to use the CLUSTER command."
62,For example:
62,db=# TRUNCATE sale_fact;
62,TRUNCATE TABLE
62,-- Insert rows without sorting
62,"db=# INSERT INTO sale_fact (username, sold_at)"
62,db-# SELECT
62,db-#
62,"md5(random()::text) AS username,"
62,db-#
62,'2020-01-01'::date + (interval '1 day') * round(random() * 365 * 2) AS sold_at
62,db-# FROM
62,db-#
62,"generate_series(1, 100000)"
62,INSERT 0 100000
62,db=# ANALYZE sale_fact;
62,ANALYZE
62,"db=# SELECT tablename, attname, correlation"
62,db-# FROM pg_stats
62,db-# WHERE tablename = 'sale_fact';
62,tablename | attname
62,correlation
62,-----------+-----------+----------------
62,sale_fact | sold_at
62,| -5.9702674e-05
62,sale_fact | id
62,sale_fact | username
62,0.010033822
62,We loaded data into the table in random order and as a result the correlation of sold_at is close to zero.
62,"To ""rearrange"" the table by sold_at, we used the CLUSTER command to sort the table on disk according to the index sale_fact_sold_at_ix:"
62,db=# CLUSTER sale_fact USING sale_fact_sold_at_ix;
62,CLUSTER
62,db=# ANALYZE sale_fact;
62,ANALYZE
62,"db=# SELECT tablename, attname, correlation"
62,db-# FROM pg_stats
62,db-# WHERE tablename = 'sale_fact';
62,tablename | attname
62,| correlation
62,-----------+----------+--------------
62,sale_fact | sold_at
62,sale_fact | id
62,| -0.002239401
62,sale_fact | username |
62,0.013389298
62,After the table was clustered we can see that the correlation for sold_at is 1.
62,CLUSTER command
62,Some things to note about the CLUSTER command:
62,Clustering the table by a specific column may affect the correlation of other column. See for example the correlation of the column id after we clustered the table by sold_at.
62,"CLUSTER is a heavy, blocking operation, so make sure you don't execute it on a live table."
62,For these two reason it's best to insert the data sorted and not rely on CLUSTER.
62,Index Columns With High Correlation Using BRIN
62,"When talking about indexes, most developers will think about B-Tree indexes. But, PostgreSQL provides other types of indexes such as BRIN:"
62,BRIN is designed for handling very large tables in which certain columns have some natural correlation with their physical location within the table
62,"BRIN stands for Block Range Index. According to the documentation, a BRIN index works best for columns with high correlation. As we've already seen in previous sections, some fields such as auto incrementing IDs and timestamps are naturally correlated with the physical structure of the table, hence they are good candidates for a BRIN index."
62,"Under some circumstances, a BRIN index can provide a better ""value for money"" in terms of size and performance compared to a similar B-Tree index."
62,BRIN Index
62,"A BRIN index works by keeping the range of values within a number of adjacent pages in the table. Say we have these values in a column, each is single table page:"
62,"1, 2, 3, 4, 5, 6, 7, 8, 9"
62,"A BRIN index works on ranges of adjacent pages in the table. If the number of adjacent pages is set to 3, the index will divide the table into the following ranges:"
62,"[1,2,3], [4,5,6], [7,8,9]"
62,"For each range, the BRIN index keeps the minimum and maximum value:"
62,"[1–3], [4–6], [7–9]"
62,"Using the index above, try to search for the value 5:"
62,[1–3] - Definitely not here
62,[4–6] - Might be here
62,[7–9] - Definitely not here
62,Using the BRIN index we managed to limit our search to blocks 4–6.
62,"Let's take another example, this time the values in the column will have a correlation close to zero, meaning they are not sorted:"
62,"[2,9,5], [1,4,7], [3,8,6]"
62,Indexing 3 adjacent blocks produces the following ranges:
62,"[2–9], [1–7], [3–8]"
62,Let's try to search for the value 5:
62,[2–9] - Might be here
62,[1–7] - Might be here
62,[3–8] - Might be here
62,"In this case the index is not limiting the search at all, hence it is useless."
62,Understanding pages_per_range
62,The number of adjacent pages is determined by the parameter pages_per_range. The number of pages per range effects the size and accuracy of the BRIN index:
62,A large pages_per_range will produce a small and less accurate index
62,A small pages_per_range will produce a bigger and more accurate index
62,The default pages_per_range is 128.
62,BRIN index with lower `pages_per_range`
62,"To demonstrate, let's create a BRIN index on ranges of 2 adjacent pages and search for the value 5:"
62,[1–2] - Definitely not here
62,[3–4] - Definitely not here
62,[5–6] - Might be here
62,[7–8] - Definitely not here
62,[9] - Definitely not here
62,"Using the index with 2 pages per range we were able to limit the search to blocks 5 and 6. When the range was 3 pages, the index limited the search to blocks 4,5 and 6."
62,Another difference between the two indexes is that when the range was 3 we only had to keep 3 ranges. When the range was 2 we had to keep 5 ranges so the index was bigger.
62,Creating a BRIN Index
62,"Using the sales_fact from before, let's create a BRIN index on the column sold_at:"
62,db=# CREATE INDEX sale_fact_sold_at_bix ON sale_fact
62,db-# USING BRIN(sold_at) WITH (pages_per_range = 128);
62,CREATE INDEX
62,This creates a BRIN index with the default pages_per_range = 128.
62,Let's try to query for a range of sale dates:
62,db=# EXPLAIN (ANALYZE)
62,db-# SELECT *
62,db-# FROM sale_fact
62,db-# WHERE sold_at BETWEEN '2020-07-01' AND '2020-07-31';
62,QUERY PLAN
62,--------------------------------------------------------------------------------------------
62,Bitmap Heap Scan on sale_fact
62,(cost=13.11..1135.61 rows=4319 width=41)
62,Recheck Cond: ((sold_at >= '2020-07-01'::date) AND (sold_at <= '2020-07-31'::date))
62,Rows Removed by Index Recheck: 23130
62,Heap Blocks: lossy=256
62,Bitmap Index Scan on sale_fact_sold_at_bix
62,(cost=0.00..12.03 rows=12500 width=0)
62,Index Cond: ((sold_at >= '2020-07-01'::date) AND (sold_at <= '2020-07-31'::date))
62,Execution Time: 8.877 ms
62,"The database used our BRIN index to get a range of sale dates, but that's not the interesting part..."
62,Optimizing pages_per_range
62,"According to the execution plan, the database removed 23,130 rows from the pages it found using the index. This may indicate that the range we set for the index it too large for this particular query. Let's try to create an index with less pages per range:"
62,db=# CREATE INDEX sale_fact_sold_at_bix64 ON sale_fact
62,db-# USING BRIN(sold_at) WITH (pages_per_range = 64);
62,CREATE INDEX
62,db=# EXPLAIN (ANALYZE)
62,db- SELECT *
62,db- FROM sale_fact
62,db- WHERE sold_at BETWEEN '2020-07-01' AND '2020-07-31';
62,QUERY PLAN
62,---------------------------------------------------------------------------------------------
62,Bitmap Heap Scan on sale_fact
62,(cost=13.10..1048.10 rows=4319 width=41)
62,Recheck Cond: ((sold_at >= '2020-07-01'::date) AND (sold_at <= '2020-07-31'::date))
62,Rows Removed by Index Recheck: 9434
62,Heap Blocks: lossy=128
62,Bitmap Index Scan on sale_fact_sold_at_bix64
62,(cost=0.00..12.02 rows=6667 width=0)
62,Index Cond: ((sold_at >= '2020-07-01'::date) AND (sold_at <= '2020-07-31'::date))
62,Execution Time: 5.491 ms
62,"With 64 pages per range the database removed less rows from the pages it found using the the index, only 9,434 were removed compared with 23,130 when the the range was 128 pages. This means the database had to do less IO and the query was slightly faster, ~5.5ms compared to ~8.9ms."
62,Testing the index with different values for pages_per_range produced the following results:
62,pages_per_range
62,Rows Removed by Index Recheck
62,128
62,"23,130"
62,"9,434"
62,874
62,446
62,446
62,"We can see that as we decrease pages_per_range, the index is more accurate and less rows are removed from the pages found using the index."
62,"Note that we optimized the query for a very specific query. This is fine for demonstration purposes, but in real life it's best to use values that meet the needs of most queries."
62,Evaluating Index Size
62,Another big selling point for BRIN indexes is their size. In previous sections we created a B-Tree index on the sold_at field. The size of the index was 2224kB. The size a BRIN index with pages_per_range=128 is only 48kb. That's 46 times smaller than the B-Tree index.
62,Schema |
62,Name
62,| Type
62,| Owner |
62,Table
62,| Size
62,--------+-----------------------+-------+-------+-----------+-------
62,public | sale_fact_sold_at_bix | index | haki
62,| sale_fact | 48 kB
62,public | sale_fact_sold_at_ix
62,| index | haki
62,| sale_fact | 2224 kB
62,"The size of a BRIN index is also affected by pages_per_range. For example, a BRIN index with pages_per_range=2 weighs 56kb, which is only slightly bigger than 48kb."
62,"Make Indexes ""Invisible"""
62,"PostgreSQL has a nice feature called transactional DDL. After years of using Oracle, I got used to DDL commands such as CREATE, DROP and ALTER ending a transaction. However, in PostgreSQL you can perform DDL commands inside a transaction, and changes will take effect only when the transaction is committed."
62,"As I recently discovered, using transactional DDL you can make indexes invisible! This comes in handy when you want to see what an execution plan looks like without some index."
62,"For example, in the sale_fact table from the previous section we created an index on sold_at. The execution plan for fetching sales made in July looked like this:"
62,db=# EXPLAIN
62,db-# SELECT *
62,db-# FROM sale_fact
62,db-# WHERE sold_at BETWEEN '2020-07-01' AND '2020-07-31';
62,QUERY PLAN
62,--------------------------------------------------------------------------------------------
62,Index Scan using sale_fact_sold_at_ix on sale_fact
62,(cost=0.42..182.80 rows=4319 width=41)
62,Index Cond: ((sold_at >= '2020-07-01'::date) AND (sold_at <= '2020-07-31'::date))P
62,"To see what the execution plan would be if the index sale_fact_sold_at_ix did not exist, we can drop the index inside a transaction and immediately rollback:"
62,db=# BEGIN;
62,BEGIN
62,db=# DROP INDEX sale_fact_sold_at_ix;
62,DROP INDEX
62,db=# EXPLAIN
62,db-# SELECT *
62,db-# FROM sale_fact
62,db-# WHERE sold_at BETWEEN '2020-07-01' AND '2020-07-31';
62,QUERY PLAN
62,---------------------------------------------------------------------------------
62,Seq Scan on sale_fact
62,(cost=0.00..2435.00 rows=4319 width=41)
62,Filter: ((sold_at >= '2020-07-01'::date) AND (sold_at <= '2020-07-31'::date))
62,db=# ROLLBACK;
62,ROLLBACK
62,"We first start a transaction using BEGIN. Then we drop the index and generate an execution plan. Notice that the execution plan now uses a full table scan, as if the index does not exist. At this point the transaction is still in progress, so the index is not dropped yet. To finish the transaction without dropping the index we rollback the transaction using the ROLLBACK command."
62,"Now, make sure the index still exists:"
62,db=# \di+ sale_fact_sold_at_ix
62,List of relations
62,Schema |
62,Name
62,| Type
62,| Owner |
62,Table
62,Size
62,--------+----------------------+-------+-------+-----------+---------
62,public | sale_fact_sold_at_ix | index | haki
62,| sale_fact | 2224 kB
62,"Other database that don't support transactional DDL provide other ways to achieve the same goal. For example, Oracle let's you mark an index as invisible, which will cause the optimizer to ignore it."
62,"CAUTION: Dropping an index inside a transaction will lock out concurrent selects, inserts, updates, and deletes on the table while the transaction is active. Use with caution in test environments, and avoid on production databases."
62,Don't Schedule Long Running Processes at Round Hours
62,"It's a known fact among investors that weird things can happen when a stock's price reaches a nice round number such as 10$, 100$, 1000$. As the following article explains:"
62,"[...] asset's price may have a difficult time moving beyond a round number, such as $50 or $100 per share. Most inexperienced traders tend to buy or sell assets when the price is at a whole number because they are more likely to feel that a stock is fairly valued at such levels."
62,"Developers in this sense are not all that different than the investors. When they need to schedule a long running process, they will usually schedule it at a round hour."
62,Typical load on a system during the night
62,"This tendency to schedule tasks at round hours can cause some unusual loads during these times. So, if you need to schedule some long running process, you have a better chance of finding a system at rest if you schedule at another time."
62,"Another good idea is to apply a random delay to the task's schedule, so it doesn't run at the same time every time. This way, even if another task is scheduled to run at the same time, it won't be a big problem. If you use systemd timer units to schedule your tasks, you can use the RandomizedDelaySec option for this."
62,Conclusion
62,"This article covers some trivial and non-trivial tips from my own experience. Some of these tips are easy to implement, and some require a deeper understanding of how the database works. Databases are the backbone of most modern systems, so taking some time to understand how they work is a good investment for any developer!"
62,This article was reviewed by the great team at pgMustard
62,Want me to send you an email
62,when I publish something new?
62,Share to show you care
62,Email
62,Twitter
62,Facebook
62,Reddit
62,Share
62,Similar articles
62,11 January 2021
62,"PostgreSQL,"
62,"Performance,"
62,SQL
62,Re-Introducing Hash Indexes in PostgreSQL
62,The Ugly Duckling of index types
62,20 October 2020
62,"SQL,"
62,"Performance,"
62,PostgreSQL
62,The Surprising Impact of Medium-Size Texts on PostgreSQL Performance
62,Why TOAST is the best thing since sliced bread
62,21 November 2019
62,"PostgreSQL,"
62,"SQL,"
62,Performance
62,12 Common Mistakes and Missed Optimization Opportunities in SQL
62,Made by Developers and Non-Developers
62,22 December 2018
62,"PostgreSQL,"
62,"SQL,"
62,Performance
62,How We Solved a Storage Problem in PostgreSQL Without Adding a Single Byte of Storage
62,A short story about a storage-heavy query and the silver bullet that solved the issue
62,17 September 2018
62,"PostgreSQL,"
62,"SQL,"
62,Performance
62,Be Careful With CTE in PostgreSQL
62,How to avoid common pitfalls with common table expressions in PostgreSQL
62,© Haki Benita 2016-2021
62,Credits
62,Appreciate
63,PostgresqlCO.NF: PostgreSQL configuration for humansWe're sorry but conf-ui doesn't work properly without JavaScript enabled. Please enable it to continue.Close
64,Apache Flink 1.12 Documentation: JDBC SQL Connector
64,v1.12
64,Home
64,Try Flink
64,Local Installation
64,Fraud Detection with the DataStream API
64,Real Time Reporting with the Table API
64,Flink Operations Playground
64,Learn Flink
64,Overview
64,Intro to the DataStream API
64,Data Pipelines & ETL
64,Streaming Analytics
64,Event-driven Applications
64,Fault Tolerance
64,Concepts
64,Overview
64,Stateful Stream Processing
64,Timely Stream Processing
64,Flink Architecture
64,Glossary
64,Application Development
64,DataStream API
64,Overview
64,Execution Mode (Batch/Streaming)
64,Event Time
64,Overview
64,Generating Watermarks
64,Builtin Watermark Generators
64,State & Fault Tolerance
64,Overview
64,Working with State
64,The Broadcast State Pattern
64,Checkpointing
64,Queryable State
64,State Backends
64,State Schema Evolution
64,Custom State Serialization
64,User-Defined Functions
64,Operators
64,Overview
64,Windows
64,Joining
64,Process Function
64,Async I/O
64,Data Sources
64,Side Outputs
64,Handling Application Parameters
64,Testing
64,Experimental Features
64,Scala API Extensions
64,Java Lambda Expressions
64,Project Configuration
64,DataSet API
64,Overview
64,Transformations
64,Iterations
64,Zipping Elements
64,Hadoop Compatibility
64,Local Execution
64,Cluster Execution
64,Batch Examples
64,Table API & SQL
64,Overview
64,Concepts & Common API
64,Streaming Concepts
64,Overview
64,Dynamic Tables
64,Time Attributes
64,Versioned Tables
64,Joins in Continuous Queries
64,Detecting Patterns
64,Query Configuration
64,Legacy Features
64,Data Types
64,Table API
64,SQL
64,Overview
64,Queries
64,CREATE Statements
64,DROP Statements
64,ALTER Statements
64,INSERT Statement
64,SQL Hints
64,DESCRIBE Statements
64,EXPLAIN Statements
64,USE Statements
64,SHOW Statements
64,Functions
64,Overview
64,System (Built-in) Functions
64,User-defined Functions
64,Modules
64,Catalogs
64,SQL Client
64,Configuration
64,Performance Tuning
64,Streaming Aggregation
64,User-defined Sources & Sinks
64,Python API
64,Overview
64,Installation
64,Table API Tutorial
64,DataStream API Tutorial
64,Table API User's Guide
64,Intro to the Python Table API
64,TableEnvironment
64,Operations
64,Data Types
64,System (Built-in) Functions
64,User Defined Functions
64,General User-defined Functions
64,Vectorized User-defined Functions
64,Conversions between PyFlink Table and Pandas DataFrame
64,Dependency Management
64,SQL
64,Catalogs
64,Metrics
64,Connectors
64,DataStream API User's Guide
64,Data Types
64,Operators
64,Dependency Management
64,Configuration
64,Environment Variables
64,FAQ
64,Data Types & Serialization
64,Overview
64,Custom Serializers
64,Managing Execution
64,Execution Configuration
64,Program Packaging
64,Parallel Execution
64,Execution Plans
64,Task Failure Recovery
64,API Migration Guides
64,Libraries
64,Event Processing (CEP)
64,State Processor API
64,Graphs: Gelly
64,Overview
64,Graph API
64,Iterative Graph Processing
64,Library Methods
64,Graph Algorithms
64,Graph Generators
64,Bipartite Graph
64,Connectors
64,DataStream Connectors
64,Overview
64,Fault Tolerance Guarantees
64,Kafka
64,Cassandra
64,Kinesis
64,Elasticsearch
64,File Sink
64,Streaming File Sink
64,RabbitMQ
64,NiFi
64,Google Cloud PubSub
64,Twitter
64,JDBC
64,Table & SQL Connectors
64,Overview
64,Formats
64,Overview
64,CSV
64,JSON
64,Confluent Avro
64,Avro
64,Debezium
64,Canal
64,Maxwell
64,Parquet
64,Orc
64,Raw
64,Kafka
64,Upsert Kafka
64,Kinesis
64,JDBC
64,Elasticsearch
64,FileSystem
64,HBase
64,DataGen
64,Print
64,BlackHole
64,Hive
64,Overview
64,Hive Catalog
64,Hive Dialect
64,Hive Read & Write
64,Hive Functions
64,Download
64,DataSet Connectors
64,Deployment
64,Overview
64,Resource Providers
64,Standalone
64,Overview
64,Docker
64,Kubernetes
64,Native Kubernetes
64,YARN
64,Mesos
64,Configuration
64,Memory Configuration
64,Set up Flink's Process Memory
64,Set up TaskManager Memory
64,Set up JobManager Memory
64,Memory tuning guide
64,Troubleshooting
64,Migration Guide
64,Command-Line Interface
64,File Systems
64,Overview
64,Common Configurations
64,Amazon S3
64,Aliyun OSS
64,Azure Blob Storage
64,Plugins
64,High Availability (HA)
64,Overview
64,ZooKeeper HA Services
64,Kubernetes HA Services
64,Metric Reporters
64,Security
64,SSL Setup
64,Kerberos
64,REPLs
64,Python REPL
64,Scala REPL
64,Advanced
64,External Resources
64,History Server
64,Logging
64,Operations
64,State & Fault Tolerance
64,Checkpoints
64,Savepoints
64,State Backends
64,Tuning Checkpoints and Large State
64,Metrics
64,REST API
64,Debugging
64,Debugging Windows & Event Time
64,Debugging Classloading
64,Application Profiling & Debugging
64,Monitoring
64,Monitoring Checkpointing
64,Monitoring Back Pressure
64,Upgrading Applications and Flink Versions
64,Production Readiness Checklist
64,Flink Development
64,Importing Flink into an IDE
64,Building Flink from Source
64,Internals
64,Jobs and Scheduling
64,Task Lifecycle
64,File Systems
64,Javadocs
64,Scaladocs
64,Pythondocs
64,Project Page
64,Pick Docs Version
64,v1.11
64,v1.10
64,v1.9
64,v1.8
64,v1.7
64,v1.6
64,v1.5
64,v1.4
64,v1.3
64,v1.2
64,v1.1
64,v1.0
64,中文版
64,Connectors
64,Table & SQL Connectors
64,JDBC
64,JDBC SQL Connector
64,Scan Source: Bounded
64,Lookup Source: Sync Mode
64,Sink: Batch
64,Sink: Streaming Append & Upsert Mode
64,Dependencies
64,How to create a JDBC table
64,Connector Options
64,Features
64,Key handling
64,Partitioned Scan
64,Lookup Cache
64,Idempotent Writes
64,Postgres Database as a Catalog
64,Data Type Mapping
64,The JDBC connector allows for reading data from and writing data into any relational databases with a JDBC driver. This document describes how to setup the JDBC connector to run SQL queries against relational databases.
64,"The JDBC sink operate in upsert mode for exchange UPDATE/DELETE messages with the external system if a primary key is defined on the DDL, otherwise, it operates in append mode and doesn’t support to consume UPDATE/DELETE messages."
64,Dependencies
64,In order to use the JDBC connector the following
64,dependencies are required for both projects using a build automation tool (such as Maven or SBT)
64,and SQL Client with SQL JAR bundles.
64,Maven dependency
64,SQL Client JAR
64,<dependency>
64,<groupId>org.apache.flink</groupId>
64,<artifactId>flink-connector-jdbc_2.11</artifactId>
64,<version>1.12.0</version>
64,</dependency>
64,Download
64,A driver dependency is also required to connect to a specified database. Here are drivers currently supported:
64,Driver
64,Group Id
64,Artifact Id
64,JAR
64,MySQL
64,mysql
64,mysql-connector-java
64,Download
64,PostgreSQL
64,org.postgresql
64,postgresql
64,Download
64,Derby
64,org.apache.derby
64,derby
64,Download
64,JDBC connector and drivers are not currently part of Flink’s binary distribution. See how to link with them for cluster execution here.
64,How to create a JDBC table
64,The JDBC table can be defined as following:
64,-- register a MySQL table 'users' in Flink SQL
64,CREATE TABLE MyUserTable (
64,"id BIGINT,"
64,"name STRING,"
64,"age INT,"
64,"status BOOLEAN,"
64,PRIMARY KEY (id) NOT ENFORCED
64,) WITH (
64,"'connector' = 'jdbc',"
64,"'url' = 'jdbc:mysql://localhost:3306/mydatabase',"
64,'table-name' = 'users'
64,"-- write data into the JDBC table from the other table ""T"""
64,INSERT INTO MyUserTable
64,"SELECT id, name, age, status FROM T;"
64,-- scan data from the JDBC table
64,"SELECT id, name, age, status FROM MyUserTable;"
64,-- temporal join the JDBC table as a dimension table
64,SELECT * FROM myTopic
64,LEFT JOIN MyUserTable FOR SYSTEM_TIME AS OF myTopic.proctime
64,ON myTopic.key = MyUserTable.id;
64,Connector Options
64,Option
64,Required
64,Default
64,Type
64,Description
64,connector
64,required
64,(none)
64,String
64,"Specify what connector to use, here should be 'jdbc'."
64,url
64,required
64,(none)
64,String
64,The JDBC database url.
64,table-name
64,required
64,(none)
64,String
64,The name of JDBC table to connect.
64,driver
64,optional
64,(none)
64,String
64,"The class name of the JDBC driver to use to connect to this URL, if not set, it will automatically be derived from the URL."
64,username
64,optional
64,(none)
64,String
64,The JDBC user name. 'username' and 'password' must both be specified if any of them is specified.
64,password
64,optional
64,(none)
64,String
64,The JDBC password.
64,scan.partition.column
64,optional
64,(none)
64,String
64,The column name used for partitioning the input. See the following Partitioned Scan section for more details.
64,scan.partition.num
64,optional
64,(none)
64,Integer
64,The number of partitions.
64,scan.partition.lower-bound
64,optional
64,(none)
64,Integer
64,The smallest value of the first partition.
64,scan.partition.upper-bound
64,optional
64,(none)
64,Integer
64,The largest value of the last partition.
64,scan.fetch-size
64,optional
64,Integer
64,"The number of rows that should be fetched from the database when reading per round trip. If the value specified is zero, then the hint is ignored."
64,scan.auto-commit
64,optional
64,true
64,Boolean
64,"Sets the auto-commit flag on the JDBC driver,"
64,"which determines whether each statement is committed in a transaction automatically. Some JDBC drivers, specifically"
64,"Postgres, may require this to be set to false in order to stream results."
64,lookup.cache.max-rows
64,optional
64,(none)
64,Integer
64,"The max number of rows of lookup cache, over this value, the oldest rows will be expired."
64,Lookup cache is disabled by default. See the following Lookup Cache section for more details.
64,lookup.cache.ttl
64,optional
64,(none)
64,Duration
64,"The max time to live for each rows in lookup cache, over this time, the oldest rows will be expired."
64,Lookup cache is disabled by default. See the following Lookup Cache section for more details.
64,lookup.max-retries
64,optional
64,Integer
64,The max retry times if lookup database failed.
64,sink.buffer-flush.max-rows
64,optional
64,100
64,Integer
64,The max size of buffered records before flush. Can be set to zero to disable it.
64,sink.buffer-flush.interval
64,optional
64,Duration
64,"The flush interval mills, over this time, asynchronous threads will flush data. Can be set to '0' to disable it. Note, 'sink.buffer-flush.max-rows' can be set to '0' with the flush interval set allowing for complete async processing of buffered actions."
64,sink.max-retries
64,optional
64,Integer
64,The max retry times if writing records to database failed.
64,Features
64,Key handling
64,"Flink uses the primary key that defined in DDL when writing data to external databases. The connector operate in upsert mode if the primary key was defined, otherwise, the connector operate in append mode."
64,"In upsert mode, Flink will insert a new row or update the existing row according to the primary key, Flink can ensure the idempotence in this way. To guarantee the output result is as expected, it’s recommended to define primary key for the table and make sure the primary key is one of the unique key sets or primary key of the underlying database table. In append mode, Flink will interpret all records as INSERT messages, the INSERT operation may fail if a primary key or unique constraint violation happens in the underlying database."
64,See CREATE TABLE DDL for more details about PRIMARY KEY syntax.
64,Partitioned Scan
64,"To accelerate reading data in parallel Source task instances, Flink provides partitioned scan feature for JDBC table."
64,All the following scan partition options must all be specified if any of them is specified. They describe how to partition the table when reading in parallel from multiple tasks.
64,"The scan.partition.column must be a numeric, date, or timestamp column from the table in question. Notice that scan.partition.lower-bound and scan.partition.upper-bound are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned."
64,scan.partition.column: The column name used for partitioning the input.
64,scan.partition.num: The number of partitions.
64,scan.partition.lower-bound: The smallest value of the first partition.
64,scan.partition.upper-bound: The largest value of the last partition.
64,Lookup Cache
64,"JDBC connector can be used in temporal join as a lookup source (aka. dimension table). Currently, only sync lookup mode is supported."
64,"By default, lookup cache is not enabled. You can enable it by setting both lookup.cache.max-rows and lookup.cache.ttl."
64,"The lookup cache is used to improve performance of temporal join the JDBC connector. By default, lookup cache is not enabled, so all the requests are sent to external database."
64,"When lookup cache is enabled, each process (i.e. TaskManager) will hold a cache. Flink will lookup the cache first, and only send requests to external database when cache missing, and update cache with the rows returned."
64,The oldest rows in cache will be expired when the cache hit to the max cached rows lookup.cache.max-rows or when the row exceeds the max time to live lookup.cache.ttl.
64,"The cached rows might not be the latest, users can tune lookup.cache.ttl to a smaller value to have a better fresh data, but this may increase the number of requests send to database. So this is a balance between throughput and correctness."
64,Idempotent Writes
64,"JDBC sink will use upsert semantics rather than plain INSERT statements if primary key is defined in DDL. Upsert semantics refer to atomically adding a new row or updating the existing row if there is a unique constraint violation in the underlying database, which provides idempotence."
64,"If there are failures, the Flink job will recover and re-process from last successful checkpoint, which can lead to re-processing messages during recovery. The upsert mode is highly recommended as it helps avoid constraint violations or duplicate data if records need to be re-processed."
64,"Aside from failure recovery, the source topic may also naturally contain multiple records over time with the same primary key, making upserts desirable."
64,"As there is no standard syntax for upsert, the following table describes the database-specific DML that is used."
64,Database
64,Upsert Grammar
64,MySQL
64,INSERT .. ON DUPLICATE KEY UPDATE ..
64,PostgreSQL
64,INSERT .. ON CONFLICT .. DO UPDATE SET ..
64,Postgres Database as a Catalog
64,The JdbcCatalog enables users to connect Flink to relational databases over JDBC protocol.
64,"Currently, PostgresCatalog is the only implementation of JDBC Catalog at the moment, PostgresCatalog only supports limited Catalog methods include:"
64,// The supported methods by Postgres Catalog.
64,PostgresCatalog.databaseExists(String databaseName)
64,PostgresCatalog.listDatabases()
64,PostgresCatalog.getDatabase(String databaseName)
64,PostgresCatalog.listTables(String databaseName)
64,PostgresCatalog.getTable(ObjectPath tablePath)
64,PostgresCatalog.tableExists(ObjectPath tablePath)
64,Other Catalog methods is unsupported now.
64,Usage of PostgresCatalog
64,Please refer to Dependencies section for how to setup a JDBC connector and Postgres driver.
64,Postgres catalog supports the following options:
64,"name: required, name of the catalog."
64,"default-database: required, default database to connect to."
64,"username: required, username of Postgres account."
64,"password: required, password of the account."
64,"base-url: required, should be of format ""jdbc:postgresql://<ip>:<port>"", and should not contain database name here."
64,CREATE CATALOG mypg WITH(
64,"'type' = 'jdbc',"
64,"'default-database' = '...',"
64,"'username' = '...',"
64,"'password' = '...',"
64,'base-url' = '...'
64,USE CATALOG mypg;
64,EnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build();
64,TableEnvironment tableEnv = TableEnvironment.create(settings);
64,String name
64,"= ""mypg"";"
64,"String defaultDatabase = ""mydb"";"
64,String username
64,"= ""..."";"
64,String password
64,"= ""..."";"
64,String baseUrl
64,"= ""..."""
64,"JdbcCatalog catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl);"
64,"tableEnv.registerCatalog(""mypg"", catalog);"
64,// set the JdbcCatalog as the current catalog of the session
64,"tableEnv.useCatalog(""mypg"");"
64,val settings = EnvironmentSettings.newInstance().inStreamingMode().build()
64,val tableEnv = TableEnvironment.create(settings)
64,val name
64,"= ""mypg"""
64,"val defaultDatabase = ""mydb"""
64,val username
64,"= ""..."""
64,val password
64,"= ""..."""
64,val baseUrl
64,"= ""..."""
64,"val catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl)"
64,"tableEnv.registerCatalog(""mypg"", catalog)"
64,// set the JdbcCatalog as the current catalog of the session
64,"tableEnv.useCatalog(""mypg"")"
64,from pyflink.table.catalog import JdbcCatalog
64,environment_settings = EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build()
64,t_env = StreamTableEnvironment.create(environment_settings=environment_settings)
64,"name = ""mypg"""
64,"default_database = ""mydb"""
64,"username = ""..."""
64,"password = ""..."""
64,"base_url = ""..."""
64,"catalog = JdbcCatalog(name, default_database, username, password, base_url)"
64,"t_env.register_catalog(""mypg"", catalog)"
64,# set the JdbcCatalog as the current catalog of the session
64,"t_env.use_catalog(""mypg"")"
64,execution:
64,planner: blink
64,...
64,current-catalog: mypg
64,# set the JdbcCatalog as the current catalog of the session
64,current-database: mydb
64,catalogs:
64,- name: mypg
64,type: jdbc
64,default-database: mydb
64,username: ...
64,password: ...
64,base-url: ...
64,PostgresSQL Metaspace Mapping
64,"PostgresSQL has an additional namespace as schema besides database. A Postgres instance can have multiple databases, each database can have multiple schemas with a default one named “public”, each schema can have multiple tables."
64,"In Flink, when querying tables registered by Postgres catalog, users can use either schema_name.table_name or just table_name. The schema_name is optional and defaults to “public”."
64,Therefor the metaspace mapping between Flink Catalog and Postgres is as following:
64,Flink Catalog Metaspace Structure
64,Postgres Metaspace Structure
64,catalog name (defined in Flink only)
64,N/A
64,database name
64,database name
64,table name
64,[schema_name.]table_name
64,"The full path of Postgres table in Flink should be ""<catalog>.<db>.`<schema.table>`"" if schema is specified, note the <schema.table> should be escaped."
64,Here are some examples to access Postgres tables:
64,"-- scan table 'test_table' of 'public' schema (i.e. the default schema), the schema name can be omitted"
64,SELECT * FROM mypg.mydb.test_table;
64,SELECT * FROM mydb.test_table;
64,SELECT * FROM test_table;
64,"-- scan table 'test_table2' of 'custom_schema' schema,"
64,-- the custom schema can not be omitted and must be escaped with table.
64,SELECT * FROM mypg.mydb.`custom_schema.test_table2`
64,SELECT * FROM mydb.`custom_schema.test_table2`;
64,SELECT * FROM `custom_schema.test_table2`;
64,Data Type Mapping
64,"Flink supports connect to several databases which uses dialect like MySQL, PostgresSQL, Derby. The Derby dialect usually used for testing purpose. The field data type mappings from relational databases data types to Flink SQL data types are listed in the following table, the mapping table can help define JDBC table in Flink easily."
64,MySQL type
64,PostgreSQL type
64,Flink SQL type
64,TINYINT
64,TINYINT
64,SMALLINT
64,TINYINT UNSIGNED
64,SMALLINT
64,INT2
64,SMALLSERIAL
64,SERIAL2
64,SMALLINT
64,INT
64,MEDIUMINT
64,SMALLINT UNSIGNED
64,INTEGER
64,SERIAL
64,INT
64,BIGINT
64,INT UNSIGNED
64,BIGINT
64,BIGSERIAL
64,BIGINT
64,BIGINT UNSIGNED
64,"DECIMAL(20, 0)"
64,BIGINT
64,BIGINT
64,BIGINT
64,FLOAT
64,REAL
64,FLOAT4
64,FLOAT
64,DOUBLE
64,DOUBLE PRECISION
64,FLOAT8
64,DOUBLE PRECISION
64,DOUBLE
64,"NUMERIC(p, s)"
64,"DECIMAL(p, s)"
64,"NUMERIC(p, s)"
64,"DECIMAL(p, s)"
64,"DECIMAL(p, s)"
64,BOOLEAN
64,TINYINT(1)
64,BOOLEAN
64,BOOLEAN
64,DATE
64,DATE
64,DATE
64,TIME [(p)]
64,TIME [(p)] [WITHOUT TIMEZONE]
64,TIME [(p)] [WITHOUT TIMEZONE]
64,DATETIME [(p)]
64,TIMESTAMP [(p)] [WITHOUT TIMEZONE]
64,TIMESTAMP [(p)] [WITHOUT TIMEZONE]
64,CHAR(n)
64,VARCHAR(n)
64,TEXT
64,CHAR(n)
64,CHARACTER(n)
64,VARCHAR(n)
64,CHARACTER VARYING(n)
64,TEXT
64,STRING
64,BINARY
64,VARBINARY
64,BLOB
64,BYTEA
64,BYTES
64,ARRAY
64,ARRAY
64,Back to top
64,Want to contribute translation?
65,Parallel hints - Developer Guide| Alibaba Cloud Documentation Center
65,Document Center
65,PolarDB-O Cloud Native Database
65,Release notes
65,Product Introduction
65,Overview
65,Architecture
65,PolarDB-O terms
65,Limits
65,Benefits
65,Pricing and Purchase
65,Billable items
65,Billing methods
65,Overview
65,Purchase procedures
65,Purchase a storage plan
65,Change the billing method from subscription to pay-as-you-go
65,Switch the billing method from pay-as-you-go to subscription
65,Instructions for purchase
65,Manage storage plans
65,View the deducted capacity of a storage plan
65,Renew or upgrade a storage plan
65,Expiration or overdue payments
65,Renew subscription clusters
65,Manual renewal
65,Auto-renewal
65,View bills
65,Billing rules of PolarDB for MySQL
65,Storage pricing
65,Billing rules of data backups that exceed the free quot
65,Billing rules of SQL Explorer (optional)
65,Configuration change fees
65,FAQ
65,FAQ
65,Quick Start
65,Migrate or Synchronize Data
65,Overview
65,Migrate data from a self-managed Oracle database to a PolarDB-O cluster
65,Configure one-way data synchronization between PolarDB-O clusters
65,Management Guide
65,Overview
65,Comparison with Oracle on database management
65,Pending events
65,Configure a whitelist for a cluster
65,Connect to PolarDB
65,View or apply for an endpoint
65,Modify or delete an endpoint
65,Connect to a database cluster
65,Cluster endpoint
65,PolarProxy features
65,PolarDB-O consistency levels
65,Read/write splitting
65,Transaction splitting
65,Create a custom cluster endpoint
65,Modify a cluster endpoint
65,Delete a custom cluster endpoint
65,Private domain names
65,Cluster management
65,Create a PolarDB-O cluster
65,Storage plan
65,Change the specifications of a cluster
65,Add or remove a read-only node
65,Set a maintenance window
65,Restart nodes
65,Upgrade the minor version
65,Release a cluster
65,Deploy a cluster across zones and change the primary zone
65,Perform a switchover
65,Account management
65,Account overview
65,Register and log on to an Alibaba Cloud account
65,Create and authorize a RAM user
65,Manage database accounts
65,Create database accounts
65,DBLink
65,Overview
65,Create a database link from PolarDB-O to PolarDB-O
65,Create a database link from PolarDB-O to Oracle
65,Use a database link to query data across databases
65,Delete a database link
65,Databases
65,Backup and restoration
65,Back up data
65,Restore data
65,Cluster recycle bin
65,FAQ about the backup feature
65,View the database storage usage
65,Data Security and Encryption
65,Configure SSL encryption
65,Configure TDE
65,Diagnostics and optimization
65,Performance monitoring
65,Performance insight
65,Manage alert rules
65,SQL Explorer
65,Configuration parameters
65,polar_comp_redwood_date
65,polar_comp_redwood_raw_names
65,polar_comp_redwood_strings
65,polar_comp_stmt_level_tx
65,polar_create_table_with_full_replica_identity
65,Custom parameters
65,Configure cluster parameters
65,Clone a cluster
65,SQL firewall
65,Developer Guide
65,Oracle compatibility
65,Connect to a PolarDB-O cluster
65,Clients and Drivers
65,Download clients and drivers
65,Use client tools
65,JDBC
65,OCI
65,ODBC
65,.NET
65,Use PHP to connect to a PolarDB cluster compatible with Oracle
65,Basic operations
65,Create a user
65,Create a database
65,Create a schema
65,Create a table
65,Delete a table
65,Create a view
65,Create a materialized view
65,Create an index
65,Create and use a sequence
65,Create and use a synonym
65,Data types
65,Data types
65,Numeric type
65,Character type
65,Date and time type
65,Boolean type
65,Binary data
65,XML type
65,VARCHAR2
65,Implicit conversion rules
65,Operators
65,Arithmetic operators
65,Logical operators
65,Comparison operators
65,Partition tables
65,Overview
65,Interval range partitioning
65,Interval range partitioning
65,Switch between range partitioning and interval range partitioning
65,Subpartitioning
65,Introduction
65,Subpartition templates
65,Partition Pruning
65,Partition pruning
65,Example - partition pruning
65,Handle stray values in a list- or range-partitioned table
65,Specify multiple partition key columns in a RANGE partitioned table
65,Command list for partitioned tables
65,Views for partitioned tables
65,External tables
65,Read and write foreign data files by using oss_fdw
65,Temporary tables
65,Global temporary tables
65,Built-in packages
65,Overview
65,DBMS_ALERT
65,DBMS_APPLICATION_INFO
65,DBMS_AQ
65,DBMS_AQADM
65,DBMS_CRYPTO
65,DBMS_LOB
65,DBMS_LOCK
65,DBMS_MVIEW
65,DBMS_OBFUSCATION_TOOLKIT
65,DBMS_OUTPUT
65,DBMS_PIPE
65,DBMS_PROFILER
65,DBMS_RANDOM
65,DBMS_RLS
65,DBMS_SESSION
65,DBMS_SQL
65,DBMS_STATS
65,DBMS_UTILITY
65,UTL_ENCODE
65,UTL_RAW
65,UTL_URL
65,Built-in functions
65,Mathematical functions
65,Date/Time functions and operators
65,Overview
65,MONTHS_BETWEEN
65,NEW_TIME
65,ADD_MONTHS
65,CURRENT DATE/TIME
65,TRUNC
65,ROUND
65,EXTRACT
65,NEXT_DAY
65,TZ_OFFSET
65,String functions
65,Pattern matching string functions
65,Overview
65,REGEXP_COUNT
65,REGEXP_SUBSTR
65,REGEXP_INSTR
65,Use the LIKE operator for pattern matching
65,Functions for formatting data types
65,Sequence manipulation functions
65,Conditional expressions
65,TO_SINGLE_BYTE and TO_MULTI_BYTE functions
65,Aggregate functions
65,Analytic function RATIO_TO_REPORT
65,Subquery expressions
65,ANYDATA
65,GROUP_ID
65,dblink_ora
65,Overview of dblink_ora
65,dblink_ora functions and procedures
65,Call dblink_ora functions
65,LNNVL function
65,FROM_TZ function
65,SYS_CONTEXT
65,Oracle catalog views
65,ALL_ALL_TABLES
65,ALL_CONS_COLUMNS
65,ALL_CONSTRAINTS
65,ALL_DB_LINKS
65,ALL_DIRECTORIES
65,ALL_IND_COLUMNS
65,ALL_INDEXES
65,ALL_JOBS
65,ALL_OBJECTS
65,ALL_PART_KEY_COLUMNS
65,ALL_PART_TABLES
65,ALL_QUEUES
65,ALL_QUEUE_TABLES
65,ALL_SEQUENCES
65,ALL_SOURCE
65,ALL_SUBPART_KEY_COLUMNS
65,ALL_SYNONYMS
65,ALL_TAB_COLS
65,ALL_TAB_COLUMNS
65,ALL_TAB_PARTITIONS
65,ALL_TAB_SUBPARTITIONS
65,ALL_TABLES
65,ALL_TRIGGERS
65,ALL_TYPES
65,ALL_USERS
65,ALL_VIEW_COLUMNS
65,ALL_VIEWS
65,DBA_ALL_TABLES
65,DBA_CONS_COLUMNS
65,DBA_CONSTRAINTS
65,DBA_DB_LINKS
65,DBA_DIRECTORIES
65,DBA_IND_COLUMNS
65,DBA_INDEXES
65,DBA_JOBS
65,DBA_OBJECTS
65,DBA_PART_KEY_COLUMNS
65,DBA_PART_TABLES
65,DBA_PROFILES
65,DBA_QUEUES
65,DBA_QUEUE_TABLES
65,DBA_ROLE_PRIVS
65,DBA_ROLES
65,DBA_SEQUENCES
65,DBA_SOURCE
65,DBA_SUBPART_KEY_COLUMNS
65,DBA_SYNONYMS
65,DBA_TAB_COLS
65,DBA_TAB_COLUMNS
65,DBA_TAB_PARTITIONS
65,DBA_TAB_SUBPARTITIONS
65,DBA_TABLES
65,DBA_TRIGGERS
65,DBA_TYPES
65,DBA_USERS
65,DBA_VIEW_COLUMNS
65,DBA_VIEWS
65,PRODUCT_COMPONENT_VERSION
65,USER_ALL_TABLES
65,USER_CONS_COLUMNS
65,USER_CONSTRAINTS
65,USER_DB_LINKS
65,USER_IND_COLUMNS
65,USER_INDEXES
65,USER_JOBS
65,USER_OBJECTS
65,USER_PART_KEY_COLUMNS
65,USER_PART_TABLES
65,USER_QUEUES
65,USER_QUEUE_TABLES
65,USER_ROLE_PRIVS
65,USER_SEQUENCES
65,USER_SOURCE
65,USER_SUBPART_KEY_COLUMNS
65,USER_SYNONYMS
65,USER_TAB_COLS
65,USER_TAB_COLUMNS
65,USER_TAB_PARTITIONS
65,USER_TAB_SUBPARTITIONS
65,USER_TABLES
65,USER_TRIGGERS
65,USER_TYPES
65,USER_USERS
65,USER_VIEW_COLUMNS
65,USER_VIEWS
65,V$VERSION
65,Views of I/O statistics and I/O latency distribution
65,Triggers
65,Overview of triggers
65,Types of triggers
65,Create a trigger
65,Trigger variables
65,Transactions and exceptions
65,Trigger examples
65,BEFORE statement-level trigger
65,AFTER statement-level trigger
65,BEFORE row-level trigger
65,AFTER row-level trigger
65,Packages
65,Package components
65,Package components
65,Package specification syntax
65,Package body syntax
65,Create a package
65,Create a package specification
65,Create a package body
65,Use a package
65,Use packages with user-defined types
65,Delete a package
65,Stored Procedure Language
65,Overview
65,Basic SPL elements
65,Character sets
65,Case sensitivity
65,Qualifiers
65,Constants
65,User-defined PL/SQL subtypes
65,Identifiers
65,SPL programs
65,Overview
65,SPL block structures
65,Anonymous blocks
65,Call a function
65,Call a procedure
65,Delete a procedure
65,Create a function
65,Delete a function
65,Create a procedure
65,Compilation errors in procedures and functions
65,Procedure and function parameters
65,Overview
65,Positional and named parameter notation
65,Parameter modes
65,Use default values in parameters
65,Subprograms - subprocedures and subfunctions
65,Overview
65,Create a subprocedure
65,Create a subfunction
65,Block relationships
65,Invoke subprograms
65,Use forward declarations
65,Overload subprograms
65,Access subprogram variables
65,Program security
65,EXECUTE privileges
65,Name resolution of database objects
65,Database object privileges
65,Rights of definers and invokers
65,Security examples
65,Variable declarations
65,Use %TYPE in variable declarations
65,Use %ROWTYPE in record declarations
65,User-defined record types and record variables
65,Declare a variable
65,Basic statements
65,NULL
65,Assignment
65,SELECT INTO
65,INSERT
65,UPDATE
65,DELETE
65,Use the RETURNING INTO clause
65,Obtain the result status
65,Control structures
65,RETURN statement
65,GOTO statement
65,CASE expression
65,CASE statement
65,Loops
65,Exception handling
65,User-defined exceptions
65,RAISE_APPLICATION_ERROR
65,PRAGMA EXCEPTION_INIT
65,IF statements
65,IF-THEN
65,IF-THEN-ELSE
65,IF-THEN-ELSE IF
65,IF-THEN-ELSIF-ELSE
65,Transaction control
65,Overview
65,COMMIT
65,ROLLBACK
65,PRAGMA AUTONOMOUS_TRANSACTION
65,Dynamic SQL
65,Static cursors
65,Overview
65,Declare a cursor
65,Open a cursor
65,Fetch rows from a cursor
65,Close a cursor
65,Use %ROWTYPE with cursors
65,Cursor attributes
65,Parameterized cursors
65,Cursor FOR loop
65,REF CURSOR and cursor variable
65,REF CURSOR overview
65,Declare a cursor variable
65,Open a cursor variable
65,Fetch rows from a cursor variable
65,Close a cursor variable
65,Usage limits
65,Examples
65,Dynamic queries with REF CURSORs
65,Collections
65,Overview
65,Associative arrays
65,Nested tables
65,Varrays
65,Collection methods
65,COUNT
65,DELETE
65,EXISTS
65,EXTEND
65,FIRST
65,LAST
65,LIMIT
65,NEXT
65,PRIOR
65,TRIM
65,Work with collections
65,TABLE()
65,Use the MULTISET UNION operator
65,Use the FORALL statement
65,Use the BULK COLLECT clause
65,Errors and messages
65,Object types and objects
65,Basic object concepts
65,Object type components
65,Create an object type
65,Create an object instance
65,Reference an object
65,Delete an object type
65,Plugins
65,Supported extensions
65,Index Advisor
65,Use the UTL_I18N plug-in
65,PG_CRON
65,polar_utility
65,Use the pldebugger plug-in
65,Use the pg_roaringbitmap plug-in
65,SQL Commands
65,Overview
65,ALTER INDEX
65,ALTER PROCEDURE
65,ALTER PROFILE
65,ALTER QUEUE
65,ALTER QUEUE TABLE
65,ALTER ROLE
65,ALTER ROLE… IDENTIFIED BY
65,ALTER SEQUENCE
65,ALTER SESSION
65,ALTER TABLE
65,ALTER TABLESPACE
65,ALTER TABLE...ADD PARTITION
65,ALTER TABLE...ADD SUBPARTITION
65,ALTER TABLE... DROP PARTITION
65,ALTER TABLE... DROP SUBPARTITION
65,ALTER TABLE...EXCHANGE PARTITION
65,ALTER TABLE…MERGE PARTITION
65,ALTER TABLE...MERGE SUBPARTITION
65,ALTER TABLE...MOVE PARTITION
65,ALTER TABLE...RENAME PARTITION
65,ALTER TABLE...SPLIT PARTITION
65,ALTER TABLE...SPLIT SUBPARTITION
65,ALTER TABLE...TRUNCATE PARTITION
65,ALTER TABLE...TRUNCATE SUBPARTITION
65,ALTER TRIGGER
65,ALTER USER… IDENTIFIED BY
65,CALL
65,COMMENT
65,COMMIT
65,CREATE DATABASE
65,CREATE FUNCTION
65,CREATE INDEX
65,CREATE MATERIALIZED VIEW
65,CREATE PACKAGE
65,CREATE PACKAGE BODY
65,CREATE PROCEDURE
65,CREATE QUEUE
65,CREATE QUEUE TABLE
65,CREATE ROLE
65,CREATE SCHEMA
65,CREATE SEQUENCE
65,CREATE SYNONYM
65,CREATE TABLE
65,CREATE TABLE AS
65,CREATE TABLE... PARTITION BY
65,CREATE TABLE ... PARTITION BY HASH
65,CREATE TRIGGER
65,CREATE TYPE
65,CREATE TYPE BODY
65,CREATE VIEW
65,DELETE
65,DROP FUNCTION
65,DROP INDEX
65,DROP PACKAGE
65,DROP PROCEDURE
65,DROP PROFILE
65,DROP QUEUE
65,DROP QUEUE TABLE
65,DROP SYNONYM
65,DROP SEQUENCE
65,DROP TABLE
65,DROP TRIGGER
65,DROP TYPE
65,DROP USER
65,DROP VIEW
65,EXEC
65,GRANT
65,GRANT on database objects
65,INSERT
65,LOCK
65,Merge Into
65,PIVOT
65,REVOKE
65,ROLLBACK
65,ROLLBACK TO SAVEPOINT
65,SAVEPOINT
65,SELECT
65,SELECT
65,SELECT list
65,CONNECT BY clause
65,DISTINCT clause
65,FOR UPDATE clause
65,FROM clause
65,GROUP BY clause
65,HAVING clause
65,INTERSECT clause
65,MINUS clause
65,ORDER BY clause
65,UNION clause
65,WHERE clause
65,Query a specified partition
65,SET CONSTRAINTS
65,SET ROLE
65,SET TRANSACTION
65,TRUNCATE
65,TRUNCATE TABLE
65,UPDATE
65,SQL tutorial
65,Get started
65,Overview
65,Create a table
65,Use rows to populate a table
65,Query a table
65,Joins between tables
65,Aggregate functions
65,Updates
65,Deletions
65,Sample database
65,Install a sample database
65,Advanced concepts
65,Views
65,Foreign keys
65,Pseudo column ROWNUM
65,Synonyms
65,Hierarchical queries
65,Define parent-child relationships
65,Select root nodes
65,Organization tree in the sample application
65,Node level
65,Order siblings
65,Use CONNECT_BY_ROOT to retrieve a root node
65,Use SYS_CONNECT_BY_PATH to retrieve a path
65,Overview
65,Optimizer hints
65,Overview
65,Default optimization mode
65,Access Method Hints
65,Specify a join order
65,Joining Relations Hint
65,Global hints
65,Use the APPEND optimizer hint
65,Parallel hints
65,Conflicting hints
65,Profiles
65,Overview
65,Create a new profile
65,Alter a profile
65,Drop a profile
65,Back up profile management functions
65,Multidimensional analysis
65,Overview
65,ROLLUP extensions
65,CUBE extensions
65,GROUPING SETS extension
65,GROUPING function
65,GROUPING_ID function
65,Performance Tuning Guide
65,Architecture of PolarDB-O
65,Main components
65,Internal architecture
65,File storage
65,Heap table structure
65,Query mechanism
65,Garbage collection mechanism
65,Locate performance issues
65,View statistics
65,Use the performance insight feature
65,Optimize cluster performance
65,Parameters
65,Specify parameters
65,Optimize SQL statements
65,Manage indexes
65,B-tree indexes
65,Hash indexes
65,GIN indexes
65,GiST index
65,BRIN indexes
65,Execution plan analysis
65,EXPLAIN syntax
65,Output of EXPLAIN statements
65,Physical optimization
65,Cost-related concepts
65,Cost calculation
65,Best practices for performance optimization
65,High CPU usage
65,High I/O
65,Network issues
65,Optimize SQL statements
65,Spatio-temporal Database
65,Overview
65,Models
65,Grid model
65,Vector pyramid
65,Path model
65,Trajectory model
65,Point cloud model
65,Raster model
65,Geometry model
65,Advanced usage
65,Enable spatio-temporal parallel query
65,Enable GPU-accelerated computing
65,Spatial-temporal object storage optimization
65,Feature signature-based storage optimization for large spatio-temporal objects
65,ST_SetTypeStorage
65,Raster SQL reference
65,Basic concepts
65,Parallel operations
65,Raster creation
65,ST_CreateRast
65,Import and export
65,ST_AsImage
65,ST_AsPNG
65,ST_AsJPEG
65,ST_ExportTo
65,ST_ImportFrom
65,Pyramid operations
65,ST_BuildPyramid
65,ST_deletePyramid
65,ST_BestPyramidLevel
65,Coordinate system conversion
65,ST_Rast2WorldCoord
65,ST_World2RastCoord
65,Pixel value operations
65,ST_ClipDimension
65,ST_AddZ
65,ST_Clip
65,ST_ClipToRast
65,ST_Values
65,ST_Update
65,S​T_MosaicFrom
65,S​T_MosaicTo
65,Overview operations
65,ST_UpdateOverview
65,ST_BuildOverview
65,S​T_EraseOverview
65,DEM operations
65,ST_Flow_direction
65,ST_Overflow
65,ST_Slope
65,ST_Hillshade
65,ST_Aspect
65,Attribute query and update
65,ST_Name
65,ST_SetName
65,ST_MetaData
65,ST_Width
65,ST_Height
65,ST_NumBands
65,ST_Value
65,ST_RasterID
65,ST_CellDepth
65,ST_CellType
65,ST_InterleavingType
65,ST_TopPyramidLevel
65,ST_Extent
65,ST_ConvexHull
65,ST_Envelope
65,ST_Srid
65,ST_SetSrid
65,ST_ScaleX
65,ST_ScaleY
65,ST_SetScale
65,ST_SkewX
65,ST_SkewY
65,ST_SetSkew
65,ST_UpperLeftX
65,ST_UpperLeftY
65,ST_SetUpperLeft
65,ST_PixelWidth
65,ST_PixelHeight
65,ST_Georeference
65,ST_IsGeoreferenced
65,ST_UnGeoreference
65,ST_SetGeoreference
65,ST_NoData
65,ST_SetNoData
65,ST_ColorTable
65,ST_SetColorTable
65,ST_Statistics
65,ST_SetStatistics
65,ST_SummaryStats
65,ST_ColorInterp
65,ST_SetColorInterp
65,ST_Histogram
65,ST_SetHistogram
65,ST_BuildHistogram
65,ST_StatsQuantile
65,ST_Quantile
65,ST_MD5Sum
65,ST_SetMD5Sum
65,ST_XMin
65,ST_YMin
65,ST_XMax
65,ST_YMax
65,ST_ChunkHeight
65,ST_ChunkWidth
65,ST_ChunkBands
65,ST_MetaItems
65,ST_SetMetaData
65,ST_BeginDateTime
65,ST_EndDateTime
65,ST_SetBeginDateTime
65,ST_SetEndDateTime
65,ST_DateTime
65,ST_SetDateTime
65,Algebra and Analysis Functions
65,ST_Reclassify
65,ST_MapAlgebra
65,Raster Image Processing
65,ST_SubRaster
65,ST_Transform
65,ST_Rescale
65,ST_Resize
65,Operators
65,Equal to operator (=)
65,Greater than operator (>)
65,Less than operator (<)
65,Greater than or equal to operator (>=)
65,Less than or equal to operator (<=)
65,Functions for Identifying Spatial Relationships
65,ST_Intersects
65,ST_Contains
65,ST_ContainsProperly
65,ST_Covers
65,ST_CoveredBy
65,ST_Disjoint
65,ST_overlaps
65,ST_Touches
65,ST_Within
65,Auxiliary functions
65,ST_AKId
65,ST_SetAccessKey
65,ST_SetAKId
65,ST_SetAKSecret
65,ST_CheckGPU
65,Variables
65,ganos.parallel.transaction
65,ganos.parallel.degree
65,ganos.raster.calculate_md5
65,ganos.raster.md5sum_chunk_size
65,ganos.raster.mosaic_must_same_nodata
65,SpatialRef SQL reference
65,ST_SrEqual
65,ST_SrReg
65,ST_SrFromEsriWkt
65,Point cloud SQL reference
65,Constructors
65,ST_makePatch
65,ST_makePoint
65,ST_Patch
65,Attribute functions
65,ST_summary
65,ST_numPoints
65,ST_asText
65,ST_get
65,ST_pcID
65,Object operations
65,ST_boundingDiagonalGeometry
65,ST_compress
65,ST_explode
65,ST_patchAvg
65,ST_envelopeGeometry
65,ST_filterGreaterThan
65,ST_filterLessThan
65,ST_filterEquals
65,ST_filterBetween
65,ST_isSorted
65,ST_patchMax
65,ST_patchMin
65,ST_patchAvg
65,ST_patchMax
65,ST_patchMin
65,ST_pointN
65,ST_sort
65,ST_range
65,ST_setPcid
65,ST_transform
65,ST_unCompress
65,ST_union
65,OGC WKB operations
65,ST_envelopeAsBinary
65,ST_asBinary
65,ST_boundingDiagonalAsBinary
65,Spatial relationship judgment
65,ST_intersects
65,Spatial processing
65,ST_intersection
65,Trajectory SQL reference
65,Terms
65,Constructors
65,Overview
65,ST_append
65,ST_makeTrajectory
65,Editing and processing functions
65,ST_attrDeduplicate
65,ST_Compress
65,ST_CompressSED
65,ST_deviation
65,ST_sort
65,Attribute metadata
65,ST_attrNullable
65,ST_attrType
65,ST_attrDefinition
65,ST_attrSize
65,ST_attrLength
65,ST_attrName
65,Event functions
65,ST_eventTypes
65,ST_addEvent
65,ST_eventTimes
65,ST_eventTime
65,ST_eventType
65,Attribute functions
65,ST_attrNullFilter
65,ST_attrIntMin
65,ST_attrIntAverage
65,ST_leafType
65,ST_trajAttrsAsInteger
65,ST_attrFloatMax
65,ST_trajAttrsAsBool
65,ST_startTime
65,ST_trajAttrsAsText
65,ST_attrFloatMin
65,ST_trajAttrs
65,ST_attrNotNullFilter
65,ST_velocityAtTime
65,ST_samplingInterval
65,ST_trajectorySpatial
65,ST_subTrajectory
65,ST_leafCount
65,ST_timeToDistance
65,ST_duration
65,ST_trajAttrsMeanMax
65,ST_accelerationAtTime
65,ST_trajectoryTemporal
65,ST_endTime
65,ST_pointAtTime
65,ST_cumulativeDistanceAtTime
65,ST_timeAtPoint
65,ST_attrFloatFilter
65,ST_subTrajectorySpatial
65,ST_timeAtDistance
65,ST_trajAttrsAsTimestamp
65,ST_trajAttrsAsDouble
65,ST_timeAtCumulativeDistance
65,ST_attrTimestampFilter
65,ST_attrIntMax
65,ST_attrFloatAverage
65,ST_attrIntFilter
65,Functions to process bounding boxes
65,ST_MakeBox
65,ST_MakeBox{Z|T|2D|2DT|3D|3DT}
65,ST_BoxndfToGeom
65,ST_Has{xy|z|t}
65,ST_{X|Y|Z|T}Min
65,ST_{X|Y|Z|T}Max
65,ST_ExpandSpatial
65,Operators to process bounding boxes
65,Basic concepts
65,INTERSECT operators
65,INCLUDE operators
65,INCLUDED operators
65,Spatial relationship judgment
65,ST_intersects
65,ST_equals
65,ST_distanceWithin
65,Spatial processing
65,ST_difference
65,ST_intersection
65,Spatial statistics
65,ST_nearestApproachDistance
65,ST_nearestApproachPoint
65,Spatio-temporal relationship judgment
65,ST_intersects
65,ST_equals
65,ST_distanceWithin
65,ST_durationWithin
65,ST_{Z|T|2D|2DT|3D|3DT}Intersects
65,ST_{2D|2DT|3D|3DT}Intersects_IndexLeft
65,ST_{2D|2DT|3D|3DT}DWithin
65,ST_{2D|2DT|3D|3DT}DWithin_IndexLeft
65,ST_{T|2D|2DT|3D|3DT}Contains
65,ST_{T|2D|2DT|3D|3DT}Within
65,Spatio-temporal processing
65,ST_intersection
65,Spatio-temporal statistics
65,ST_nearestApproachPoint
65,ST_nearestApproachDistance
65,Distance measurement
65,ST_euclideanDistance
65,ST_length
65,ST_mdistance
65,Similarity analysis
65,ST_lcsSimilarity
65,ST_lcsDistance
65,ST_lcsSubDistance
65,ST_JaccardSimilarity
65,Indexing
65,GiST indexing
65,TrajGiST indexing
65,Variables
65,ganos.trajectory.attr_string_length
65,GeomGrid SQL reference
65,Usage notes
65,Functions for output
65,ST_AsText
65,ST_AsBinary
65,ST_AsGeometry
65,ST_AsBox
65,Functions for input
65,ST_GridFromText
65,ST_GridFromBinary
65,Functions to query spatial relationships
65,ST_Intersects
65,ST_Contains
65,ST_Within
65,Operators
65,Functions to compute grids
65,ST_AsGrid
65,Geometry Pyramid SQL reference
65,Usage notes
65,Functions to build pyramids
65,ST_BuildPyramid
65,Functions to delete pyramids
65,ST_DeletePyramid
65,Functions to view pyramids
65,ST_Tile
65,ST_AsPng
65,Map services
65,Publish geometry data
65,Publish raster data
65,Overview
65,Desktop applications
65,Use OpenJump to access Ganos data
65,Use QGIS to access Ganos data
65,Use uDig to access Ganos data
65,FAQ
65,Load raster data
65,Load vector data
65,Trajectory FAQ
65,Best practice
65,Trajectory best practices
65,API Reference
65,API overview
65,Use RAM for resource authorization
65,RAM role linked to Apsara PolarDB
65,Call methods
65,Common parameters
65,Request structures
65,Signatures
65,Regions
65,DescribeRegions
65,Cluster management
65,CreateDBCluster
65,DeleteDBCluster
65,DescribeDBClusters
65,DescribeDBClusterAttribute
65,DescribeTasks
65,ModifyDBClusterMaintainTime
65,ModifyDBClusterDescription
65,Deployment architecture
65,ModifyDBClusterPrimaryZone
65,FailoverDBCluster
65,Renewal management
65,ModifyAutoRenewAttribute
65,DescribeAutoRenewAttribute
65,DescribeDBClusterAvailableResources
65,Data Security
65,Whitelist management
65,DescribeDBClusterAccessWhitelist
65,ModifyDBClusterAccessWhitelist
65,SSL encryption
65,DescribeDBClusterSSL
65,ModifyDBClusterSSL
65,Node management
65,CreateDBNodes
65,ModifyDBNodeClass
65,RestartDBNode
65,DeleteDBNodes
65,Cluster parameters
65,ModifyDBClusterParameters
65,DescribeDBClusterParameters
65,Connection points
65,CreateDBEndpointAddress
65,CreateDBClusterEndpoint
65,DescribeDBClusterEndpoints
65,ModifyDBClusterEndpoint
65,ModifyDBEndpointAddress
65,DeleteDBEndpointAddress
65,DeleteDBClusterEndpoint
65,Logs
65,DescribeDBClusterAuditLogCollector
65,ModifyDBClusterAuditLogCollector
65,Account management
65,CreateAccount
65,DescribeAccounts
65,ModifyAccountDescription
65,GrantAccountPrivilege
65,RevokeAccountPrivilege
65,DeleteAccount
65,CheckAccountName
65,ModifyAccountPassword
65,Database management
65,CreateDatabase
65,DescribeDatabases
65,DeleteDatabase
65,CheckDBName
65,DescribeDBInitializeVariable
65,DBLink
65,CreateDBLink
65,DescribeDBLinks
65,DeleteDBLink
65,Backup management
65,CreateBackup
65,DeleteBackup
65,DescribeBackups
65,DescribeBackupTasks
65,DescribeBackupLogs
65,DescribeDetachedBackups
65,DescribeDBClustersWithBackups
65,Backup policy
65,DescribeBackupPolicy
65,DescribeLogBackupPolicy
65,ModifyBackupPolicy
65,ModifyLogBackupPolicy
65,Tag management
65,TagResources
65,ListTagResources
65,UntagResources
65,Pending events
65,DescribePendingMaintenanceAction
65,DescribePendingMaintenanceActions
65,ModifyPendingMaintenanceAction
65,Monitoring management
65,DescribeDBNodePerformance
65,DescribeDBClusterMonitor
65,DescribeDBClusterPerformance
65,ModifyDBClusterMonitor
65,Appendixes
65,Client error codes
65,Cluster status
65,Character set tables
65,Performance metric monitoring
65,SDK
65,Reference
65,Download the SDK
65,FAQ
65,All Products
65,Search
65,Document Center
65,PolarDB-O Cloud Native Database
65,Developer Guide
65,SQL tutorial
65,Optimizer hints
65,Parallel hints
65,all-products-head
65,This Product
65,This Product
65,All Products
65,Parallel hints
65,Document Center
65,Parallel hints
65,"Last Updated: Jan 05, 2021"
65,The PARALLEL optimizer hint is used to force parallel scanning.The NO_PARALLEL optimizer
65,hint prevents usage of a parallel scan.
65,SynopsisPARALLEL (table [ parallel_degree | DEFAULT ])
65,NO_PARALLEL (table)
65,Description
65,Parallel scanning allows multiple background workers to simultaneously scan a table
65,"in a specified query. Compared with other methods such as a sequential scan, this"
65,scan provides improved performance.
65,Parameters
65,Parameter
65,Description
65,table
65,The table in which a parallel hint is used.
65,parallel_degree | DEFAULT
65,The value of the parallel_degree parameter is a positive integer that specifies the
65,"desired number of workers to be used in a parallel scan. If this parameter is set,"
65,the smaller value between this parameter and the configuration parameter max_parallel_workers_per_gather
65,is used as the planned number of workers. For more information about the max_parallel_workers_per_gather
65,"parameter, visit max_parallel_workers_per_gather."
65,"If DEFAULT is set, the maximum possible parallel degree is used."
65,"If both parallel_degree and DEFAULT are omitted, the query optimizer determines the"
65,"parallel degree. In this case, if the table parameter has been set with the parallel_workers"
65,"storage parameter, the value of parallel_workers is used as the parallel degree. Otherwise,"
65,the optimizer uses the maximum possible parallel degree specified by DEFAULT. For
65,"more information about the parallel_workers storage parameter, visit parallel_workers."
65,"Regardless of the circumstance, the parallel degree never exceeds the value of max_parallel_workers_per_gather."
65,Examples
65,The following configuration parameter settings are valid:SHOW max_worker_processes;
65,max_worker_processes
65,----------------------
65,(1 row)
65,SHOW max_parallel_workers_per_gather;
65,max_parallel_workers_per_gather
65,---------------------------------
65,(1 row)The following example shows the default scan on the pgbench_accounts table. A sequential
65,scan is shown in the query plan.
65,SET trace_hints TO on;
65,EXPLAIN SELECT * FROM pgbench_accounts;
65,QUERY PLAN
65,---------------------------------------------------------------------------
65,Seq Scan on pgbench_accounts
65,(cost=0.00..53746.15 rows=2014215 width=97)
65,"(1 row)The following example uses the PARALLEL hint. In the query plan, the Gather node that"
65,launches the background workers specifies that two workers are planned to be used.
65,"Note If trace_hints is set to on, the INFO: [HINTS] lines are displayed to indicate that"
65,PARALLEL has been supported by pgbench_accounts and other hints. For the remaining
65,"examples, these lines are not displayed. These examples show the same output, where"
65,trace_hints is reset to off.
65,EXPLAIN SELECT /*+ PARALLEL(pgbench_accounts) */ * FROM pgbench_accounts;
65,INFO:
65,[HINTS] SeqScan of [pgbench_accounts] rejected due to PARALLEL hint.
65,INFO:
65,[HINTS] PARALLEL on [pgbench_accounts] accepted.
65,INFO:
65,[HINTS] Index Scan of [pgbench_accounts].[pgbench_accounts_pkey] rejected due to PARALLEL hint.
65,QUERY PLAN
65,-----------------------------------------------------------------------------------------
65,Gather
65,(cost=1000.00..244418.06 rows=2014215 width=97)
65,Workers Planned: 2
65,Parallel Seq Scan on pgbench_accounts
65,(cost=0.00..41996.56 rows=839256 width=97)
65,(3 rows)The following example shows an increased value of max_parallel_workers_per_gather:SET max_parallel_workers_per_gather TO 6;
65,SHOW max_parallel_workers_per_gather;
65,max_parallel_workers_per_gather
65,---------------------------------
65,(1 row)The same query on pgbench_accounts is used again with no specified parallel degree
65,in the PARALLEL hint. The number of planned workers has been determined by the optimizer
65,and increased to 4.
65,EXPLAIN SELECT /*+ PARALLEL(pgbench_accounts) */ * FROM pgbench_accounts;
65,QUERY PLAN
65,-----------------------------------------------------------------------------------------
65,Gather
65,(cost=1000.00..241061.04 rows=2014215 width=97)
65,Workers Planned: 4
65,Parallel Seq Scan on pgbench_accounts
65,(cost=0.00..38639.54 rows=503554 width=97)
65,(3 rows)A value of 6 is specified for the parallel degree parameter of the PARALLEL hint.
65,The value is returned as the planned number of workers in the following example:
65,EXPLAIN SELECT /*+ PARALLEL(pgbench_accounts 6) */ * FROM pgbench_accounts;
65,QUERY PLAN
65,-----------------------------------------------------------------------------------------
65,Gather
65,(cost=1000.00..239382.52 rows=2014215 width=97)
65,Workers Planned: 6
65,Parallel Seq Scan on pgbench_accounts
65,(cost=0.00..36961.03 rows=335702 width=97)
65,(3 rows)The same query is used with the DEFAULT setting for the parallel degree. The results
65,indicate that the maximum allowable number of workers is planned.
65,EXPLAIN SELECT /*+ PARALLEL(pgbench_accounts DEFAULT) */ * FROM pgbench_accounts;
65,QUERY PLAN
65,-----------------------------------------------------------------------------------------
65,Gather
65,(cost=1000.00..239382.52 rows=2014215 width=97)
65,Workers Planned: 6
65,Parallel Seq Scan on pgbench_accounts
65,(cost=0.00..36961.03 rows=335702 width=97)
65,"(3 rows)The pgbench_accounts table is modified. In this table, the parallel_workers storage"
65,parameter is set to 3.
65,Note This format in which the ALTER TABLE statement sets the parallel_workers parameter
65,is not compatible with Oracle databases.
65,The parallel_workers parameter is set by the PSQL \d+ statement.ALTER TABLE pgbench_accounts SET (parallel_workers=3);
65,\d+ pgbench_accounts
65,"Table ""public.pgbench_accounts"""
65,Column
65,Type
65,| Modifiers | Storage
65,| Stats target | Description
65,----------+---------------+-----------+----------+--------------+-------------
65,aid
65,| integer
65,| not null
65,| plain
65,bid
65,| integer
65,| plain
65,abalance | integer
65,| plain
65,filler
65,| character(84) |
65,| extended |
65,Indexes:
65,"""pgbench_accounts_pkey"" PRIMARY KEY, btree (aid)"
65,"Options: fillfactor=100, parallel_workers=3If the PARALLEL hint is provided with no parallel degree, the returned number of planned"
65,workers is the value of the parallel_workers parameter.
65,EXPLAIN SELECT /*+ PARALLEL(pgbench_accounts) */ * FROM pgbench_accounts;
65,QUERY PLAN
65,-----------------------------------------------------------------------------------------
65,Gather
65,(cost=1000.00..242522.97 rows=2014215 width=97)
65,Workers Planned: 3
65,Parallel Seq Scan on pgbench_accounts
65,(cost=0.00..40101.47 rows=649747 width=97)
65,(3 rows)The parallel degree value or DEFAULT in the PARALLEL hint overwrites the parallel_workers
65,setting.
65,"The following example shows the NO_PARALLEL hint. If trace_hints is set to on, the"
65,INFO: [HINTS] message is displayed to indicate that the parallel scan has been rejected
65,due to the NO_PARALLEL hint.
65,EXPLAIN SELECT /*+ NO_PARALLEL(pgbench_accounts) */ * FROM pgbench_accounts;
65,INFO:
65,[HINTS] Parallel SeqScan of [pgbench_accounts] rejected due to NO_PARALLEL hint.
65,QUERY PLAN
65,---------------------------------------------------------------------------
65,Seq Scan on pgbench_accounts
65,(cost=0.00..53746.15 rows=2014215 width=97)
65,(1 row)
65,Previous: Use the APPEND optimizer hint
65,Next: Conflicting hints
65,How helpful was this page?
65,What might be the problems?
65,More suggestions?
65,Send Feedback
65,Thank you! We've received your
65,feedback.
65,Free Trial
65,Free Trial
66,JDBC To Other Databases - Spark 2.4.7 Documentation
66,2.4.7
66,Overview
66,Programming Guides
66,Quick Start
66,"RDDs, Accumulators, Broadcasts Vars"
66,"SQL, DataFrames, and Datasets"
66,Structured Streaming
66,Spark Streaming (DStreams)
66,MLlib (Machine Learning)
66,GraphX (Graph Processing)
66,SparkR (R on Spark)
66,API Docs
66,Scala
66,Java
66,Python
66,"SQL, Built-in Functions"
66,Deploying
66,Overview
66,Submitting Applications
66,Spark Standalone
66,Mesos
66,YARN
66,Kubernetes
66,More
66,Configuration
66,Monitoring
66,Tuning Guide
66,Job Scheduling
66,Security
66,Hardware Provisioning
66,Building Spark
66,Contributing to Spark
66,Third Party Projects
66,Spark SQL Guide
66,Getting Started
66,Data Sources
66,Generic Load/Save Functions
66,Parquet Files
66,ORC Files
66,JSON Files
66,Hive Tables
66,JDBC To Other Databases
66,Avro Files
66,Troubleshooting
66,Performance Tuning
66,Distributed SQL Engine
66,PySpark Usage Guide for Pandas with Apache Arrow
66,Migration Guide
66,Reference
66,JDBC To Other Databases
66,Spark SQL also includes a data source that can read data from other databases using JDBC. This
66,functionality should be preferred over using JdbcRDD.
66,This is because the results are returned
66,as a DataFrame and they can easily be processed in Spark SQL or joined with other data sources.
66,The JDBC data source is also easier to use from Java or Python as it does not require the user to
66,provide a ClassTag.
66,"(Note that this is different than the Spark SQL JDBC server, which allows other applications to"
66,run queries using Spark SQL).
66,To get started you will need to include the JDBC driver for your particular database on the
66,"spark classpath. For example, to connect to postgres from the Spark Shell you would run the"
66,following command:
66,bin/spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar
66,Tables from the remote database can be loaded as a DataFrame or Spark SQL temporary view using
66,the Data Sources API. Users can specify the JDBC connection properties in the data source options.
66,user and password are normally provided as connection properties for
66,"logging into the data sources. In addition to the connection properties, Spark also supports"
66,the following case-insensitive options:
66,Property NameMeaning
66,url
66,"The JDBC URL to connect to. The source-specific connection properties may be specified in the URL. e.g., jdbc:postgresql://localhost/test?user=fred&password=secret"
66,dbtable
66,The JDBC table that should be read from or written into. Note that when using it in the read
66,path anything that is valid in a FROM clause of a SQL query can be used.
66,"For example, instead of a full table you could also use a subquery in parentheses. It is not"
66,allowed to specify `dbtable` and `query` options at the same time.
66,query
66,A query that will be used to read data into Spark. The specified query will be parenthesized and used
66,as a subquery in the FROM clause. Spark will also assign an alias to the subquery clause.
66,"As an example, spark will issue a query of the following form to the JDBC Source."
66,SELECT <columns> FROM (<user_specified_query>) spark_gen_alias
66,Below are couple of restrictions while using this option.
66,It is not allowed to specify `dbtable` and `query` options at the same time.
66,It is not allowed to specify `query` and `partitionColumn` options at the same time. When specifying
66,"`partitionColumn` option is required, the subquery can be specified using `dbtable` option instead and"
66,partition columns can be qualified using the subquery alias provided as part of `dbtable`.
66,Example:
66,"spark.read.format(""jdbc"")"
66,".option(""url"", jdbcUrl)"
66,".option(""query"", ""select c1, c2 from t1"")"
66,.load()
66,driver
66,The class name of the JDBC driver to use to connect to this URL.
66,"partitionColumn, lowerBound, upperBound"
66,"These options must all be specified if any of them is specified. In addition,"
66,numPartitions must be specified. They describe how to partition the table when
66,reading in parallel from multiple workers.
66,"partitionColumn must be a numeric, date, or timestamp column from the table in question."
66,Notice that lowerBound and upperBound are just used to decide the
66,"partition stride, not for filtering the rows in table. So all rows in the table will be"
66,partitioned and returned. This option applies only to reading.
66,numPartitions
66,The maximum number of partitions that can be used for parallelism in table reading and
66,writing. This also determines the maximum number of concurrent JDBC connections.
66,"If the number of partitions to write exceeds this limit, we decrease it to this limit by"
66,calling coalesce(numPartitions) before writing.
66,queryTimeout
66,The number of seconds the driver will wait for a Statement object to execute to the given
66,"number of seconds. Zero means there is no limit. In the write path, this option depends on"
66,"how JDBC drivers implement the API setQueryTimeout, e.g., the h2 JDBC driver"
66,checks the timeout of each query instead of an entire JDBC batch.
66,It defaults to 0.
66,fetchsize
66,"The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows). This option applies only to reading."
66,batchsize
66,"The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing. It defaults to 1000."
66,isolationLevel
66,"The transaction isolation level, which applies to current connection. It can be one of NONE, READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, or SERIALIZABLE, corresponding to standard transaction isolation levels defined by JDBC's Connection object, with default of READ_UNCOMMITTED. This option applies only to writing. Please refer the documentation in java.sql.Connection."
66,sessionInitStatement
66,"After each database session is opened to the remote DB and before starting to read data, this option executes a custom SQL statement (or a PL/SQL block). Use this to implement session initialization code. Example: option(""sessionInitStatement"", """"""BEGIN execute immediate 'alter session set ""_serial_direct_read""=true'; END;"""""")"
66,truncate
66,"This is a JDBC writer related option. When SaveMode.Overwrite is enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it. This can be more efficient, and prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. It defaults to false. This option applies only to writing."
66,cascadeTruncate
66,"This is a JDBC writer related option. If enabled and supported by the JDBC database (PostgreSQL and Oracle at the moment), this options allows execution of a TRUNCATE TABLE t CASCADE (in the case of PostgreSQL a TRUNCATE TABLE ONLY t CASCADE is executed to prevent inadvertently truncating descendant tables). This will affect other tables, and thus should be used with care. This option applies only to writing. It defaults to the default cascading truncate behaviour of the JDBC database in question, specified in the isCascadeTruncate in each JDBCDialect."
66,createTableOptions
66,"This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., CREATE TABLE t (name string) ENGINE=InnoDB.). This option applies only to writing."
66,createTableColumnTypes
66,"The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g: ""name CHAR(64), comments VARCHAR(1024)""). The specified types should be valid spark sql data types. This option applies only to writing."
66,customSchema
66,"The custom schema to use for reading data from JDBC connectors. For example, ""id DECIMAL(38, 0), name STRING"". You can also specify partial fields, and the others use the default type mapping. For example, ""id DECIMAL(38, 0)"". The column names should be identical to the corresponding column names of JDBC table. Users can specify the corresponding data types of Spark SQL instead of using the defaults. This option applies only to reading."
66,pushDownPredicate
66,"The option to enable or disable predicate push-down into the JDBC data source. The default value is true, in which case Spark will push down filters to the JDBC data source as much as possible. Otherwise, if set to false, no filter will be pushed down to the JDBC data source and thus all filters will be handled by Spark. Predicate push-down is usually turned off when the predicate filtering is performed faster by Spark than by the JDBC data source."
66,// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
66,// Loading data from a JDBC source
66,val jdbcDF = spark.read
66,".format(""jdbc"")"
66,".option(""url"", ""jdbc:postgresql:dbserver"")"
66,".option(""dbtable"", ""schema.tablename"")"
66,".option(""user"", ""username"")"
66,".option(""password"", ""password"")"
66,.load()
66,val connectionProperties = new Properties()
66,"connectionProperties.put(""user"", ""username"")"
66,"connectionProperties.put(""password"", ""password"")"
66,val jdbcDF2 = spark.read
66,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties)"
66,// Specifying the custom data types of the read schema
66,"connectionProperties.put(""customSchema"", ""id DECIMAL(38, 0), name STRING"")"
66,val jdbcDF3 = spark.read
66,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties)"
66,// Saving data to a JDBC source
66,jdbcDF.write
66,".format(""jdbc"")"
66,".option(""url"", ""jdbc:postgresql:dbserver"")"
66,".option(""dbtable"", ""schema.tablename"")"
66,".option(""user"", ""username"")"
66,".option(""password"", ""password"")"
66,.save()
66,jdbcDF2.write
66,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties)"
66,// Specifying create table column data types on write
66,jdbcDF.write
66,".option(""createTableColumnTypes"", ""name CHAR(64), comments VARCHAR(1024)"")"
66,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties)"
66,"Find full example code at ""examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala"" in the Spark repo."
66,// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
66,// Loading data from a JDBC source
66,Dataset<Row> jdbcDF = spark.read()
66,".format(""jdbc"")"
66,".option(""url"", ""jdbc:postgresql:dbserver"")"
66,".option(""dbtable"", ""schema.tablename"")"
66,".option(""user"", ""username"")"
66,".option(""password"", ""password"")"
66,.load();
66,Properties connectionProperties = new Properties();
66,"connectionProperties.put(""user"", ""username"");"
66,"connectionProperties.put(""password"", ""password"");"
66,Dataset<Row> jdbcDF2 = spark.read()
66,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties);"
66,// Saving data to a JDBC source
66,jdbcDF.write()
66,".format(""jdbc"")"
66,".option(""url"", ""jdbc:postgresql:dbserver"")"
66,".option(""dbtable"", ""schema.tablename"")"
66,".option(""user"", ""username"")"
66,".option(""password"", ""password"")"
66,.save();
66,jdbcDF2.write()
66,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties);"
66,// Specifying create table column data types on write
66,jdbcDF.write()
66,".option(""createTableColumnTypes"", ""name CHAR(64), comments VARCHAR(1024)"")"
66,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties);"
66,"Find full example code at ""examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java"" in the Spark repo."
66,# Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
66,# Loading data from a JDBC source
66,jdbcDF = spark.read \
66,".format(""jdbc"") \"
66,".option(""url"", ""jdbc:postgresql:dbserver"") \"
66,".option(""dbtable"", ""schema.tablename"") \"
66,".option(""user"", ""username"") \"
66,".option(""password"", ""password"") \"
66,.load()
66,jdbcDF2 = spark.read \
66,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"","
66,"properties={""user"": ""username"", ""password"": ""password""})"
66,# Specifying dataframe column data types on read
66,jdbcDF3 = spark.read \
66,".format(""jdbc"") \"
66,".option(""url"", ""jdbc:postgresql:dbserver"") \"
66,".option(""dbtable"", ""schema.tablename"") \"
66,".option(""user"", ""username"") \"
66,".option(""password"", ""password"") \"
66,".option(""customSchema"", ""id DECIMAL(38, 0), name STRING"") \"
66,.load()
66,# Saving data to a JDBC source
66,jdbcDF.write \
66,".format(""jdbc"") \"
66,".option(""url"", ""jdbc:postgresql:dbserver"") \"
66,".option(""dbtable"", ""schema.tablename"") \"
66,".option(""user"", ""username"") \"
66,".option(""password"", ""password"") \"
66,.save()
66,jdbcDF2.write \
66,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"","
66,"properties={""user"": ""username"", ""password"": ""password""})"
66,# Specifying create table column data types on write
66,jdbcDF.write \
66,".option(""createTableColumnTypes"", ""name CHAR(64), comments VARCHAR(1024)"") \"
66,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"","
66,"properties={""user"": ""username"", ""password"": ""password""})"
66,"Find full example code at ""examples/src/main/python/sql/datasource.py"" in the Spark repo."
66,# Loading data from a JDBC source
66,"df <- read.jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", user = ""username"", password = ""password"")"
66,# Saving data to a JDBC source
66,"write.jdbc(df, ""jdbc:postgresql:dbserver"", ""schema.tablename"", user = ""username"", password = ""password"")"
66,"Find full example code at ""examples/src/main/r/RSparkSQLExample.R"" in the Spark repo."
66,CREATE TEMPORARY VIEW jdbcTable
66,USING org.apache.spark.sql.jdbc
66,OPTIONS (
66,"url ""jdbc:postgresql:dbserver"","
66,"dbtable ""schema.tablename"","
66,"user 'username',"
66,password 'password'
66,INSERT INTO TABLE jdbcTable
66,SELECT * FROM resultTable
67,Database Performance Blog – Percona
67,Percona Live
67,About Us
67,Contact Us
67,Services
67,Support
67,MySQL Support
67,MongoDB Support
67,MariaDB Support
67,PostgreSQL Support
67,DBaaS Support
67,High Availability Support
67,Flexible Pricing
67,Support Tiers
67,Technical Account Managers
67,Managed Services
67,Percona Managed Database Services
67,Percona Advanced Managed Database Service
67,Consulting
67,Percona Cloud Cover
67,Percona Open Source Advance
67,Percona and Microsoft Azure Partnership
67,Policies
67,Training
67,Products
67,MySQL Database Software
67,Percona Distribution for MySQL
67,Percona Server for MySQL
67,Percona XtraDB Cluster
67,Percona XtraBackup
67,MongoDB Database Software
67,Percona Distribution for MongoDB
67,Percona Server for MongoDB
67,Percona Backup for MongoDB
67,PostgreSQL Database Software
67,Percona Monitoring and Management
67,Percona Kubernetes Operators
67,Open Source Database Tools
67,Percona Toolkit
67,Percona DBaaS Command Line Tool
67,Solutions
67,Eliminate Vendor Lock-In
67,Embrace the Cloud
67,Optimize Database Performance
67,Reduce Costs and Complexity
67,Resources
67,Calculators
67,2020 Survey Results
67,Solution Briefs
67,White Papers
67,Webinars
67,Case Studies
67,Datasheets
67,Ebooks
67,Videos
67,Technical Presentations
67,Documentation
67,About
67,About Percona
67,Contact Us
67,Customers
67,Careers
67,Percona Lifestyle
67,In The News
67,Percona Live
67,Events
67,Community
67,Forums
67,Community Blog
67,PMM Community Contributions
67,Apr
67,2021
67,Deploying a MongoDB Proof of Concept on Google Cloud Platform
67,"By Michael Patrick Cloud, MongoDB, Percona Software cloud, Google Cloud, MongoDB, percona server for MongoDB, Percona Software 0 Comments"
67,"Michael Patrick2021-04-09T10:19:55-04:00Recently, I needed to set up a Proof of Concept (POC) and wanted to do it on Google Cloud Platform (GCP).  After documenting the process, it seemed it might be helpful for others looking for the most basic guide possible to get a Mongo server up and running on GCP.  The process below will […]"
67,Read more
67,Apr
67,2021
67,MySQL 101: Basic MySQL Server Triage
67,"By Michael Patrick MySQL, Open Source, Percona Services, Percona Software MySQL, mysql-and-variants, Percona services, Percona Software 0 Comments"
67,"Michael Patrick2021-04-07T13:56:00-04:00So your MySQL server has crashed.  What do you do now?  When a server is down, in my opinion, there are two steps that are essential and both are extremely important and neither should be neglected:"
67,Save diagnostic information for determining the root cause analysis (RCA).
67,Get the server back up and running.
67,Too many people rush […]
67,Read more
67,Apr
67,2021
67,Percona Kubernetes Operators and Azure Blob Storage
67,"By Sergey Pronin Cloud, MongoDB, MySQL, Percona Software Azure, cloud, containers, Kubernetes, Kubernetes Operator, MongoDB, MySQL, mysql-and-variants, Percona Software 0 Comments"
67,"Sergey Pronin2021-04-07T10:57:44-04:00Percona Kubernetes Operators allow users to simplify deployment and management of MongoDB and MySQL databases on Kubernetes. Both operators allow users to store backups on S3-compatible storage and leverage Percona XtraBackup and Percona Backup for MongoDB to deliver backup and restore functionality. Both backup tools do not work with Azure Blob Storage, which is […]"
67,Read more
67,Apr
67,2021
67,What’s New at Percona Live ONLINE 2021?
67,"By Rachel Pescador Percona Events, Percona Live percona live 0 Comments"
67,"Rachel Pescador2021-04-06T10:21:49-04:00Percona Live ONLINE is a community-focused event for database developers, administrators, and decision-makers to share their knowledge and experiences."
67,We would love you to join us on May 12-13 for this year’s event.
67,Register and attend for FREE!
67,This year Percona Live is going to be our biggest and best event yet!
67,"Free, just like our software, Percona […]"
67,Read more
67,Apr
67,2021
67,Monitoring OPNSense Firewall with Percona Monitoring and Management
67,"By Peter Zaitsev Monitoring, Open Source, Percona Software Monitoring, OPNSense, Percona Monitoring and Management, Percona Software 0 Comments"
67,"Peter Zaitsev2021-04-06T08:15:21-04:00I try to use Open Source when a good solution exists, so for my home firewall, I’m using OPNSense  – a very powerful FreeBSD-based firewall appliance with great features along with a powerful GUI."
67,"One of the plugins available with OPNSense is  node_exporter, which exposes a lot of operating system metrics through the Prometheus protocol."
67,Installing […]
67,Read more
67,12…997Next
67,How Can We Help?
67,"Percona's experts can maximize your application performance with our open source database support, managed services or consulting."
67,Contact us
67,Subscribe Want to get weekly updates listing the latest blog posts? Subscribe now and we'll send you an update every Friday at 1pm ET.
67,Subscribe to our blog
67,CategoriesMySQL(3399)Insight for DBAs(1597)Percona Software(1548)Percona Events(874)MongoDB(571)Insight for Developers(492)Benchmarks(345)Percona Live(335)Webinars(301)Cloud(297)PostgreSQL(189)Monitoring(185)MariaDB(159)Percona Services(153)Security(130)ProxySQL(130)Hardware and Storage(106)Storage Engine(56)Database Trends(55)Percona Announcements(12)   Percona Blog RSS Feed
67,Upcoming WebinarsOptimize and Troubleshoot MySQL using PMM
67,MongoDB Backups Overview
67,Introduction to pg_stat_monitor
67,Moving your Database to the Cloud: Top 3 Things to Consider
67,What’s Old Is New; What’s Coming Is Here: Percona Offerings for MySQL 5.6 and DBaaS in PMM
67,All Webinars
67,Services
67,Support
67,Managed Services
67,Consulting
67,Training
67,Products
67,MySQL Software
67,MongoDB Software
67,PostgreSQL Distribution
67,Kubernetes
67,Monitoring & Management
67,Resources
67,Solution Briefs
67,White Papers
67,Webinars
67,Case Studies
67,Datasheets
67,Documentation
67,More
67,Blog
67,Community Blog
67,Technical Forum Help
67,About
67,Customers
67,Newsroom
67,About
67,Careers
67,Contact Us
67,Sales & General Inquiries
67,(888) 316-9775 (USA)
67,(208) 473-2904 (USA)
67,+44 203 608 6727 (UK)
67,0-808-169-6490 (UK)
67,0-800-724-4569 (GER)
67,"MySQL, InnoDB, MariaDB and MongoDB are trademarks of their respective owners. Proudly running Percona Server for MySQL"
67,Terms of Use |
67,Privacy |
67,Copyright |
67,Legal
67,Copyright © 2006-2021 Percona LLC.
68,How do I speed up my Redshift queries? | Segment Documentation
68,Home
68,Getting Started
68,Getting started with Segment
68,What is Segment?
68,A simple Segment installation
68,Planning a full installation
68,A full Segment installation
68,Sending data to destinations
68,Testing and debugging
68,What's next?
68,Guides
68,An introduction to Segment
68,Segment developer overview
68,Segment data user overview
68,Segment workspace admin overview
68,Filtering your Segment data
68,How does Segment handle duplicate data?
68,How can I ignore internet bots?
68,What is the difference between Segment and tag managers?
68,What is Replay?
68,How To Guides
68,How-to Guides index
68,How do I automate multi-channel re-engagement campaigns?
68,Should I collect data on the client or server?
68,How do I collect page views on the server side?
68,How do I create a push notification?
68,How do we track your customers across channels and devices?
68,How do I set up a dynamic coupon program to reward loyal customers?
68,How do I forecast LTV with SQL and Excel for e-commerce?
68,How do I import historical data?
68,How do I join user profiles?
68,How do I measure my advertising funnel?
68,How do I measure the ROI of my Marketing Campaigns?
68,How do I migrate code from other analytics tools?
68,What role does Segment play in Attribution?
68,How do we set up event-triggered notifications or alerts?
68,Usage and Billing
68,Account management
68,Billing and account FAQs
68,"MTUs, Throughput and Billing"
68,Do you offer discounts or coupons?
68,What is the Segment Startup Program?
68,Connections
68,Connections overview
68,The Segment Spec
68,Spec overview
68,Spec: Page
68,Spec: Screen
68,Spec: Track
68,Spec: Group
68,Spec: Identify
68,Spec: Common Fields
68,Native Mobile Spec
68,What is the native mobile spec?
68,Packaging SDKs for mobile destinations
68,Spec: Semantic Events
68,Spec: B2B SaaS
68,Spec: Ecommerce Events
68,Ecommerce tracking plans
68,Video Spec
68,Best practices for identifying users
68,Best practices for Event calls
68,Sources
68,Sources overview
68,Sources catalog
68,Using the Source Debugger
68,All about Cloud Sources
68,Set up a custom domain proxy in Segment
68,Visual Tagger
68,Destinations
68,Destinations overview
68,Add a destination
68,Destinations catalog
68,Destination Filters
68,Functions
68,Functions overview
68,Source Functions
68,Destination Functions
68,Functions environment
68,Functions usage limits
68,Storage Destinations
68,Storage Destinations overview
68,Storage Destinations catalog
68,Segment Data Lakes
68,Data Lakes overview
68,Set up Data Lakes
68,Sync Reports and error reporting
68,Data Lakes vs Warehouses
68,Data Warehouses
68,Warehouses overview
68,Warehouses schemas
68,Warehouse selective sync
68,Warehouse Health dashboards
68,Choosing a warehouse
68,Warehouse FAQs
68,How do you add users?
68,What does my warehouse error mean?
68,Redshift cluster and Redshift connector limitations
68,How do I speed up my redshift queries?
68,How do I test my connections?
68,What are my data export options?
68,How do I use Schema Controls?
68,How do I check if data is reaching a destination?
68,How do I find my write key?
68,Integration error codes
68,Are there limits on my Segment Schema?
68,Rate Limits and SLA
68,Data Residency
68,Personas
68,Personas overview
68,Personas Quickstart Guide
68,Personas Identity Resolution
68,Overview
68,Onboarding
68,Use Cases
68,External IDs
68,Settings
68,E-Commerce example
68,Personas Space set up
68,Computed Traits
68,SQL Traits
68,Audiences
68,Overview
68,Account-level Audiences
68,Using Personas data
68,Personas and Warehouses
68,Profile API
68,Personas and GDPR
68,Personas FAQs
68,Rate Limits
68,Privacy
68,Privacy overview
68,Privacy Portal
68,Detect PII
68,Data Controls and Alerts
68,GDPR
68,Complying with GDPR
68,User deletion and suppression
68,Privacy FAQs
68,Protocols
68,Protocols overview
68,Create a Tracking Plan
68,Data collection best practices
68,The Tracking Plan
68,Tracking Plan Libraries
68,Validate with violations
68,Connect Sources to your Tracking Plan
68,Review and resolve event violations
68,Forward violations
68,Enforce with data controls
68,Customize your schema controls
68,Forward blocked events
68,Transform to fix bad data
68,Protocols Extensions
68,Protocols APIs
68,Typewriter
68,Anomaly Detection
68,Protocols FAQs
68,Segment App
68,Introducing the Segment Web App
68,Workspace Home
68,Access Management
68,Identity & Access Management overview
68,Concepts
68,Roles
68,Manage workspace members
68,Label-based access control
68,Audit trail
68,Single Sign On
68,Multi-Factor Authentication (MFA)
68,Picking a secure password
68,Prod and Testing environments in Segment
68,How do I verify my email address?
68,Partners
68,Glossary
68,Config API
68,Help
68,Release Notes
68,Back to Segment.com
68,Log in
68,Sign Up
68,Home
68,Getting Started
68,Getting started with Segment
68,What is Segment?
68,A simple Segment installation
68,Planning a full installation
68,A full Segment installation
68,Sending data to destinations
68,Testing and debugging
68,What's next?
68,Guides
68,An introduction to Segment
68,Segment developer overview
68,Segment data user overview
68,Segment workspace admin overview
68,Filtering your Segment data
68,How does Segment handle duplicate data?
68,How can I ignore internet bots?
68,What is the difference between Segment and tag managers?
68,What is Replay?
68,How To Guides
68,How-to Guides index
68,How do I automate multi-channel re-engagement campaigns?
68,Should I collect data on the client or server?
68,How do I collect page views on the server side?
68,How do I create a push notification?
68,How do we track your customers across channels and devices?
68,How do I set up a dynamic coupon program to reward loyal customers?
68,How do I forecast LTV with SQL and Excel for e-commerce?
68,How do I import historical data?
68,How do I join user profiles?
68,How do I measure my advertising funnel?
68,How do I measure the ROI of my Marketing Campaigns?
68,How do I migrate code from other analytics tools?
68,What role does Segment play in Attribution?
68,How do we set up event-triggered notifications or alerts?
68,Usage and Billing
68,Account management
68,Billing and account FAQs
68,"MTUs, Throughput and Billing"
68,Do you offer discounts or coupons?
68,What is the Segment Startup Program?
68,Connections
68,Connections overview
68,The Segment Spec
68,Spec overview
68,Spec: Page
68,Spec: Screen
68,Spec: Track
68,Spec: Group
68,Spec: Identify
68,Spec: Common Fields
68,Native Mobile Spec
68,What is the native mobile spec?
68,Packaging SDKs for mobile destinations
68,Spec: Semantic Events
68,Spec: B2B SaaS
68,Spec: Ecommerce Events
68,Ecommerce tracking plans
68,Video Spec
68,Best practices for identifying users
68,Best practices for Event calls
68,Sources
68,Sources overview
68,Sources catalog
68,Using the Source Debugger
68,All about Cloud Sources
68,Set up a custom domain proxy in Segment
68,Visual Tagger
68,Destinations
68,Destinations overview
68,Add a destination
68,Destinations catalog
68,Destination Filters
68,Functions
68,Functions overview
68,Source Functions
68,Destination Functions
68,Functions environment
68,Functions usage limits
68,Storage Destinations
68,Storage Destinations overview
68,Storage Destinations catalog
68,Segment Data Lakes
68,Data Lakes overview
68,Set up Data Lakes
68,Sync Reports and error reporting
68,Data Lakes vs Warehouses
68,Data Warehouses
68,Warehouses overview
68,Warehouses schemas
68,Warehouse selective sync
68,Warehouse Health dashboards
68,Choosing a warehouse
68,Warehouse FAQs
68,How do you add users?
68,What does my warehouse error mean?
68,Redshift cluster and Redshift connector limitations
68,How do I speed up my redshift queries?
68,How do I test my connections?
68,What are my data export options?
68,How do I use Schema Controls?
68,How do I check if data is reaching a destination?
68,How do I find my write key?
68,Integration error codes
68,Are there limits on my Segment Schema?
68,Rate Limits and SLA
68,Data Residency
68,Personas
68,Personas overview
68,Personas Quickstart Guide
68,Personas Identity Resolution
68,Overview
68,Onboarding
68,Use Cases
68,External IDs
68,Settings
68,E-Commerce example
68,Personas Space set up
68,Computed Traits
68,SQL Traits
68,Audiences
68,Overview
68,Account-level Audiences
68,Using Personas data
68,Personas and Warehouses
68,Profile API
68,Personas and GDPR
68,Personas FAQs
68,Rate Limits
68,Privacy
68,Privacy overview
68,Privacy Portal
68,Detect PII
68,Data Controls and Alerts
68,GDPR
68,Complying with GDPR
68,User deletion and suppression
68,Privacy FAQs
68,Protocols
68,Protocols overview
68,Create a Tracking Plan
68,Data collection best practices
68,The Tracking Plan
68,Tracking Plan Libraries
68,Validate with violations
68,Connect Sources to your Tracking Plan
68,Review and resolve event violations
68,Forward violations
68,Enforce with data controls
68,Customize your schema controls
68,Forward blocked events
68,Transform to fix bad data
68,Protocols Extensions
68,Protocols APIs
68,Typewriter
68,Anomaly Detection
68,Protocols FAQs
68,Segment App
68,Introducing the Segment Web App
68,Workspace Home
68,Access Management
68,Identity & Access Management overview
68,Concepts
68,Roles
68,Manage workspace members
68,Label-based access control
68,Audit trail
68,Single Sign On
68,Multi-Factor Authentication (MFA)
68,Picking a secure password
68,Prod and Testing environments in Segment
68,How do I verify my email address?
68,Partners
68,Glossary
68,Config API
68,Config API overview
68,API design
68,Authentication
68,Destination Filter Query Language
68,Reference
68,Creating a Javascript web source and Google Analytics destination
68,Help
68,Release Notes
68,Back to Connections
68,Email Marketing
68,ActiveCampaign
68,Airship
68,AutopilotHQ
68,Blueshift
68,Braze
68,Bronto
68,Courier
68,Customer.io
68,Drip
68,Eloqua
68,Email Aptitude
68,Emarsys
68,Exponea
68,Extole Platform
68,Freshmarketer
68,Gainsight PX
68,Gist
68,HubSpot
68,Insider
68,Intercom
68,Iterable
68,KISSmetrics
68,Kahuna
68,Klaviyo
68,Leanplum
68,MailChimp
68,Mailjet
68,Marketo Static Lists
68,Marketo V2
68,MoEngage
68,Moosend
68,Movable Ink
68,Nudgespot
68,OneSignal
68,Pardot
68,PersistIQ
68,Personyze
68,RadiumOne Connect
68,Responsys
68,Sailthru
68,Salescamp CRM
68,Salesforce Marketing Cloud
68,Seg
68,Selligent Marketing Cloud
68,Swrve
68,Talon.One
68,Trustpilot
68,UserIQ
68,Userlist
68,Vero
68,Voucherify
68,WebEngage
68,Wigzo
68,Wishpond
68,Xtremepush
68,Zaius
68,Zendesk Connect
68,hydra
68,Advertising
68,AdLearn Open Platform
68,AdQuick
68,AdRoll
68,AdWords Remarketing Lists
68,Adikteev
68,Adtriba
68,AppNexus
68,Bing Ads
68,Blueshift
68,ByteGain
68,Criteo App & Web Events
68,Criteo Offline Conversions
68,DoubleClick Floodlight
68,EPICA
68,Everflow
68,Facebook App Events
68,Facebook Conversions API
68,Facebook Offline Conversions
68,Facebook Pixel
68,Firebase
68,Flurry
68,Google Ads (Classic)
68,Google Ads (Gtag)
68,HasOffers
68,Inkit
68,Kevel
68,Kitemetrics
68,LinkedIn Insight Tag
68,MediaMath
68,Millennial Media
68,Nanigans
68,Perfect Audience
68,Personas Display & Video 360
68,Personas Facebook Custom Audiences
68,Pinterest Audiences
68,Pinterest Tag
68,Podsights
68,Quantcast
68,QuanticMind
68,Quora Conversion Pixel
68,RadiumOne Connect
68,ShareASale
68,SimpleReach
68,Snapchat Audiences
68,TV Squared
68,TrafficGuard
68,Twitter Ads
68,Yellowhammer
68,Zaius
68,Analytics
68,AdLearn Open Platform
68,AdQuick
68,Adobe Analytics
68,Adtriba
68,Alexa
68,Algolia Insights
68,Amazon Kinesis
68,Amazon Kinesis Firehose
68,Amazon S3
68,Amplitude
68,Anodot
68,Asayer
68,Auryc
68,Beamer
68,Blendo
68,Bucket
68,ByteGain
68,Calixa
68,Candu
68,Chartbeat
68,ClearBrain
68,CleverTap
68,Clicky
68,Countly
68,CrowdPower
68,Cruncher
68,Custify
68,CustomFit.ai
68,CustomerSuccessBox
68,Data Lakes
68,DataBrain
68,EMMA
68,EPICA
68,Emarsys
68,Everflow
68,Experiments by GrowthHackers
68,Exponea
68,Facebook App Events
68,FactorsAI
68,Firebase
68,Flurry
68,FoxMetrics
68,FunnelFox
68,Gainsight
68,Gainsight PX
68,Gameball
68,Gauges
68,GoSquared
68,Google Analytics
68,Google Cloud Storage
68,Heap
68,HitTail
68,HubSpot
68,IBM UBX
68,Indicative
68,Inkit
68,Interana
68,KISSmetrics
68,Keen
68,Kitemetrics
68,Kubit
68,Lantern
68,LaunchDarkly Events
68,Librato
68,Localytics
68,Lytics
68,Madkudu
68,Matomo
68,Maxia
68,Mixpanel
68,MoEngage
68,Mutiny
68,New Relic
68,Nielsen DCR
68,Parsely
68,Pendo
68,Pointillist
68,PostHog
68,ProfitWell
68,Quantcast
68,Refersion
68,Retina
68,Richpanel
68,SIGNL4 Alerting
68,SMBStreams
68,ScopeAI
68,SegMetrics
68,Serenytics
68,Sherlock
68,Singular
68,SlicingDice
68,Smartlook
68,Split
68,Startdeliver
68,Stories
68,Stormly
68,Strikedeck
68,Survicate
68,Swrve
68,Tamber
68,Tractionboard
68,TrafficGuard
68,Treasure Data
68,Unwaffle
68,UserIQ
68,UserLeap
68,Vidora
68,WalkMe
68,WebEngage
68,Whale Watch
68,Wigzo
68,Windsor
68,Woopra
68,Xtremepush
68,Yandex Metrica
68,Youbora
68,comScore
68,goedle.io
68,hydra
68,Enrichment
68,AdLearn Open Platform
68,Clearbit Enrichment
68,Clearbit Reveal
68,Cruncher
68,Hull
68,Madkudu
68,Modern Pricing
68,Noora
68,Refiner
68,SMBStreams
68,Survicate
68,Vidora
68,A/B Testing
68,AdLearn Open Platform
68,Algolia Insights
68,Apptimize
68,ConvertFlow
68,Criteo Offline Conversions
68,CustomFit.ai
68,Experiments by GrowthHackers
68,Exponea
68,Freshmarketer
68,FunnelEnvy
68,Insider
68,LaunchDarkly Events
68,Leanplum
68,Modern Pricing
68,Monetate
68,Mutiny
68,Optimizely Full Stack
68,Optimizely Web
68,Personyze
68,PostHog
68,Proof Experiences
68,Split
68,Tamber
68,Taplytics
68,Trackier
68,Visual Website Optimizer
68,Customer Success
68,AdQuick
68,Asayer
68,Beamer
68,Calixa
68,Candu
68,ChurnZero
68,ClientSuccess
68,Courier
68,Custify
68,CustomFit.ai
68,CustomerSuccessBox
68,DataBrain
68,Elevio
68,Emarsys
68,EnjoyHQ
68,Gainsight
68,Gainsight PX
68,Hawkei
68,Help Scout
68,Inkit
68,Interana
68,Intercom
68,Kustomer
68,Learndot
68,Lou
68,Moesif API Analytics
68,Nat
68,Natero
68,Noora
68,Planhat
68,Ramen
68,Refiner
68,Retently
68,Richpanel
68,Salescamp CRM
68,Salesmachine
68,SatisMeter
68,Savio
68,ScopeAI
68,Sherlock
68,Slack
68,Snapboard
68,Startdeliver
68,Stonly
68,Stormly
68,Strikedeck
68,Talon.One
68,Totango
68,Trustpilot
68,Unwaffle
68,Upcall
68,UserIQ
68,UserLeap
68,UserVoice
68,Vitally
68,Voucherify
68,WalkMe
68,Windsor
68,Zendesk
68,Zopim
68,hydra
68,Performance Monitoring
68,AdQuick
68,Asayer
68,Atatus
68,BugHerd
68,Bugsnag
68,Button
68,Callingly
68,Crittercism
68,Cruncher
68,Custify
68,CustomerSuccessBox
68,Errorception
68,Experiments by GrowthHackers
68,FunnelFox
68,Hawkei
68,Keen
68,Lantern
68,Moesif API Analytics
68,New Relic
68,Pingdom
68,ProductBird
68,Retina
68,Rollbar
68,SIGNL4 Alerting
68,SegMetrics
68,Sentry
68,Serenytics
68,Snapboard
68,Stormly
68,Track JS
68,TrafficGuard
68,Attribution
68,Adjust
68,Adtriba
68,AppsFlyer
68,Attribution
68,Branch Metrics
68,Button
68,Convertro
68,Criteo Offline Conversions
68,Dreamdata IO
68,EMMA
68,Impact Partnership Cloud
68,Improvely
68,Interana
68,Kitemetrics
68,Kochava
68,Localytics
68,Lou
68,Refersion
68,Retina
68,Rockerbox
68,Singular
68,Stormly
68,TUNE
68,Tapstream
68,TrafficGuard
68,Deep Linking
68,Adjust
68,AppsFlyer
68,Branch Metrics
68,Button
68,EMMA
68,Singular
68,Video
68,Adobe Analytics
68,Chartbeat
68,Nielsen DCR
68,Parsely
68,Youbora
68,comScore
68,SMS & Push Notifications
68,Airship
68,Batch
68,Beamer
68,Blueshift
68,Braze
68,Callingly
68,CleverTap
68,Courier
68,Customer.io
68,EMMA
68,Emarsys
68,Flurry
68,Gameball
68,Insider
68,Iterable
68,Kahuna
68,Leanplum
68,Localytics
68,MoEngage
68,OneSignal
68,Regal Voice
68,SIGNL4 Alerting
68,Selligent Marketing Cloud
68,Swrve
68,Tamber
68,User.com
68,WebEngage
68,Wigzo
68,Xtremepush
68,Zaius
68,Zendesk Connect
68,Marketing Automation
68,Airship
68,ByteGain
68,Callingly
68,ClearBrain
68,Courier
68,CrowdPower
68,CustomFit.ai
68,DataBrain
68,EPICA
68,Emarsys
68,Everflow
68,Experiments by GrowthHackers
68,Extole Platform
68,Freshmarketer
68,FunnelEnvy
68,Gist
68,Inkit
68,Insider
68,Kitemetrics
68,Mutiny
68,Nat
68,PersistIQ
68,Personyze
68,Proof Experiences
68,Regal Voice
68,Retina
68,Salescamp CRM
68,SegMetrics
68,Singular
68,Startdeliver
68,Stories
68,Stormly
68,Strikedeck
68,Tamber
68,Unwaffle
68,Upcall
68,Userlist
68,Vidora
68,Voucherify
68,WebEngage
68,Xtremepush
68,hydra
68,Personalization
68,Algolia Insights
68,Amazon Personalize
68,Appcues
68,Button
68,ByteGain
68,Candu
68,Chameleon
68,ClearBrain
68,ConvertFlow
68,Criteo Offline Conversions
68,Cruncher
68,CustomFit.ai
68,DataBrain
68,EPICA
68,Exponea
68,FunnelEnvy
68,FunnelFox
68,Gainsight PX
68,Gameball
68,Hello Bar
68,Hull
68,Insider
68,Leanplum
68,Lytics
68,Modern Pricing
68,Monetate
68,Movable Ink
68,Mutiny
68,Nat
68,Optimizely Full Stack
68,Optimizely Web
68,Personyze
68,ProductBird
68,Proof Experiences
68,Regal Voice
68,Selligent Marketing Cloud
68,Snapboard
68,Spinnakr
68,Startdeliver
68,Stonly
68,Talon.One
68,Tamber
68,Unwaffle
68,Upcall
68,User.com
68,UserLeap
68,Userpilot
68,Voucherify
68,WebEngage
68,Wigzo
68,Wishpond
68,Xtremepush
68,goedle.io
68,tray.io
68,Raw Data
68,Amazon EventBridge
68,Amazon Kinesis
68,Amazon Kinesis Firehose
68,Amazon Lambda
68,Amazon S3
68,Anodot
68,Asayer
68,Azure Function
68,Blendo
68,Calixa
68,Cruncher
68,Data Lakes
68,Experiments by GrowthHackers
68,FunnelFox
68,Google Cloud Function
68,Google Cloud PubSub
68,Google Cloud Storage
68,Hull
68,Interana
68,Iron.io
68,Keen
68,Mammoth
68,Maxia
68,PostHog
68,Repeater
68,Serenytics
68,SlicingDice
68,Stitch Data
68,Stories
68,Treasure Data
68,Vidora
68,Webhooks
68,Xplenty
68,Zapier
68,tray.io
68,Referrals
68,Ambassador
68,Attribution
68,Extole Platform
68,Friendbuy
68,Gameball
68,Impact Partnership Cloud
68,Refersion
68,SaaSquatch
68,Talkable
68,Talon.One
68,Trustpilot
68,Voucherify
68,Feature Flagging
68,Apptimize
68,LaunchDarkly Events
68,Optimizely Full Stack
68,Optimizely Web
68,Split
68,Heatmaps & Recordings
68,Auryc
68,Crazy Egg
68,Freshmarketer
68,FullStory
68,Hotjar
68,Inspectlet
68,Lucky Orange
68,MouseStats
68,Mouseflow
68,Navilytics
68,PostHog
68,Smartlook
68,Stories
68,WalkMe
68,mabl
68,Surveys
68,Auryc
68,Beamer
68,Canny
68,ConvertFlow
68,Custify
68,CustomerSuccessBox
68,Delighted
68,Gainsight
68,Gainsight PX
68,Lucky Orange
68,Noora
68,Pendo
68,ProductBird
68,Promoter.io
68,Qualaroo
68,Ramen
68,Refiner
68,Retently
68,SatisMeter
68,Savio
68,Stonly
68,Strikedeck
68,Survicate
68,Trustpilot
68,UserIQ
68,UserLeap
68,UserVoice
68,WalkMe
68,WebEngage
68,Wootric
68,CRM
68,Braze
68,ByteGain
68,Calixa
68,CrowdPower
68,Emarsys
68,EnjoyHQ
68,Firebase
68,Freshsales
68,FunnelFox
68,GoSquared
68,HubSpot
68,Kustomer
68,Moesif API Analytics
68,Nat
68,Noora
68,ProductBird
68,Richpanel
68,Sailthru
68,Salescamp CRM
68,Salesforce
68,SegMetrics
68,Startdeliver
68,Strikedeck
68,Userlist
68,WebEngage
68,Whale Alerts
68,Wigzo
68,Windsor
68,Wishpond
68,Security & Fraud
68,Castle
68,Moesif API Analytics
68,SIGNL4 Alerting
68,Singular
68,TrafficGuard
68,Livechat
68,Courier
68,Drift
68,Gist
68,GoSquared
68,Intercom
68,LiveChat
68,Lucky Orange
68,Olark
68,Richpanel
68,SnapEngage
68,Zopim
68,Tag Managers
68,FunnelEnvy
68,Google Tag Manager
68,Email
68,User.com
68,Back to Connections
68,Server
68,.NET
68,Clojure
68,HTTP API
68,Java
68,Node.js
68,PHP
68,Pixel Tracking API
68,Python
68,Ruby
68,Email Marketing
68,ActiveCampaign
68,Airship
68,AutopilotHQ
68,Blueshift
68,Braze
68,Customer.io
68,Drip
68,Facebook Lead Ads
68,Intercom
68,Iterable
68,Klenty
68,Leanplum
68,Mailchimp
68,Mailjet
68,Mandrill
68,Nudgespot
68,Selligent Marketing Cloud
68,SendGrid
68,Vero
68,CRM
68,Aircall
68,HubSpot
68,Moesif API Analytics
68,Salesforce
68,Helpdesk
68,Aircall
68,Zendesk
68,Customer Success
68,Aircall
68,Beamer
68,Candu
68,Moesif API Analytics
68,ProveSource
68,Refiner
68,Marketing Automation
68,Airship
68,Foursquare Pilgrim
68,ProveSource
68,Regal Voice
68,Selligent Marketing Cloud
68,SMS & Push Notifications
68,Airship
68,Beamer
68,Braze
68,Leanplum
68,Regal Voice
68,Selligent Marketing Cloud
68,Twilio
68,Custom
68,Amazon S3
68,Analytics
68,Amplitude Cohorts
68,Beamer
68,Candu
68,Looker
68,Moesif API Analytics
68,Pendo
68,Youbora
68,Mobile
68,Android
68,React Native
68,Xamarin
68,iOS
68,Surveys
68,Beamer
68,Delighted
68,Refiner
68,Wootric
68,Personalization
68,Candu
68,Foursquare Pilgrim
68,Leanplum
68,ProveSource
68,Regal Voice
68,Livechat
68,Chatlio
68,Advertising
68,Facebook Ads
68,Google Ads
68,ProveSource
68,Enrichment
68,Foursquare Pilgrim
68,Herow
68,Radar
68,Refiner
68,Referrals
68,Friendbuy
68,Website
68,Javascript
68,Shopify (by Littledata)
68,Feature Flagging
68,LaunchDarkly
68,A/B Testing
68,LaunchDarkly
68,Leanplum
68,Performance Monitoring
68,Moesif API Analytics
68,Ott
68,Roku (alpha)
68,Payments
68,Stripe
68,Home
68,Connections
68,Storage
68,Warehouses
68,How do I speed up my Redshift queries?
68,How do I speed up my Redshift queries?
68,On this page
68,Common Causes for Slow Queries
68,Pro-tips for Segment Warehouses
68,"Waiting minutes and minutes, maybe even an hour, for your queries to compute is an unfortunate reality for many growing companies. Whether your data has grown faster than your cluster, or you’re running too many jobs in parallel, there are lots of reasons your queries might be slowing down."
68,"To help you improve your query performance, this guide takes you through common issues and how to mitigate them."
68,Common Causes for Slow Queries
68,1. Not enough space
68,"As your data volume grows and your team writes more queries, you might be running out of space in your cluster."
68,"To check if you’re getting close to your max, run this query. It will tell you the percentage of storage used in your cluster. We recommend never exceeding 75-80% of your storage capacity. If you’re nearing capacity, consider adding some more nodes."
68,Learn how to resize your cluster here.
68,2. Inefficient queries
68,"Another thing you’ll want to check is if your queries are efficient. For example, if you’re scanning an entire dataset with a query, you’re probably not making the best use of your compute resources."
68,A few tips for writing performant queries:
68,Consider using INNER joins as they are are more efficient that LEFT joins.
68,Stay away from UNION whenever possible.
68,Specify multiple levels of conditionals when you can.
68,Use EXPLAIN to show the query execution plan and cost.
68,"To learn more about writing beautiful SQL, check out these resources:"
68,Periscope on Query Performance
68,Mode on Performance Tuning SQL Queries
68,Chartio on Improving Query Performance
68,3. Multiple ETL processes and queries running
68,Some databases like Redshift have limited computing resources. Running multiple queries or ETL processes that insert data into your warehouse at the same time will compete for compute power.
68,"If you have multiple ETL processes loading into your warehouse at the same time, especially when analysts are also trying to run queries, everything will slow down. Try to schedule them at different times and when your cluster is least active."
68,"If you’re a Segment Business Tier customer, you can schedule your sync times under Warehouses Settings."
68,"In addition, you might want to take advantage of Redshift’s Workload Management that helps ensure fast-running queries won’t get stuck behind long ones."
68,4. Default WLM Queue Configuration
68,"As mentioned before, Redshift schedules and prioritizes queries using Workload Management. Each queue is configured to distribute resources in ways that can optimize for your use-case."
68,"The default configuration is a single queue with only 5 queries running concurrently, but we’ve discovered that the default only works well for very low-volume warehouses. More often than not, adjusting this configuration can significantly improve your sync times."
68,"Before our SQL statements, we use set query_group to ""segment""; to group all of our queries together. This allows you to easily create a queue just for Segment that can be isolated from your own queries. The maximum concurrency that Redshift supports is 50 across all query groups, and resources like memory are distributed evenly across all those queries."
68,Our initial recommendation is for 2 WLM queues:
68,a queue for the segment query group with a concurrency of 10
68,leave the default queue with a concurrency of 5
68,"Generally, we are responsible for most writes in the databases we connect to, so having a higher concurrency allows us to write as quickly as possible. However, if you are also using the same database for your own ETL process, you may want to use the same concurrency for both groups. In addition, you may even require additional queues if you have other applications writing to the database."
68,"Each cluster may have different needs, so feel free to stray from this recommendation if another configuration works better for your use-case. AWS provides some guidelines, and of course you can always contact us as we’re more than happy to share what we have learned while working with Redshift."
68,Pro-tips for Segment Warehouses
68,"In addition to following performance best practices, here are a few more optimizations to consider if you’re using Segment Warehouses."
68,Factors that affect load times
68,"When Segment is actively loading data into your data warehouse, we’re competing for cluster space and storage with any other jobs you might be running. Here are the parameters that influence your load time for Segment Warehouses."
68,"Volume of data. Our pipeline needs to load and deduplicate data for each sync, so simply having more volume means these operations will take longer."
68,"Number of sources. When we start a sync of your data into your warehouse, we kick off a new job for every source you have in Segment. So the more sources you have, the longer your load time could take. This is where the WLM queue and the concurrency setting can make a big difference."
68,"Number and size of columns. Column sizes and the number of columns also affect load time. If you have very long property values or lots of properties per event, the load may take longer as well."
68,Performance optimizations
68,"To make sure you have enough headroom for quick queries while using Segment Warehouses, here are some tips!"
68,"Size up your cluster. If you find your queries are getting slow at key times during the day, add more nodes to give enough room for us to load data and for your team to run their queries."
68,"Disable unused sources. If you’re not actively analyzing data from a source, consider disabling the source for your Warehouse (available for business tier). If you don’t use a source anymore—perhaps you were just playing around with it for testing, you might even want to remove it completely. This will kick off fewer jobs in our ETL process."
68,"Schedule syncs during off times. If you’re concerned about query times and you don’t mind data that’s a little stale, you can schedule your syncs to run when most of your team isn’t actively using the database. (Available for business tier customers.)"
68,"Schedule regular vacuums. Make sure to schedule regular vacuums for your cluster, so old deleted data isn’t taking up space."
68,"We hope these steps will speed up your workflow! If you need any other help, feel free to contact us."
68,This page was last modified: 24 Feb 2021
68,Need support?
68,"Questions? Problems? Need more info? Contact us, and we can help!"
68,Visit our Support page
68,Was this page helpful?
68,Yes
68,Thanks for your feedback!
68,Can we improve this doc? Send us feedback!
68,Get started with Segment
68,Segment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.
68,Request Demo
68,Create free account
68,On this page
68,Common Causes for Slow Queries
68,Pro-tips for Segment Warehouses
68,Was this page helpful?
68,Yes
68,Thanks for your feedback!
68,Can we improve this doc? Send us feedback!
68,Product
68,Connections
68,Protocols
68,Personas
68,Integrations Catalog
68,Pricing
68,Security
68,GDPR
68,For Developers
68,Documentation
68,Segment API
68,Build on Segment
68,Open Source
68,Engineering Team
68,Company
68,Careers
68,Blog
68,Press
68,FTFY Podcast
68,Events
68,Support
68,Help Center
68,Contact us
68,Resources
68,Recipes
68,Security Bulletins
68,Become a Partner
68,"© 2021 Segment.io, Inc."
68,Privacy
68,Terms
68,Website Data Collection Preferences
68,Send
68,Send
68,Send
69,Alfresco Docs - Configure databases
69,In: content-services
69,In: All docs
69,Close
69,Docs
69,Services
69,Content
69,Process
69,Process Services
69,Process Automation
69,Governance
69,Integrations
69,Synchronize
69,Sync Service
69,Desktop Sync
69,Transform
69,Transform Service
69,Document Transformation Engine
69,Media Management
69,Search
69,Search and Insight Engine
69,Search Services
69,Federation
69,Federation
69,Identity
69,Identity Service
69,SAML Module
69,Intelligence
69,Intelligence
69,Content Stores
69,Amazon S3
69,Microsoft Azure
69,Amazon Glacier
69,EMC Centera
69,Business Connectors
69,Salesforce
69,SAP
69,Microsoft 365
69,Microsoft Outlook
69,Microsoft Office
69,Google Docs
69,Applications
69,Digital Workspace
69,Digital Workspace
69,Mobile Workspace
69,Application Development Framework
69,Content Mobile
69,Alfresco Content Services
69,6.0
69,7.0Latest
69,6.2
69,6.1
69,6.0
69,Community
69,Reference
69,Tutorials
69,Supported Platforms
69,Table of contents
69,Introduction
69,Install
69,Overview
69,Install with zip
69,Overview
69,Install on Tomcat
69,Install Alfresco Module Package
69,Install additional software
69,Install using containers
69,Overview
69,Install using Docker Compose
69,Install using Helm
69,Customization
69,Upgrade
69,Configure
69,Overview
69,Subsystems
69,Databases
69,title
69,Repository
69,File servers
69,Email
69,LibreOffice
69,ActiveMQ
69,Smart Folders
69,Overview
69,FAQ
69,Content models
69,Mobile
69,Administer
69,Overview
69,Admin Tools
69,Repository Admin Console
69,Share Admin Tools
69,Support Tools
69,Import and transfer tools
69,Manage security
69,Users and groups
69,Authentication and sync
69,Authorization
69,Auditing
69,Backup and migrate
69,Backup and restore
69,Migration
69,High availability features
69,Clustering
69,Multi-tenancy
69,Content replication
69,Licenses
69,File and folder templates
69,File metadata extraction
69,Workflows
69,Transformations
69,Content stores
69,Database table cleanup
69,JMX reference
69,Troubleshooting
69,Using
69,Overview
69,Alfresco Share
69,Profiles and dashboards
69,Sites
69,Overview
69,Features
69,Content
69,Overview
69,Manage content
69,Files and folders
69,Folder rules
69,Tasks and workflows
69,Search
69,Smart Folders
69,Roles and permissions
69,Develop
69,Software architecture
69,Software development kit (SDK)
69,Extension packaging (modules)
69,Extension points overview
69,Platform extension points
69,Overview
69,Content model
69,Actions
69,Web scripts
69,JavaScript Root Objects
69,Behavior policies
69,Scheduled jobs
69,Metadata Extractors
69,Mimetypes
69,Content Transformers (and Renditions)
69,Permissions and roles
69,Data lists
69,Ratings
69,Bootstrapping content
69,Patches
69,Module components
69,Subsystems
69,Authentication
69,Content Stores
69,Audit Log
69,Admin Console Components
69,Share UI extension points
69,Overview
69,Share configuration
69,Document Library
69,Share Themes
69,Site Presets
69,Web Scripts
69,Surf Pages
69,Surf Dashlets
69,Surf Widgets
69,Surf Extension Modules
69,Aikau Menus
69,Aikau Pages
69,Aikau Dashlets
69,Aikau Widgets
69,Evaluators
69,JavaScript Root Objects
69,Form Controls
69,Form Processor
69,Form Processor Filters
69,Form Field Validation Handlers
69,Modifying out-of-the-box code
69,Useful tools
69,ReST API guide
69,Overview
69,Install and authenticate
69,Get Repository Information
69,Managing Folders and Files
69,Managing Sites
69,Managing People and Groups
69,Managing Audit Applications and Logs
69,Searching for content
69,Reference
69,Web Scripts
69,Surf Framework
69,FreeMarker
69,Share Document Library
69,Repository JavaScript root objects
69,Introduction to Aikau
69,Table of contents
69,Configure databases
69,You can configure supported databases for use with Content Services:
69,Amazon Relational Database Service (RDS) in the cloud
69,"Choose either Amazon Aurora, MySQL, Oracle, PostgreSQL, or Microsoft SQL Server"
69,MySQL or MariaDB
69,Oracle
69,PostgreSQL
69,Microsoft SQL Server
69,"Before continuing, check the Supported platforms page for the correct driver version for your chosen database."
69,Amazon RDS
69,"Amazon RDS makes it easy to set up, operate, and scale a relational database in the cloud. It’s a web service running in the cloud and provides relational database for use with Content Services. Amazon RDS supports and gives you online access to the capabilities of the following relational database management systems (RDBMS):"
69,Amazon Aurora
69,MySQL
69,Oracle
69,PostgreSQL
69,Microsoft SQL Server
69,"As good practice, when using an Amazon’s Elastic Compute Cloud (EC2) environment, you may want to use Amazon’s Simple Storage Service (S3) where an S3 bucket is used as a content store. See Alfresco Content Connector for AWS S3 for more information."
69,Read the following sections to configure a database for Amazon RDS.
69,Amazon Aurora database on Amazon RDS
69,"You can configure an Aurora database on Amazon RDS for use with Content Services. Amazon Aurora is a MySQL-compatible relational database management system, and has the following prerequisites:"
69,Setup Amazon RDS using the AWS Management Console.
69,Content Services deployed on an Amazon EC2 instance
69,Note: Aurora support is only available when running in Amazon Web Services (AWS).
69,To configure the database:
69,Use the ssh command to connect to the Amazon EC2 instance using a provided .ppk key.
69,"For Amazon Linux, the user name is ec2-user."
69,"For RHEL5, the user name is either root or ec2-user."
69,"For Ubuntu, the user name is ubuntu. For SUSE Linux, the user name is root."
69,Execute sudo su to change to root.
69,Install Content Serviceson your Amazon EC2 instance.
69,Install the Aurora database connector.
69,This release requires mysql-connector-java-5.x.x.jar for compatibility with the SQL Server database. Check the Supported platforms page for the correct driver.
69,Download the driver from the MySQL site.
69,Copy the JDBC driver into the <TOMCAT_HOME>/lib directory.
69,Install and use a database tool to connect to the Amazon RDS.
69,Create a database named alfresco.
69,Create a user named alfresco.
69,Set the new user’s password to alfresco.
69,Open the <classpathRoot>/alfresco-global.properties file.
69,Locate the following property:
69,dir.root=
69,Edit this to set an absolute path to point to the directory in which you want to store Content Services data. For example: dir.root=C:/Alfresco/alf_data
69,Set and uncomment the database connection properties as shown below:
69,db.name=alfresco2
69,db.username=alfresco
69,db.password=alfresco
69,db.host=auroraqadb-cluster.cluster-clqevmd2v8y9.us-east-1.rds.amazonaws.com
69,db.port=13306
69,db.prefix=mysql
69,db.pool.max=275
69,# MySQL database connection
69,db.driver=org.gjt.mm.mysql.Driver
69,db.url=jdbc:mysql://${db.host}/${db.name}?${db.params}
69,db.url=jdbc:mysql://${db.host}:${db.port}/${db.name}?${db.params}
69,Save the file.
69,Restart the Content Services server.
69,MySQL database on Amazon RDS
69,"You can configure a MySQL database on Amazon RDS for use with Content Services, with the following prerequisites:"
69,Setup Amazon RDS using the AWS Management Console
69,Amazon EC2 instance
69,Use the ssh command to connect to the Amazon EC2 instance using a provided .ppk key.
69,"For Amazon Linux, the user name is ec2-user."
69,"For RHEL5, the user name is either root or ec2-user."
69,"For Ubuntu, the user name is ubuntu. For SUSE Linux, the user name is root."
69,Execute sudo su to change to root.
69,Install Content Services using one of the options provided.
69,Install the MySQL database connector.
69,"The MySQL database connector is required when installing with MySQL, and allows the MySQL database to talk to the server. Check the Supported platforms page for the correct driver."
69,Download mysql-connector-java-5.x.x from the MySQL download site.
69,Copy the JAR file into the /lib directory.
69,"For example, for Tomcat, copy the JAR file into the <TOMCAT_HOME>/lib directory."
69,Install and use a database tool to connect to the Amazon RDS.
69,Create a database named alfresco.
69,Create a user named alfresco.
69,Set the new user’s password to alfresco.
69,Open the <classpathRoot>/alfresco-global.properties file.
69,Edit the following line with an absolute path to point to the directory in which you want to store Content Services data.
69,For example: dir.root=C:/Alfresco/alf_data
69,Set and uncomment the database connection properties as shown below:
69,db.name=alfresco
69,db.username=alfresco
69,db.password=alfresco
69,db.host=alfqa-mysql5-6-19a.cw4mo3qj8qdu.us-east-1.rds.amazonaws.com
69,db.port=3306
69,db.pool.max=275
69,# MySQL connection
69,db.driver=org.gjt.mm.mysql.Driver
69,db.url=jdbc:mysql://${db.host}:${db.port}/${db.name}?useUnicode=yes&characterEncoding=UTF-8
69,Note: Ensure that these database connection properties aren’t commented out.
69,Save the file.
69,Restart the Content Services server.
69,Oracle database on Amazon RDS
69,"You can configure an Oracle database on Amazon RDS for use with Content Services, with the following prerequisites:"
69,Setup Amazon RDS using the AWS Management Console
69,Amazon EC2 instance
69,"The Oracle database is case sensitive, so any configuration setting that you add into the alfresco.global.properties file must match the case used in Oracle."
69,Use the ssh command to connect to the Amazon EC2 instance using a provided .ppk key.
69,"For Amazon Linux, the user name is ec2-user."
69,"For RHEL5, the user name is either root or ec2-user."
69,"For Ubuntu, the user name is ubuntu. For SUSE Linux, the user name is root."
69,Execute sudo su to change to root.
69,Install Content Services using one of the options provided.
69,Install the Oracle database connector to allow the database to talk to the server.
69,Download ojdbc7.jar from the Oracle download site.
69,Copy the JAR file into the /lib directory.
69,"For example, for Tomcat, copy the JAR file into the <TOMCAT_HOME>/lib directory."
69,Install and use a database tool to connect to the Amazon RDS.
69,Increase the available connections.
69,"In the SQL*Plus Console, run these commands:"
69,alter system set processes=275 scope=spfile sid='*';
69,alter system set sessions=305 scope=spfile sid='*';
69,alter system set transactions=330 scope=spfile sid='*';
69,Restart the database.
69,Create a database named alfresco.
69,Create a user named alfresco.
69,Set the new user’s password to alfresco.
69,Open the <classpathRoot>/alfresco-global.properties.sample file.
69,Edit the following line with an absolute path to point to the directory in which you want to store Content Services data. For example:
69,dir.root=C:/Alfresco/alf_data
69,Set and uncomment the database connection properties as shown below:
69,db.name=alfresco
69,db.username=alfresco
69,db.password=alfresco
69,db.host=alfrescoora12.cw4mo3qj8qdu.us-east-1.rds.amazonaws.com
69,db.port=1433
69,db.pool.max=275
69,db.txn.isolation=4096
69,# Oracle database connection
69,db.driver=oracle.jdbc.OracleDriver
69,db.url=jdbc:oracle:thin:@${db.host}:${db.port}:${db.name}
69,Note: Ensure that these database connection properties aren’t commented out.
69,Save the file without the .sample extension.
69,Restart the Content Services server.
69,PostgreSQL database on Amazon RDS
69,"You can configure a PostgreSQL database on Amazon RDS for use with Content Services, with the following prerequisites:"
69,Setup Amazon RDS using the AWS Management Console
69,Amazon EC2 instance
69,To configure the database:
69,Use the ssh command to connect to the Amazon EC2 instance using a provided .ppk key.
69,"For Amazon Linux, the user name is ec2-user."
69,"For RHEL5, the user name is either root or ec2-user."
69,"For Ubuntu, the user name is ubuntu. For SUSE Linux, the user name is root."
69,Execute sudo su to change to root.
69,Install Content Services using one of the options provided.
69,Install the PostgreSQL database connector to allow the database to talk to the server.
69,Download postgresql-42.x.jar from the PostgreSQL download site.
69,Copy the JAR file into the /lib directory.
69,"For example, for Tomcat, copy the JAR file into the <TOMCAT_HOME>/lib directory."
69,Install and use a database tool to connect to the Amazon RDS PostgreSQL datasource.
69,If Content Services is installed as standard with no configuration then psql from the installation folder can be used.
69,Create a database named alfresco.
69,Create a user named alfresco.
69,This user must have write permissions on all tables and sequences.
69,Set the new user’s password to alfresco.
69,Open the <classpathRoot>/alfresco-global.properties file.
69,Locate the line: dir.root=./alf_data
69,Edit this to set an absolute path to point to the directory in which you want to store Content Services data. For example:
69,dir.root=C:/Alfresco/alf_data
69,Uncomment and set the database connection properties.
69,db.name=alfresco
69,db.username=alfresco
69,db.password=alfresco
69,db.host=postgressql-alfresco.cw4mo3qj8qdu.us-east-1.rds.amazonaws.com
69,db.port=5432
69,db.pool.max=275
69,# PostgreSQL connection (requires postgresql-8.2-504.jdbc3.jar or equivalent)
69,db.driver=org.postgresql.Driver
69,db.url=jdbc:postgresql://${db.host}:${db.port}/${db.name}
69,Note: Ensure that these database connection properties aren’t commented out.
69,Save the file.
69,Restart the Content Services server.
69,SQL Server database on Amazon RDS
69,"You can configure a SQL Server database on Amazon RDS for use with Content Services, with the following prerequisites:"
69,Setup Amazon RDS using the AWS Management Console.
69,Amazon EC2 instance
69,To configure the database:
69,Use the ssh command to connect to the Amazon EC2 instance using a provided .ppk key.
69,"For Amazon Linux, the user name is ec2-user."
69,"For RHEL5, the user name is either root or ec2-user."
69,"For Ubuntu, the user name is ubuntu. For SUSE Linux, the user name is root."
69,Execute sudo su to change to root.
69,Install Content Services using one of the options provided.
69,Install the Microsoft SQL Server database connector to allow the database to talk to the server.
69,Check the Supported platforms page for the correct driver version.
69,Download sqljdbc4.jar from the Microsoft SQL Server download site.
69,Copy the JDBC driver into the <TOMCAT_HOME>/lib directory.
69,Install and use a database tool to connect to the Amazon RDS.
69,Create a database named alfresco.
69,Enable snapshot isolation mode with the following command:
69,ALTER DATABASE alfresco SET ALLOW_SNAPSHOT_ISOLATION ON;
69,Create a user named alfresco.
69,Set the new user’s password to alfresco.
69,Open the <classpathRoot>/alfresco-global.properties file.
69,Locate the property: dir.root=
69,Edit this to set an absolute path to point to the directory in which you want to store Content Services data. For example:
69,dir.root=C:/Alfresco/alf_data
69,Set and uncomment the database connection properties as shown below:
69,db.name=alfresco
69,db.username=alfresco
69,db.password=alfresco
69,db.host=sql-alfresco.cw4mo3qj8qdu.us-east-1.rds.amazonaws.com
69,db.port=1433
69,db.pool.max=275
69,db.txn.isolation=4096
69,# SQL Server connection
69,db.driver=com.microsoft.sqlserver.jdbc.SQLServerDriver
69,db.url=jdbc:sqlserver://${db.host}:${db.port};databaseName=${db.name}
69,Save the file.
69,Restart the Content Services server.
69,MySQL and MariaDB
69,You can configure a MySQL or MariaDB database connection (with a MySQL JDBC driver) for use with Content Services.
69,Install the MySQL database connector to allow the database to talk to the Content Services server.
69,"The connector is a JAR file, for example, mysql-connector-java-5.x.x."
69,Check the Supported platforms page for the correct driver version.
69,Download the database connector from the MySQL site.
69,Copy the JAR file into the /lib directory.
69,"For example, for Tomcat, copy the JAR file into the <TOMCAT_HOME>/lib directory."
69,Create a database named alfresco.
69,"If you’re using MySQL and require the use of non-US-ASCII characters, you need to set the encoding for internationalization. This allows you to store content with accents in the repository. The database must be created with the UTF-8 character set and the utf8_bin collation. Although MySQL is a unicode database, and Unicode strings in Java, the JDBC driver might corrupt your non-English data. Ensure that you keep the ?useUnicode=yes&characterEncoding=UTF-8 parameters at the end of the JDBC URL."
69,Note: You also must ensure that the MySQL database is set to use UTF-8 and InnoDB. See Optimizing MySQL for more information.
69,Increase the maximum connections setting in the MySQL configuration file.
69,"Locate the configuration file, for example:"
69,Linux: /etc/my.cnf
69,Windows: c:\Users\All Users\MySQL\MySQL Server 5.x\my.ini
69,"In the mysqld section, add or edit the max_connections property:"
69,max_connections = 275
69,Restart the database.
69,Create a user named alfresco.
69,Set the new user’s password to alfresco.
69,Navigate to the <ALFRESCO_HOME>/alf_data/ directory and empty the <contentstore> directory.
69,"This is because the contentstore must be consistent with the database. Step 2 created an empty database, and so the contentstore must also be empty."
69,Open the <classpathRoot>/alfresco-global.properties.sample file.
69,Edit the following line with an absolute path to point to the directory in which you want to store Content Services data. For example:
69,dir.root=C:/Alfresco/alf_data
69,Uncomment the following properties:
69,db.driver=com.mysql.jdbc.Driver
69,db.url=jdbc:mysql://${db.host}:${db.port}/${db.name}?useUnicode=yes&characterEncoding=UTF-8
69,Set the other database connection properties.
69,db.name=alfresco
69,db.username=alfresco
69,db.password=alfresco
69,db.host=localhost
69,db.port=3306
69,db.pool.max=275
69,Note: Ensure that these database connection properties aren’t commented out.
69,Copy the keystore directory from the alf_data directory in the old location to the alf_data directory in the new location (specified in step 7).
69,(Optional) Enable case sensitivity.
69,"The default, and ideal, database setting for Content Services is to be case-insensitive. For example, the user name properties in the <configRoot>/classes/alfresco/repository.properties file are:"
69,# Are user names case sensitive?
69,user.name.caseSensitive=false
69,domain.name.caseSensitive=false
69,domain.separator=
69,"If your preference is to set the database to be case-sensitive, add the following line to the alfresco-global.properties file:"
69,user.name.caseSensitive=true
69,Save the file without the .sample extension.
69,Restart the Content Services server.
69,"If you receive JDBC errors, ensure the location of the MySQL JDBC drivers are on the system path, or add them to the relevant lib directory of the application server."
69,Optimize MySQL
69,There are some settings that are required to optimize MySQL to work with Content Services. The following table represents the specific settings in the MySQL configuration wizard that enable MySQL to work effectively.
69,Configuration wizard dialog
69,Setting
69,Server Type
69,Choose Dedicated MySQL Server Machine. The option selected determines the memory allocation.
69,Database usage
69,Choose Transactional Database Only. This creates a database that uses InnoDB as its storage engine.
69,InnoDB Tablespace
69,Accept the default drive and path.
69,Concurrent Connections
69,Select Decision Support (DSS) OLAP. This sets the approximate number of concurrent connections to the server.
69,Networking and Strict Mode Options
69,"Accept the default networking options (Enable TCP/IP Networking, Port Number 3306), and the default server SQL mode (Enable Strict Mode)."
69,Character Set
69,Select Best Support for Multilingualism. This sets the default character set to be UTF-8 (set in character-set-server).
69,Security Options
69,"Select Modify Security Settings. Type the root password admin, then retype the password."
69,"By default, table aliases are case sensitive on Unix but not on Windows or Mac OS X."
69,Use the following variable setting to enable MySQL server to handle case sensitivity of database and table names:
69,lower_case_table_names=1
69,Using this variable setting allows MySQL to convert all table names to lowercase on storage and lookup. This behavior also applies to database names and table aliases. This setting also prevents data transfer problems between platforms and between file systems with varying case sensitivity.
69,See the MySQL site for more information on this variable.
69,Oracle
69,"You can configure an Oracle RDBMS database for use with Content Services. The Oracle database is case sensitive, so any configuration setting that you add into alfresco-global.properties must match the case used in Oracle."
69,Note: The Oracle database must be created with the AL32UTF8 character set.
69,"Note: Alfresco supports RAC as a single instance Oracle database as the customers will benefit from high availability and resiliency. As Content Services requires a sequentially ordered transaction ID, customers will not see a performance improvement from deploying on Oracle RAC."
69,Note: The Oracle Thin driver is recommended. Check the Supported platforms page for the correct driver.
69,Create a database named alfresco.
69,Create a user named alfresco.
69,"This user must have Connect and Resource privileges in Oracle, and write permissions on all tables and sequences."
69,Set the new user’s password to alfresco.
69,Ensure the alfresco user has the required privileges to create and modify tables.
69,"You can remove these privileges once the server has started, but they might also be required for upgrades."
69,"Note: When connecting to Oracle database 12c, you must configure privileges on tablespace USERS to avoid the following error:"
69,ORA-01950: no privileges on tablespace 'USERS'
69,You can do this by using one of the following commands:
69,ALTER USER <username> QUOTA <QUOTE_M> ON <tablespace name>
69,GRANT UNLIMITED TABLESPACE TO <username>
69,Open the <classpathRoot>/alfresco-global.properties.sample file.
69,Locate the line: dir.root=./alf_data
69,Edit the line with an absolute path to point to the directory in which you want to store Alfresco data. For example:
69,dir.root=C:/Alfresco/alf_data
69,Set and uncomment the Oracle database connection properties:
69,db.name=alfresco
69,db.username=alfresco
69,db.password=alfresco
69,db.host=localhost
69,db.port=1521
69,db.pool.max=275
69,# Oracle connection
69,db.driver=oracle.jdbc.OracleDriver
69,db.url= jdbc:oracle:thin:@${db.host}:${db.port}:${db.name}
69,"If you’re using the oci configuration, change the URL syntax:"
69,db.url=jdbc:oracle:oci:@${db.host}:${db.port}:${db.name}
69,The Oracle connection URL in this example is basic. Typical Oracle connection strings can be used with the Oracle driver (Thin/Oracle Call Interface (OCI)). The Thin driver is recommended over the OCI driver.
69,"For database URLs and specifiers, see the Oracle documentation:"
69,Database URLs and Database Specifiers
69,Thin-style Service Name Syntax
69,"You can use standard (OCI/Thin) connection URL, Oracle service, and Oracle DNS service URL without any issues."
69,"Note: If you’re using the OCI URL, you need an Oracle client on the Alfresco host. For more information, see OCI Instant Client."
69,Save the file without the .sample extension.
69,Copy the Oracle JDBC driver JAR into /lib.
69,"CAUTION: Don’t put multiple driver JARs in the application or the application server lib directory. Only include the driver JAR that’s advised in these instructions. Remove any others, if present."
69,Restart the Alfresco server.
69,Note: If you receive JDBC errors:
69,Ensure the location of the Oracle JDBC driver is on the system path or added to the relevant lib directory of the application server.
69,"Check if you have LD_LIBRARY_PATH in use in your environment to remove the old Oracle client (for example, /home/oracle/app/oracle/product/11.2.0/client_1/lib) and add the full path to the current ojdbc7.jar. If you don’t have this environment variable, don’t add it."
69,"Note: The JDBC driver for Oracle is in the JAR file: ojdbc7.jar. However, if you see the following error, then add the Doracle.jdbc.thinLogonCapability=o3 parameter to JAVA_OPTS:"
69,java.sql.SQLException: OAUTH marshaling failure
69,PostgreSQL
69,You can configure a PostgreSQL database for use with Content Services.
69,Install the PostgreSQL database connector to allow the database to talk to the Content Services server.
69,"The database connector is a JAR file, for example, postgresql-x.x.jar."
69,Check the Supported platforms page for the correct driver.
69,Download the latest database connector from the PostgreSQL download site.
69,Copy the JAR file into the /lib directory.
69,"For example, for Tomcat, copy the JAR file into the <TOMCAT_HOME>/lib directory."
69,Increase the maximum connections setting in the PostgreSQL configuration file.
69,Locate the configuration file:
69,Linux: /var/lib/pgsql/<version-of-postgresql\>/data/postgresql.conf
69,Windows: C:\Program Files\PostgreSQL\<version-of-postgresql>\data\postgresql.conf
69,Add or edit the max_connections property:
69,max_connections = 275
69,Restart the database.
69,Create a database named alfresco.
69,Create a user named alfresco.
69,This user must have write permissions on all tables and sequences.
69,Set the new user’s password to alfresco.
69,Ensure the alfresco user has the required privileges to create and modify tables.
69,Open the <classpathRoot>/alfresco-global.properties.sample file.
69,Locate the line: dir.root=./alf_data
69,Edit the line with an absolute path to point to the directory in which you want to store Content Services data. For example:
69,dir.root=C:/Alfresco/alf_data
69,Uncomment the following properties:
69,# PostgreSQL connection (requires postgresql-8.2-504.jdbc3.jar or equivalent)
69,db.driver=org.postgresql.Driver
69,db.url=jdbc:postgresql://${db.host}:${db.port}/${db.name}
69,Set the other database connection properties.
69,db.name=alfresco
69,db.username=alfresco
69,db.password=alfresco
69,db.host=localhost
69,db.port=5432
69,db.pool.max=275
69,Note: Ensure that these database connection properties aren’t commented out.
69,Save the file without the .sample extension.
69,"To allow password-authenticated connections through TCP/IP, ensure that the PostgreSQL configuration file, pg_hba.conf, contains the following line:"
69,host all all `127.0.0.1/32` password
69,Restart the Content Services server.
69,"If you receive JDBC errors, ensure the location of the PostgreSQL JDBC drivers are on the system path, or add them to the relevant lib directory of the application server."
69,Microsoft SQL Server
69,"You can configure a Microsoft SQL Server database for use with Content Services. To modify the default database configuration, you must edit values in the <classpathRoot>/alfresco-global.properties file."
69,Install the Microsoft SQL Server database connector to allow the database to talk to the Content Services server.
69,Check the Supported platforms page for the correct driver version.
69,Download the JDBC driver from the Microsoft SQL Server download site.
69,Copy the JDBC driver into the <TOMCAT_HOME>/lib directory.
69,Increase the available connections setting in the Microsoft SQL Server configuration file.
69,Follow the instructions in Configuring the user connections option to update the setting.
69,Create a database named alfresco.
69,Create the database using default collation settings.
69,Create a user named alfresco.
69,"This user must have write permissions on all tables and sequences. For example, you can provide these permissions by granting your database user (in this case, the alfresco user) the db_owner role."
69,See Database-Level Roles{target=”_blank”} for more information.
69,Set the new user’s password to alfresco.
69,Ensure the alfresco user has the required privileges to create and modify tables.
69,"This can be removed once the server has started, but may be required during upgrades."
69,Enable snapshot isolation mode with the following command:
69,ALTER DATABASE alfresco SET ALLOW_SNAPSHOT_ISOLATION ON;
69,Ensure that the TCP connectivity is enabled on the fixed port number 1433.
69,Open the <classpathRoot>/alfresco-global.properties.sample file.
69,Locate the property: dir.root=
69,Edit the line with an absolute path to point to the directory in which you want to store Content Services data. For example:
69,dir.root=C:/Alfresco/alf_data
69,Set the database connection properties:
69,db.name=alfresco
69,db.username=alfresco
69,db.password=alfresco
69,db.host=localhost
69,db.port=1433
69,db.pool.max=275
69,Add the following properties to register the driver and set up the connection:
69,db.driver=com.microsoft.sqlserver.jdbc.SQLServerDriver
69,db.url=jdbc:sqlserver://${db.host}:${db.port};databaseName=${db.name};lockTimeout=1000;
69,db.txn.isolation=4096
69,Save the file without the .sample extension.
69,Restart the Content Services server.
69,"If you receive JDBC errors, ensure the location of the SQL Server JDBC drivers are on the system path, or add them to the relevant lib directory of the application server."
69,Optimize Microsoft SQL Server
69,Make sure you manage Microsoft SQL Server to optimize performance.
69,"To ensure that your performance doesn’t degrade, it’s useful to carry out the following weekly maintenance operations on your SQL server, especially in repositories with a high transaction count and frequency:"
69,Recompute statistics by running the command:
69,EXEC sp_updatestats
69,Clear the buffers by running the command:
69,DBCC DROPCLEANBUFFERS
69,Clear the cache by running the command:
69,DBCC FREEPROCCACHE
69,Run an index fragmentation check and also:
69,Rebuild anything that’s >30% fragmented
69,Reorganize anything that’s between 5% and 30% fragmented
69,See Reorganize and rebuild indexes for more information.
69,Advanced configuration properties
69,"As an administrator, you need to edit some advanced properties to customize your database configuration. Many properties, however, don’t need to be edited."
69,The advanced database configuration properties are categorized into two groups based on their relevance:
69,properties that you SHOULD edit
69,properties that you COULD edit
69,The following table describes the properties that you SHOULD edit:
69,Property
69,Description
69,db.txn.isolation
69,"The JDBC code number for the transaction isolation level, corresponding to those in the java.sql.Connection class. The value of -1 indicates that the database’s default transaction isolation level should be used. For the Microsoft SQL Server JDBC driver, the special value of 4096 should be used to enable snapshot isolation. The default value is -1"
69,db.pool.initial
69,The number of connections opened when the pool is initialized. The default value is 10
69,db.pool.validate.query
69,"The SQL query that is used to ensure that your connections are still alive. This is useful if your database closes long-running connections after periods of inactivity.For Oracle database, use `SELECT 1 from dual`For MySQL database, use `SELECT 1`For SQL Server database, use `SELECT 1`For PostgreSQL database, use `SELECT 1`"
69,The following table describes the properties that you COULD edit:
69,Property
69,Description
69,db.pool.statements.enable
69,A Boolean property. When set to true it indicates that all pre-compiled statements used on a connection will be kept open and cached for reuse. The default value is true
69,db.pool.statements.max
69,The maximum number of pre-compiled statements to cache for each connection. Note that Oracle doesn’t allow more that 50 by default. The default value is 40
69,db.pool.idle
69,The maximum number of connections that aren’t in use but kept open. The default value is 10
69,db.pool.max
69,The maximum number of connections in the pool. See the note below for more information on this property. The default value is 275
69,db.pool.min
69,The minimum number of connections in the pool. The default value is 10
69,db.pool.wait.max
69,Time (in milliseconds) to wait for a connection to be returned before generating an exception when connections are unavailable. A value of 0 or -1 indicates that the exception shouldn’t be generated. The default value is 5000
69,db.pool.validate.borrow
69,A Boolean property. When set to true it indicates that connections will be validated before being borrowed from the pool. The default value is true
69,db.pool.validate.return
69,A Boolean property. When set to true it indicates that connections will be validated before being returned to the pool. The default value is false
69,db.pool.evict.interval
69,"Indicates the interval (in milliseconds) between eviction runs. If the value of this property is zero or less, idle objects won’t be evicted in the background. The default value is 600000"
69,db.pool.evict.idle.min
69,The minimum number of milliseconds that a connection may remain idle before it’s eligible for eviction. The default value is 1800000
69,db.pool.evict.validate
69,A Boolean property. When set to true it indicates that the idle connections will be validated during eviction runs. The default value is false
69,db.pool.abandoned.detect
69,A Boolean property. When set to true it indicates that a connection is considered abandoned and eligible for removal if it’s been idle longer than the db.pool.abandoned.time. The default value is false
69,db.pool.abandoned.time
69,The time in seconds before an abandoned connection can be removed. The default value is 300
69,"The db.pool.max property is the most important. By default, each Content Services instance is configured to use up to a maximum of 275. All operations require a database connection, which places an upper limit on the amount of concurrent requests a single instance can service from all protocols."
69,"Most Java application servers have higher default settings for concurrent access (Tomcat allows up to 200 concurrent HTTP requests by default). Coupled with other threads in Content Services (non-HTTP protocol threads, background jobs, and so on) this can quickly result in excessive contention for database connections, manifesting as poor performance for users."
69,"If you’re using Content Services in anything other than a single-user evaluation mode, increase the maximum size of the database connection pool to at least the following setting."
69,[number of application server worker threads] + 75
69,"For a Tomcat default HTTP worker thread configuration, and with all other thread pools left at the defaults, this means this property should be set to at least 275."
69,"To increase the database connection pool, add the db.pool.max property to the alfresco.global.properties file, and set it to the recommended value of 275, for example:"
69,db.pool.max=275
69,"For clarity, add this property immediately after the other database properties."
69,"Important: After increasing the size of the database connection pools, you must also increase the number of concurrent connections your database can handle to at least the size of the cumulative connection pools. In a cluster, each node has its own independent database connection pool. You must configure sufficient database connections for all of the cluster nodes to be able to connect simultaneously. We recommend that you configure at least 10 more connections to the database than are configured cumulatively across all of the connection pools to ensure that you can still connect to the database, even if Content Services saturates its own connection pools. Remember to factor in cluster nodes (which can each use up to 275 database connections) as well as connections required by other applications that are using the same database server as Content Services."
69,The precise mechanism for reconfiguring your database’s connection limit depends on the relational database product you’re using. Contact your DBA for configuration details.
69,Validate your database
69,Validate your database to ensure that it meets the prerequisites for a Content Services installation.
69,"Note: We’re unable to provide specialized support for maintaining or tuning your relational database. You MUST have an experienced, certified DBA on staff to support your installation(s). Typically this is not a full time role once the database is configured and tuned, and automated maintenance processes are in place. However, an experienced, certified DBA is required to get to this point."
69,Maintenance and tuning
69,"As with any application that uses a relational database, regular maintenance and tuning of the database and schema is necessary. Specifically, all of the database servers that Content Services supports require a minimum level of index statistics maintenance at frequent, regular intervals. Unless your DBA suggests otherwise, Alfresco recommends daily maintenance."
69,"Note: Relying on your database’s automated statistics gathering mechanism might not be optimal – consult an experienced, certified DBA for your database to confirm this."
69,"Note: Index maintenance on most databases is an expensive, and in some cases, blocking operation that can severely impact performance while in progress. Consult your experienced, certified DBA regarding best practices for scheduling these operations in your database."
69,The following table describes example commands for specific databases. These commands are for illustration only. You must validate the commands required for your environment with your DBA.
69,Database
69,Example maintenance commands
69,MySQL
69,"ANALYZE (ANALYZE TABLE Statement)Consult with an experienced, certified MySQL DBA who has InnoDB experience (Content Services can’t use a MyISAM database and hence an InnoDB-experienced MySQL DBA is required)."
69,PostgreSQL
69,"VACUUM and ANALYZE (Routine Database Maintenance Tasks)Consult with an experienced, certified PostgreSQL DBA."
69,Oracle
69,"See Database Performance Tuning Guide (depending on version)Consult with an experienced, certified Oracle DBA."
69,Microsoft SQL Server
69,"ALTER INDEX REBUILD (Transact-SQL)UPDATE STATISTICS (Transact-SQL)Consult with an experienced, certified MS SQL Server DBA."
69,Edit this page
69,Suggest an edit on GitHub
69,Additional resources
69,Alfresco Forums
69,Alfresco University
69,Legacy documentation
69,Support resources
69,Support Handbook
69,Customer Support Portal
69,"© 2021 Alfresco Software, Inc. All Rights Reserved."
69,Legal
69,Cookies
69,Privacy
69,CCPA
69,Terms
69,This website uses cookies in order to offer you the most relevant information. Please accept cookies for optimal performance. This documentation is subject to the Alfresco documentation terms.Accept cookies
70,Babelfish for Aurora PostgreSQL - Steve Stedman
70,↓ Skip to Main Content
70,Steve Stedman
70,Main Navigation
70,Menu
70,HomeAbout Steve StedmanBooksLatest 100 PostsOther SitesContact SteveSpeakingCurrent AbstractsSpeaker Review FormSQL Saturday 212 in Redmond WASQL Saturday 198 in Vancouver BCSQL Saturday 166 OlympiaSQL Saturday 172 Portland OregonSeattle Code Camp 2012SQL Saturday 114 in Vancouver BCSQL Saturday 108 in Redmond WASQL Server ConsultingMy ResumeNewsletterRemote DBATraining ClassesCommon Table ExpressionsIntroduction to SQL ServerIntroduction to SQL Server Reporting Services (SSRS)New Analytic Functions in SQL Server 2012Performance Tuning for DBA’sSQL Server Performance for DevelopersWhat’s New in TSQL 2012Virtual SQL ClassesCommon Table Expressions (CTE)ExamsIndex ExamPossible Topics For Virtual ClassesTSQL Basics: SQL Server Join TypesSQL Server HealthDatabase Corruption WorksheetSQL Server Performance TuningSQL Server Performance Tuning TipsDatabase Corruption ChallengeCurrent ScoresWeek 1 Database Corruption ChallengeWeek 2 Database Corruption ChallengeWeek 2 Challenge DetailsWeek 2 Corrupt DatabaseWeek 3 Database Corruption ChallengeWeek 3 Challenge DetailsWeek 3 Corrupt DatabaseWeek 4 Database Corruption ChallengeWeek 4 Challenge DetailsWeek 4 Extra ClueWeek 4 Corrupt DatabaseWeek 5 Database Corruption ChallengeWeek 5 Challenge DetailsWeek 5 Extra CluesWeek 5 Corrupt DatabaseWeek 6 Database Corruption ChallengeWeek 6 Challenge DetailsWeek 6 Extra ClueWeek 6 Corrupt DatabaseWeek 7 Database Corruption ChallengeWeek 7 Challenge DetailsWeek 7 Extra ClueWeek 8 Database Corruption ChallengeWeek 8 – Extra ClueWeek 9 Database Corruption ChallengeWeek 10 Database Corruption ChallengeIndex FragmentationTop 20 SQL Statements by Cache SizeLatest SQL Server UpdatesVideo
70,Home › SQL Server › Babelfish for Aurora PostgreSQL
70,Babelfish for Aurora PostgreSQL
70,SteveStedman
70,"Posted on December 1, 2020"
70,Posted in SQL Server
70,No Comments
70,"Tagged with , SQL Server"
70,Amazon goes after Microsoft’s SQL Server with Babelfish for Aurora PostgreSQL.
70,"What is interesting is that a direct port of SQL Server to PostgreSQL or Aurora PostgreSQL can be challenging due to incompatibilities in data types, and in functions."
70,What is promised in Babelfish is that it will create a layer between standard SQL Server TSQL and PostgreSQL to be that translation layer similar in concept to the fictional Babelfish translator from the Hitchhikers Guide to the Galaxy book.
70,"Babelfish provides translations for the SQL language TSQL, but also SQL commands, cursors, catalog views, data types, triggers, stored procedures, and functions."
70,My primary concerns would be around compatibility and performance.
70,"Compatibility: There is the discussion around this on the concept of correctness where they would behave the same on PostgreSQL as they would on SQL Server. But there are also statements that it is designed to fail if using something specific to SQL Server that is not compatible on PostgreSQL, with the point that it would fail to let the developer know it is not supported rather than defaulting to some possibly incorrect behavior."
70,"Performance: Given that SQL Server has amazing performance, and many amazing performance features built in, and I am sure that PostgreSQL has good performance. How well will things perform when being processed on one platform and then being translated to another format for data delivery? Will we have access to execution plans that get translated from SQL Server TSQL to PostgreSQL? If so that would be really cool."
70,"More on performance. Where to we start on performance tuning on PostgreSQL? That is not something I have done yet, I am sure there are tools to help, but are they as mature as the tools and 3rd party products on SQL Server to help with finding the performance issues."
70,"Babelfish is said to be launching in 2021, which means we will soon have a chance to give it a try."
70,"I am curious to find out what it would take to translate, for instance can you run CREATE TABLE statements with SQL data types and have them translate to PostgreSQL types. For instance a UNIQUEIDENTIFIER column on SQL need to map to a CHAR(16) on PostgreSQL. I am curious how that and other data types would work."
70,Will SQL Server management studio work when connecting to PostgreSQL using Babelfish?
70,I guess it would be interesting to give it a try with a small application or some part of a larger application and see how it behaves.
70,What are your thoughts on this?
70,Steve and the team at Stedman Solutions are here for all your SQL Server needs.
70,Contact us today for your
70,free 30 minute consultation..
70,We are ready to help!
70,Post navigation
70,Previous Post is
70,‹ Sales To Aggressive?Next Post is TSQL Query to find the user the SQL services run as ›
70,Leave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment Name *
70,E-mail *
70,Website
70,Time limit is exhausted. Please reload CAPTCHA.
70,eight
70,Database Health Monitor Download the Version 2 of the Database Health Monitor. Make your job as a DBA easier and more productive.
70,"SQL Server Performance Tuning Need help with SQL Server Performance Tuning, contact Steve at Stedman Solutions, LLC for help, or take a look at the performance tuning page at the SteveStedman.com blog. We can help with tuning a single query or with figuring out why your SQL Server is running slow. Free 30 minute consultation."
70,"Newsletter Signup Stay informed of Database Corruption Challenge events, and other things happening at SteveStedman.com with my newsletter."
70,Newsletter signup form.
70,Search
70,Search for:
70,Poster Download Click to Download TsqlJoinTypes.pdf
70,Click to Download TsqlJoinTypes.pdf
70,Stedman Solutions
70,Corruption Worksheet Download the Corruption Worksheet
70,Tag Cloud70-461 Training
70,Agile
70,Analytic TSQL
70,Classes
70,Code Camp
70,Corruption
70,CTE
70,CTE Book
70,Database Health
70,DBA
70,DBCC Commands
70,DevTeach
70,Fail
70,JSON and XML month
70,LEAN
70,Other
70,PASS Chapter
70,PASS Summit
70,Performance
70,Performance Tuning
70,Podcast
70,Query Certification
70,Recovery
70,Replication
70,Server Health
70,SQL 2008
70,SQL 2012
70,SQL 2014
70,SQL 2017
70,SQL 2019
70,SQL Daily Checkup
70,SQL Saturday
70,SQL Server
70,SSMS
70,SSRS
70,Startup Weekend
70,Steve
70,Summit 13
70,Summit 2016
70,Technical Debt
70,TSQL
70,TSQL 2012
70,Uncategorized
70,Uzility
70,Video
70,Archives
70,April 2021 (4)
70,March 2021 (13)
70,February 2021 (19)
70,January 2021 (15)
70,December 2020 (1)
70,November 2020 (2)
70,October 2020 (1)
70,September 2020 (1)
70,August 2020 (5)
70,July 2020 (8)
70,June 2020 (30)
70,May 2020 (2)
70,April 2020 (5)
70,March 2020 (3)
70,February 2020 (4)
70,January 2020 (1)
70,December 2019 (2)
70,November 2019 (1)
70,October 2019 (2)
70,September 2019 (2)
70,August 2019 (6)
70,July 2019 (1)
70,June 2019 (1)
70,May 2019 (3)
70,April 2019 (1)
70,March 2019 (4)
70,February 2019 (8)
70,January 2019 (1)
70,December 2018 (1)
70,November 2018 (10)
70,October 2018 (2)
70,September 2018 (1)
70,August 2018 (3)
70,June 2018 (5)
70,May 2018 (2)
70,March 2018 (1)
70,February 2018 (2)
70,December 2017 (1)
70,November 2017 (3)
70,October 2017 (2)
70,September 2017 (4)
70,August 2017 (3)
70,July 2017 (1)
70,June 2017 (1)
70,May 2017 (1)
70,April 2017 (5)
70,March 2017 (1)
70,February 2017 (3)
70,January 2017 (7)
70,December 2016 (1)
70,November 2016 (8)
70,October 2016 (9)
70,September 2016 (6)
70,August 2016 (5)
70,July 2016 (5)
70,June 2016 (5)
70,May 2016 (5)
70,April 2016 (8)
70,March 2016 (7)
70,February 2016 (4)
70,January 2016 (3)
70,December 2015 (3)
70,November 2015 (2)
70,October 2015 (2)
70,September 2015 (13)
70,August 2015 (13)
70,July 2015 (19)
70,June 2015 (18)
70,May 2015 (30)
70,April 2015 (23)
70,March 2015 (35)
70,February 2015 (21)
70,January 2015 (5)
70,December 2014 (2)
70,November 2014 (4)
70,October 2014 (3)
70,September 2014 (1)
70,August 2014 (1)
70,July 2014 (3)
70,June 2014 (2)
70,May 2014 (3)
70,April 2014 (10)
70,January 2014 (1)
70,December 2013 (4)
70,November 2013 (11)
70,October 2013 (9)
70,September 2013 (9)
70,August 2013 (12)
70,July 2013 (7)
70,June 2013 (27)
70,May 2013 (27)
70,April 2013 (19)
70,March 2013 (3)
70,February 2013 (5)
70,January 2013 (3)
70,December 2012 (2)
70,November 2012 (18)
70,October 2012 (12)
70,September 2012 (4)
70,August 2012 (4)
70,July 2012 (2)
70,June 2012 (12)
70,May 2012 (3)
70,April 2012 (8)
70,March 2012 (25)
70,February 2012 (9)
70,January 2012 (8)
70,December 2011 (9)
70,November 2011 (15)
70,October 2011 (2)
70,September 2011 (1)
70,July 2011 (1)
70,June 2011 (1)
70,April 2011 (1)
70,March 2011 (1)
70,January 2011 (1)
70,September 2010 (1)
70,April 2010 (3)
70,February 2010 (1)
70,January 2010 (1)
70,November 2009 (3)
70,September 2009 (2)
70,May 2009 (5)
70,April 2009 (4)
70,March 2009 (4)
70,December 2008 (1)
70,November 2008 (2)
70,October 2008 (2)
70,September 2008 (4)
70,August 2008 (1)
70,May 2008 (1)
70,March 2008 (1)
70,August 2007 (1)
70,June 2007 (1)
70,May 2006 (1)
70,January 2006 (1)
70,April 2005 (1)
70,March 2005 (1)
70,February 2003 (1)
70,September 1999 (1)
70,June 1997 (1)
70,January 1992 (1)
70,February 1991 (1)
70,January 1991 (1)
70,December 1990 (1)
70,January 1982 (1)
70,Login
70,Log in
70,Entries feed
70,Comments feed
70,WordPress.org
70,2021
70,Steve Stedman
70,| Powered by
70,Responsive Theme
71,JSONB PostgreSQL: How to Store & Index JSON Data in Postgres
71,Toggle navigation
71,Products
71,Managed MySQL
71,Overview
71,AWS
71,Azure
71,DigitalOcean
71,Google Cloud
71,Managed PostgreSQL
71,Overview
71,AWS
71,Azure
71,DigitalOcean
71,Google Cloud
71,Managed Redis™
71,Overview
71,AWS
71,Azure
71,DigitalOcean
71,Google Cloud
71,Managed MongoDB®
71,Overview
71,AWS
71,Azure
71,DigitalOcean
71,Private/On-Prem
71,Pricing
71,Enterprise
71,Blog
71,Login
71,Sign Up
71,Using JSONB in PostgreSQL: How to Effectively Store & Index JSON Data in PostgreSQL
71,Posted:
71,"July 17, 2020"
71,"+1 Tweet Share Share PinShares 0JSON stands for JavaScript Object Notation. It is an open standard format which organizes data into key/value pairs and arrays detailed in RFC 7159. JSON is the most common format used by web services to exchange data, store documents, unstructured data, etc. In this post, we are going to show you tips and techniques on how to effectively store and index JSON data in PostgreSQL."
71,"You can also check out our Working with JSON Data in PostgreSQL vs. MongoDB webinar in partnership with PostgresConf to learn more on the topic, and check out our SlideShare page to download the slides."
71,What’s in this article?
71,Why Store JSON in PostgreSQL?
71,Timeline of JSON Support in PostgreSQL
71,JSONB Patterns & Antipatterns
71,JSONB Data Structures
71,JSONB Operators & Functions
71,JSONB Indexes
71,SQL/JSON & JSONPath
71,Why Store JSON in PostgreSQL?
71,Why should a relational database even care about unstructured data? It turns out that there are a few scenarios where it is useful.
71,Schema flexibility
71,"One of the main reasons to store data using the JSON format is schema flexibility. Storing your data in JSON is useful when your schema is fluid and is changing frequently. If you store each of the keys as columns, it will result in frequent DML operations – this can be difficult when your data set is large - for example, event tracking, analytics, tags, etc. Note: If a particular key is always present in your document, it might make sense to store it as a first class column. We discuss more about this approach in section “JSON Patterns & Antipatterns” below."
71,Nested objects
71,"If your data set has nested objects (single or multi-level), in some cases, it is easier to handle them in JSON instead of denormalizing the data into columns or multiple tables."
71,Syncing with external data sources
71,"Often times an external system is providing data as JSON, so it might be a temporary store before data is ingested into other parts of the system. For example, Stripe transactions."
71,Timeline of JSON Support in PostgreSQL
71,JSON support in PostgreSQL was introduced in 9.2 and has steadily improved in every release going forward.
71,Wave 1: PostgreSQL 9.2  (2012) added support for JSON data type
71,JSON database in 9.2 was fairly limited (and probably overhyped at that point) – basically a glorified string with some JSON validation thrown in. It is useful to validate incoming JSON and store in the database. More details are provided below.
71,Wave 2: PostgreSQL 9.4 (2014) added support for JSONB data type
71,"JSONB stands for “JSON Binary” or “JSON better” depending on whom you ask. It is a decomposed binary format to store JSON. JSONB supports indexing the JSON data, and is very efficient at parsing and querying the JSON data. In most cases, when you work with JSON in PostgreSQL, you should be using JSONB."
71,Wave 3: PostgreSQL 12 (2019) added support for SQL/JSON standard and JSONPATH queries
71,JSONPath brings a powerful JSON query engine to PostgreSQL.
71,When Should You Use JSON vs. JSONB?
71,"In most cases, JSONB is what you should be using. However, there are some specific cases where JSON works better:"
71,JSON preserves the original formatting (a.k.a whitespace) and ordering of the keys.
71,JSON preserves duplicate keys.
71,"JSON is faster to ingest vs. JSONB – however, if you do any further processing, JSONB will be faster."
71,"For example, if you’re just ingesting JSON logs and not querying them in any way, then JSON might be a better option for you. For the purposes of this blog, when we refer to JSON support in PostgreSQL, we will refer to JSONB going forward."
71,Using JSONB in PostgreSQL: How to Effectively Store & Index JSON Data in PostgreSQLClick To Tweet
71,JSONB Patterns & Antipatterns
71,"If PostgreSQL has great support for JSONB, why do we need columns anymore? Why not just create a table with a JSONB blob and get rid of all columns like the schema below:"
71,"CREATE TABLE test(id int, data JSONB, PRIMARY KEY (id));"
71,"At the end of the day, columns are still the most efficient technique to work with your data. JSONB storage has some drawbacks vs. traditional columns:"
71,PostreSQL does not store column statistics for JSONB columns
71,"PostgreSQL maintains statistics about the distributions of values in each column of the table - most common values (MCV), NULL entries, histogram of distribution. Based on this data, the PostgreSQL query planner makes smart decisions on the plan to use for the query. At this point, PostgreSQL does not store any stats for JSONB columns or keys. This can sometimes result in poor choices like using nested loop joins vs. hash joins, etc. A more detailed example of this is provided in this blog post – When To Avoid JSONB In A PostgreSQL Schema."
71,JSONB storage results in a larger storage footprint
71,"JSONB storage does not deduplicate the key names in the JSON. This can result in considerably larger storage footprint compared to MongoDB BSON on WiredTiger or traditional column storage. I ran a simple test with the below JSONB model storing about 10 million rows of data, and here are the results – In some ways this is similar to the MongoDB MMAPV1 storage model where the keys in JSONB were stored as-is without any compression. One long-term fix is to move the key names to a table level dictionary and refer this dictionary instead of storing the key names repeatedly. Until then, the workaround might be to use more compact names (unix-style) instead of more descriptive names. For example, if you’re storing millions of instances of a particular key, it would be better storage-wise to name it “pb” instead of “publisherName”."
71,"The most efficient way to leverage JSONB in PostgreSQL is to combine columns and JSONB. If a key appears very frequently in your JSONB blobs, it is probably better off being stored as a column. Use JSONB as a “catch all” to handle the variable parts of your schema while leveraging traditional columns for fields that are more stable."
71,JSONB Data Structures
71,"Both JSONB and MongoDB BSON are essentially tree structures, using multi-level nodes to store the parsed JSONB data. MongoDB BSON has a very similar structure."
71,Images source
71,JSONB & TOAST
71,"Another important consideration for storage is how JSONB interacts with TOAST (The Oversize Attribute Storage Technique). Typically, when the size of your column exceeds the TOAST_TUPLE_THRESHOLD (2kb default), PostgreSQL will attempt to compress the data and fit in 2kb. If that doesn’t work, the data is moved to out-of-line storage. This is what they call “TOASTing” the data. When the data is fetched, the reverse process “deTOASTting” needs to happen. You can also control the TOAST storage strategy:"
71,Extended – Allows for out-of-line storage and compression (using pglz). This is the default option.
71,"External – Allows for out-of-line storage, but not compression."
71,"If you’re experiencing delays due to the TOAST compression or decompression, one option is to proactively set the column storage to ‘EXTENDED’. For all of the details, please refer to this PostgreSQL doc."
71,JSONB Operators & Functions
71,PostgreSQL provides a variety of operators to work on JSONB. From the docs:
71,Operator
71,Description
71,"Get JSON array element (indexed from zero, negative integers count from the end)"
71,Get JSON object field by key
71,->>
71,Get JSON array element as text
71,->>
71,Get JSON object field as text
71,Get JSON object at the specified path
71,#>>
71,Get JSON object at the specified path as text
71,Does the left JSON value contain the right JSON path/value entries at the top level?
71,Are the left JSON path/value entries contained at the top level within the right JSON value?
71,Does the string exist as a top-level key within the JSON value?
71,Do any of these array strings exist as top-level keys?
71,Do all of these array strings exist as top-level keys?
71,Concatenate two jsonb values into a new jsonb value
71,Delete key/value pair or string element from left operand. Key/value pairs are matched based on their key value.
71,Delete multiple key/value pairs or string elements from left operand. Key/value pairs are matched based on their key value.
71,Delete the array element with specified index (Negative integers count from the end). Throws an error if top level container is not an array.
71,"Delete the field or element with specified path (for JSON arrays, negative integers count from the end)"
71,Does JSON path return any item for the specified JSON value?
71,"Returns the result of JSON path predicate check for the specified JSON value. Only the first item of the result is taken into account. If the result is not Boolean, then null is returned."
71,PostgreSQL also provides a variety of Creation Functions and Processing Functions to work with the JSONB data.
71,JSONB Indexes
71,"JSONB provides a wide array of options to index your JSON data. At a high-level, we are going to dig into 3 different types of indexes – GIN, BTREE and HASH. Not all index types support all operator classes, so planning is needed to design your indexes based on the type of operators and queries that you plan on using."
71,GIN Indexes
71,GIN stands for “Generalized Inverted indexes”. From the docs:
71,"“GIN is designed for handling cases where the items to be indexed are composite values, and the queries to be handled by the index need to search for element values that appear within the composite items. For example, the items could be documents, and the queries could be searches for documents containing specific words.”"
71,GIN supports two operator classes:
71,"jsonb_ops (default) – ?, ?|, ?&, @>, @@, @? [Index each key and value in the JSONB element]"
71,"jsonb_pathops – @>, @@, @? [Index only the values in the JSONB element]"
71,CREATE INDEX datagin ON books USING gin (data);
71,"Existence Operators (?, ?|, ?& )"
71,"These operators can be used to check for the existence of top-level keys in the JSONB. Let’s create a GIN index on the data JSONB column. For example, find all books that are available in braille. The JSON looks something like this:"
71,"""{""tags"": {""nk594127"": {""ik71786"": ""iv678771""}}, ""braille"": false, ""keywords"": [""abc"", ""kef"", ""keh""], ""hardcover"": true, ""publisher"": ""EfgdxUdvB0"", ""criticrating"": 1}"
71,demo=# select * from books where data ? 'braille';
71,id | author | isbn | rating | data
71,---------+-----------------+------------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------
71,------------------
71,"1000005 | XEI7xShT8bPu6H7 | 2kD5XJDZUF | 0 | {""tags"": {""nk455671"": {""ik937456"": ""iv506075""}}, ""braille"": true, ""keywords"": [""abc"", ""kef"", ""keh""], ""hardcover"": false, ""publisher"": ""zSfZIAjGGs"", """
71,"criticrating"": 4}"
71,.....
71,demo=# explain analyze select * from books where data ? 'braille';
71,QUERY PLAN
71,---------------------------------------------------------------------------------------------------------------------
71,Bitmap Heap Scan on books (cost=12.75..1005.25 rows=1000 width=158) (actual time=0.033..0.039 rows=15 loops=1)
71,Recheck Cond: (data ? 'braille'::text)
71,Heap Blocks: exact=2
71,-> Bitmap Index Scan on datagin (cost=0.00..12.50 rows=1000 width=0) (actual time=0.022..0.022 rows=15 loops=1)
71,Index Cond: (data ? 'braille'::text)
71,Planning Time: 0.102 ms
71,Execution Time: 0.067 ms
71,(7 rows)
71,"As you can see from the explain output, the GIN index that we created is being used for the search. What if we wanted to find books that were in braille or in hardcover?"
71,"demo=# explain analyze select * from books where data ?| array['braille','hardcover'];"
71,QUERY PLAN
71,---------------------------------------------------------------------------------------------------------------------
71,Bitmap Heap Scan on books (cost=16.75..1009.25 rows=1000 width=158) (actual time=0.029..0.035 rows=15 loops=1)
71,"Recheck Cond: (data ?| '{braille,hardcover}'::text[])"
71,Heap Blocks: exact=2
71,-> Bitmap Index Scan on datagin (cost=0.00..16.50 rows=1000 width=0) (actual time=0.023..0.023 rows=15 loops=1)
71,"Index Cond: (data ?| '{braille,hardcover}'::text[])"
71,Planning Time: 0.138 ms
71,Execution Time: 0.057 ms
71,(7 rows)
71,"The GIN index supports the “existence” operators only on “top-level” keys. If the key is not at the top level, then the index will not be used. It will result in a sequential scan:"
71,demo=# select * from books where data->'tags' ? 'nk455671';
71,id | author | isbn | rating | data
71,---------+-----------------+------------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------
71,------------------
71,"1000005 | XEI7xShT8bPu6H7 | 2kD5XJDZUF | 0 | {""tags"": {""nk455671"": {""ik937456"": ""iv506075""}}, ""braille"": true, ""keywords"": [""abc"", ""kef"", ""keh""], ""hardcover"": false, ""publisher"": ""zSfZIAjGGs"", """
71,"criticrating"": 4}"
71,"685122 | GWfuvKfQ1PCe1IL | jnyhYYcF66 | 3 | {""tags"": {""nk455671"": {""ik615925"": ""iv253423""}}, ""publisher"": ""b2NwVg7VY3"", ""criticrating"": 0}"
71,(2 rows)
71,demo=# explain analyze select * from books where data->'tags' ? 'nk455671';
71,QUERY PLAN
71,----------------------------------------------------------------------------------------------------------
71,Seq Scan on books (cost=0.00..38807.29 rows=1000 width=158) (actual time=0.018..270.641 rows=2 loops=1)
71,Filter: ((data -> 'tags'::text) ? 'nk455671'::text)
71,Rows Removed by Filter: 1000017
71,Planning Time: 0.078 ms
71,Execution Time: 270.728 ms
71,(5 rows)
71,The way to check for existence in nested docs is to use “expression indexes”. Let’s create an index on data->tags:
71,CREATE INDEX datatagsgin ON books USING gin (data->'tags');
71,demo=# select * from books where data->'tags' ? 'nk455671';
71,id | author | isbn | rating | data
71,---------+-----------------+------------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------
71,------------------
71,"1000005 | XEI7xShT8bPu6H7 | 2kD5XJDZUF | 0 | {""tags"": {""nk455671"": {""ik937456"": ""iv506075""}}, ""braille"": true, ""keywords"": [""abc"", ""kef"", ""keh""], ""hardcover"": false, ""publisher"": ""zSfZIAjGGs"", """
71,"criticrating"": 4}"
71,"685122 | GWfuvKfQ1PCe1IL | jnyhYYcF66 | 3 | {""tags"": {""nk455671"": {""ik615925"": ""iv253423""}}, ""publisher"": ""b2NwVg7VY3"", ""criticrating"": 0}"
71,(2 rows)
71,demo=# explain analyze select * from books where data->'tags' ? 'nk455671';
71,QUERY PLAN
71,------------------------------------------------------------------------------------------------------------------------
71,Bitmap Heap Scan on books (cost=12.75..1007.75 rows=1000 width=158) (actual time=0.031..0.035 rows=2 loops=1)
71,Recheck Cond: ((data ->'tags'::text) ? 'nk455671'::text)
71,Heap Blocks: exact=2
71,-> Bitmap Index Scan on datatagsgin (cost=0.00..12.50 rows=1000 width=0) (actual time=0.021..0.021 rows=2 loops=1)
71,Index Cond: ((data ->'tags'::text) ? 'nk455671'::text)
71,Planning Time: 0.098 ms
71,Execution Time: 0.061 ms
71,(7 rows)
71,Note: An alternative here is to use the @> operator:
71,"select * from books where data @> '{""tags"":{""nk455671"":{}}}'::jsonb;"
71,"However, this only works if the value is an object. So, if you’re unsure if the value is an object or a primitive value, it could lead to incorrect results."
71,"Path Operators @>, <@"
71,The “path” operator can be used for multi-level queries of your JSONB data. Let’s use it similar to the ? operator above:
71,"select * from books where data @> '{""braille"":true}'::jsonb;"
71,"demo=# explain analyze select * from books where data @> '{""braille"":true}'::jsonb;"
71,QUERY PLAN
71,---------------------------------------------------------------------------------------------------------------------
71,Bitmap Heap Scan on books (cost=16.75..1009.25 rows=1000 width=158) (actual time=0.040..0.048 rows=6 loops=1)
71,"Recheck Cond: (data @> '{""braille"": true}'::jsonb)"
71,Rows Removed by Index Recheck: 9
71,Heap Blocks: exact=2
71,-> Bitmap Index Scan on datagin (cost=0.00..16.50 rows=1000 width=0) (actual time=0.030..0.030 rows=15 loops=1)
71,"Index Cond: (data @> '{""braille"": true}'::jsonb)"
71,Planning Time: 0.100 ms
71,Execution Time: 0.076 ms
71,(8 rows)
71,The path operators support querying nested objects or top-level objects:
71,"demo=# select * from books where data @> '{""publisher"":""XlekfkLOtL""}'::jsonb;"
71,id | author | isbn | rating | data
71,-----+-----------------+------------+--------+-------------------------------------------------------------------------------------
71,"346 | uD3QOvHfJdxq2ez | KiAaIRu8QE | 1 | {""tags"": {""nk88"": {""ik37"": ""iv161""}}, ""publisher"": ""XlekfkLOtL"", ""criticrating"": 3}"
71,(1 row)
71,"demo=# explain analyze select * from books where data @> '{""publisher"":""XlekfkLOtL""}'::jsonb;"
71,QUERY PLAN
71,--------------------------------------------------------------------------------------------------------------------
71,Bitmap Heap Scan on books (cost=16.75..1009.25 rows=1000 width=158) (actual time=0.491..0.492 rows=1 loops=1)
71,"Recheck Cond: (data @> '{""publisher"": ""XlekfkLOtL""}'::jsonb)"
71,Heap Blocks: exact=1
71,-> Bitmap Index Scan on datagin (cost=0.00..16.50 rows=1000 width=0) (actual time=0.092..0.092 rows=1 loops=1)
71,"Index Cond: (data @> '{""publisher"": ""XlekfkLOtL""}'::jsonb)"
71,Planning Time: 0.090 ms
71,Execution Time: 0.523 ms
71,The queries can be multi-level as well:
71,"demo=# select * from books where data @> '{""tags"":{""nk455671"":{""ik937456"":""iv506075""}}}'::jsonb;"
71,id | author | isbn | rating | data
71,---------+-----------------+------------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------
71,------------------
71,"1000005 | XEI7xShT8bPu6H7 | 2kD5XJDZUF | 0 | {""tags"": {""nk455671"": {""ik937456"": ""iv506075""}}, ""braille"": true, ""keywords"": [""abc"", ""kef"", ""keh""], ""hardcover"": false, ""publisher"": ""zSfZIAjGGs"", """
71,"criticrating"": 4}"
71,(1 row)
71,GIN Index “pathops” Operator Class
71,"GIN also supports a “pathops” option to reduce the size of the GIN index. When you use the pathops option, the only operator support is the “@>” so you need to be careful with your queries. From the docs:"
71,"“The technical difference between a jsonb_ops and a jsonb_path_ops GIN index is that the former creates independent index items for each key and value in the data, while the latter creates index items only for each value in the data”"
71,You can create a GIN pathops index as follows:
71,CREATE INDEX dataginpathops ON books USING gin (data jsonb_path_ops);
71,"On my small dataset of 1 million books, you can see that the pathops GIN index is smaller – you should test with your dataset to understand the savings:"
71,public | dataginpathops | index | sgpostgres | books | 67 MB |
71,public | datatagsgin | index | sgpostgres | books | 84 MB |
71,Let’s rerun our query from before with the pathops index:
71,"demo=# select * from books where data @> '{""tags"":{""nk455671"":{""ik937456"":""iv506075""}}}'::jsonb;"
71,id | author | isbn | rating | data
71,---------+-----------------+------------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------
71,------------------
71,"1000005 | XEI7xShT8bPu6H7 | 2kD5XJDZUF | 0 | {""tags"": {""nk455671"": {""ik937456"": ""iv506075""}}, ""braille"": true, ""keywords"": [""abc"", ""kef"", ""keh""], ""hardcover"": false, ""publisher"": ""zSfZIAjGGs"", """
71,"criticrating"": 4}"
71,(1 row)
71,"demo=# explain select * from books where data @> '{""tags"":{""nk455671"":{""ik937456"":""iv506075""}}}'::jsonb;"
71,QUERY PLAN
71,-----------------------------------------------------------------------------------------
71,Bitmap Heap Scan on books (cost=12.75..1005.25 rows=1000 width=158)
71,"Recheck Cond: (data @> '{""tags"": {""nk455671"": {""ik937456"": ""iv506075""}}}'::jsonb)"
71,-> Bitmap Index Scan on dataginpathops (cost=0.00..12.50 rows=1000 width=0)
71,"Index Cond: (data @> '{""tags"": {""nk455671"": {""ik937456"": ""iv506075""}}}'::jsonb)"
71,(4 rows)
71,"However, as mentioned above, the “pathops” option does not support all of the scenarios that the default operator class supports. With a “pathops” GIN index, all these queries are not able to leverage the GIN index. To summarize, you have a smaller index but it supports a more limited use case."
71,select * from books where data ? 'tags'; => Sequential scan
71,"select * from books where data @> '{""tags"" :{}}'; => Sequential scan"
71,"select * from books where data @> '{""tags"" :{""k7888"":{}}}' => Sequential scan"
71,B-Tree indexes
71,"B-tree indexes are the most common index type in relational databases. However, if you index an entire JSONB column with a B-tree index, the only useful operators are “=”, <, <=, >, >=. Essentially, this can only be used for whole object comparisons, which has a very limited use case."
71,"A more common scenario is to use B-tree “expression indexes”. For a primer, refer here – Indexes on Expressions. B-tree expression indexes can support the common comparison operators ‘=’, ‘<’, ‘>’, ‘>=’, ‘<=’. As you might recall, GIN indexes don’t support these operators. Let’s consider the case when we want to retrieve all books with a data->criticrating > 4. So, you would build a query something like this:"
71,demo=# select * from books where data->'criticrating' > 4;
71,ERROR: operator does not exist: jsonb >= integer
71,LINE 1: select * from books where data->'criticrating'
71,>= 4;
71,HINT: No operator matches the given name and argument types. You might need to add explicit type casts.
71,"Well, that doesn’t work since the ‘->’ operator returns a JSONB type. So we need to use something like this:"
71,demo=# select * from books where (data->'criticrating')::int4 > 4;
71,"If you’re using a version prior to PostgreSQL 11, it gets more ugly. You need to first query as text and then cast it to integer:"
71,demo=# select * from books where (data->'criticrating')::int4 > 4;
71,"For expression indexes, the index needs to be an exact match with the query expression. So, our index would look something like this:"
71,demo=# CREATE INDEX criticrating ON books USING BTREE (((data->'criticrating')::int4));
71,CREATE INDEX
71,demo=# explain analyze select * from books where (data->'criticrating')::int4 = 3;
71,QUERY PLAN
71,----------------------------------------------------------------------------------------------------------------------------------
71,Index Scan using criticrating on books (cost=0.42..4626.93 rows=5000 width=158) (actual time=0.069..70.221 rows=199883 loops=1)
71,Index Cond: (((data -> 'criticrating'::text))::integer = 3)
71,Planning Time: 0.103 ms
71,Execution Time: 79.019 ms
71,(4 rows)
71,demo=# explain analyze select * from books where (data->'criticrating')::int4 = 3;
71,QUERY PLAN
71,----------------------------------------------------------------------------------------------------------------------------------
71,Index Scan using criticrating on books (cost=0.42..4626.93 rows=5000 width=158) (actual time=0.069..70.221 rows=199883 loops=1)
71,Index Cond: (((data -> 'criticrating'::text))::integer = 3)
71,Planning Time: 0.103 ms
71,Execution Time: 79.019 ms
71,(4 rows)
71,From above we can see that the BTREE index is being used as expected.
71,Hash Indexes
71,"If you are only interested in the ""="" operator, then Hash indexes become interesting. For example, consider the case when we are looking for a particular tag on a book. The element to be indexed can be a top level element or deeply nested."
71,E.g. tags->publisher = XlekfkLOtL
71,CREATE INDEX publisherhash ON books USING HASH ((data->'publisher'));
71,"Hash indexes also tend to be smaller in size than B-tree or GIN indexes. Of course, this ultimately depends on your data set."
71,demo=# select * from books where data->'publisher' = 'XlekfkLOtL'
71,demo-# ;
71,id | author | isbn | rating | data
71,-----+-----------------+------------+--------+-------------------------------------------------------------------------------------
71,"346 | uD3QOvHfJdxq2ez | KiAaIRu8QE | 1 | {""tags"": {""nk88"": {""ik37"": ""iv161""}}, ""publisher"": ""XlekfkLOtL"", ""criticrating"": 3}"
71,(1 row)
71,demo=# explain analyze select * from books where data->'publisher' = 'XlekfkLOtL';
71,QUERY PLAN
71,-----------------------------------------------------------------------------------------------------------------------
71,Index Scan using publisherhash on books (cost=0.00..2.02 rows=1 width=158) (actual time=0.016..0.017 rows=1 loops=1)
71,Index Cond: ((data -> 'publisher'::text) = 'XlekfkLOtL'::text)
71,Planning Time: 0.080 ms
71,Execution Time: 0.035 ms
71,(4 rows)
71,Special Mention: GIN Trigram Indexes
71,PostgreSQL supports string matching using trigram indexes. Trigram indexes work by breaking up text into trigrams. Trigrams are basically words broken up into sequences of 3 letters. More information can be found in the documentation. GIN indexes support the “gin_trgm_ops” class that can be used to index the data in JSONB. You can choose to use expression indexes to build the trigram index on a particular column.
71,CREATE EXTENSION pg_trgm;
71,CREATE INDEX publisher ON books USING GIN ((data->'publisher') gin_trgm_ops);
71,demo=# select * from books where data->'publisher' LIKE '%I0UB%';
71,id |
71,author
71,isbn
71,| rating |
71,data
71,----+-----------------+------------+--------+---------------------------------------------------------------------------------
71,4 | KiEk3xjqvTpmZeS | EYqXO9Nwmm |
71,"0 | {""tags"": {""nk3"": {""ik1"": ""iv1""}}, ""publisher"": ""MI0UBqZJDt"", ""criticrating"": 1}"
71,(1 row)
71,"As you can see in the query above, we can search for any arbitrary string occurring at any potion. Unlike the B-tree indexes, we are not restricted to left anchored expressions."
71,demo=# explain analyze select * from books where data->'publisher' LIKE '%I0UB%';
71,QUERY PLAN
71,--------------------------------------------------------------------------------------------------------------------
71,Bitmap Heap Scan on books
71,(cost=9.78..111.28 rows=100 width=158) (actual time=0.033..0.033 rows=1 loops=1)
71,Recheck Cond: ((data -> 'publisher'::text) ~~ '%I0UB%'::text)
71,Heap Blocks: exact=1
71,Bitmap Index Scan on publisher
71,(cost=0.00..9.75 rows=100 width=0) (actual time=0.025..0.025 rows=1 loops=1)
71,Index Cond: ((data -> 'publisher'::text) ~~ '%I0UB%'::text)
71,Planning Time: 0.213 ms
71,Execution Time: 0.058 ms
71,(7 rows)
71,Special Mention: GIN Array Indexes
71,"JSONB has great built-in support for indexing arrays. Let's consider an example of indexing an array of strings using a GIN index in the case when our JSONB data contains a ""keyword"" element and we would like to find rows with particular keywords:"
71,"{""tags"": {""nk780341"": {""ik397357"": ""iv632731""}}, ""keywords"": [""abc"", ""kef"", ""keh""], ""publisher"": ""fqaJuAdjP5"", ""criticrating"": 2}"
71,CREATE INDEX keywords ON books USING GIN ((data->'keywords') jsonb_path_ops);
71,"demo=# select * from books where data->'keywords' @> '[""abc"", ""keh""]'::jsonb;"
71,author
71,isbn
71,| rating |
71,data
71,---------+-----------------+------------+--------+-----------------------------------------------------------------------------------------------------------------------------------
71,1000003 | zEG406sLKQ2IU8O | viPdlu3DZm |
71,"4 | {""tags"": {""nk263020"": {""ik203820"": ""iv817928""}}, ""keywords"": [""abc"", ""kef"", ""keh""], ""publisher"": ""7NClevxuTM"", ""criticrating"": 2}"
71,1000004 | GCe9NypHYKDH4rD | so6TQDYzZ3 |
71,"4 | {""tags"": {""nk780341"": {""ik397357"": ""iv632731""}}, ""keywords"": [""abc"", ""kef"", ""keh""], ""publisher"": ""fqaJuAdjP5"", ""criticrating"": 2}"
71,(2 rows)
71,"demo=# explain analyze select * from books where data->'keywords' @> '[""abc"", ""keh""]'::jsonb;"
71,QUERY PLAN
71,---------------------------------------------------------------------------------------------------------------------
71,Bitmap Heap Scan on books
71,(cost=54.75..1049.75 rows=1000 width=158) (actual time=0.026..0.028 rows=2 loops=1)
71,"Recheck Cond: ((data -> 'keywords'::text) @> '[""abc"", ""keh""]'::jsonb)"
71,Heap Blocks: exact=1
71,Bitmap Index Scan on keywords
71,(cost=0.00..54.50 rows=1000 width=0) (actual time=0.014..0.014 rows=2 loops=1)
71,"Index Cond: ((data -> 'keywords'::text) @&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;gt; '[""abc"", ""keh""]'::jsonb)"
71,Planning Time: 0.131 ms
71,Execution Time: 0.063 ms
71,(7 rows)
71,"The order of the items in the array on the right does not matter. For example, the following query would return the same result as the previous:"
71,"demo=# explain analyze select * from books where data->'keywords' @> '[""keh"",""abc""]'::jsonb;"
71,"All elements in the right side array of the containment operator need to be present - basically like an ""AND"" operator. If you want ""OR"" behavior, you can construct it in the WHERE clause:"
71,"demo=# explain analyze select * from books where (data->'keywords' @> '[""abc""]'::jsonb OR data->'keywords' @> '[""keh""]'::jsonb);"
71,More details on the behavior of the containment operators with arrays can be found in the documentation.
71,SQL/JSON & JSONPath
71,"SQL standard added support for JSON  in SQL - SQL/JSON Standard-2016. With the PostgreSQL 12/13 releases, PostgreSQL has one of the best implementations of the SQL/JSON standard. For more details refer to the PostgreSQL 12 announcement."
71,"One of the core features of SQL/JSON is support for the JSONPath language to query JSONB data. JSONPath allows you to specify an expression (using a syntax similar to the property access notation in Javascript) to query your JSONB data. This makes it simple and intuitive, but is also very powerful to query your JSONB data. Think of  JSONPath as the logical equivalent of XPath for XML."
71,.key
71,Returns an object member with the specified key.
71,[*]
71,Wildcard array element accessor that returns all array elements.
71,Wildcard member accessor that returns the values of all members located at the top level of the current object.
71,.**
71,"Recursive wildcard member accessor that processes all levels of the JSON hierarchy of the current object and returns all the member values, regardless of their nesting level."
71,Refer to JSONPath documentation for the full list of operators. JSONPath also supports a variety of filter expressions.
71,JSONPath Functions
71,PostgreSQL 12 provides several functions to use JSONPath to query your JSONB data. From the docs:
71,jsonb_path_exists - Checks whether JSONB path returns any item for the specified JSON value.
71,"jsonb_path_match - Returns the result of JSONB path predicate check for the specified JSONB value. Only the first item of the result is taken into account. If the result is not Boolean, then null is returned."
71,jsonb_path_query - Gets all JSONB items returned by JSONB path for the specified JSONB value. There are also a couple of other variants of this function that handle arrays of objects.
71,Let's start with a simple query - finding books by publisher:
71,"demo=# select * from books where data @@ '$.publisher == ""ktjKEZ1tvq""';"
71,id | author | isbn | rating | data
71,---------+-----------------+------------+--------+----------------------------------------------------------------------------------------------------------------------------------
71,"1000001 | 4RNsovI2haTgU7l | GwSoX67gLS | 2 | {""tags"": {""nk542369"": {""ik55240"": ""iv305393""}}, ""keywords"": [""abc"", ""def"", ""geh""], ""publisher"": ""ktjKEZ1tvq"", ""criticrating"": 0}"
71,(1 row)
71,"demo=# explain analyze select * from books where data @@ '$.publisher == ""ktjKEZ1tvq""';"
71,QUERY PLAN
71,--------------------------------------------------------------------------------------------------------------------
71,Bitmap Heap Scan on books (cost=21.75..1014.25 rows=1000 width=158) (actual time=0.123..0.124 rows=1 loops=1)
71,"Recheck Cond: (data @@ '($.""publisher"" == ""ktjKEZ1tvq"")'::jsonpath)"
71,Heap Blocks: exact=1
71,-> Bitmap Index Scan on datagin (cost=0.00..21.50 rows=1000 width=0) (actual time=0.110..0.110 rows=1 loops=1)
71,"Index Cond: (data @@ '($.""publisher"" == ""ktjKEZ1tvq"")'::jsonpath)"
71,Planning Time: 0.137 ms
71,Execution Time: 0.194 ms
71,(7 rows)
71,You can rewrite this expression as a JSONPath filter:
71,"demo=# select * from books where jsonb_path_exists(data,'$.publisher ?(@ == ""ktjKEZ1tvq"")');"
71,"You can also use very complex query expressions. For example, let's select books where print style = hardcover and price = 100:"
71,"select * from books where jsonb_path_exists(data, '$.prints[*] ?(@.style==""hc"" &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; @.price == 100)');"
71,"However, index support for JSONPath is very limited at this point - this makes it dangerous to use JSONPath in the where clause. JSONPath support for indexes will be improved in subsequent releases."
71,"demo=# explain analyze select * from books where jsonb_path_exists(data,'$.publisher ?(@ == ""ktjKEZ1tvq"")');"
71,QUERY PLAN
71,------------------------------------------------------------------------------------------------------------
71,Seq Scan on books (cost=0.00..36307.24 rows=333340 width=158) (actual time=0.019..480.268 rows=1 loops=1)
71,"Filter: jsonb_path_exists(data, '$.""publisher""?(@ == ""ktjKEZ1tvq"")'::jsonpath, '{}'::jsonb, false)"
71,Rows Removed by Filter: 1000028
71,Planning Time: 0.095 ms
71,Execution Time: 480.348 ms
71,(5 rows)
71,Projecting Partial JSON
71,Another great use case for JSONPath is projecting partial JSONB from the row that matches. Consider the following sample JSONB:
71,demo=# select jsonb_pretty(data) from books where id = 1000029;
71,jsonb_pretty
71,-----------------------------------
71,"""tags"": {"
71,"""nk678947"": {"
71,"""ik159670"": ""iv32358"
71,"""prints"": ["
71,"""price"": 100,"
71,"""style"": ""hc"""
71,"""price"": 50,"
71,"""style"": ""pb"""
71,"""braille"": false,"
71,"""keywords"": ["
71,"""abc"","
71,"""kef"","
71,"""keh"""
71,"""hardcover"": true,"
71,"""publisher"": ""ppc3YXL8kK"","
71,"""criticrating"": 3"
71,Select only the publisher field:
71,"demo=# select jsonb_path_query(data, '$.publisher') from books where id = 1000029;"
71,jsonb_path_query
71,------------------
71,"""ppc3YXL8kK"""
71,(1 row)
71,Select the prints field (which is an array of objects):
71,"demo=# select jsonb_path_query(data, '$.prints') from books where id = 1000029;"
71,jsonb_path_query
71,---------------------------------------------------------------
71,"[{""price"": 100, ""style"": ""hc""}, {""price"": 50, ""style"": ""pb""}]"
71,(1 row)
71,Select the first element in the array prints:
71,"demo=# select jsonb_path_query(data, '$.prints[0]') from books where id = 1000029;"
71,jsonb_path_query
71,-------------------------------
71,"{""price"": 100, ""style"": ""hc""}"
71,(1 row)
71,Select the last element in the array prints:
71,"demo=# select jsonb_path_query(data, '$.prints[$.size()]') from books where id = 1000029;"
71,jsonb_path_query
71,------------------------------
71,"{""price"": 50, ""style"": ""pb""}"
71,(1 row)
71,Select only the hardcover prints from the array:
71,"demo=# select jsonb_path_query(data, '$.prints[*] ?(@.style==""hc"")') from books where id = 1000029;"
71,jsonb_path_query
71,-------------------------------
71,"{""price"": 100, ""style"": ""hc""}"
71,(1 row)
71,We can also chain the filters:
71,"demo=# select jsonb_path_query(data, '$.prints[*] ?(@.style==""hc"") ?(@.price ==100)') from books where id = 1000029;"
71,jsonb_path_query
71,-------------------------------
71,"{""price"": 100, ""style"": ""hc""}"
71,(1 row)
71,"In summary, PostgreSQL provides a powerful and versatile platform to store and process JSON data. There are several gotcha's that you need to be aware of, but we are optimistic that it will be fixed in future releases."
71,More tips for you
71,Which Is the Best PostgreSQL GUI?
71,"PostgreSQL graphical user interface (GUI) tools help these open source database users to manage, manipulate, and visualize their data. In this post, we discuss the top 5 GUI tools for administering your PostgreSQL deployments. Learn more"
71,Managing High Availability in PostgreSQL
71,Managing high availability in your PostgreSQL hosting is very important to ensuring your clusters maintain exceptional uptime and strong operational performance so your data is always available to your application. Learn more
71,PostgreSQL Connection Pooling: Part 1 – Pros & Cons
71,"In modern apps, clients open a lot of connections. Developers are discouraged from holding a database connection while other operations take place. “Open a connection as late as possible, close as soon as possible”. Learn more"
71,+1 Tweet Share Share PinShares 0
71,Join the ScaleGrid Newsletter and never miss out!
71,"Check out Related PostsWhich Is The Best PostgreSQL GUI? 2021 ComparisonPostgreSQL graphical user interface (GUI) tools help open source database users to manage, manipulat...Read Full ArticleScaleGrid DigitalOcean Support for MySQL, PostgreSQL and Redis™ Now Av...PALO ALTO, Calif., June 9, 2020 – ScaleGrid, a leading Database-as-a-Service (DBaaS) provider, has j...Read Full ArticlePostgreSQL Connection Pooling: Part 1 - Pros & ConsA long time ago, in a galaxy far far away, ‘threads’ were a programming novelty rarely used and seld...Read Full Article"
71,Dharshan is the founder of ScaleGrid.io (formerly MongoDirector.com). He is an experienced MongoDB developer and administrator. He can be reached for further comment at @dharshanrg
71,Start Your Free 30 Day Trial
71,ASDFADSF FDASFDA
71,Syntax error at
71,CREATE INDEX datatagsgin ON books USING gin (data->’tags’);
71,should be
71,CREATE INDEX datatagsgin ON books USING gin ((data->’tags’));
71,machty
71,It’s pretty hard to follow some of the examples when you’re using machine-y sample data like `”{“tags”: {“nk594127″: {“ik71786″: “iv678771″}}`.
71,Search:
71,About us:
71,ScaleGrid is a fully managed database hosting service for
71,"MongoDB® , Redis™, MySQL, and PostgreSQL"
71,on public and private clouds.
71,You may also be interested in
71,mysqldump Best Practices: Part 2 – Migrations Guide
71,ScaleGrid Adds Oracle Cloud for Managed Database Hosting
71,Which Is The Best PostgreSQL GUI? 2021 Comparison
71,Oracle Cloud Breakdown – Database Hosting Costs on OCI
71,How To Set Up MySQL on DigitalOcean
71,0 Shares +1 Tweet Share Share Pin
71,"ScaleGrid provides a fully managed Database-as-a-Service (DBaaS) solution used by thousands of developers, startups, and enterprise customers including UPS, Polaris, and Adobe. The ScaleGrid platform supports MongoDB® Database, Redis™, MySQL, and PostgreSQL on both public and private clouds, including Amazon AWS, Microsoft Azure, Google Cloud Platform, DigitalOcean, and VMware, and automates your time-consuming tasks at any scale so you can focus on your product instead of operations."
71,Start a FREE 30-Day Trial
71,MongoDB
71,Compare MongoDB DBaaS AWS Azure DigitalOcean
71,Redis™
71,Compare Redis™ DBaaS AWS
71,Azure
71,DigitalOcean
71,Google Cloud Platform
71,MySQL
71,Compare MySQL DBaaS AWS Azure
71,DigitalOcean
71,Google Cloud Platform
71,PostgreSQL
71,Compare PostgreSQL DBaaS AWS Azure
71,DigitalOcean
71,Google Cloud Platform
71,Pricing
71,Startup Program
71,Switching Providers
71,About Us
71,Customer Stories
71,Press
71,Contact
71,Email support@scalegrid.io
71,Resources
71,Blog
71,Documentation
71,Support Status
71,API Reference
71,Terms of Service
71,GDPR DPA
71,CCPA DPA
71,Privacy Policy Subprocessors
71,"MONGO®, MongoDB® and MongoDB® & Design are registered trademarks of MongoDB®, Inc. Redis is a trademark of Redis Labs Ltd. Any rights therein are reserved to Redis Labs Ltd. Any use by ScaleGrid is for referential purposes only and does not indicate any sponsorship, endorsement or affiliation between Redis and ScaleGrid.Greenplum™ is a trademark of Pivotal Software, Inc. in the U.S. and other countries."
72,PostgreSQL Learning Path at Pluralsight - SQL Authority with Pinal Dave
72,"April Discount: Comprehensive Database Performance Health Check | TestimonialsConsultingTrainingFree VideosAll ArticlesInterview Questions and AnswersSQL Tips and TricksSQL PerformanceSQL PuzzleBig DataBlog StatsSQL BooksSearch SQLAuthorityDownloadsHire MeHealth CheckTrainingPostgreSQL Learning Path at PluralsightNovember 16, 2020Pinal DaveSQLNo CommentsWhen I started my career, I was very much confused about which database language I should learn. I started my career with SQL Server and learned for many years. However, as I moved to the consulting field, I started to realize that the world is bigger than just one technology. In the real world, we often face multiple technologies together. One of the most popular languages besides SQL Server, I encountered is PostgreSQL. Today I want to share PostgreSQL Learning Path at Pluralsight where you can learn this technology systematically.Learn PostgreSQL – Learning PathCourse 1: PostgreSQL: Getting Started PostgreSQL is commonly known as Postgres and is often referred to as the world’s most advanced open source database. In this course, we will go over the basics of PostgreSQL. We will cover topics ranging from installations to writing basic queries and retrieving data from tables. We will also explore the logic of join, and a few best practices.Course 2: PostgreSQL: Introduction to SQL Queries In this course, we will learn about various data types and their impact on performance and database designs. We will learn about various table operations, schemas, keys, and constraints. We will also learn how to efficiently retrieve data and make a modification to data with insert, update, and delete operations.Course 3: PostgreSQL: Advanced SQL Queries In this course, we will discuss advanced queries for PostgreSQL. We will learn about functions and operators, type conversions, and transactions.Course 4: PostgreSQL: Advanced Server Programming If you’re a database developer looking to expand your skills and understanding of triggers, rules, and procedural language in PostgreSQL, this course is for you.Course 5: PostgreSQL: Index Tuning and Performance Optimization Data is critical to any application, and database performance goes hand-in-hand with it. In this course, you’ll get to see some ways to maximize database performance with PostgreSQL, covering indexes, best practices, and more.I hope you find this PostgreSQL Learning Path helpful. If you have a Pluralsight subscription, you can watch it for free. If you do not have a Pluralsight subscription, you can still watch the course for FREE by signing up for a trial account.Reference: Pinal Dave (https://blog.sqlauthority.com)"
72,"Pluralsight, PostgreSQLPrevious PostSQL SERVER – What is Latch?Next PostSQL SERVER – Logical Processing Order of the SELECT StatementRelated PostsHeroku – How to Deploy Postgres Add-Ons?September 26, 2020SQLAuthority News – Truly Amazing Experience at Pluralsight Author Summit February 2013March 14, 2013Learn PostgreSQL – New Technology WeekDecember 24, 2019Leave a Reply Cancel reply"
72,"Pinal Dave is an SQL Server Performance Tuning Expert and independent consultant with over 17 years of hands-on experience. He holds a Masters of Science degree and numerous database certifications.Pinal has authored 12 SQL Server database books and 37 Pluralsight courses. To freely share his knowledge and help others build their expertise, Pinal has also written more than 5,500 database tech articles on his blog at https://blog.sqlauthority.com.Pinal is an experienced and dedicated professional with a deep commitment to flawless customer service. If you need help with any SQL Server Performance Tuning Issues, please feel free to reach out at pinal@sqlauthority.com.Pinal is also a CrossFit Level 1 Trainer (CF-L1) and CrossFit Level 2 Trainer (CF-L2).Nupur Dave is a social media enthusiast and an independent consultant. She primarily focuses on the database domain, helping clients build short and long term multi-channel campaigns to drive leads for their sales pipeline.Exclusive NewsletterWebsite Is your SQL Server running slow and you want to speed it up without sharing server credentials? In my Comprehensive Database Performance Health Check, we can work together remotely and resolve your biggest performance troublemakers in less than 4 hours.Once you learn my business secrets, you will fix the majority of problems in the future.Have you ever opened any PowerPoint deck when you face SQL Server Performance Tuning emergencies? SQL Server Performance Tuning Practical Workshop is my MOST popular training with no PowerPoint presentations and 100% practical demonstrations.Essentially I share my business secrets to optimize SQL Server performance."
72,SQL Interview Q & ATestimonialsSearchPrivacy Policy© 2006 – 2021 All rights reserved. pinal @ SQLAuthority.com
72,Menu
72,Go to mobile version
75,Tips February 2021
75,Java Performance Tuning
75,Java(TM) - see bottom of page
75,|home
75,|services
75,|training
75,|newsletter
75,|tuning tips
75,|tool reports
75,|articles
75,|resources
75,|about us
75,|site map
75,|contact us
75,Tools: |
75,GC log analysers|
75,Multi-tenancy tools|
75,Books|
75,SizeOf|
75,Thread analysers|
75,Heap dump analysers|
75,Our valued sponsors who help make this site possible
75,JProfiler: Get rid of your performance problems and memory leaks!
75,"Training online: Concurrency, Threading, GC, Advanced Java and more ..."
75,Tips February 2021
75,JProfiler
75,Get rid of your performance problems and memory leaks!
75,Modern Garbage Collection Tuning
75,Java Performance Training Courses
75,COURSES AVAILABLE NOW. We can provide training courses to handle all your Java performance needs
75,"Java Performance Tuning, 2nd ed"
75,The classic and most comprehensive book on tuning Java
75,Java Performance Tuning Newsletter
75,Your source of Java performance news. Subscribe now!
75,Enter email:
75,Training online
75,Threading Essentials course
75,JProfiler
75,Get rid of your performance problems and memory leaks!
75,Back to newsletter 243 contents
75,https://lkorinth.github.io/posts/2020-11-27-metaspace.html
75,"Metaspace in OpenJDK 16 (Page last updated November 2020, Added 2021-02-25, Author Leo Korinth, Publisher lkorinth). Tips:"
75,For small class loaders small class loaders huge savings can be made in Metaspace with JDK 16+.
75,Metaspace allocates memory outside the Java heap.
75,"Metaspace can allocate memory within a specified address space, allowing class headers to us 32-bit indexes on 64-bit machines. Related flags are -XX:CompressedClassSpaceSize, -XX:+UseCompressedClassPointers and -XX:+UseCompressedOops."
75,https://www.morling.dev/blog/talking-to-postgres-through-java-16-unix-domain-socket-channels/
75,"Talking to Postgres Through Java 16 Unix-Domain Socket Channels (Page last updated February 2021, Added 2021-02-25, Author Gunnar Morling, Publisher morling). Tips:"
75,Unix domain sockets are both more secure and also more efficient than TCP/IP loopback connections.
75,A really interesting feature of Unix domain sockets is the ability to transfer open file descriptors from one process to another.
75,https://inside.java/2021/02/03/jep380-unix-domain-sockets-channels/
75,"JEP-380: Unix domain socket channels (Page last updated February 2021, Added 2021-02-25, Author Michael McMahon, Publisher Inside Java). Tips:"
75,Unix domain sockets bypasses the TCP/IP stack with consequential improvements in latency and CPU usage.
75,"Server instance is ServerSocketChannel.open(StandardProtocolFamily.UNIX).bind(UnixDomainSocketAddress.of(""/foo/bar.socket"")), a client instance SocketChannel.open(StandardProtocolFamily.UNIX).bind(UnixDomainSocketAddress.of(""/foo/bar.socket""))."
75,"It is good practice when cleaning up after a Unix domain server socket shuts down to ensure that its socket file gets deleted, as the file is otherwise persisted."
75,https://nipafx.dev/java-unix-domain-sockets/
75,"Code-First Unix Domain Socket Tutorial (Page last updated March 2021, Added 2021-03-29, Author Nicolai Parlog, Publisher nipafx.dev). Tips:"
75,Java's socket channel / server-socket channel API can use Unix domain sockets (since Java 16) for faster and more secure inter-process communication on the same host
75,The Unix domain socket file needs to have the correct permissions for the program for all instances of the program (ie for all users who launch the program).
75,"Obtain a UnixDomainSocketAddress (eg using the static of() method), and open using ServerSocketChannel/SocketChannel.open(StandardProtocolFamily.UNIX) then bind() to the address. Finally the server should ServerSocketChannel.accept() and the client should SocketChannel.connect(). The resulting IO is executed the same way as for other sockets."
75,Unix domain sockets have faster setup times and higher data throughput than TCP/IP loopback connections. They also have better security (because standard file-based access control is used).
75,Jack Shirazi
75,Back to newsletter 243 contents
75,Last Updated: 2021-03-29
75,Copyright © 2000-2021 Fasterj.com. All Rights Reserved.
75,All trademarks and registered trademarks appearing on JavaPerformanceTuning.com are the property of their respective owners.
75,Java is a trademark or registered trademark of Oracle Corporation in the United States and other countries. JavaPerformanceTuning.com is not connected to Oracle Corporation and is not sponsored by Oracle Corporation.
75,URL: http://www.JavaPerformanceTuning.com/news/newtips243.shtml
75,RSS Feed: http://www.JavaPerformanceTuning.com/newsletters.rss
75,Trouble with this page? Please contact us
77,IDM 6.5 > Installation Guide
77,Toggle navigation DocsIDM 6.5 ›
77,Installation Guide
77,Getting StartedRelease Notes
77,Getting Started
77,Installation Guide
77,Using Identity ManagementSamples Guide
77,Integrator's Guide
77,Password Synchronization Plugin Guide
77,ReferenceConnector Release Notes
77,Connector Reference
77,Connector Developer's Guide
77,Self-Service REST API Reference
77,OpenICF Javadoc
77,Search in All booksThis bookThis versionThis productSearchFeedbackPDFAboutLatest update: 6.5.1.0Preface1. About This Guide2. Accessing Documentation Online3. Using the ForgeRock.org Site1. Preparing to Install and Run Servers1.1. Before You Install1.1.1. Java Prerequisites1.1.2. Application Container1.2. Installing and Running Servers1.3. Installing IDM as a Service1.3.1. Installing as a Windows Service1.3.2. Installing as a Linux Service1.3.2.1. Setting up a Systemd Service1.3.2.2. Setting up a SysV Service (Red Hat)1.3.2.3. Setting up a SysV Service (Ubuntu)1.4. Getting Started With the REST Interface1.4.1. Format REST Output For Readability1.5. IDM User Interfaces1.6. About the Repository1.7. Starting a New Project2. Selecting a Repository2.1. Using the Default DS Repository2.2. Using an External DS Repository2.3. Database Access Rights For a JDBC Repository2.4. Configuring Case Insensitivity For a JDBC Repository2.5. Setting Up a MySQL Repository2.6. Setting Up a Microsoft SQL Repository2.7. Setting Up an Oracle DB Repository2.8. Setting Up a PostgreSQL Repository2.9. Setting Up an IBM DB2 Repository2.9.1. Configuring Kerberos Authentication With a DB2 Repository3. Removing and Moving Server Software4. Updating Servers4.1. Preparing Systems for An Update4.2. Migrating Your Existing Server Configuration4.2.1. Migrating Configuration Files4.2.2. Migrating boot.properties4.2.3. Migrating Security Settings4.2.4. Migrating Custom Scripts4.2.5. Migrating Provisioner Files4.2.6. Migrating Custom Workflows4.3. Updating the IDM Repository4.3.1. Upgrade Your Existing Repository4.3.2. Create a New Repository4.4. Migrating Data from Previous Versions of IDM4.4.1. Configuring the Migration service4.4.2. Running Your Migration4.5. Updating a Clustered Deployment4.6. Updating UI Customizations4.7. Updating to IDM 6.5.1.04.8. Placing a Server in Maintenance Mode4.9. Applying Patch Bundle ReleasesA. Installing on a Read-Only VolumeA.1. Preparing Your SystemA.2. Redirect Output Through Configuration FilesA.3. Additional DetailsIDM Glossary
77,"Guide to installing, updating, and uninstalling"
77,ForgeRock® Identity Management software. This software offers
77,flexible services for automating management of the identity life cycle.
77,Preface
77,ForgeRock Identity Platform™ serves as the basis for
77,our simple and comprehensive Identity and Access Management solution.
77,"We help our customers deepen their relationships with their customers,"
77,and improve the productivity and connectivity of their employees and partners.
77,"For more information about ForgeRock and about the platform, see https://www.forgerock.com."
77,1. About This Guide
77,This guide shows you how to install ForgeRock Identity Management services for
77,"identity management, provisioning, and compliance. Unless you are planning an"
77,"evaluation or test installation, read the Release Notes before you get started."
77,This guide is written for anyone installing ForgeRock Identity Management software
77,"to manage identities, and to ensure compliance with identity management"
77,regulations.
77,It covers the install and removal (uninstall) procedures that you
77,theoretically perform only once per version. It aims to provide you with at
77,least some idea of what happens behind the scenes when you perform the steps.
77,You do not need a complete understanding of ForgeRock Identity Management
77,"software to learn something from this guide, though a background in identity"
77,management and maintaining web application software can help. You do need
77,some background in managing services on your operating systems and in your
77,"application servers. You can nevertheless get started with this guide, and"
77,then learn more as you go along.
77,"If you have a previous version of ForgeRock Identity Management software installed,"
77,"see ""Compatibility"" in the Release Notes before"
77,you install this version.
77,2. Accessing Documentation Online
77,ForgeRock publishes comprehensive documentation online:
77,The ForgeRock
77,Knowledge Base
77,"offers a large and increasing number of up-to-date, practical articles"
77,that help you deploy and manage ForgeRock software.
77,"While many articles are visible to community members,"
77,"ForgeRock customers have access to much more,"
77,including advanced information for customers using ForgeRock software
77,in a mission-critical capacity.
77,"ForgeRock product documentation, such as this document,"
77,aims to be technically accurate and complete
77,with respect to the software documented.
77,It is visible to everyone and covers all product features
77,and examples of how to use them.
77,3. Using the ForgeRock.org Site
77,The
77,ForgeRock.org site
77,"has links to source code for ForgeRock open source software,"
77,as well as links to the ForgeRock forums and technical blogs.
77,"If you are a ForgeRock customer,"
77,raise a support ticket instead of using the forums.
77,ForgeRock support professionals will get in touch to help you.
77,Chapter 1. Preparing to Install and Run Servers
77,"This chapter covers the tasks required to prepare, install and start"
77,IDM.
77,Note
77,This documentation set includes a separate
77,Samples Guide. When you have
77,"read the first two chapters of this document, use the"
77,Samples Guide to test a number of different deployment
77,scenarios.
77,1.1. Before You Install
77,This section covers what you need to know before you install IDM.
77,1.1.1. Java Prerequisites
77,"For details of the supported Java Environment, see ""Preparing the Java Environment"" in the Release Notes."
77,"On Windows systems, you must set the JAVA_HOME"
77,environment variable to point to the root of a valid Java installation. The
77,following steps indicate how to set the JAVA_HOME
77,environment variable on Windows Server 2008 R2. Adjust the steps for your
77,specific environment:
77,Locate your JRE Installation Directory. If you have not changed the
77,"installation path for the Java Runtime Environment during installation, it"
77,will be in a directory under C:\Program Files\Java\.
77,Select Start > Control Panel > System and Security > System.
77,Click Advanced System Settings.
77,Click Environment Variables.
77,"Under System Variables, click New."
77,Enter the Variable name (JAVA_HOME) and set the
77,"Variable value to the JRE installation directory, for example"
77,C:\Program Files\Java\jre8.
77,Click OK.
77,"On Linux systems, if startup.sh reports"
77,"JAVA_HOME not available, Java is needed to run"
77,"IDM and you've already installed Java, use the following"
77,steps to set JAVA_HOME:
77,Open the user shell configuration file found in your home directory.
77,Add the JAVA_HOME variable to the user shell
77,"configuration file, setting the value to /usr. In"
77,"Bash, this would appear as export JAVA_HOME=""/usr""."
77,1.1.2. Application Container
77,IDM runs in an OSGi container with an embedded Servlet container
77,and an embedded noSQL database. By default the OSGi container is Apache
77,Felix (Felix) and the default Servlet container is Jetty. No other
77,configuration is supported.
77,1.2. Installing and Running Servers
77,Follow the procedures in this section to install and run IDM. To
77,"set up the server on a read-only volume, read"
77,"""Installing on a Read-Only Volume""."
77,To Install IDM
77,Follow these steps to install IDM:
77,Make sure you have an appropriate version of Java installed:
77,$ java -version
77,"java version ""1.8.0_121"""
77,Java(TM) SE Runtime Environment (build 1.8.0_121-b13)
77,"Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)"
77,"For a description of the Java requirements, see ""Before You Install"" in the Release Notes."
77,Download IDM from the
77,ForgeRock BackStage download site.
77,Releases on the ForgeRock BackStage download site are thoroughly validated for ForgeRock
77,"customers who run the software in production deployments, and for those who"
77,want to try or test a given release.
77,Unpack the contents of the .zip file into the install directory:
77,$ unzip ~/Downloads/IDM-6.5.1.0.zip
77,Archive:
77,IDM-6.5.1.0.zip
77,inflating: openidm/.checksums.csv
77,creating: openidm/bundle/
77,extracting: openidm/bundle/openidm-audit-6.5.1.0.jar
77,...
77,"By default, IDM listens for HTTP and HTTPS connections on"
77,"ports 8080 and 8443, respectively. To change these port numbers, edit the"
77,following settings in your resolver/boot.properties
77,file:
77,openidm.port.httpopenidm.port.https
77,"When you deploy IDM in production, you must"
77,"set openidm.host to the URL of your deployment, in the"
77,"same resolver/boot.properties file. Otherwise, calls to"
77,the /admin endpoint are not properly redirected.
77,"Deployment URLs will vary, depending on whether you're using a load"
77,balancer. While IDM documentation does not specify how you'd
77,"configure a load balancer, you'll need to configure IDM in"
77,"a cluster as described in ""Configuring an IDM Instance as Part of a Cluster"" in the Integrator's Guide, and specifically"
77,"in ""Deploying Securely Behind a Load Balancer"" in the Integrator's Guide."
77,"Before running IDM in production, replace the default embedded"
77,DS repository with a supported repository.
77,"For more information, see ""Selecting a Repository""."
77,To Start IDM
77,"To run IDM as a background process, see ""Starting, Stopping, and Running the Server"" in the Integrator's Guide."
77,"Follow these steps to run IDM interactively:Start the Felix container, load all services, and start a command"
77,shell to allow you to manage the container:
77,Start IDM (UNIX):$ cd /path/to/openidm
77,$ ./startup.sh
77,Using OPENIDM_HOME:
77,/path/to/openidm
77,Using PROJECT_HOME:
77,/path/to/openidm
77,Using OPENIDM_OPTS:
77,-Xmx1024m -Xms1024m
77,Using LOGGING_CONFIG: -Djava.util.logging.config.file=/path/to/openidm/conf/logging.properties
77,"-> OpenIDM version ""6.5.1.0"""
77,OpenIDM ready
77,Start IDM (Windows):C:\> cd \path\to\openidm
77,C:\> startup.bat
77,"""Using OPENIDM_HOME:"
77,"\path\to\openidm"""
77,"""Using PROJECT_HOME:"
77,"\path\to\openidm"""
77,"""Using OPENIDM_OPTS:"
77,"-Xmx1024m -Xms1024m -Dfile.encoding=UTF-8"""
77,"""Using LOGGING_CONFIG: -Djava.util.logging.config.file=\path\to\openidm\conf\logging.properties"""
77,"-> OpenIDM version ""6.5.1.0"""
77,OpenIDM ready
77,"At the OSGi console -> prompt, you"
77,"can enter commands such as help for usage, or"
77,ps to view the bundles installed. For a list of the
77,"core services and their states, run the following command:-> scr list"
77,BundleId Component Name Default State
77,Component Id State
77,PIDs (Factory PID)
77,org.forgerock.openidm.config.enhanced.starter
77,enabled
77,1] [active
77,] org.forgerock.openidm.config.enhanced.starter
77,org.forgerock.openidm.config.manage
77,enabled
77,0] [active
77,] org.forgerock.openidm.config.manage
77,10]
77,org.forgerock.openidm.datasource.jdbc
77,enabled
77,10]
77,org.forgerock.openidm.repo.jdbc
77,enabled
77,11]
77,org.forgerock.openidm.repo.ds
77,enabled
77,48] [active
77,] org.forgerock.openidm.repo.ds
77,...
77,"-> A default startup does not include certain configurable services,"
77,which will indicate an unsatisfied state until they
77,are included in the configuration. As you work through the sample
77,"configurations described later in this guide, you will notice that these"
77,services are active.Startup errors and messages are logged to the console by default.
77,You can also view these messages in the log files at
77,/path/to/openidm/logs.
77,"Alternatively, you can manage the container and services from the Apache"
77,Felix Web Console.
77,Use these hints to connect to the Apache Felix Web Console:
77,Default URL: https://localhost:8443/system/console
77,Default user name: admin
77,Default password: admin
77,Select Main > Components to see core services and their respective states.
77,To Stop IDM
77,You can stop IDM from the -> prompt in the
77,"OSGi console, or through the Apache Felix Web Console. Both of these"
77,options stop the Felix container.
77,"In the OSGi console, enter the shutdown command at the"
77,-> prompt:
77,-> shutdown
77,...
77,"In the Apache Felix Web Console, select Web Console > System Information"
77,to stop the container.
77,"On Unix systems, you can stop IDM by using the"
77,"shutdown.sh script, located in the"
77,/path/to/openidm directory:
77,$ ./shutdown.sh
77,./shutdown.sh
77,Stopping OpenIDM (31391)1.3. Installing IDM as a Service
77,The following sections describe how to install and run IDM as a
77,"service, on Windows and Linux systems:"
77,1.3.1. Installing as a Windows Service
77,You can install IDM to run as a Windows service so that the
77,server starts and stops automatically when Windows starts and stops. You
77,must be logged in as an administrator to install a Windows service.
77,Note
77,"On a 64-bit Windows server, you must have a 64-bit Java version installed"
77,"to start the service. If a 32-bit Java version is installed, you will be"
77,"able to install IDM as a service, but starting the service"
77,will fail.
77,Before you launch the
77,"service.bat file, which registers the"
77,"service within the Windows registry, make sure that your"
77,JAVA_HOME environment variable points to a
77,valid 64-bit version of the JRE or JDK. If you have already installed the
77,service with the JAVA_HOME environment variable
77,"pointing to a 32-bit JRE or JDK, delete the service first, then reinstall"
77,the service.
77,"Unpack the IDM-6.5.1.0.zip file, as described previously, and navigate"
77,to the install-directory\bin
77,directory:
77,C:\>cd openidm\bin
77,C:\openidm\bin>
77,Run the service.bat command with the
77,"/install option, specifying the name that the service"
77,should run as:
77,C:\openidm\bin>service.bat /install openidm
77,"ForgeRock Identity Management Server successfully installed as ""openidm"" service"
77,Use the Windows Service manager to manage the IDM service.
77,Running as a Windows Service
77,"By default, the IDM service is run by Local System,"
77,which is a system-level service account built in to Windows. Before deploying
77,"to production, it is recommended you switch to an account with fewer permissions."
77,"The account running the IDM service needs to be able to read,"
77,"write, and execute only the directories related to IDM. For"
77,"more information about service accounts, see"
77,Service
77,Accounts in the Microsoft documentation.
77,"Use the Windows Service Manager to start, stop, or restart the service."
77,"If you want to uninstall the IDM service, first use the Windows"
77,Service Manager to stop IDM and then run the following command:
77,C:\install-directory\openidm\bin>service.bat /uninstall openidm
77,"Service ""openidm"" removed successfully"
77,"If desired, you can then set up IDM with a specific project"
77,directory:
77,C:\install-directory\openidm\bin>service.bat /install openidm -p C:\project-directory
77,"ForgeRock Identity Management Server successfully installed as ""openidm"" service"
77,You can also manage configuration details with the Procrun monitor
77,application. IDM includes the associated
77,prunmgr.exe executable in the
77,C:\install-directory\openidm\bin directory.
77,"For example, you can open the Windows service configuration application"
77,"for IDM with the following command, where ES"
77,stands for Edit Service Configuration
77,C:\install-directory\openidm\bin>prunmgr.exe //ES/openidm
77,The prunmgr.exe executable also includes the monitor
77,application functionality described in the following Apache Commons
77,page on the:
77,"Procrun monitor Application. However,"
77,IDM does not include the Procrun service application.
77,"For example, if you've configured IDM as a Windows service, you"
77,can start and stop it with the following commands:
77,C:\install-directory\openidm\bin>prunmgr.exe //MR/openidm
77,C:\install-directory\openidm\bin>prunmgr.exe //MQ/openidm
77,"In these commands, MR is the option to Monitor"
77,"and Run IDM, and MQ stands for"
77,"Monitor Quit, which stops the IDM service."
77,1.3.2. Installing as a Linux Service
77,IDM provides a script that can generate SysV
77,or Systemd service initialization scripts. You can start
77,"the script as the root user, or configure it to start during the boot process."
77,"When IDM runs as a service, logs are written to the installation"
77,directory.
77,"If you have not yet installed IDM, follow the steps in"
77,"""To Install IDM""."
77,Review the options by running the following script:
77,$ cd /path/to/openidm/bin
77,$ ./create-openidm-rc.sh
77,Usage: ./create-openidm-rc.sh --[systemd|chkconfig|lsb]
77,Outputs OpenIDM init file to stdout for the given system
77,--systemd
77,Generate Systemd init script. This is preferred for all modern distros.
77,--chkconfig
77,Generate SysV init script with chkconfig headers (RedHat/CentOS)
77,--lsb
77,Generate SysV init script with LSB headers (Debian/Ubuntu)
77,...
77,The following sections describe how you can create each of these scripts:
77,1.3.2.1. Setting up a Systemd Service
77,If you're running relatively standard versions of Red Hat Enterprise Linux
77,"(CentOS Linux) version 7.x, or Ubuntu 16.04 and later, you'll want to"
77,"set up a systemd service script. To set up such a script, navigate to the"
77,"/path/to/openidm/bin directory, and run the following"
77,command:
77,$ ./create-openidm-rc.sh --systemd
77,"As noted in the output, you can set up the IDM service on a"
77,standard systemd-based Linux distribution with the following commands:
77,$ ./create-openidm-rc.sh --systemd > openidm.service
77,$ sudo cp openidm.service /etc/systemd/system/
77,$ systemctl enable openidm
77,$ systemctl start openidm
77,"To stop the IDM service, run the following command:"
77,$ systemctl stop openidm
77,You can modify the openidm.service script.
77,The following excerpt would run IDM with a startup script in the
77,/home/idm/project directory:
77,[Unit]
77,Description=ForgeRock OpenIDM
77,After=network.target auditd.target
77,[Service]
77,Type=simple
77,SuccessExitStatus=143
77,Environment=JAVA_HOME=/usr
77,User=testuser
77,ExecStart=/root/openidm/startup.sh -p /home/idm/project
77,ExecStop=/root/openidm/shutdown.sh
77,[Install]
77,WantedBy=multi-user.target
77,Run the following commands to reload the configuration and then start the
77,IDM service script:
77,$ systemctl daemon-reload
77,$ systemctl start openidm1.3.2.2. Setting up a SysV Service (Red Hat)
77,If you're running relatively standard versions of Red Hat Enterprise Linux
77,"(CentOS Linux) version 6.x, you'll want to set up a SysV service script,"
77,with runlevels controlled through the chkconfig command.
77,"To set up such a script, navigate to the"
77,"/path/to/openidm/bin directory, and run the following"
77,command:
77,$ ./create-openidm-rc.sh --chkconfig
77,You can then set up and start the IDM service
77,"on a Linux distribution that uses SysV init scripts, with the following"
77,commands:
77,$ ./create-openidm-rc.sh --chkconfig
77,> openidm
77,$ sudo cp openidm /etc/init.d/
77,$ sudo chmod u+x /etc/init.d/openidm
77,$ sudo chkconfig --add openidm
77,$ sudo chkconfig openidm on
77,$ sudo service openidm start
77,"To stop the IDM service, run the following command:"
77,$ sudo service openidm stop
77,You can modify the /etc/init.d/openidm script. The
77,following excerpt would run IDM with the
77,startup.sh script in the
77,/path/to/openidm directory:
77,"START_CMD=""PATH=$JAVA_BIN_PATH:$PATH;nohup $OPENIDM_HOME/startup.sh >$OPENIDM_HOME/logs/server.out 2>&1 &"""
77,You can modify this line to point to some
77,/path/to/production directory:
77,"START_CMD=""PATH=$JAVA_BIN_PATH:$PATH;nohup $OPENIDM_HOME/startup.sh -p /path/to/production >$OPENIDM_HOME/logs/server.out 2>&1 &"""
77,Run the following commands to reload the configuration and then start the
77,IDM service script:
77,$ sudo service openidm start
77,"If you run Linux with SELinux enabled, change the file context of the"
77,newly copied script with the following command:
77,$ sudo restorecon /etc/init.d/openidm
77,Verify the change to SELinux contexts with the
77,"ls -Z /etc/init.d command. For consistency, change the"
77,user context to match other scripts in the same directory with the
77,sudo chcon -u system_u /etc/init.d/openidm command.
77,1.3.2.3. Setting up a SysV Service (Ubuntu)
77,"If you're running relatively standard older versions of Ubuntu Linux,"
77,"versions which support SysV services, you'll want to set up a SysV service"
77,"script, with runlevels controlled through the update-rc.d"
77,"command. To set up such a script, navigate to the"
77,"/path/to/openidm/bin directory, and run the following"
77,command:
77,$ ./create-openidm-rc.sh --lsb
77,You can then set up and start the IDM service
77,"on a Linux distribution that uses SysV init scripts, with the following"
77,commands:
77,$ ./create-openidm-rc.sh --lsb
77,> openidm
77,$ sudo cp openidm /etc/init.d/
77,$ sudo chmod u+x /etc/init.d/openidm
77,$ sudo update-rc.d openidm defaults
77,$ sudo service openidm start
77,"To stop the IDM service, run the following command:"
77,$ sudo service openidm stop
77,You can modify the /etc/init.d/openidm script. The
77,following excerpt would run IDM with the
77,startup.sh script in the
77,/path/to/openidm directory:
77,"START_CMD=""PATH=$JAVA_BIN_PATH:$PATH;nohup $OPENIDM_HOME/startup.sh >$OPENIDM_HOME/logs/server.out 2>&1 &"""
77,You can modify this line to point to some
77,/path/to/production directory:
77,"START_CMD=""PATH=$JAVA_BIN_PATH:$PATH;nohup $OPENIDM_HOME/startup.sh -p /path/to/production >$OPENIDM_HOME/logs/server.out 2>&1 &"""
77,You can then run the following commands to reload the configuration and then
77,start the IDM service script:
77,$ sudo service openidm restart1.4. Getting Started With the REST Interface
77,ForgeRock Identity Management provides RESTful access to users in its repository.
77,"To access the repository over REST, you can use a browser-based REST"
77,"client, such as the Simple REST Client for Chrome, or"
77,RESTClient for Firefox. Alternatively you can use the
77,command-line utility that is included with most operating systems. For more
77,"information about curl, see"
77,https://github.com/bagder/curl.
77,IDM is accessible over the regular and secure HTTP ports of the
77,"Jetty Servlet container, 8080, and 8443. Most of the command-line examples"
77,"in this documentation set use the regular HTTP port, to avoid you having to"
77,"use certificates just to test IDM. In a production deployment,"
77,install a CA-signed certificate and restrict REST access to a secure
77,(HTTPS) port.
77,"To run curl over the secure port, 8443, you must either"
77,"include the --insecure option, or follow the"
77,"instructions in ""Restricting REST Access to the HTTPS Port"" in the Integrator's Guide. You can use those instructions with the"
77,"self-signed certificate that is generated when IDM starts, or"
77,with a *.crt file provided by a certificate authority.
77,Note
77,Some of the examples in this documentation set use client-assigned IDs
77,(such as bjensen and scarter) when
77,creating objects because it makes the examples easier to read. If you create
77,"objects using the Admin UI, they are created with server-assigned IDs"
77,(such as 55ef0a75-f261-47e9-a72b-f5c61c32d339).
77,"Generally, immutable server-assigned UUIDs are used in production"
77,environments.
77,Access the following URL to obtain the JSON representation of all
77,users in the IDM repository:
77,$ curl \
77,"--header ""X-OpenIDM-Username: openidm-admin"" \"
77,"--header ""X-OpenIDM-Password: openidm-admin"" \"
77,--request GET \
77,http://localhost:8080/openidm/managed/user/?_queryId=query-all-ids
77,"When you first install IDM with an empty repository, no users"
77,exist.
77,Create a user joe by sending a RESTful POST.
77,The following curl commands create a managed user in
77,"the repository, and set the user's ID to jdoe:"
77,Create joe (UNIX):$ curl \
77,"--header ""Content-Type: application/json"" \"
77,"--header ""X-OpenIDM-Username: openidm-admin"" \"
77,"--header ""X-OpenIDM-Password: openidm-admin"" \"
77,--request POST \
77,--data '{
77,"""userName"":""joe"","
77,"""givenName"":""joe"","
77,"""sn"":""smith"","
77,"""mail"":""joe@example.com"","
77,"""telephoneNumber"":""555-123-1234"","
77,"""password"":""TestPassw0rd"","
77,"""description"":""My first user"","
77,"""_id"":""joe"""
77,}' \
77,http://localhost:8080/openidm/managed/user?_action=create
77,"""_id"": ""joe"","
77,"""_rev"": ""00000000c03fd7aa"","
77,"""userName"": ""joe"","
77,"""givenName"": ""joe"","
77,"""sn"": ""smith"","
77,"""mail"": ""joe@example.com"","
77,"""telephoneNumber"": ""555-123-1234"","
77,"""description"": ""My first user"","
77,"""accountStatus"": ""active"","
77,"""effectiveRoles"": [],"
77,"""effectiveAssignments"": []"
77,Create joe (Windows):C:\> curl ^
77,"--header ""Content-Type: application/json"" ^"
77,"--header ""X-OpenIDM-Username: openidm-admin"" ^"
77,"--header ""X-OpenIDM-Password: openidm-admin"" ^"
77,--request POST ^
77,"--data ""{"
77,"\""userName\"":\""joe\"","
77,"\""givenName\"":\""joe\"","
77,"\""sn\"":\""smith\"","
77,"\""mail\"":\""joe@example.com\"","
77,"\""telephoneNumber\"":\""555-123-1234\"","
77,"\""password\"":\""TestPassw0rd\"","
77,"\""description\"":\""My first user\"","
77,"\""_id\"":\""joe\"""
77,"}"" ^"
77,http://localhost:8080/openidm/managed/user?_action=create
77,Fetch the newly created user from the repository with a RESTful
77,GET:$ curl \
77,"--header ""X-OpenIDM-Username: openidm-admin"" \"
77,"--header ""X-OpenIDM-Password: openidm-admin"" \"
77,--request GET \
77,http://localhost:8080/openidm/managed/user/joe
77,"""_id"": ""joe"","
77,"""_rev"": ""00000000c03fd7aa"","
77,"""userName"": ""joe"","
77,"""givenName"": ""joe"","
77,"""sn"": ""smith"","
77,"""mail"": ""joe@example.com"","
77,"""telephoneNumber"": ""555-123-1234"","
77,"""description"": ""My first user"","
77,"""accountStatus"": ""active"","
77,"""effectiveRoles"": [],"
77,"""effectiveAssignments"": []"
77,}1.4.1. Format REST Output For Readability
77,"By default, curl-based REST calls return the JSON object"
77,on one line.
77,"Without a bit of help, the JSON output is formatted all on one line. One"
77,"example is shown below, and it is difficult to read:"
77,"{""mail"":""joe@example.com"",""sn"":""smith"",""passwordAttempts"":""0"","
77,"""lastPasswordAttempt"":""Mon Apr 14 2014 11:13:37 GMT-0800 (GMT-08:00)"","
77,"""address2"":"""",""givenName"":""joe"",""effectiveRoles"":[""internal/role/openidm-authorized""],"
77,"""password"":{""$crypto"":{""type"":""x-simple-encryption"",""value"":{""data"":"
77,"""OBFVL9cG8uaLoo1N+SMJ3g=="",""cipher"":""AES/CBC/PKCS5Padding"",""iv"":"
77,"""7rlV4EwkwdRHkt19F8g22A=="",""key"":""openidm-sym-default""}}},""country"":"""","
77,"""city"":"""",""_rev"": ""00000000c03fd7aa"",""lastPasswordSet"":"""",""postalCode"":"""","
77,"""_id"":""joe3"",""description"":""My first user"",""accountStatus"":""active"",""telephoneNumber"":"
77,"""555-123-1234"",""roles"":[""internal/role/openidm-authorized""],""effectiveAssignments"":{},"
77,"""postalAddress"":"""",""stateProvince"":"""",""userName"":""joe3""}"
77,At least two options are available to clean up this output.
77,The standard way to format JSON output is with a JSON parser such as
77,jq. You can
77,"""pipe"" the output of a REST call to jq, as follows:"
77,$ curl \
77,"--header ""X-OpenIDM-Username: openidm-admin"" \"
77,"--header ""X-OpenIDM-Password: openidm-admin"" \"
77,--request GET \
77,"""http://localhost:8080/openidm/managed/user/joe"" \"
77,| jq .
77,The ForgeRock REST API includes an optional _prettyPrint
77,request parameter. The default value is false. To
77,"use the ForgeRock REST API to format output, add a parameter such as"
77,?_prettyPrint=true or
77,"&_prettyPrint=true, depending on whether it is added"
77,"to the end of an existing request parameter. In this case, the following"
77,command would return formatted output:
77,$ curl \
77,"--header ""X-OpenIDM-Username: openidm-admin"" \"
77,"--header ""X-OpenIDM-Password: openidm-admin"" \"
77,--request GET \
77,"""http://localhost:8080/openidm/managed/user/joe?_prettyPrint=true"""
77,Note that most command-line examples in this guide do not show this
77,"parameter, although the output is formatted for readability."
77,1.5. IDM User Interfaces
77,"You can manage IDM using Web-based user interfaces, called the"
77,UI in this documentation set.
77,"IDM provides UIs at two different endpoints,"
77,/ and /admin. We refer to the
77,administrative tools available at each endpoint as the End User UI and
77,"the Administrative UI (Admin UI), respectively."
77,The End User UI allows regular (non-administrative) users to manage their
77,"account data, consent, workflows, and shared resources. If self-service is"
77,"enabled, regular users can also self-register and reset their own passwords."
77,"For more information, see ""Configuring User Self-Service"" in the Integrator's Guide."
77,"In essence, the End User UI supports day-to-day administrative tasks for"
77,end users of an IDM system.
77,"In contrast, the Admin UI allows an administrator to define the server"
77,configuration. Administrators would access the Admin UI to learn about
77,"IDM during initial system setup, and when they identify new"
77,requirements.
77,The Admin UI also lets you configure connections to external data
77,"stores, and to specify the reconciliation and synchronization configuration"
77,between data stores.
77,"When IDM is running on the localhost system, you can access"
77,these UIs at https://localhost:8443/ and
77,"https://localhost:8443/admin, respectively."
77,1.6. About the Repository
77,"By default, IDM installs an embedded ForgeRock Directory Services"
77,(DS) instance for use as its repository. This makes it easy to
77,"get started. Before you use IDM in production, you must replace"
77,the embedded DS repository with a supported repository. For more
77,"information, see ""Selecting a Repository""."
77,You can query the internal repository directly by using the LDAP command-line
77,"utilities provided with DS. For example, the following command"
77,returns all the objects in the repository of a default IDM
77,project:
77,$ ldapsearch \
77,--hostname localhost \
77,--port 31389 \
77,"--bindDN ""cn=Directory Manager"" \"
77,--bindPassword password \
77,"--baseDN ""dc=openidm,dc=forgerock,dc=com"" \"
77,"""(objectclass=*)"""
77,"dn: dc=openidm,dc=forgerock,dc=com"
77,objectClass: top
77,objectClass: domain
77,dc: openidm
77,"dn: ou=links,dc=openidm,dc=forgerock,dc=com"
77,objectClass: top
77,objectClass: organizationalUnit
77,ou: links
77,"dn: ou=internal,dc=openidm,dc=forgerock,dc=com"
77,objectClass: top
77,objectClass: organizationalUnit
77,ou: internal
77,"dn: ou=users,ou=internal,dc=openidm,dc=forgerock,dc=com"
77,objectClass: top
77,objectClass: organizationalUnit
77,ou: users
77,...
77,"For more information about the DS command-line utilities, see the"
77,DS Tools
77,Reference.
77,1.7. Starting a New Project
77,"When you extract the IDM .zip file, you have a default project"
77,under /path/to/openidm. You can use this project to test
77,"customizations, but you should not run the default project in production."
77,Set up a new project as follows:
77,Create a directory for your new project:
77,$ mkdir /path/to/my-project
77,Note that the automated update process does not work for projects that are
77,subdirectories of the default project. You should therefore create your new
77,project directory somewhere outside of
77,/path/to/openidm/.
77,Set up a minimal configuration:
77,If your project will be similar to any of the sample configurations
77,(described in the
77,Samples Guide) copy the
77,contents of the sample to your new project.
77,For example:
77,$ cp -r /path/to/openidm/samples/sync-with-ldap/* /path/to/my-project/
77,You can then customize the sample configuration according to your
77,requirements.
77,"If you do not want to start with one of the sample configurations, copy"
77,the conf/ and script/
77,directories from the default project to your new project directory:
77,$ cd /path/to/openidm
77,$ cp -pr conf /path/to/my-project/
77,$ cp -pr script /path/to/my-project/
77,You can then customize the basic configuration according to your
77,requirements.
77,Start your new project as follows:
77,$ cd /path/to/openidm
77,$ ./startup.sh -p /path/to/my-projectChapter 2. Selecting a Repository
77,"By default, IDM uses an embedded ForgeRock Directory Services"
77,(DS) instance for its internal repository. This means that you do
77,not need to install a database in order to evaluate the software. Before using
77,"IDM in production, however, you must replace the embedded"
77,DS repository with a supported repository.
77,"In production environments, the following repositories are supported:"
77,External DS instance
77,"See ""Using an External DS Repository""."
77,Important
77,Both the default embedded and the external DS repositories do
77,not support storage of audit data. Audit logging to
77,the repository is disabled by default. Do not enable logging to the
77,repository if you are using a DS repository.
77,MySQL
77,"See ""Setting Up a MySQL Repository""."
77,MariaDB
77,"The instructions in ""Setting Up a MySQL Repository"" work equally well"
77,for MariaDB.
77,Microsoft SQL
77,"See ""Setting Up a Microsoft SQL Repository""."
77,PostgreSQL
77,"See ""Setting Up a PostgreSQL Repository""."
77,Oracle Database (Oracle DB)
77,"See ""Setting Up an Oracle DB Repository""."
77,IBM DB2 Database
77,"See ""Setting Up an IBM DB2 Repository""."
77,"For supported versions, see ""Supported Repositories"" in the Release Notes."
77,This chapter describes how to set up IDM to work with each of
77,"these supported repositories, and lists the minimum rights required for"
77,database installation and operation.
77,"For information about the repository configuration, and how to map"
77,"IDM objects to database tables or to DS LDAP objects,"
77,"see ""Managing the Repository"" in the Integrator's Guide."
77,2.1. Using the Default DS Repository
77,"By default, IDM uses the conf/repo.ds.json"
77,file to start an embedded DS instance. The embedded DS
77,repository is not supported in production environments.
77,The embedded DS server has the following configuration by default:
77,hostname - localhost
77,ldapPort - 31389
77,bindDN - cn=Directory Manager
77,bindPassword - password
77,adminPort - 34444
77,"To change the administrative port of the embedded DS server, add"
77,an adminPort property to your project's
77,conf/repo.ds.json file before you start
77,"IDM. To change any of the other default values, add an"
77,"ldapConnectionFactories property, as shown in the"
77,following example.
77,This excerpt of a repo.ds.json sets the
77,administrative port to 4444. The example changes the bind
77,password to MyPassw0rd but shows the structure of the
77,entire ldapConnectionFactories property for reference:
77,"""embedded"": true,"
77,"""maxConnectionAttempts"" : 5,"
77,"""adminPort"": 4444,"
77,"""ldapConnectionFactories"": {"
77,"""bind"": {"
77,"""primaryLdapServers"": [{ ""hostname"": ""localhost"", ""port"": 31389 }]"
77,"""root"": {"
77,"""authentication"": {"
77,"""simple"": { ""bindDn"": ""cn=Directory Manager"", ""bindPassword"": ""MyPassw0rd"" }"
77,"""queries"": {"
77,...
77,It is not necessary to add the entire ldapConnectionFactories
77,block to your configuration file but you must respect the JSON structure. For
77,"example, to change only the hostname, you would need to"
77,add at least the following:
77,...
77,"""ldapConnectionFactories"": {"
77,"""bind"": {"
77,"""primaryLdapServers"": [{ ""hostname"": ""my-hostname"" }]"
77,"""queries"": {"
77,...
77,"If you do not specify a connection property here, IDM assumes the"
77,default.
77,You can also configure an external DS instance as a repository.
77,"For more information, see ""Using an External DS Repository""."
77,Note
77,If you are running Red Hat Enterprise Linux 6 or an AWS-based Ubuntu 16.04
77,"system and do not have your own public key certificate, include the hostname"
77,"of your system in your /etc/hosts file. Otherwise, an"
77,attempt to start IDM will fail with an
77,UnknownHostException error.
77,2.2. Using an External DS Repository
77,IDM supports the use of a single external DS instance as
77,"a repository. You can use a replicated instance for backup purposes, but"
77,using multiple replicated instances (in a multimaster DS deployment)
77,is not supported.
77,"To configure a DS instance as an external IDM repository,"
77,follow these steps:
77,"If you have not yet installed DS, download it from the"
77,ForgeRock BackStage download site
77,and extract the zip archive.
77,"Install DS with the idm-repo profile, as"
77,described in the DS
77,Installation Guide.
77,"This step configures DS on the localhost, listening on ports"
77,31389 and 34444 so that it does not
77,conflict with the default ports used in the LDAP samples. You can use any
77,hostname and available ports in the setup. If you use a different host and
77,"an LDAP port other than 31389, change the"
77,primaryLdapServers property in your
77,repo.ds-external.json file accordingly.
77,"In your IDM installation, remove the default DS"
77,repository configuration file (repo.ds.json) from
77,your project's conf/ directory. For example:
77,$ cd /path/to/openidm/my-project/conf/
77,$ rm repo.ds.json
77,Copy the external DS repository configuration file
77,(repo.ds-external.json) to your project's
77,conf directory and rename it
77,repo.ds.json:
77,$ cd /path/to/openidm/
77,$ cp db/ds/conf/repo.ds-external.json my-project/conf/repo.ds.json
77,If your DS instance is not running on the
77,"localhost and listening for LDAP connections on port 31389,"
77,adjust the primaryLdapServers property in that file to
77,match your DS setup.
77,Start IDM with the configuration for your project. For example:
77,$ cd /path/to/openidm
77,$ ./startup.sh -p my-project
77,Executing ./startup.sh...
77,Using OPENIDM_HOME:
77,/path/to/openidm
77,Using PROJECT_HOME:
77,/path/to/my-project
77,Using OPENIDM_OPTS:
77,-Xmx1024m -Xms1024m
77,Using LOGGING_CONFIG: -Djava.util.logging.config.file=/path/to/my-project/conf/logging.properties
77,"-> OpenIDM version ""6.5.1.0"""
77,OpenIDM ready2.3. Database Access Rights For a JDBC Repository
77,"In general, IDM requires minimal access rights to the JDBC"
77,repository for daily operation. This section lists the minimum permissions
77,"required, and suggests a strategy for restricting database access in your"
77,deployment.
77,The JDBC repository used by IDM requires only one
77,relevant user - the service account that is used to
77,"create the tables. Generally, the details of this account are configured in"
77,the repository connection file
77,"(datasource.jdbc-default.json). By default, the username"
77,and password for this account are openidm and
77,"openidm, regardless of the database type."
77,All other users are created by the
77,db/database-type/scripts/openidm.sql
77,"script. The openidm user account must have SELECT, UPDATE,"
77,"INSERT, and DELETE permissions on all the openidm tables that are created by"
77,"this script, by the scripts that create the tables specific to the Activiti"
77,"workflow engine, and by the script that sets up the audit tables if you are"
77,using the repository audit event handler.
77,2.4. Configuring Case Insensitivity For a JDBC Repository
77,A DS repository is case-insensitive by default. The supported JDBC
77,repositories are generally case-sensitive by default. Case-sensitivity can
77,"cause issues if queries expect results to be returned, regardless of case."
77,"For example, with the default configuration of a MySQL database, a search for"
77,an email address of scarter@example.com might return a
77,"results, while a search for scarter@EXAMPLE.COM might"
77,return an Unable to find account error.
77,"If you need to support case-insensitive queries, you must configure a"
77,"case-insensitive collation in your JDBC repository, on the specific columns"
77,that require it.
77,"For example, for a generic managed object mapping in MySQL or MariaDB, change"
77,the default collation of the managedobjectproperties.propvalue
77,column to utf8_general_ci. Note that this changes
77,case-sensitivity for all managed object properties. To
77,"change case-sensitivity for all the properties of a specific object, specify"
77,a different table for the propertiesTable entry in your
77,"repo.jdbc.json for that object, and adjust the collation"
77,on that table. To change case-sensitivity only for certain properties of an
77,"object, use an explicit mapping."
77,"For a PostgreSQL repository, use an explicit table structure if you require"
77,case-insensitivity. Managing case-insensitivity at scale with generic tables
77,in PostgreSQL is not supported. For more information about generic and
77,"explicit object mappings, see ""Generic and Explicit Mappings With a JDBC Repository"" in the Integrator's Guide."
77,"To set the collation for an Oracle DB repository, see the corresponding"
77,Oracle
77,documentation.
77,"To set the collation for a SQL Server repository, see the corresponding"
77,Windows
77,documentation.
77,"To set the collation for a DB2 repository, see the corresponding"
77,DB2
77,documentation.
77,2.5. Setting Up a MySQL Repository
77,After you have installed MySQL on the local host and before
77,"starting IDM for the first time, configure the server"
77,"to use the new repository, as described in the following sections."
77,This procedure assumes that a password has already been set for the MySQL
77,root user:
77,Download MySQL
77,"Connector/J, version 5.1 or later from the MySQL website. Unpack the"
77,"delivery, and copy the .jar into the openidm/bundle"
77,directory:
77,$ cp mysql-connector-java-version-bin.jar /path/to/openidm/bundle/
77,Make sure that IDM is stopped:
77,$ cd /path/to/openidm/
77,$ ./shutdown.sh
77,"OpenIDM is not running, not stopping."
77,Remove the default DS repository configuration file
77,(repo.ds.json) from your project's
77,conf/ directory. For example:
77,$ cd /path/to/openidm/my-project/conf/
77,$ rm repo.ds.json
77,Copy the MySQL database connection configuration file
77,(datasource.jdbc-default.json) and the database table
77,configuration file (repo.jdbc.json) to your project's
77,conf directory:
77,$ cd /path/to/openidm/
77,$ cp db/mysql/conf/datasource.jdbc-default.json my-project/conf/
77,$ cp db/mysql/conf/repo.jdbc.json my-project/conf/
77,"If you have previously set up a MySQL repository for IDM, you"
77,must drop the openidm database and users before you
77,continue:
77,mysql> drop database openidm;
77,"Query OK, 21 rows affected (0.63 sec)"
77,mysql> drop user openidm;
77,"Query OK, 0 rows affected (0.02 sec)"
77,mysql> drop user openidm@localhost;
77,"Query OK, 0 rows affected (0.00 sec)"
77,Import the IDM data definition language script into MySQL:
77,$ cd /path/to/mysql
77,$ mysql -u root -p < /path/to/openidm/db/mysql/scripts/openidm.sql
77,Enter password:
77,$ Note
77,"If you see errors like Access denied for user 'root'@'localhost',"
77,and are deploying on a new
77,"installation of Ubuntu 16.04 and above, the UNIX_SOCKET plugin may be"
77,"installed, which applies Linux root credentials to"
77,"MySQL. In that case, substitute sudo mysql -u root"
77,for mysql -u root -p in the commands in this section.
77,This step creates an openidm database for use
77,"as the internal repository, and a user openidm with"
77,password openidm who has all the required privileges to
77,update the database:$ mysql -u root -p
77,Enter password:
77,Welcome to the MySQL monitor.
77,Commands end with ; or \g.
77,Your MySQL connection id is 18
77,Server version: 5.5.19 MySQL Community Server (GPL)
77,...
77,mysql> use openidm;
77,Reading table information for completion of table and column names
77,You can turn off this feature to get a quicker startup with -A
77,Database changed
77,mysql> show tables;
77,+---------------------------+
77,| Tables_in_openidm
77,+---------------------------+
77,| clusteredrecontargetids
77,| clusterobjectproperties
77,| clusterobjects
77,| configobjectproperties
77,| configobjects
77,| genericobjectproperties
77,| genericobjects
77,| ...
77,| schedulerobjects
77,| schedulerobjectproperties |
77,| uinotification
77,| updateobjectproperties
77,| updateobjects
77,+---------------------------+
77,Exit the mysql console.
77,mysql> exit
77,Bye
77,Create the IDM database user.
77,"If you are running MySQL 5.7 or higher, run the following script:"
77,$ cd /path/to/mysql
77,$ mysql -u root -p < /path/to/openidm/db/mysql/scripts/createuser.sql
77,Enter password:
77,"If you are running a MySQL version prior to 5.7, run the following script:"
77,$ cd /path/to/mysql
77,$ mysql -u root -p < /path/to/openidm/db/mysql/scripts/createuser.mysql56.sql
77,Enter password:
77,Run the three scripts that set up the tables required by the Activiti
77,workflow engine.
77,"If you are running MySQL 5.6.4 or higher, run the following scripts:"
77,$ cd /path/to/mysql
77,$ mysql -D openidm -u root -p < /path/to/openidm/db/mysql/scripts/activiti.mysql.create.engine.sql
77,Enter password:
77,$ mysql -D openidm -u root -p < /path/to/openidm/db/mysql/scripts/activiti.mysql.create.history.sql
77,Enter password:
77,$ mysql -D openidm -u root -p < /path/to/openidm/db/mysql/scripts/activiti.mysql.create.identity.sql
77,Enter password:
77,"If you are running a MySQL version prior to 5.6.4, run the following scripts:"
77,$ cd /path/to/mysql
77,$ mysql -D openidm -u root -p < /path/to/openidm/db/mysql/scripts/activiti.mysql55.create.engine.sql
77,Enter password:
77,$ mysql -D openidm -u root -p < /path/to/openidm/db/mysql/scripts/activiti.mysql55.create.history.sql
77,Enter password:
77,$ mysql -D openidm -u root -p < /path/to/openidm/db/mysql/scripts/activiti.mysql.create.identity.sql
77,Enter password:
77,"If you are planning to direct audit logs to this repository, run the script that"
77,sets up the audit tables:
77,$ mysql -D openidm -u root -p < openidm/db/mysql/scripts/audit.sql
77,Enter password:
77,Update the connection configuration to reflect your MySQL deployment. The
77,default connection configuration in the
77,datasource.jdbc-default.json file is as follows:
77,"""driverClass"" : ""com.mysql.jdbc.Driver"","
77,"""jdbcUrl"" : ""jdbc:mysql://&{openidm.repo.host}:&{openidm.repo.port}/openidm?allowMultiQueries=true&characterEncoding=utf8"","
77,"""databaseName"" : ""openidm"","
77,"""username"" : ""openidm"","
77,"""password"" : ""openidm"","
77,"""connectionTimeout"" : 30000,"
77,"""connectionPool"" : {"
77,"""type"" : ""hikari"","
77,"""minimumIdle"" : 20,"
77,"""maximumPoolSize"" : 50"
77,Specify the values for openidm.repo.host and
77,openidm.repo.port in one of the following ways:
77,Set the values in resolver/boot.properties or your
77,"project's conf/system.properties file, for example:"
77,openidm.repo.host = localhost
77,openidm.repo.port = 3306
77,Set the properties in the OPENIDM_OPTS environment
77,variable and export that variable before startup. You must include the
77,JVM memory options when you set this variable. For example:
77,"$ export OPENIDM_OPTS=""-Xmx1024m -Xms1024m -Dopenidm.repo.host=localhost -Dopenidm.repo.port=3306"""
77,$ ./startup.sh -p /path/to/openidm/my-project
77,Executing ./startup.sh...
77,Using OPENIDM_HOME:
77,/path/to/openidm
77,Using PROJECT_HOME:
77,/path/to/openidm
77,Using OPENIDM_OPTS:
77,-Xmx1024m -Xms1024m -Dopenidm.repo.host=localhost -Dopenidm.repo.port=3306
77,Using LOGGING_CONFIG: -Djava.util.logging.config.file=/path/to/openidm/conf/logging.properties
77,Using boot properties at /path/to/openidm/resolver/boot.properties
77,"-> OpenIDM version ""6.5.1.0"""
77,OpenIDM readyTip
77,"In a production environment, configure a secure connection to the repository."
77,"When you have set up MySQL for use as the internal repository, start the"
77,"server to check that the setup has been successful. After startup, you should"
77,"see that repo.jdbc is active, whereas"
77,repo.ds is enabled but not
77,active:
77,$ cd /path/to/openidm
77,$ ./startup.sh -p my-project
77,Using OPENIDM_HOME:
77,/path/to/openidm
77,Using OPENIDM_OPTS:
77,-Xmx1024m -Xms1024m
77,Using LOGGING_CONFIG:
77,-Djava.util.logging.config.file=/path/to/openidm/conf/logging.properties
77,Using boot properties at /path/to/openidm/resolver/boot.properties
77,-> scr list
77,BundleId Component Name Default State
77,Component Id State
77,PIDs (Factory PID)
77,org.forgerock.openidm.config.enhanced.starter
77,enabled
77,1] [active
77,] org.forgerock.openidm.config.enhanced.starter
77,org.forgerock.openidm.config.manage
77,enabled
77,0] [active
77,] org.forgerock.openidm.config.manage
77,10]
77,org.forgerock.openidm.datasource.jdbc
77,enabled
77,10]
77,org.forgerock.openidm.repo.jdbc
77,enabled
77,48] [active
77,] org.forgerock.openidm.repo.jdbc
77,11]
77,org.forgerock.openidm.repo.ds
77,enabled
77,...2.6. Setting Up a Microsoft SQL Repository
77,"These instructions are specific to Microsoft SQL Server 2012 R2 Standard Edition,"
77,running on a Windows Server 2012 R2 system. Adapt the instructions for your
77,environment.
77,"When you install Microsoft SQL Server, pay attention to the following"
77,specific configuration requirements:
77,"During the Feature Selection installation step, make sure that at least"
77,"SQL Server Replication, Full Text Search, and Management Tools - Basic are"
77,selected.
77,These instructions require SQL Management Studio so make sure that you
77,include Management Tools in the installation.
77,"During the Database Engine Configuration step, select Mixed Mode (SQL"
77,Server authentication and Windows authentication). IDM
77,requires SQL Server authentication.
77,TCP/IP must be enabled and configured for the correct IP address and port.
77,"To configure TCP/IP, follow these steps:"
77,Navigate to SQL Server Configuration Manager.
77,"Expand the SQL Server Network Configuration item and select ""Protocols"
77,"for MSSQLSERVER""."
77,Check that TCP/IP is Enabled.
77,Select the IP Addresses tab and set the addresses and ports on which the
77,server will listen.
77,"For this sample procedure, scroll down to IPAll and set TCP Dynamic"
77,Ports to 1433 (the default port for Microsoft SQL).
77,Click OK.
77,Restart Microsoft SQL Server for the configuration changes to take effect.
77,"To restart the server, select SQL Server Services in the left pane,"
77,double click SQL Server (MSSQLSERVER) and click Restart.
77,"If you have a firewall enabled, ensure that the port you configured in"
77,the previous step is open for IDM to access Microsoft SQL.
77,"After you have installed Microsoft SQL on the local host, install IDM,"
77,"if you have not already done so, but do not start the"
77,instance. Import the data definition and configure IDM to use
77,"the Microsoft SQL repository, as described in the following steps:"
77,Use SQL Management Studio to import the IDM data definition
77,language script into Microsoft SQL:
77,Navigate to SQL Server Management Studio.
77,"On the Connect to Server panel, select Windows Authentication and click"
77,Connect.
77,Select File > Open > File and navigate to the data definition language
77,script
77,(path\to\openidm\db\mssql\scripts\openidm.sql).
77,Click Open to open the file.
77,Click Execute to run the script.
77,This step creates an openidm database for use as the
77,"internal repository, and a user openidm with password"
77,openidm who has all the required privileges to update
77,the database. You might need to refresh the view in SQL Server Management
77,Studio to see the openidm database in the Object
77,Explorer.
77,Expand Databases > openidm > Tables. You should see the IDM
77,"tables in the openidm database, as shown in the following example."
77,Execute the three scripts that set up the tables required by the Activiti
77,workflow engine:
77,"You can use the sqlcmd command to execute the scripts,"
77,for example:
77,PS C:\Users\Administrator> sqlcmd -S localhost -d openidm ^
77,-i C:\path\to\openidm\db\mssql\scripts\activiti.mssql.create.engine.sql
77,PS C:\Users\Administrator> sqlcmd -S localhost -d openidm ^
77,-i C:\path\to\openidm\db\mssql\scripts\activiti.mssql.create.history.sql
77,PS C:\Users\Administrator> sqlcmd -S localhost -d openidm ^
77,-i C:\path\to\openidm\db\mssql\scripts\activiti.mssql.create.identity.sqlNote
77,When you run the activiti.mssql.create.engine.sql
77,"script, you might see the following warning in the log:"
77,Warning! The maximum key length is 900 bytes. The index 'ACT_UNIQ_PROCDEF' has maximum
77,"length of 1024 bytes. For some combination of large values, the insert/update operation will fail."
77,It is very unlikely that the key length will be an issue in your
77,"deployment, and you can safely ignore this warning."
77,"If you are going to direct audit logs to this repository, run the script"
77,that sets up the audit tables:
77,PS C:\Users\Administrator> sqlcmd -S localhost -d openidm ^
77,-i C:\path\to\openidm\db\mssql\scripts\audit.sql
77,Download the Microsoft JDBC Drivers for SQL Server:
77,Download the JDBC Drivers from Microsoft's
77,download site. IDM requires at least version 7.2 of the
77,"driver, which supports OSGi by default."
77,Extract the driver JAR files using 7-zip or an equivalent file management
77,application.
77,Copy the JAR file that corresponds to your Java environment to the
77,\path\to\openidm\bundle directory. For example:
77,copy mssql-jdbc-7.4.1.jre8.jar \path\to\openidm\bundle
77,Download the JDBC
77,OSGi Service Package JAR and place it in the
77,\path\to\openidm\bundle directory:
77,IDM was tested with version 1.0.0 of the service package.
77,Remove the default DS repository configuration file
77,(repo.ds.json) from your project's
77,conf/ directory. For example:
77,C:\> cd \path\to\openidm\my-project\conf\
77,.\> del repo.ds.json
77,Copy the
77,database connection configuration file for Microsoft SQL
77,(datasource.jdbc-default.json) and the database table
77,configuration file (repo.jdbc.json) to your project's
77,configuration directory. For example:
77,C:\> cd \path\to\openidm
77,.\> copy db\mssql\conf\datasource.jdbc-default.json my-project\conf\
77,.\> copy db\mssql\conf\repo.jdbc.json my-project\conf\
77,Update the connection configuration to reflect your Microsoft SQL deployment. The
77,default connection configuration in the
77,datasource.jdbc-default.json file is as follows:
77,"""driverClass"" : ""com.microsoft.sqlserver.jdbc.SQLServerDriver"","
77,"""jdbcUrl"" : ""jdbc:sqlserver://&{openidm.repo.host}:&{openidm.repo.port};instanceName=default;databaseName=openidm;applicationName=OpenIDM"","
77,"""databaseName"" : ""openidm"","
77,"""username"" : ""openidm"","
77,"""password"" : ""openidm"","
77,"""connectionTimeout"" : 30000,"
77,"""connectionPool"" : {"
77,"""type"" : ""hikari"","
77,"""minimumIdle"" : 20,"
77,"""maximumPoolSize"" : 50"
77,Specify the values for openidm.repo.host and
77,openidm.repo.port in one of the following ways:
77,Set the values in resolver/boot.properties or your
77,"project's conf/system.properties file, for example:"
77,openidm.repo.host = localhost
77,openidm.repo.port = 1433
77,Set the properties in the OPENIDM_OPTS environment variable before startup.
77,You must include the JVM memory options when you set this variable. For example::
77,"set:OPENIDM_OPTS=""-Xmx1024m -Xms1024m -Dopenidm.repo.host=localhost -Dopenidm.repo.port=1433"""
77,"After you have set up Microsoft SQL Server as the repository, make sure that IDM starts"
77,without errors.
77,2.7. Setting Up an Oracle DB Repository
77,"Before you set up Oracle DB as the IDM repository, confer with your"
77,"Oracle DBA to create the database schema, tables, and users. This section"
77,assumes that you have configured an Oracle DB with
77,Local
77,Naming Parameters (tnsnames.ora)
77,and a service user for
77,IDM.
77,Important
77,IDM supports two connection pools for an Oracle DB:
77,"Hikari Connection Pool (HikariCP), described in the"
77,HikariCP
77,GitHub Repository
77,"Oracle Universal Connection Pool (Oracle UCP), described in the"
77,Universal
77,Connection Pool for JDBC Developer's Guide
77,Many steps in this procedure will depend on your connection pool type.
77,To Set Up Oracle as an IDM Repository
77,"As the appropriate schema owner, import the IDM schema using the"
77,data definition language script
77,(/path/to/openidm/db/oracle/scripts/openidm.sql).
77,Run the scripts that set up the tables required by the Activiti workflow
77,engine.
77,"Use the Oracle SQL Developer Data Modeler to run the scripts, as described"
77,in the corresponding Oracle
77,documentation.
77,Run the following scripts:
77,/path/to/openidm/db/oracle/scripts/activiti.oracle.create.engine.sql
77,/path/to/openidm/db/oracle/scripts/activiti.oracle.create.history.sql
77,/path/to/openidm/db/oracle/scripts/activiti.oracle.create.identity.sql
77,"If you are planning to direct audit logs to this repository, run the script"
77,that sets up audit tables.
77,Use the Oracle SQL Developer Data Modeler to run the following script:
77,/path/to/openidm/db/oracle/scripts/audit.sql
77,"Set the host and port of the Oracle DB instance, either in the"
77,resolver/boot.properties file or through the
77,OPENIDM_OPTS environment variable.
77,"If you use the resolver/boot.properties file, set"
77,values for the following variables:
77,openidm.repo.host = localhostopenidm.repo.port = 1521
77,"If you use the OPENIDM_OPTS environment variable,"
77,include the JVM memory options when you set the repo host and port. For
77,example:
77,"export OPENIDM_OPTS=""-Xmx1024m -Xms1024m -Dopenidm.repo.host=localhost -Dopenidm.repo.port=1521"""
77,Remove the default DS repository configuration file
77,(repo.ds.json) from your project's
77,conf/ directory. For example:
77,rm /path/to/openidm/my-project/conf/repo.ds.json
77,Copy the Oracle DB repository configuration file
77,(repo.jdbc.json) to your project's configuration
77,directory:
77,cp /path/to/openidm/db/oracle/conf/repo.jdbc.json my-project/conf/For OracleUCP only
77,Edit the repo.jdbc.json file as follows:
77,"""dbType"" : ""ORACLE"","
77,"""useDataSource"" : ""ucp-oracle"","
77,...
77,Copy the connection configuration file to your project's configuration
77,directory and edit the file for your Oracle DB deployment. The connection
77,configuration file depends on the connection pool that you use:
77,For Hikari CP
77,Copy the following file:
77,cp /path/to/openidm/db/oracle/conf/datasource.jdbc-default.json my-project/conf/
77,Edit the file to reflect your deployment. The default configuration for
77,a HikariCP connection pool is as follows:
77,"""driverClass"" : ""oracle.jdbc.OracleDriver"","
77,"""jdbcUrl"" : ""jdbc:oracle:thin:@//&{openidm.repo.host}:&{openidm.repo.port}/DEFAULTCATALOG"","
77,"""databaseName"" : ""openidm"","
77,"""username"" : ""openidm"","
77,"""password"" : ""openidm"","
77,"""connectionTimeout"" : 30000,"
77,"""connectionPool"" : {"
77,"""type"" : ""hikari"","
77,"""minimumIdle"" : 20,"
77,"""maximumPoolSize"" : 50"
77,The jdbcUrl corresponds to the URL of the Oracle DB
77,"listener, including the service name, based on your configured Local"
77,Naming Parameters tnsnames.ora. Set this parameter
77,according to your database environment.
77,The DEFAULTCATALOG refers to the SID (system
77,"identifier), for example, orcl."
77,The username and password
77,correspond to the credentials of the service user that connects from
77,IDM.
77,For Oracle UCP
77,Copy the following file:
77,cp /path/to/openidm/db/oracle/conf/datasource.jdbc-ucp-oracle.json my-project/conf/
77,Edit the file to reflect your
77,deployment. The default connection
77,configuration for an Oracle UCP connection pool is as follows:
77,"""databaseName"" : ""openidm"","
77,"""jsonDataSource"" : {"
77,"""class"" : ""oracle.ucp.jdbc.PoolDataSourceImpl"","
77,"""settings"" : {"
77,"""connectionFactoryClassName"" : ""oracle.jdbc.pool.OracleDataSource"","
77,"""url"" : ""jdbc:oracle:thin:@&{openidm.repo.host}:&{openidm.repo.port}:SID"","
77,"""user"" : ""openidm"","
77,"""password"" : ""openidm"","
77,"""connectionTimeout"" : ""30000"","
77,"""minPoolSize"" : 20,"
77,"""maxPoolSize"" : 50"
77,The url corresponds to the URL of the Oracle DB
77,"listener, including the service ID (SID), based on"
77,your configured Local Naming Parameters tnsnames.ora.
77,"Set this property to the appropriate value for your environment, for"
77,example: jdbc:oracle:thin:@localhost:1521:orcl.
77,The user and password correspond
77,to the credentials of the service user that connects from IDM.
77,"Create an OSGi bundle for the Oracle DB driver, as follows:"
77,Download the JDBC drivers for your Oracle DB version.
77,"The files that you download depend on your Oracle DB version, and on"
77,whether you are using HikariCP or Oracle UCP. Because the version numbers
77,"change with minor updates, you must search for the precise corresponding"
77,files on oracle.com:
77,Download the ojdbc*.jar file that corresponds to
77,your Oracle DB version.
77,Download
77,bnd-2.4.0.jar.
77,This file lets you create OSGi bundles. For more information
77,"about bnd, see"
77,http://bnd.bndtools.org/.
77,For OracleUCP only
77,Download the following files:
77,ucp.jarons.jar
77,Copy the downloaded files to the
77,/path/to/openidm/db/oracle/scripts directory.
77,Create a bnd file and edit it to match the version information for your JDBC driver.
77,You can use the sample bnd file located in openidm/db/mssql/scripts.
77,"Copy that file to the directory with the JDBC driver, and rename it ojdbc8.bnd:"
77,cd /path/to/openidm/db
77,cp mssql/scripts/sqljdbc4.bnd oracle/scripts/ojdbc8.bnd
77,Edit the file for your Oracle version. The resulting file should look similar to the following:
77,version=12.2.0.1
77,Export-Package: *;version=6.5.1.0-7
77,Bundle-Name: Oracle Database 12.2.0.1 JDBC Driver
77,Bundle-SymbolicName: oracle.jdbc.OracleDriver
77,Bundle-Version: 6.5.1.0-7
77,Import-Package: *;resolution:=optionalNote
77,"Do not include trailing zeros in the version number. For example, for Oracle 12.2.0.1.0, set"
77,the version string to version=12.2.0.1.
77,From the /path/to/openidm/db/oracle/scripts
77,"directory, run the following command to create the OSGi bundle, replacing"
77,the * with your Oracle DB driver version:
77,java -jar bnd-2.4.0.jar wrap --properties ojdbc*.bnd --output ojdbc*-osgi.jar ojdbc*.jarFor OracleUCP only
77,Create bnd files for the
77,ucp.jar and ons.jar files.
77,The following examples assume version 12.2.0 Oracle JDBC drivers:
77,ucp.bndversion=12.2.0
77,Export-Package: oracle.ucp.*;version=${version}
77,Bundle-Name: Oracle Universal Connection Pool
77,Bundle-SymbolicName: oracle.ucp
77,Bundle-Version: ${version}
77,Import-Package: *;resolution:=optional
77,DynamicImport-Package: *ons.bndversion=12.2.0
77,Export-Package: *;version=${version}
77,Bundle-Name: Oracle ONS
77,Bundle-SymbolicName: oracle.ons
77,Bundle-Version: ${version}
77,Import-Package: *;resolution:=optional
77,Save the bnd files in the
77,"/path/to/openidm/db/oracle/scripts directory,"
77,then run the following commands to create the corresponding OSGi
77,bundles:
77,cd /path/to/openidm/db/oracle/scripts
77,java -jar bnd-2.4.0.jar wrap --properties ucp.bnd --output ucp-osgi.jar ucp.jar
77,java -jar bnd-2.4.0.jar wrap --properties ons.bnd --output ons-osgi.jar ons.jar
77,You can ignore any private references warnings that
77,are logged when you build these bundles.
77,Move all the OSGi bundle files to the openidm/bundle
77,directory.
77,"When you have set up Oracle DB for use as the internal repository, make"
77,sure that the server starts without errors.
77,2.8. Setting Up a PostgreSQL Repository
77,This procedure assumes that PostgreSQL is installed and running on the local
77,"host. For supported versions, see"
77,"""Supported Repositories"" in the Release Notes."
77,"Before starting IDM for the first time,"
77,"configure the server to use a PostgreSQL repository, as described in the"
77,following procedure:
77,The path/to/openidm/db/postgresql/scripts/createuser.pgsql
77,"script sets up an openidm database and user, with a"
77,default password of openidm. The script also grants the
77,appropriate permissions.
77,Edit this script if you want to change the password of the
77,"openidm user, for example:"
77,$ more /path/to/openidm/db/postgresql/scripts/createuser.pgsql
77,create user openidm with password 'mypassword';
77,create database openidm encoding 'utf8' owner openidm;
77,grant all privileges on database openidm to openidm;
77,"Edit the Postgres client authentication configuration file,"
77,pg_hba.conf. Add the following entries for the
77,following users: postgres and openidm:
77,local
77,all
77,openidm
77,trust
77,local
77,all
77,postgres
77,trust
77,"As the postgres user, execute the"
77,createuser.pgsql script as follows:
77,$ psql -U postgres < /path/to/openidm/db/postgresql/scripts/createuser.pgsql
77,CREATE ROLE
77,CREATE DATABASE
77,GRANT
77,Execute the openidm.pgsql script as the new
77,openidm user that you created in the first step:
77,$ psql -U openidm < /path/to/openidm/db/postgresql/scripts/openidm.pgsql
77,CREATE SCHEMA
77,CREATE TABLE
77,CREATE TABLE
77,CREATE TABLE
77,CREATE INDEX
77,CREATE INDEX
77,...
77,START TRANSACTION
77,INSERT 0 1
77,INSERT 0 1
77,COMMIT
77,CREATE INDEX
77,CREATE INDEX
77,Your database has now been initialized.
77,Run the three scripts that set up the tables required by the Activiti
77,workflow engine:
77,$ psql -d openidm -U openidm < /path/to/openidm/db/postgresql/scripts/activiti.postgres.create.engine.sql
77,$ psql -d openidm -U openidm < /path/to/openidm/db/postgresql/scripts/activiti.postgres.create.history.sql
77,$ psql -d openidm -U openidm < /path/to/openidm/db/postgresql/scripts/activiti.postgres.create.identity.sql
77,"If you plan to direct audit logs to this repository, run the script that"
77,sets up the audit tables:
77,$ psql -d openidm -U openidm < /path/to/openidm/db/postgresql/scripts/audit.pgsql
77,Remove the default DS repository configuration file
77,(repo.ds.json) from your project's
77,conf/ directory. For example:
77,$ cd /path/to/openidm/my-project/conf/
77,$ rm repo.ds.json
77,Copy the
77,database connection configuration file for PostgreSQL
77,(datasource.jdbc-default.json) and the database table
77,file (repo.jdbc.json) to your project's configuration
77,directory. For example:
77,$ cd /path/to/openidm
77,$ cp db/postgresql/conf/datasource.jdbc-default.json my-project/conf/
77,$ cp db/postgresql/conf/repo.jdbc.json my-project/conf/
77,Update the connection configuration to reflect your PostgreSQL deployment.
77,The default connection configuration in the
77,datasource.jdbc-default.json file is as follows:
77,"""driverClass"" : ""org.postgresql.Driver"","
77,"""jdbcUrl"" : ""jdbc:postgresql://&{openidm.repo.host}:&{openidm.repo.port}/openidm"","
77,"""databaseName"" : ""openidm"","
77,"""username"" : ""openidm"","
77,"""password"" : ""openidm"","
77,"""connectionTimeout"" : 30000,"
77,"""connectionPool"" : {"
77,"""type"" : ""hikari"","
77,"""minimumIdle"" : 20,"
77,"""maximumPoolSize"" : 50"
77,"If you changed the password in step 1 of this procedure, edit the"
77,datasource.jdbc-default.json file to set the value for
77,the password field to whatever password you set for
77,the openidm user.
77,Specify the values for openidm.repo.host and
77,openidm.repo.port in one of the following ways:
77,Set the values in your resolver/boot.properties file:
77,openidm.repo.host = localhost
77,openidm.repo.port = 5432
77,Set the properties in the OPENIDM_OPTS environment
77,variable and export that variable before startup. You must include the JVM
77,memory options when you set this variable. For example:
77,"$ export OPENIDM_OPTS=""-Xmx1024m -Xms1024m -Dopenidm.repo.host=localhost -Dopenidm.repo.port=5432"""
77,$ cd /path/to/openidm
77,$ ./startup.sh -p my-project
77,Executing ./startup.sh...
77,Using OPENIDM_HOME:
77,/path/to/openidm
77,Using PROJECT_HOME:
77,/path/to/openidm
77,Using OPENIDM_OPTS:
77,-Xmx1024m -Xms1024m -Dopenidm.repo.host=localhost -Dopenidm.repo.port=5432
77,Using LOGGING_CONFIG: -Djava.util.logging.config.file=/path/to/openidm/conf/logging.properties
77,Using boot properties at /path/to/openidm/resolver/boot.properties
77,"-> OpenIDM version ""6.5.1.0"""
77,OpenIDM ready
77,PostgreSQL is now set up for use as the internal repository.
77,Start IDM with the configuration for your project. Monitor the
77,"console for the success of your setup. After startup, run a"
77,"scr list command. In the output, you should see that"
77,"repo.jdbc is active, whereas"
77,repo.ds is enabled but not
77,active:
77,$ cd /path/to/openidm
77,$ ./startup.sh -p my-project
77,Using OPENIDM_HOME:
77,/path/to/openidm
77,Using OPENIDM_OPTS:
77,-Xmx1024m -Xms1024m
77,Using LOGGING_CONFIG:
77,-Djava.util.logging.config.file=/path/to/openidm/conf/logging.properties
77,Using boot properties at /path/to/openidm/resolver/boot.properties
77,-> scr list
77,BundleId Component Name Default State
77,Component Id State
77,PIDs (Factory PID)
77,org.forgerock.openidm.config.enhanced.starter
77,enabled
77,1] [active
77,] org.forgerock.openidm.config.enhanced.starter
77,org.forgerock.openidm.config.manage
77,enabled
77,0] [active
77,] org.forgerock.openidm.config.manage
77,10]
77,org.forgerock.openidm.datasource.jdbc
77,enabled
77,10]
77,org.forgerock.openidm.repo.jdbc
77,enabled
77,48] [active
77,] org.forgerock.openidm.repo.jdbc
77,11]
77,org.forgerock.openidm.repo.ds
77,enabled
77,...
77,Set up indexes to tune the PostgreSQL repository according to your specific deployment.
77,Important
77,No indexes are set by default. If you do not tune the repository correctly by creating the
77,"required indexes, the performance of your service can be severely impacted. For example,"
77,setting too many indexes can have an adverse effect on performance during managed object
77,"creation. Conversely, not indexing fields that are searched will severely impact search"
77,performance.
77,IDM includes a /path/to/openidm/db/postgresql/scripts/default_schema_optimization.pgsql
77,script that sets up a number of indexes. This script includes extensive comments on the indexes
77,that are being created. Review the script before you run it to ensure that all the indexes are
77,suitable for your deployment.
77,"When you have refined the script for your deployment, execute the script as a user with"
77,"superuser privileges, so that the required extensions can be created. By default, this is the"
77,postgres user:
77,$ psql -U postgres openidm < /path/to/openidm/db/postgresql/scripts/default_schema_optimization.pgsql
77,CREATE INDEX
77,CREATE INDEX
77,CREATE INDEX
77,CREATE INDEX
77,CREATE INDEX
77,CREATE INDEX2.9. Setting Up an IBM DB2 Repository
77,This section makes the following assumptions about the DB2 environment. If
77,"these assumptions do not match your DB2 environment, adapt the subsequent"
77,instructions accordingly.
77,"DB2 is running on the localhost, and is listening on the default port"
77,(50000).
77,The user db2inst1 is configured as the DB2 instance
77,"owner, and has the password Passw0rd1."
77,This section assumes that you will use basic username/password authentication.
77,"For instructions on configuring Kerberos authentication with a DB2 repository,"
77,"see ""Configuring Kerberos Authentication With a DB2 Repository""."
77,"Before you start, make sure that the server is stopped."
77,$ cd /path/to/openidm/
77,$ ./shutdown.sh
77,"OpenIDM is not running, not stopping."
77,"Configure IDM to use the DB2 repository, as described in the"
77,following steps:
77,"Create an OSGi bundle for the DB2 JDBC driver, as follows:"
77,Download the DB2 JDBC driver for your database version from the IBM download site and place it in the
77,openidm/db/db2/scripts directory.
77,Use either the db2jcc.jar or
77,"db2jcc4.jar, depending on your DB2 version. For more"
77,"information, see the DB2"
77,JDBC Driver Versions.
77,$ ls /path/to/db/db2/scripts/
77,db2jcc.jar
77,openidm.sql
77,Create a bnd file and edit it to match the version
77,information for your JDBC driver.
77,You can use the sample bnd file located in
77,openidm/db/mssql/scripts. Copy that file to the
77,directory with the JDBC driver:
77,$ cd /path/to/openidm/db
77,$ cp mssql/scripts/sqljdbc4.bnd db2/scripts/
77,$ ls db2/scripts
77,db2jcc.jar
77,openidm.sql
77,sqljdbc4.bnd
77,The JDBC driver version information for your driver is located in the
77,Specification-Version property in the MANIFEST file of
77,the driver.
77,$ cd /path/to/openidm/db/db2/scripts
77,$ unzip -q -c db2jcc.jar META-INF/MANIFEST.MF
77,Manifest-Version: 1.0
77,Created-By: 1.4.2 (IBM Corporation)
77,Edit the bnd file to match the JDBC driver version:
77,$ more sqljdbc4.bnd
77,...
77,version=1.0
77,Export-Package: *;version=${version}
77,Bundle-Name: IBM JDBC DB2 Driver
77,Bundle-SymbolicName: com.ibm.db2.jcc.db2driver
77,Bundle-Version: ${version}
77,Download the bnd JAR file
77,(bnd-2.4.0.jar)
77,that lets you create OSGi bundles. For more information about
77,"bnd, see http://bnd.bndtools.org/."
77,Place the bnd JAR file in the same directory
77,as the JDBC driver:
77,$ ls /path/to/openidm/db/db2/scripts
77,bnd-2.4.0.jar
77,db2jcc.jar
77,Change to the directory in which the script files are located and run the
77,following command to create the OSGi bundle:
77,$ cd /path/to/openidm/db/db2/scripts$ java -jar bnd-2.4.0.jar wrap --properties sqljdbc4.bnd --output db2jcc-osgi.jar db2jcc.jar
77,"This command creates an OSGi bundle, as defined by the --output"
77,option: db2jcc-osgi.jar:
77,$ ls
77,bnd-2.4.0.jar
77,db2jcc-osgi.jar
77,db2jcc.jar
77,Move the OSGi bundle fle to the openidm/bundle directory:
77,$ mv db2jcc-osgi.jar /path/to/openidm/bundle/
77,Remove the default DS repository configuration file
77,(repo.ds.json) from your project's
77,conf/ directory. For example:
77,$ cd /path/to/openidm/my-project/conf/
77,$ rm repo.ds.json
77,Copy the database connection configuration file for DB2
77,(datasource.jdbc-default.json) and the database table
77,configuration file (repo.jdbc.json) to your project's
77,configuration directory. For example:
77,$ cd /path/to/openidm/
77,$ cp db/db2/conf/datasource.jdbc-default.json my-project/conf/
77,$ cp db/db2/conf/repo.jdbc.json my-project/conf/
77,Update the connection configuration to reflect your DB2 deployment. The
77,default connection configuration in the
77,datasource.jdbc-default.json file is as follows:
77,"""driverClass"" : ""com.ibm.db2.jcc.DB2Driver"","
77,"""jdbcUrl"" : ""jdbc:db2://&{openidm.repo.host}:&{openidm.repo.port}/dopenidm:retrieveMessagesFromServerOnGetMessage=true;"","
77,"""databaseName"" : ""sopenidm"","
77,"""username"" : ""openidm"","
77,"""password"" : ""openidm"","
77,"""connectionTimeout"" : 30000,"
77,"""connectionPool"" : {"
77,"""type"" : ""hikari"","
77,"""minimumIdle"" : 20,"
77,"""maximumPoolSize"" : 50"
77,Specify the values for openidm.repo.host and
77,openidm.repo.port in one of the following ways:
77,Set the values in resolver/boot.properties or your
77,"project's conf/system.properties file, for example:"
77,openidm.repo.host = localhost
77,openidm.repo.port = 50000
77,Set the properties in the OPENIDM_OPTS environment
77,variable and export that variable before startup. You must include the JVM
77,memory options when you set this variable. For example:
77,"$ export OPENIDM_OPTS=""-Xmx1024m -Xms1024m -Dopenidm.repo.host=localhost -Dopenidm.repo.port=50000"""
77,$ cd /path/to/openidm
77,$ ./startup.sh -p my-project
77,Executing ./startup.sh...
77,Using OPENIDM_HOME:
77,/path/to/openidm
77,Using PROJECT_HOME:
77,/path/to/openidm
77,Using OPENIDM_OPTS:
77,-Xmx1024m -Xms1024m -Dopenidm.repo.host=localhost -Dopenidm.repo.port=50000
77,Using LOGGING_CONFIG: -Djava.util.logging.config.file=/path/to/openidm/conf/logging.properties
77,Using boot properties at /path/to/openidm/resolver/boot.properties
77,"-> OpenIDM version ""6.5.1.0"""
77,OpenIDM ready
77,Create a user database for IDM (dopenidm).
77,$ db2 create database dopenidm
77,Import the IDM data definition language script into your DB2
77,instance.
77,$ cd /path/to/openidm
77,$ db2 -i -tf db/db2/scripts/openidm.sql
77,The database schema is defined in the SOPENIDM database.
77,"You can show the list of tables in the repository, using the"
77,"db2 list command, as follows:"
77,$ db2 LIST TABLES for all
77,Table/View
77,Schema
77,Type
77,Creation time
77,------------------------------- --------------- ----- --------------------------
77,CLUSTEROBJECTPROPERTIES
77,SOPENIDM
77,2015-10-01-11.58.05.968933
77,CLUSTEROBJECTS
77,SOPENIDM
77,2015-10-01-11.58.05.607075
77,CONFIGOBJECTPROPERTIES
77,SOPENIDM
77,2015-10-01-11.58.01.039999
77,CONFIGOBJECTS
77,SOPENIDM
77,2015-10-01-11.58.00.570231
77,GENERICOBJECTPROPERTIES
77,SOPENIDM
77,2015-10-01-11.57.59.583530
77,GENERICOBJECTS
77,SOPENIDM
77,2015-10-01-11.57.59.152221
77,INTERNALUSER
77,SOPENIDM
77,2015-10-01-11.58.04.060990
77,LINKS
77,SOPENIDM
77,2015-10-01-11.58.01.349194
77,MANAGEDOBJECTPROPERTIES
77,SOPENIDM
77,2015-10-01-11.58.00.261556
77,MANAGEDOBJECTS
77,SOPENIDM
77,2015-10-01-11.57.59.890152
77,...
77,"Connect to the openidm database, then run the three scripts that set up the"
77,tables required by the Activiti workflow engine:
77,$ db2 connect to dopenidm
77,$ db2 -i -tf /path/to/openidm/db/db2/scripts/activiti.db2.create.engine.sql
77,$ db2 -i -tf /path/to/openidm/db/db2/scripts/activiti.db2.create.history.sql
77,$ db2 -i -tf /path/to/openidm/db/db2/scripts/activiti.db2.create.identity.sql
77,"If you plan to direct audit logs to this repository, run the script that"
77,sets up the audit tables:
77,$ db2 -i -tf /path/to/openidm/db/db2/scripts/audit.sql
77,"When you have set up DB2 for use as the internal repository, start the"
77,"server to check that the setup has been successful. After startup, you should"
77,"see that repo.jdbc is active, whereas"
77,repo.ds is enabled but not
77,active:
77,$ cd /path/to/openidm
77,$ ./startup.sh -p my-project
77,Using OPENIDM_HOME:
77,/path/to/openidm
77,Using OPENIDM_OPTS:
77,-Xmx1024m -Xms1024m
77,Using LOGGING_CONFIG:
77,-Djava.util.logging.config.file=/path/to/openidm/conf/logging.properties
77,Using boot properties at /path/to/openidm/resolver/boot.properties
77,-> scr list
77,BundleId Component Name Default State
77,Component Id State
77,PIDs (Factory PID)
77,org.forgerock.openidm.config.enhanced.starter
77,enabled
77,1] [active
77,] org.forgerock.openidm.config.enhanced.starter
77,org.forgerock.openidm.config.manage
77,enabled
77,0] [active
77,] org.forgerock.openidm.config.manage
77,10]
77,org.forgerock.openidm.datasource.jdbc
77,enabled
77,10]
77,org.forgerock.openidm.repo.jdbc
77,enabled
77,48] [active
77,] org.forgerock.openidm.repo.jdbc
77,11]
77,org.forgerock.openidm.repo.ds
77,enabled
77,...2.9.1. Configuring Kerberos Authentication With a DB2 Repository
77,"By default, IDM uses the username and password configured in the"
77,repository connection configuration file
77,(conf/datasource.jdbc-default.json) to connect to the
77,DB2 repository. You can configure IDM to use Kerberos
77,authentication instead.
77,"In this scenario, IDM acts as a client and"
77,"requests a Kerberos ticket for a service, which is DB2,"
77,through the JDBC driver.
77,This section assumes that you have configured DB2 for Kerberos
77,"authentication. If that is not the case, follow the instructions in the"
77,corresponding DB2
77,documentation before you read this section.
77,The following diagram shows how the ticket is obtained and how the keytab is
77,referenced from IDM's jaas.conf file.
77,Using Kerberos to Connect to a DB2 Repository
77,To configure IDM for Kerberos authentication:
77,"Create a keytab file, specifically for use by IDM."
77,A Kerberos keytab file (krb5.keytab) is an encrypted
77,copy of the host's key. The keytab enables DB2 to validate the Kerberos
77,ticket that it receives from IDM. You must create a keytab
77,file on the host that IDM runs on. The keytab file must be
77,secured in the same way that you would secure any password file.
77,"Specifically, only the user running IDM should have read and"
77,write access to this file.
77,"Create a keytab for DB2 authentication, in the file"
77,openidm/security/idm.keytab/:
77,$ kadmin -p kadmin/admin -w password
77,$ kadmin: ktadd -k /path/to/openidm/security/idm.keytab db2/idm.example.com
77,Make sure that the DB2 user has read access to the keytab.
77,Copy the DB2 Java Authentication and Authorization Service (JAAS)
77,configuration file to the IDM security
77,directory:
77,$ cd path/to/openidm
77,$ cp db/db2/conf/jaas.conf security/
77,"By default, IDM assumes that the keytab is in the file"
77,openidm/security/idm.keytab and that the principal
77,identity is db2/idm.example.com@EXAMPLE.COM. Change the
77,following lines in the jaas.conf file if you are using
77,a different keytab:
77,"keyTab=""security/idm.keytab"""
77,"principal=""db2/idm.example.com@EXAMPLE.COM"""
77,Adjust the authentication details in your DB2 connection configuration
77,file (conf/datasource.jdbc-default.json). Edit that
77,file to remove password field and change the username
77,to the instance owner (db2). The following excerpt
77,shows the modified file:
77,...
77,"""databaseName"" : ""sopenidm"","
77,"""username"" : ""db2"","
77,"""connectionTimeout"" : 30000,"
77,...
77,"Edit your project's conf/system.properties file, to"
77,add the required Java options for Kerberos authentication.
77,"In particular, add the following two lines to that file:"
77,db2.jcc.securityMechanism=11
77,java.security.auth.login.config=security/jaas.conf
77,Restart IDM.
77,Chapter 3. Removing and Moving Server Software
77,This chapter covers uninstallation of an IDM server.
77,To Remove IDM(Optional)
77,"Stop the server if it is running, as described in"
77,"""To Stop IDM""."
77,Remove the directory where you installed the software:
77,"$ rm -rf /path/to/openidm(Optional) If you use a JDBC database for the repository, you can"
77,drop the openidm database.Chapter 4. Updating Servers
77,This chapter describes how to update an existing deployment to IDM
77,6.5.1.0.
77,The update process is largely dependent on your deployment and
77,on the extent to which you have customized IDM. Engage
77,ForgeRock Support
77,for help with updating an existing deployment.
77,The automated update process available with previous IDM versions is no
77,longer supported. This chapter describes the manual process required to update
77,"an existing IDM deployment. At a high level, the manual update process involves"
77,the following steps:
77,Create a new installation of IDM.
77,Migrate your existing configuration to the new installation. To migrate your
77,"configuration, copy your IDM 6.0 configuration files to the new"
77,"IDM 6.5.1.0 installation, making the changes"
77,"specified later in this chapter and in ""Required Changes to IDM"" in the Release Notes,"
77,then systematically enable each IDM 6.5.1.0 feature
77,that you need.
77,"Your 6.0 configuration includes any customizations made to the Admin UI,"
77,generally with files in the openidm/ui/admin/extension
77,directory.
77,Note: you cannot copy configuration files directly from a version prior to
77,IDM 6. You must either manually migrate your customizations to the new
77,"configuration files, or run the old update process for each major version"
77,"successively, so from 4 to 4.5 then from 4.5 to 5, and so on. Instructions for"
77,updating to each major version are available in the Installation
77,Guide for that version.
77,Migrate your existing data to the new installation. To migrate you can
77,either:
77,"Use your existing repository, making the changes specified later in this"
77,"chapter and in ""Updating to IDM 6.5"" in the Release Notes."
77,Create a new repository and migrate existing data to it with the new data
77,"migration service. For more information about the data migration service,"
77,"see ""Migrating Data from Previous Versions of IDM""."
77,4.1. Preparing Systems for An Update
77,Take the following steps before you start to update your deployment:
77,Update your Java environment.
77,"IDM requires a supported Java environment, as described in"
77,"""Preparing the Java Environment"" in the Release Notes."
77,"If your server uses an older version, install a newer Java version before"
77,"you update and follow the instructions in ""Java Prerequisites""."
77,Download the new software.
77,Download and extract IDM-6.5.1.0.zip from the
77,ForgeRock BackStage download site.
77,Back up your existing deployment by archiving the
77,openidm directory and the contents of your repository.
77,"If you have encrypted or obfuscated any properties in configuration files,"
77,decrypt them into their plain text values before you start the update.
77,"When you have completed the update, you can encrypt or obfuscate the"
77,values again.
77,Save your audit data. If you want to preserve a record of the audit logs
77,"collected while IDM 6.0 was running, save these log files manually"
77,before you start the update process. You can find the log files in the
77,/path/to/openidm/audit/ directory. For more
77,"information on these files, see"
77,"""Audit Event Topics"" in the Integrator's Guide."
77,Tip
77,File rotation tip: When you delete files in the
77,"/path/to/openidm/audit/ directory, IDM"
77,creates new files as needed in the same directory.
77,4.2. Migrating Your Existing Server Configuration
77,"Before you start migrating your configuration, be sure to review ""Compatibility"" in the Release Notes and ""Updating to IDM 6.5"" in the Release Notes. These"
77,include important compatibility notes and instructions for updating your
77,IDM installation.
77,"In most cases, the configuration files from your IDM 6.0 installation"
77,"will work without further modification, and can be migrated directly to your"
77,"new 6.5.1.0 installation. There are, however, changes in some"
77,files that are required for your migration to work correctly. These changes
77,"are covered in ""Updating to IDM 6.5"" in the Release Notes, and noted"
77,where appropriate below.
77,4.2.1. Migrating Configuration Files
77,Because there is no automated way to migrate a customized configuration to
77,"IDM 6.5.1.0, you will need to migrate these files"
77,"to your new installation manually. Before migrating your configuration, note"
77,the new files added in IDM 6.5.1.0:
77,endpoint-removeRepoPathFromRelationships.json: Used
77,to modify existing relationship references to no longer include
77,"repo/ in the path. For more information, see ""Changes to repo/internal"" in the Release Notes."
77,endpoint-updateInternalUserAndInternalRoleEntries.json:
77,Used to update internal users and internal roles to accommodate new
77,"privileges functionality. For more information, see ""Enabling Privileges"" in the Release Notes."
77,internal.json: Provides access to relationship
77,resource collections for internal objects. This is primarily related to
77,"new privileges functionality. For more information, see ""Enabling Privileges"" in the Release Notes."
77,java.security: Used by IDM to extend the
77,security properties defined in your JDK's java.security file. For more
77,"information, see ""Configuring IDM to Support an HSM Provider"" in the Integrator's Guide."
77,notification-passwordUpdate.json: Part of IDM's
77,"new notification service. For more information, see ""Configuring Notifications"" in the Integrator's Guide."
77,notification-profileUpdate.json: Part of IDM's
77,"new notification service. For more information, see ""Configuring Notifications"" in the Integrator's Guide."
77,notificationFactory.json: Part of IDM's
77,new notification service. If you are planning to use the old notification
77,"system, you should either delete or disable this file, and add your"
77,existing endpoint-usernotifications.json file. For
77,"more information, see ""Configuring Notifications"" in the Integrator's Guide."
77,"repo.init.json: Used by IDM when starting,"
77,"defining initial internal users and roles. For more information, see"
77,"""Internal Users"" in the Integrator's Guide and"
77,"""Roles and Authentication"" in the Integrator's Guide."
77,secrets.json: Used by IDM for managing
77,security and encryption related configuration. This information was
77,previously stored in boot.properties. For more
77,"information, see ""Changes for the New Secrets Service"" in the Release Notes."
77,ui.context-enduser.json: Used by the new IDM
77,"End User UI. If you intend to use the old End User UI, this file should"
77,be removed and replaced by your existing
77,ui.context-selfservice.json file.
77,Also note that two files are no longer included by default in the new
77,conf/ directory:
77,endpoint-usernotifications.json: This file was used
77,by the old notification service (IDM versions 6.0 and prior). Do
77,not migrate this file if you are planning to use the new notification
77,service. Any customizations to this file should be adapted to use the
77,new notification service instead.
77,ui.context-selfservice.json: This file was used by
77,the old End User UI. Do not migrate this file if you are planning to use
77,the new End User UI. Any customizations to this file should be adapted to
77,use the new UI instead.
77,"For all other files found in conf/, double-check ""Compatibility"" in the Release Notes to ensure any"
77,"customizations you've made are compatible with changes in IDM 6.5,"
77,"then copy your old configuration to your new installation. In some cases,"
77,you may find it easier to adapt your customizations to the new version of the
77,"file instead, in particular if you plan to use IDM's new features."
77,"For example, the following files changed significantly from the previous"
77,version of IDM:
77,managed.json
77,policy.json
77,repo.ds.json and repo.jdbc.json
77,authorization.json
77,While the older versions of these files should still work (as long as you
77,"perform any changes noted as required in ""Required Changes to IDM"" in the Release Notes and migrate the"
77,"scripts noted in ""Migrating Custom Scripts""), it may be easier to"
77,migrate your customizations to the new version of these files instead.
77,4.2.2. Migrating boot.properties
77,"On the IDM 6.5 server, edit the"
77,resolver/boot.properties file to match any
77,customizations that you made on your IDM 6 server.
77,"Specifically, check the following elements:"
77,"The HTTP, HTTPS, and mutual authentication ports are specified in the"
77,resolver/boot.properties file. If you changed the
77,"default ports in your IDM 6 deployment, make"
77,sure that the corresponding ports are specified in this file.
77,Security-related configurations (such as secrets and encryption) have
77,been moved to the new conf/secrets.json file. Any
77,customizations you've made to these fields should be migrated to use
77,secrets.json. For more information about
77,"secrets.json, see ""Accessing IDM Keys and Certificates"" in the Integrator's Guide."
77,Notes about further changes to your configuration related to the new
77,"secrets service can be found in ""Changes for the New Secrets Service"" in the Release Notes."
77,Check that the keystore and truststore passwords match the current
77,passwords for the keystore and truststore of your IDM
77,6 deployment.
77,Depending on the level of customization you have made in your current
77,"deployment, it might be simpler to start with your IDM"
77,"6 boot.properties file, and copy"
77,all customized settings from that file to the corresponding IDM
77,"6.5 file. As a best practice, you should keep all"
77,configuration customizations (including new properties and changed settings)
77,in a single location. You can then copy and paste these changes as
77,appropriate.
77,4.2.3. Migrating Security Settings
77,Copy the contents of your IDM 6
77,security/ folder to the IDM 6.5
77,"instance. By default, this contains three files:"
77,keystore.jceksrealm.propertiestruststore
77,"If you made no changes to realm.properties, use the"
77,"newer version of the file. If you use the older file, adjust"
77,openidm-authorized to be
77,internal/role/openidm-authorized.
77,Warning
77,If you do not copy your old truststore and keystore files to your new
77,"instance, you will be unable to decrypt anything that was encrypted by your"
77,old instance of IDM.
77,"As noted in ""Migrating boot.properties"", security and"
77,encryption-related properties previously stored in
77,boot.properties are now stored in
77,conf/secrets.json. Further notes about changes to
77,your configuration files related to the new secrets service can be found in
77,"""Changes for the New Secrets Service"" in the Release Notes."
77,4.2.4. Migrating Custom Scripts
77,Migrate any custom scripts or default scripts that you have
77,modified to the script directory of your
77,"IDM 6.5.1.0 instance. In general, custom and"
77,customized scripts should be located in the script
77,directory of the existing IDM deployment.
77,"For custom scripts, review ""Compatibility"" in the Release Notes. If"
77,you're confident that the scripts will work as intended on IDM
77,"6.5.1.0, then copy these scripts to the new instance."
77,"If you modified an existing script, compare the default versions of the"
77,IDM 6.0 and IDM 6.5.1.0 scripts. If nothing
77,"has changed between the default versions, review your customizations against"
77,"""Compatibility"" in the Release Notes. If you"
77,"are confident that your changes will work as intended on the new version,"
77,copy the customized scripts to the new script
77,directory.
77,If a default script has changed since the IDM 6
77,"release, you will need to test that your customizations work with the new"
77,default script before porting your changes to that new script.
77,Note
77,Some scripts that were previously found in defaults/
77,are no longer included with IDM. Be sure to check that any scripts
77,you were previously using are still present in your new installation.
77,"Two examples are onDelete-user-cleanup.js, which was"
77,referenced in the managed/user onDelete action; and
77,"userNotifications.js, which was called by"
77,endpoint-usernotifications.json.
77,Special attention should be paid to the changes made in
77,"access.js, which received a large number of changes in"
77,the new version of IDM. If you are planning to use the new features
77,"of IDM, you may find it easier to migrate your customizations to the"
77,"new file, rather than adapt your old access file to the new features."
77,"If you modify any shell scripts, such as startup.sh, you must migrate"
77,your changes manually to the new version of the script.
77,4.2.5. Migrating Provisioner Files
77,Modify any customized provisioner configurations in your existing
77,deployment to point to the connectors that are provided with IDM
77,"6.5.1.0. Specifically, make sure that the"
77,connectorRef properties reflect the new connector
77,"versions, where applicable. For example:"
77,"""connectorRef"" : {"
77,"""bundleName"": ""org.forgerock.openicf.connectors.ldap-connector"","
77,"""bundleVersion"": ""[1.5.19.0,1.6.0.0)"","
77,"""connectorName"": ""org.identityconnectors.ldap.LdapConnector"""
77,"Alternatively, copy the connector .jar files from your existing"
77,deployment into the openidm/connectors directory of
77,the new installation.
77,4.2.6. Migrating Custom Workflows
77,"Previously, Activiti workflow templates used JQuery and Handlebars. If your"
77,"deployment includes existing workflows, you must"
77,rewrite these to use Vue JS if you want to view them in the new End User UI.
77,The new UI does not support older workflow templates that use JQuery and
77,Handlebars.
77,"To rewrite existing workflows for the new UI, you must have a basic"
77,understanding of the Vue JS framework and how to create components. For more
77,"information, see the"
77,Vue documentation.
77,"For an example of a workflow template written for the new UI, see"
77,/path/to/samples/provisioning-with-workflow/workflow/contactorOnboarding.bar.
77,This archive file includes the workflow definition
77,(contactorOnboarding.bpmn20.xml) and the corresponding
77,JavaScript template (contractorForm.js) to render the
77,workflow in the new UI.
77,If you previously generated your workflows with a bpmn file (and never
77,"created custom JavaScript files), the new UI will just generate these as"
77,before and you will not have to convert them.
77,"""In progress"" workflows must be compatible with IDM 6.5, or must be rewritten before you"
77,"upgrade to IDM 6.5. You will not be able to complete incompatible ""in progress"""
77,workflows on an upgraded system.
77,4.3. Updating the IDM Repository
77,"After migrating your configuration files to your new IDM installation,"
77,You will need to handle the data stored in your IDM repository. There
77,"are two options: upgrade your existing repository, or create a new repository."
77,"Once the repository has been updated or created and populated, complete the"
77,"IDM 6.5.1.0 installation, as described in ""Preparing to Install and Run Servers"". Your new IDM instance should be ready for"
77,testing to ensure all scripts and functionality are working as intended.
77,4.3.1. Upgrade Your Existing Repository
77,"Copy or connect to your existing repository, making updates as needed for"
77,newer features and functionality. This has the benefit of not needing any
77,"data migration, but does require running a series of provided scripts to"
77,"modify the repository, in order to make use of any of the new capabilities"
77,of IDM.
77,If you intend to use your existing repository with your new IDM
77,"instance, please review the steps found in ""Updating to IDM 6.5"" in the Release Notes,"
77,"following all required instructions, as well as any recommended or"
77,feature-specific instructions appropriate for your deployment.
77,There are a few steps you will need to take when preparing your existing
77,repository for use with the new version of IDM:
77,"Clear all configobjects related tables. For example,"
77,in MySQL run:
77,DELETE FROM openidm.configobjects;
77,DELETE FROM openidm.configobjectproperties;
77,Run each of the schema update scripts and make any configuration
77,"modifications identified in ""Required Changes to IDM"" in the Release Notes."
77,Important
77,"When you run the alter_uinotification.sql update script, you might see"
77,an error similar to the following:
77,Column createDate to be modified to NOT NULL is already NOT NULL
77,You can safely ignore this error.
77,"If you are using a managed relational database service such as Amazon RDS, be aware that"
77,some update scripts might require root level access to the system tables in the underlying
77,database.
77,"Specifically, certain PostgreSQL update scripts require access to the pg_attribute"
77,"table. Because the database service master user is not the same as the PostgreSQL root user,"
77,"such scripts might fail with a permissions error. In this case, investigate the failing"
77,"script, and use an ALTER TABLE command on the specific IDM table"
77,instead.
77,Run each of the schema update scripts and configuration modifications
77,"identified in ""Enabling New Features in IDM"" in the Release Notes"
77,for the new features you wish to enable.
77,Launch IDM and run the following Groovy script to clear the
77,reconprogressstate data in your repository:
77,def result = openidm.query(
77,"""repo/reconprogressstate"", [ ""_queryFilter"" : ""true"", ""_fields"" : ""_id"" ]).result;"
77,for ( item in result ) {
77,"openidm.delete(""repo/reconprogressstate/"" + item[""_id""], null);"
77,"return result.size() + "" reconprogressstate records deleted"";"
77,"This script will work regardless of the type of repository, and can be"
77,sent as a REST call. For example:
77,curl \
77,"--header ""X-OpenIDM-Username: openidm-admin"" \"
77,"--header ""X-OpenIDM-Password: openidm-admin"" \"
77,"--header ""Content-Type: application/json"" \"
77,--request POST \
77,--data '{
77,"""type"":""groovy"","
77,"""source"":""def result = openidm.query(\""repo/reconprogressstate\"", [ \""_queryFilter\"" : \""true\"", \""_fields\"" : \""_id\"" ]).result; for ( item in result ) { openidm.delete(\""repo/reconprogressstate/\"" + item[\""_id\""], null); }; return result.size() + \"" reconprogressstate records deleted\"";"""
77,}' \
77,"""http://localhost:8080/openidm/script?_action=eval"""
77,"""1 reconprogressstate records deleted"""
77,Verify all scripts and functions behave as expected.
77,Note
77,"For particularly large repositories, there are additional optimizations"
77,recommended to improve performance during your upgrade. When running the
77,removeRepoPathFromRelationships endpoint:
77,"If you are using PostgreSQL, creating an index of _id"
77,in the openidm.relationships table can help improve
77,performance. For example:
77,"create unique index on openidm.relationships(json_extract_path_text(fullobject, VARIADIC ARRAY['_id'::text]));"
77,Once your migration is complete and you have performed any other updates
77,"necessary for your installation, this index can be safely removed."
77,Increase _pageSize for the
77,removeRepoPathFromRelationships endpoint. This value
77,can be found in
77,bin/defaults/script/update/removeRepoPathFromRelationships.js.
77,"The default is set to 1000 records per page, but can be increased depending"
77,on the resources for your installation.
77,4.3.2. Create a New Repository
77,"Set up a new repository, following the steps found in ""Selecting a Repository"". This has the benefit of being already"
77,"configured for all the new capabilities in IDM, but does require"
77,migrating your existing data to the new repository.
77,"If you intend to use a new repository with your new IDM instance,"
77,"please see ""Migrating Data from Previous Versions of IDM"" for more information. Please"
77,"note, if you choose to create a new repository, you will still need to"
77,update your configuration files to effectively make use of the new features
77,"listed in ""Updating to IDM 6.5"" in the Release Notes."
77,4.4. Migrating Data from Previous Versions of IDM
77,IDM 6.5 includes a data migration service to help
77,move information stored in IDM to a new deployment. This service
77,"is off by default. To enable it, copy migration.json"
77,from samples/example-configurations/conf/
77,into your conf/ directory.
77,"Migration is run from your new installation, using your previous deployment"
77,as a data source. The data migration service supports importing information
77,from IDM instances back to version 4. If you are migrating from a
77,"version of IDM earlier than that, you will need to follow previous"
77,update instructions to get your deployment into a state where it can be
77,migrated using this service.
77,Note
77,"Because the migration service migrates information that may be encrypted,"
77,"such as passwords, it is important to make sure you have copied the"
77,truststore and keystore files from
77,your previous deployment prior to starting the
77,migration.
77,"By default, the data migration service will import:"
77,Internal Roles
77,Internal Users
77,Internal User Metadata
77,Managed Roles
77,Managed Users
77,Managed Assignments
77,Links and Relationships
77,Scheduler jobs
77,Note
77,"If you are migrating scheduler jobs from IDM 4.0 or 4.5, you will"
77,need to modify the entry in migration.json to be:
77,"""source"" : ""scheduler"","
77,"""target"" : ""scheduler/job"""
77,"If you have added additional object types (for example, managed devices),"
77,modify migration.json to include these objects.
77,4.4.1. Configuring the Migration service
77,The data migration service is configured through
77,"migration.json, and comes preconfigured for migrating"
77,the information that IDM stores in a default installation. If you
77,"have made additional customizations, you may need to modify this file to"
77,include your custom data. Several properties are available to help with
77,migration:
77,connection
77,Connection provides configuration information for the source IDM
77,instance you are migrating from. Available properties:
77,instanceUrl
77,The URI for the source IDM instance.
77,userName
77,Used for authenticating on the source IDM instance.
77,password
77,Used for authenticating on the source IDM instance.
77,socketTimeout
77,"The TCP socket timeout, when waiting for HTTP responses. If you do not set a time duration, the default is 10 seconds."
77,Example valid time duration values:4 days59 minutes and 1 millisecond1 minute and 10 seconds42 millisunlimitednonezeromappings
77,A list of mappings you wish to migrate from your old IDM instance
77,to your new deployment.
77,Each mapping can contain:
77,source
77,This is the only property that is required for data
77,migration. The source should be the path to the resource within the
77,"repo; for example, repo/managed/user."
77,target
77,"The path to the resource within the target repository. By default,"
77,this will be the same as the source path.
77,runTargetPhase
77,Specifies whether the migration should run the target phase of
77,"reconciliation. By default, this is set to false."
77,reconSourceQueryPaging
77,Specifies whether the migration service should use paging when
77,"querying the source installation. By default, this is set to false."
77,"If you have a large data set and are concerned about memory usage,"
77,you may wish to turn paging on.
77,reconSourceQueryPageSize
77,"Specifies the number of results to return per page, if paging is"
77,"turned on. By default, 1000 results per page are returned."
77,allowEmptySourceSet
77,Specifies whether the migration service should continue if it
77,encounters an empty source mapping. This is enabled by default.
77,properties
77,"An array of properties you wish to perform additional actions on,"
77,such as modifying the contents of a property during the migration.
77,(This follows the pattern you would find in a standard reconciliation.
77,"For more information about transforming data during a reconciliation,"
77,"see""Transforming Attributes in a Mapping"" in the Integrator's Guide.)"
77,policies
77,An array of policies you wish to apply to the data being migrated.
77,onCreate
77,The script used by the migration service for creating the data that
77,"is being migrated in the new installation. By default, this points to"
77,a Groovy script: update/mapLegacyObject.groovy.
77,onUpdate
77,The script used by the migration service for updating the data that
77,"is being migrated in the new installation. By default, this points to"
77,a Groovy script: update/mapLegacyObject.groovy.
77,onDelete
77,"If you wish to also delete data during your migration, you can specify"
77,"an onDelete script with this property. By default, this property is"
77,empty.
77,correlationQuery
77,"You can specify a custom correlation query. By default, this is:"
77,"""var map = {'_queryFilter': '_id eq \""' + source._id + '\""'}; map;"""
77,"For more information about writing correlation queries, see ""Correlating Source Objects With Existing Target Objects"" in the Integrator's Guide."
77,validSource
77,You can specify a script to validate the source object prior to
77,"migration. By default, this property is empty."
77,endpoint
77,"By default, the migration service endpoint is migration."
77,You can use the endpoint property to change this if
77,needed.
77,Note
77,"If your IDM repository is large, you may be able to improve"
77,migration performance by turning on paging (using
77,"reconSourceQueryPaging), and increasing the query page"
77,size using reconSourceQueryPageSize in your
77,migration.json file. This defaults to 1000 records
77,per page. The most effective page size will vary depending on the resources
77,available to your installation.
77,Since the data migration service is performing a reconciliation between
77,"your old installation and your new installation, the types of optimizations"
77,used in other types of reconciliations should also be effective with the
77,data migration service. For more information about reconciliation
77,"optimization, see ""Optimizing Reconciliation Performance"" in the Integrator's Guide."
77,4.4.2. Running Your Migration
77,"Before you run your migration, ensure that you have done the following:"
77,Paused any scheduled jobs on the source deployment
77,Configured your conf/migration.json and
77,update/mapLegacyObject.groovy files on the new
77,IDM installation
77,Moved your config files from the old deployment to the new one
77,"When you launch the new IDM installation, a new"
77,migration endpoint should be available. This endpoint
77,supports the following actions:
77,migrate: Triggers a migration of all legacy objects
77,from the remote system. Optionally takes a mapping parameter in order to
77,specify a specific mapping to migrate. For example:
77,$ curl \
77,"--header ""X-OpenIDM-Username: openidm-admin"" \"
77,"--header ""X-OpenIDM-Password: openidm-admin"" \"
77,--request POST \
77,"""http://localhost:8080/openidm/migration?_action=migrate&mapping=repoManagedUser_repoManagedUser"""
77,status: Returns the last status for all reconciliations
77,triggered via the migration service.
77,mappingConfigurations: Returns the full list of
77,migration mapping configurations.
77,mappingNames: Returns the list of migration mapping
77,names.
77,The period of time a migration takes will depend on the amount of
77,information being migrated. Migrated data will retain the same object IDs
77,they had in the previous deployment.
77,4.5. Updating a Clustered Deployment
77,Follow these general steps when you are updating servers in a cluster:
77,Redirect client traffic to a different IDM system or cluster.
77,Shut down every node in the cluster.
77,Update one node in the cluster.
77,Clone the first node to the other nodes in that cluster.
77,4.6. Updating UI CustomizationsNote
77,"The End User UI has been completely rewritten, using a new framework. This"
77,"greatly improves customization and maintainability, but does mean you will"
77,need to adapt your old UI customizations by hand to the new system.
77,"If you have a custom Admin UI, save any custom files from the"
77,openidm/ui/admin/extension subdirectory.
77,Delete the existing openidm/ui/admin/extension
77,subdirectory.
77,Copy the default UI files from the
77,openidm/ui/admin/default subdirectory with the
77,following command:
77,$ cd /path/to/openidm/ui
77,$ cp -r admin/default/. admin/extension
77,Review any custom UI files from your IDM 6.0 deployment and compare
77,them against the IDM 6.5.1.0 version of these files.
77,Apply custom changes to each new IDM 6.5 UI
77,file in the openidm/ui/admin/extension and subdirectory.
77,4.7. Updating to IDM 6.5.1.0
77,"In the following sections, run these procedures:"
77,"""Preparing Your 6.5.1.0 Update"".""Updating to Version 6.5.1.0""."
77,You can run an update to IDM 6.5.1.0 from IDM version 6.5.0 using
77,"the command-line interface script, cli.sh. If you are updating from an"
77,"IDM deployment prior to 6.5.0, use the manual migration steps in ""Migrating Your Existing Server Configuration""."
77,Important
77,The automated update process is not supported on Windows platforms.
77,Preparing Your 6.5.1.0 Update
77,Download IDM-6.5.1.0.zip
77,from ForgeRock's BackStage
77,site.
77,Copy IDM-6.5.1.0.zip to /path/to/openidm/bin/update.
77,Back up your 6.5 deployment. Save any customized *.json configuration
77,"files, located in your project's /conf directory. Save your custom directories."
77,"If you have a read-only deployment, mount the directory in read-write mode before starting the update."
77,"If you integrated IDM with AM, disable the authentication modules that you used."
77,"If you have a custom resolver/boot.properties file in your IDM 6.5 deployment,"
77,"you will need to copy the file to your 6.5.1.0 deployment. For example, if you"
77,"have an IDM 6.5 deployment, extract IDM-6.5.1.0.zip to a temporary directory."
77,"Then, copy the custom IDM 6.5 resolver/boot.properties"
77,file to your 6.5.1.0 resolver directory.
77,"If you have not customized the default resolver/boot.properties file in your deployment, you can simply"
77,overwrite it with version 6.5.1.0 of this file.
77,Start IDM. IDM must be running when you launch an update.
77,Note
77,"You must use the CLI to update your system. As of IDM 6.5, the facility to update"
77,servers through the Admin UI has been removed.
77,Disable and re-enable connections to AM.
77,Updating to Version 6.5.1.0
77,Go to your IDM installation:
77,$ cd /path/to/openidm
77,Run the cli.sh command with the --skipRepoUpdatePreview option. There are no
77,repository updates during a patch release:
77,$ ./cli.sh update \
77,--acceptLicense \
77,--user openidm-admin:openidm-admin \
77,--url http://localhost:8080/openidm \
77,--skipRepoUpdatePreview \
77,"IDM-6.5.1.0.zipThe update process continues and completes with ""Scheduler has been resumed.""Executing ./cli.sh..."
77,Starting shell in /Users/namespace/Downloads/openidm
77,License was accepted via command line argument.
77,Repository update preview was skipped.
77,Pausing the Scheduler
77,Scheduler has been paused.
77,Waiting for running jobs to finish.
77,All running jobs have finished.
77,Entering into maintenance mode...
77,Now in maintenance mode.
77,Installing the update archive IDM-6.5.0.2.zip
77,Update procedure is still processing...
77,Update procedure is still processing...
77,Update procedure is still processing...
77,Update procedure is still processing...
77,Update procedure is still processing...
77,Update procedure is still processing...
77,The update process is complete.
77,Exiting maintenance mode...
77,No longer in maintenance mode.
77,Resuming the job scheduler.
77,"Scheduler has been resumed..After update, check if you have multiple versions of the bundle/openidm-repo-opendj-<version> files, for example,"
77,bundle/openidm-repo-opendj-6.5.0.x.jar.
77,"Manually remove the oldest version, which should be the file generated from the release before the 6.5.0.2 update.Manually update the conf/ui-themeconfig.json. Change the Bootstrap version to"
77,css/bootstrap-3.4.1-custom.css from the previous 3.3.7 version.
77,"The update does not touch this file, to avoid overwriting possible customization of the UI theme."
77,"If you have a custom Jetty configuration, see the"
77,Release Notes
77,for an important change in functionality.
77,"If you have customized your version 6 deployment, you might find files with the following"
77,extensions: .old and .new.
77,"On Linux/UNIX systems, you can locate these files with the following commands:"
77,$ cd /path/to/openidm
77,"$ find . -type f -name ""*.old*"""
77,"$ find . -type f -name ""*.new*"""
77,Files with the .old-unix_time extension are saved
77,from your configuration before starting this update process. Files with the
77,.new-unix_time extension are files from the
77,IDM 6 configuration that have not been included in your updated installation.
77,"For example, if you find a system.properties.new-unix_time"
77,"file in your project directory, IDM is still using the version of this file before the"
77,update (which would still be named system.properties).
77,"To make sure that you have a completely upgraded configuration, analyze the new features in any"
77,"files with the .new-unix_time extension, and copy"
77,those changes into your existing configuration. If you have similar files with multiple
77,".new-unix_time extensions, use the file with the"
77,latest unix_time.
77,Restart your server.
77,Your update has successfully completed.
77,4.8. Placing a Server in Maintenance Mode
77,The Maintenance Service disables non-essential services of a running
77,"IDM instance, in preparation for an update to a later"
77,"version. When maintenance mode is enabled, services such as recon, sync,"
77,"scheduling, and workflow are disabled. The complete list of disabled services"
77,is output to the log file.
77,The router remains functional and requests to the maintenance
77,endpoint continue to be serviced. Requests to endpoints that are serviced by
77,a disabled component return the following response:
77,404 Resource endpoint-name not found
77,"Before you enable maintenance mode, you should temporarily suspend all"
77,"scheduled tasks. For more information, see"
77,"""Pausing Scheduled Jobs"" in the Integrator's Guide."
77,You can enable and disable maintenance mode over the REST interface.
77,"To enable maintenance mode, run the following command:"
77,$ curl \
77,"--header ""X-OpenIDM-Username: openidm-admin"" \"
77,"--header ""X-OpenIDM-Password: openidm-admin"" \"
77,--request POST \
77,"""http://localhost:8080/openidm/maintenance?_action=enable"""
77,"""maintenanceEnabled"": true"
77,"To disable maintenance mode, run the following command:"
77,$ curl \
77,"--header ""X-OpenIDM-Username: openidm-admin"" \"
77,"--header ""X-OpenIDM-Password: openidm-admin"" \"
77,--request POST \
77,"""http://localhost:8080/openidm/maintenance?_action=disable"""
77,"""maintenanceEnabled"": false"
77,"To check whether a server is in maintenance mode, run the following command:"
77,$ curl \
77,"--header ""X-OpenIDM-Username: openidm-admin"" \"
77,"--header ""X-OpenIDM-Password: openidm-admin"" \"
77,--request POST \
77,"""http://localhost:8080/openidm/maintenance?_action=status"""
77,"""maintenanceEnabled"": false"
77,"If the server is in maintenance mode, the command returns"
77,"""maintenanceEnabled"": true, otherwise it returns"
77,"""maintenanceEnabled"": false."
77,4.9. Applying Patch Bundle Releases
77,ForgeRock issues periodic patch bundle releases containing bug fixes and
77,improvements to IDM. You can view the available list of patch
77,bundles and download them from the ForgeRock
77,BackStage site.
77,"When a patch bundle is available, you can view the key fixes provided"
77,"in that patch bundle release, in the"
77,Release Notes.
77,Appendix A. Installing on a Read-Only Volume
77,Some enterprises choose to enhance security of their applications by
77,installing them on a dedicated read-only (ro) filesystem volume. This appendix
77,describes how you can set up IDM on such a volume.
77,This appendix assumes that you have prepared the read-only volume appropriate
77,for your Linux/UNIX installation environment.
77,A.1. Preparing Your System
77,"Before you continue, read ""Preparing to Install and Run Servers"", as well as the"
77,prerequisites described in
77,"""Before You Install"" in the Release Notes."
77,This appendix assumes that you have set up a regular Linux
77,user named idm and a dedicated volume for the
77,/idm directory.
77,"Configure the dedicated volume device,"
77,/dev/volume in the
77,"/etc/fstab file, as follows:"
77,/dev/volume
77,/idm
77,ext4
77,"ro,defaults"
77,"1,2"
77,"When you run the mount -a command, the"
77,/dev/volume volume device gets
77,mounted on the /idm directory.
77,You can switch between read-write and read-only mode for the
77,/idm volume with the following commands:
77,"$ sudo mount -o remount,rw /idm"
77,"$ sudo mount -o remount,ro /idm"
77,"You can confirm the result with the mount command, which"
77,should show whether the /idm volume is mounted in
77,read-only or read-write mode:
77,/dev/volume on /idm type ext4 (ro)
77,Set up the /idm volume in read-write mode:
77,"$ sudo mount -o remount,rw /idm"
77,"With the following commands, you can unpack the IDM binary in the"
77,"/idm directory, and give user idm"
77,ownership of all files in that directory:
77,$ sudo unzip /idm/IDM-6.5.1.zip
77,$ sudo chown -R idm.idm /idmA.2. Redirect Output Through Configuration Files
77,"In this section, you will modify appropriate configuration files to redirect"
77,data to writable volumes. This procedure assumes that you have a user
77,idm with Linux administrative (superuser) privileges.
77,"Create an external directory where IDM can send logging,"
77,"auditing, and internal repository information."
77,$ sudo mkdir -p /var/log/openidm/audit
77,$ sudo mkdir /var/log/openidm/logs
77,$ sudo mkdir -p /var/cache/openidm/felix-cache
77,$ sudo mkdir /var/run/openidmNote
77,You can also route audit data to a remote data store. For an example of how
77,"to send audit data to a MySQL repository, see ""Directing Audit Information To a MySQL Database"" in the Samples Guide."
77,Give user idm ownership of the newly created
77,directories:
77,$ sudo chown -R idm.idm /var/log/openidm
77,$ sudo chown -R idm.idm /var/cache/openidm
77,$ sudo chown -R idm.idm /var/run/openidm
77,"Open the audit configuration file for your project,"
77,project-dir/conf/audit.json.
77,Make sure handlerForQueries is set to
77,json.
77,Redirect the logDirectory property to the newly
77,created /var/log/openidm/audit subdirectory:
77,"""auditServiceConfig"" : {"
77,"""handlerForQueries"" : ""json"","
77,"""availableAuditEventHandlers"" : ["
77,"""org.forgerock.audit.handlers.csv.CsvAuditEventHandler"","
77,"""org.forgerock.audit.handlers.elasticsearch.ElasticsearchAuditEventHandler"","
77,"""org.forgerock.audit.handlers.jms.JmsAuditEventHandler"","
77,"""org.forgerock.audit.handlers.json.JsonAuditEventHandler"","
77,"""org.forgerock.audit.handlers.json.stdout.JsonStdoutAuditEventHandler"","
77,"""org.forgerock.openidm.audit.impl.RepositoryAuditEventHandler"","
77,"""org.forgerock.openidm.audit.impl.RouterAuditEventHandler"","
77,"""org.forgerock.audit.handlers.splunk.SplunkAuditEventHandler"","
77,"""org.forgerock.audit.handlers.syslog.SyslogAuditEventHandler"""
77,...
77,"""eventHandlers"" : ["
77,"""class"" : ""org.forgerock.audit.handlers.json.JsonAuditEventHandler"","
77,"""config"" : {"
77,"""name"" : ""json"","
77,"""logDirectory"" : ""/var/log/openidm/audit"","
77,"""buffering"" : {"
77,"""maxSize"" : 100000,"
77,"""writeInterval"" : ""100 millis"""
77,"""topics"" : [ ""access"", ""activity"", ""recon"", ""sync"", ""authentication"", ""config"" ]"
77,...
77,Open the logging configuration file for your project:
77,project-dir/conf/logging.properties.
77,Find the java.util.logging.FileHandler.pattern property
77,and redirect it as shown:
77,java.util.logging.FileHandler.pattern = /var/log/openidm/logs/openidm%u.log
77,Open the configuration properties file for your project:
77,project-dir/conf/config.properties.
77,Activate and redirect the org.osgi.framework.storage
77,property as follows:
77,"# If this value is not absolute, then the felix.cache.rootdir controls"
77,# how the absolute location is calculated. (See buildNext property)
77,org.osgi.framework.storage=&{felix.cache.rootdir|&{user.dir}}/felix-cache
77,# The following property is used to convert a relative bundle cache
77,# location into an absolute one by specifying the root to prepend to
77,# the relative cache path. The default for this property is the
77,# current working directory.
77,felix.cache.rootdir=/var/cache/openidmNote
77,You may want to set up additional redirection. Watch for the following
77,configuration details:
77,"Connectors. Depending on the connector, and the read-only volume, you"
77,may need to configure connectors to direct output to writable volumes.
77,"Scripts. If you're using Groovy, examine the"
77,conf/script.json file for your project. Make sure
77,that output such as to the groovy.target.directory
77,"is directed to an appropriate location, such as"
77,idm.data.dir
77,A.3. Additional Details
77,"In a production environment, you must configure a supported repository, as"
77,"described in ""Selecting a Repository""."
77,"Disable monitoring of JSON configuration files. To do so, open the"
77,project-dir/conf/system.properties
77,"file, and activate the following option:"
77,openidm.fileinstall.enabled=false
77,"You should address one more detail, the value of the"
77,OPENIDM_PID_FILE in the startup.sh
77,and shutdown.sh scripts.
77,"For RHEL 6 and Ubuntu 14.04 systems, the default shell is bash."
77,You can set the value of OPENIDM_PID_FILE for user
77,idm by adding the following line to
77,/home/idm/.bashrc:
77,export OPENIDM_PID_FILE=/var/run/openidm/openidm.pid
77,"If you have set up a different command line shell, adjust your changes"
77,accordingly.
77,"When you log in again as user idm, your"
77,OPENIDM_PID_FILE variable should redirect the process
77,"identifier file, openidm.pid to the"
77,"/var/run/openidm directory, ready for access by the"
77,shutdown.sh script.
77,"You need to set up security keystore and truststore files, either by"
77,importing a signed certificate or by generating a self-signed certificate.
77,"For more information, see ""Securing and Hardening Servers"" in the Integrator's Guide."
77,"While the volume is still mounted in read-write mode, start IDM"
77,normally:
77,$ ./startup.sh -p project-dir
77,The first startup of IDM either processes the signed certificate
77,"that you added, or generates a self-signed certificate."
77,Stop IDM:
77,-> shutdown
77,You can now mount the /idm directory in read-only mode.
77,The configuration in /etc/fstab ensures that Linux
77,mounts the /idm directory in read-only mode the next
77,time that system is booted.
77,"$ sudo mount -o remount,ro /idm"
77,"You can now start IDM, configured on a secure read-only volume."
77,$ ./startup.sh -p project-dirIDM Glossarycorrelation query
77,A correlation query specifies an expression that matches existing entries in
77,a source repository to one or more entries on a target repository. While a
77,"correlation query may be built with a script, it is not"
77,a correlation script.
77,"As noted in ""Correlating Source Objects With Existing Target Objects"" in the Integrator's Guide,"
77,"you can set up a query definition, such as_queryId,"
77,"_queryFilter, or_queryExpression, possibly"
77,with the help of alinkQualifier.
77,correlation script
77,"A correlation script matches existing entries in a source repository, and"
77,returns the IDs of one or more matching entries on a target repository.
77,While it skips the intermediate step associated with acorrelation
77,"query, a correlation script can be relatively complex, based on"
77,the operations of the script.
77,entitlement
77,An entitlement is a collection of attributes that can be added to a user
77,"entry via roles. As such, it is a specialized type of"
77,assignment. A user or device with an entitlement gets
77,access rights to specified resources. An entitlement is a property of a
77,managed object.
77,JCE
77,"Java Cryptographic Extension, which is part of the Java Cryptography"
77,"Architecture, provides a framework for encryption, key generation, and"
77,digital signatures.
77,"JSONJavaScript Object Notation, a lightweight data interchange format"
77,"based on a subset of JavaScript syntax. For more information, see the"
77,JSON site.JSON Pointer
77,A JSON Pointer defines a string syntax for identifying a specific value
77,"within a JSON document. For information about JSON Pointer syntax, see the"
77,JSON Pointer RFC.
77,JWT
77,JSON Web Token. As noted in the JSON
77,"Web Token draft IETF Memo, ""JSON Web Token (JWT) is a compact"
77,"URL-safe means of representing claims to be transferred between two parties."""
77,"For IDM, the JWT is associated with the JWT_SESSION"
77,authentication module.
77,managed object
77,An object that represents the identity-related data managed by
77,"IDM. Managed objects are configurable, JSON-based data structures"
77,that IDM stores in its pluggable repository. The default
77,"configuration of a managed object is that of a user, but you can define any"
77,"kind of managed object, for example, groups or roles."
77,mappingA policy that is defined between a source object and a target object
77,during reconciliation or synchronization. A mapping can also define a trigger
77,"for validation, customization, filtering, and transformation of source and"
77,target objects.OSGi
77,A module system and service platform for the Java programming language that
77,"implements a complete and dynamic component model. For a good introduction,"
77,see the
77,OSGi
77,site. Currently only the
77,Apache Felix container is
77,supported.
77,"reconciliationDuring reconciliation, comparisons are made between managed objects"
77,and objects on source or target systems. Reconciliation can result in one
77,"or more specified actions, including, but not limited to, synchronization."
77,"resourceAn external system, database, directory server, or other source of"
77,identity data to be managed and audited by the identity management system.
77,RESTRepresentational State Transfer. A software architecture style for
77,"exposing resources, using the technologies and protocols of the World Wide Web."
77,"REST describes how distributed data objects, or resources, can be defined and"
77,addressed.
77,role
77,IDM distinguishes between two distinct role types - provisioning
77,"roles and authorization roles. For more information, see"
77,"""Working With Managed Roles"" in the Integrator's Guide."
77,source object
77,"In the context of reconciliation, a source object is a data object on the"
77,"source system, that IDM scans before attempting to find a"
77,"corresponding object on the target system. Depending on the defined mapping,"
77,IDM then adjusts the object on the target system (target object).
77,"synchronizationThe synchronization process creates, updates, or deletes objects on a"
77,"target system, based on the defined mappings from the source system."
77,Synchronization can be scheduled or on demand.system object
77,"A pluggable representation of an object on an external system. For example,"
77,a user entry that is stored in an external LDAP directory is represented as a
77,system object in IDM for the period during which IDM
77,requires access to that entry. System objects follow the same RESTful
77,resource-based design principles as managed objects.
77,target object
77,"In the context of reconciliation, a target object is a data object on the"
77,"target system, that IDM scans after locating its corresponding"
77,"object on the source system. Depending on the defined mapping, IDM"
77,then adjusts the target object to match the corresponding source object.
77,XForgeRock Identity Management 6.5Installation GuideMark CraigLana FrostPaul BryanAndi EgloffLaszlo HordosMatthias TristlMike JangForgeRock AS
77,201 Mission St.
77,Suite 2900
77,"San Francisco, CA 94105"
77,USA
77,+1 415-599-1100 (US)
77,www.forgerock.com
77,"Guide to installing, updating, and uninstalling"
77,ForgeRock® Identity Management software. This software offers
77,flexible services for automating management of the identity life cycle.
77,Build Number:
77,6.5.1.0-7Build Date:
77,2021-03-05T11:43:43.840845Legal Notice (Documentation)
77,Copyright © 2011-2018 ForgeRock AS.×Read a different version of :
78,Lab3: Query Plan Management :: Amazon Aurora Labs for PostgreSQL
78,Amazon Aurora Labs for PostgreSQL
78,Overview of Labs
78,Prerequisites
78,I am in a workshop using Event Engine
78,Signing in to the AWS console using Event Engine
78,I need to deploy lab environment manually
78,Getting started
78,Signing in to the AWS Console
78,Setup Lab Environment with Aurora cluster
78,Setup the Lab without Aurora cluster
78,Lab1: Creating a New Aurora Cluster Manually
78,Configure Cloud9 and Initialize Database
78,Configure the Cloud9 workstation
78,"Connect, Verify and Initialize DB Instance"
78,Lab2: Fast Cloning
78,Lab3: Query Plan Management
78,Lab4: Cluster Cache Management
78,4.1 Setup cluster cache management
78,4.2 Benchmarking with Cluster Cache management
78,Lab5: Database Activity Streaming
78,5.1: Setup KMS for Database Activity Streaming
78,5.2: Database Activity Streams in action
78,Lab6: RDS Performance Insights
78,Lab7: Create dataset and Auto Scale
78,Lab8: Test Fault Tolerance
78,Lab9: Aurora Global Database
78,9.0: Prerequisites
78,9.1: Establish Global Database
78,9.2: Monitor a Global Database
78,9.3: Failover: Promote a Region
78,Failback: Optional Lab
78,Lab10: Aurora Serverless
78,Create an Aurora Serverless DB Cluster
78,Use Aurora Serverless with AWS Lambda and RDS Data API
78,Lab11: Aurora Machine Learning
78,Clean Up
78,Contributors & Revision History
78,More Resources
78,"Privacy | Site Terms | © 2021, Amazon Web Services, Inc. or its affiliates. All rights reserved."
78,Edit this page
78,Amazon Aurora Labs for PostgreSQL > Lab3: Query Plan Management
78,Lab3: Query Plan Management
78,"With query plan management (QPM), you can control execution plans for a set of statements that you want to manage. You can do the following:"
78,"Improve plan stability by forcing the optimizer to choose from a small number of known, good plans."
78,Optimize plans centrally and then distribute the best plans globally.
78,Identify indexes that aren’t used and assess the impact of creating or dropping an index.
78,Automatically detect a new minimum-cost plan discovered by the optimizer.
78,"Try new optimizer features with less risk, because you can choose to approve only the plan changes that improve performance."
78,For additional details on the Query Plan Management please refer official documentation
78,Managing Query Execution Plans for Aurora PostgreSQL.
78,"Query plan management is available with Amazon Aurora PostgreSQL version 10.5-compatible (Aurora 2.1.0) and later, or Amazon Aurora PostgreSQL version 9.6.11-compatible (Aurora 1.4.0) and later. The quickest way to enable QPM is to use the automatic plan capture, which enables the plan capture for all SQL statements that run at least two times."
78,"In this lab, we will walk through the process of enabling QPM with automatic plan capture, evolving captured query plans to manually accept them and fixing query plans by using optimizer hints."
78,Prerequisites
78,This lab requires the following lab modules to be completed first:
78,Prerequisites
78,"Creating a New Aurora Cluster (optional, if creating a cluster manually)"
78,Configure Cloud9 and Initialize Database
78,1. Quick start guide on using QPM with automatic capture
78,Here are the steps to configure and enable QPM on your Aurora PostgreSQL cluster to automatically capture and control execution plans for a set of SQL statements.
78,1.1 Modify the Amazon Aurora DB Cluster Parameters related to the QPM.
78,a. Open the Amazon RDS service console Parameters group section.
78,"b. In the list, choose the parameter group for your Aurora PostgreSQL DB cluster. You can find the DB Cluster parameter group name by selecting the CloudFormation Stack with description “Amazon Aurora PostgreSQL Labs Stackset” in the Cloudformation Console and referring to the Value for key apgcustomclusterparamgroup in the Outputs tab.The DB cluster must use a parameter group other than the default, because you can’t change values in a default parameter group."
78,"For more information, see Creating a DB Cluster Parameter Group."
78,c. Click on the DB cluster parameter group name selected above and then click on Edit Parameters.
78,"In Parameter Filter field, enter rds.enable_plan_management to reveal the filtered parameter. Set value of rds.enable_plan_management to 1 and click on Save changes."
78,e. Click on the database parameter group name and click on Edit Parameters.
78,f. We need to change two paramaters
78,Modify the value for apg_plan_mgmt.capture_plan_baselines parameter to automatic
78,Modify the value for apg_plan_mgmt.use_plan_baselines to true.
78,g. Click on the Preview changes to verify the changes and click Save changes.
78,h. Click Databases on the left navigation panel and wait for the status of the instance to change to available. The parameter changes will take effect after an DB instance reboot as suggested on the configuration tab of the Aurora writer and reader instances.
78,i. Reboot the writer node by selecting it and going to the Actions menu. The reader will automatically be rebooted after the writer reboots.
78,j. Wait for the Status of Writer and Reader nodes to become Available.
78,1.2 Create and verify the apg_plan_mgmt extension for your DB instance.
78,a. Open a Cloud9 terminal window and create the apg_plan_mgmt extension for your DB instance.
78,psql
78,CREATE EXTENSION apg_plan_mgmt;
78,"select extname,extversion from pg_extension where extname='apg_plan_mgmt';"
78,You should see output similar to the following. The extension version will vary depending on the Aurora PostgreSQL version.
78,b. Review all QPM related parameters are modified to the appropriate value by pasting the following queries.
78,show rds.enable_plan_management;
78,show apg_plan_mgmt.capture_plan_baselines;
78,show apg_plan_mgmt.use_plan_baselines;
78,The output should be:
78,rds.enable_plan_management
78,----------------------------
78,apg_plan_mgmt.capture_plan_baselines
78,--------------------------------------
78,automatic
78,apg_plan_mgmt.use_plan_baselines
78,----------------------------------
78,c. exit the psql command line by pressing Ctrl+d or typing \q and pressing Enter.
78,1.3 Run synthetic workload with automatic capture.
78,"a. Open a terminal window in Cloud9 and run pgbench (a PostgreSQL benchmarking tool) to generate a simulated workload, which runs same queries for a specified period. With automatic capture enabled, QPM captures plans for each query that runs at least twice."
78,pgbench --progress-timestamp -M prepared -n -T 100 -P 1
78,-c 500 -j 500 -b tpcb-like@1 -b select-only@20
78,b. Open another terminal window on Cloud9 to query apg_plan_mgmt.dba_plans table to view the managed statements and the execution plans for the SQL statements started with the pgbench tool.
78,Then run the following commands:
78,psql
78,"SELECT sql_hash,"
78,"plan_hash,"
78,"status,"
78,"enabled,"
78,sql_text
78,FROM
78,apg_plan_mgmt.dba_plans;
78,sql_hash
78,plan_hash
78,status
78,| enabled |
78,sql_text
78,-------------+-------------+----------+---------+-------------
78,474705734 |
78,695325866 | Approved | t
78,| SELECT abalance FROM pgbench_accounts WHERE aid = $1;
78,-2033469270 | -1987991358 | Approved | t
78,| UPDATE pgbench_tellers SET tbalance = tbalance + $1 WHERE tid = $2;
78,-1677381765 |
78,-225188843 | Approved | t
78,| UPDATE pgbench_branches SET bbalance = bbalance + $1 WHERE bid = $2;
78,(3 rows)
78,c. Turn off automatic capture of query plans.
78,Capturing all plans with automatic capture has little runtime overhead and can be enabled in production. We are turning off the automatic capture to make sure that we don’t capture SQL statements outside the pgbench workload. This can be turned off by setting the apg_plan_mgmt.capture_plan_baselines parameter to off from the DB parameter group.
78,Verify parameter settings using PSQL.
78,show apg_plan_mgmt.capture_plan_baselines;
78,apg_plan_mgmt.capture_plan_baselines
78,--------------------------------------
78,Off
78,d. Let’s verify that the execution plan for one of the managed statements is same as the plan captured by QPM.
78,Execute explain plan on one of the managed statements. The explain plan output shows that the SQL hash and the plan hash matches with the QPM approved plan for that statement.
78,explain (hashes true) UPDATE pgbench_tellers SET tbalance = tbalance + 100 WHERE tid = 200;
78,Output:
78,QUERY PLAN
78,----------------------------------------------------------------------
78,Update on pgbench_tellers
78,(cost=0.14..8.16 rows=1 width=358)
78,Index Scan using pgbench_tellers_pkey on pgbench_tellers
78,(cost=0.14..8.16 rows=1 width=358)
78,Index Cond: (tid = 200)
78,"SQL Hash: -2033469270, Plan Hash: -1987991358"
78,"SQL Hash and Plan Hash values could vary during your lab runs, so please note your specific values for later use in lab."
78,"In addition to automatic plan capture, QPM has manual plan capture capability, which offers a mechanism to capture execution plans for known problematic queries. Capturing the plans automatically is recommended generally. However, there are situations where capturing plans manually would be the best option, such as:"
78,"You don’t want to enable plan management at the Database level, but you do want to control a few critical SQL statements only."
78,You want to save the plan for a specific set of literals or parameter values that are causing a performance problem.
78,2. QPM Plan adaptability with plan evolution mechanism
78,"If the optimizer’s generated plan is not a stored plan, the optimizer captures and stores it as a new unapproved plan to preserve stability for the QPM-managed SQL statements."
78,"Query plan management provides techniques and functions to add, maintain, and improve execution plans and thus provides Plan adaptability. Users can instruct QPM on demand or periodically to evolve all the stored plans to see if there is a better minimum cost plan available than any of the approved plans."
78,"QPM provides apg_plan_mgmt.evolve_plan_baselines function to compare plans based on their actual performance. Depending on the outcome of your performance experiments, you can change a plan’s status from unapproved to either approved or rejected. You can instead decide to use the apg_plan_mgmt.evolve_plan_baselines function to temporarily disable a plan if it does not meet your requirements."
78,"For additional details about the QPM Plan evolution, see Evaluating Plan Performance."
78,"For the first use case, we’ll walk through an example on how QPM helps ensure plan stability where various changes can result in plan regression."
78,"In most cases, you set up QPM to use automatic plan capture so that plans are captured for all statements that run two or more times. However, you can also capture plans for a specific set of statements that you specify manually."
78,"To do this, you set apg_plan_mgmt.capture_plan_baselines = off in the DB parameter group (which is the default) and apg_plan_mgmt.capture_plan_baselines = manual at the session level."
78,a. Enable manual plan capture to instruct QPM to capture the execution plan of the desired SQL statements manually.
78,SET apg_plan_mgmt.capture_plan_baselines = manual;
78,b. Run explain plan for a specific query so that QPM can capture the execution plan (the following output for the explain plan is truncated for brevity).
78,explain (hashes true)
78,"SELECT Sum(delta),"
78,Sum(bbalance)
78,FROM
78,"pgbench_history h,"
78,pgbench_branches b
78,WHERE
78,b.bid = h.bid
78,"AND b.bid IN ( 1, 2, 3 )"
78,AND mtime BETWEEN (SELECT Min(mtime)
78,FROM
78,pgbench_history mn) AND (SELECT Max(mtime)
78,FROM
78,pgbench_history mx);
78,Output:
78,QUERY PLAN
78,----------------------------------------------------------------------
78,Aggregate
78,(cost=23228.13..23228.14 rows=1 width=16)
78,InitPlan 1 (returns $1)
78,Finalize Aggregate
78,(cost=6966.00..6966.01 rows=1 width=8)
78,Gather
78,(cost=6965.89..6966.00 rows=1 width=8)
78,Workers Planned: 1
78,Partial Aggregate
78,(cost=5965.89..5965.90 rows=1 width=8)
78,Parallel Seq Scan on pgbench_history mn
78,(cost=0.00..5346.11 rows=247911 width=8)
78,InitPlan 2 (returns $3)
78,Finalize Aggregate
78,(cost=6966.00..6966.01 rows=1 width=8)
78,Gather
78,(cost=6965.89..6966.00 rows=1 width=8)
78,Workers Planned: 1
78,Partial Aggregate
78,(cost=5965.89..5965.90 rows=1 width=8)
78,Parallel Seq Scan on pgbench_history mx
78,(cost=0.00..5346.11 rows=247911 width=8)
78,Nested Loop
78,(cost=0.00..9292.95 rows=632 width=8)
78,Join Filter: (h.bid = b.bid)
78,Seq Scan on pgbench_history h
78,(cost=0.00..9188.74 rows=2107 width=8)
78,Filter: ((mtime >= $1) AND (mtime <= $3))
78,Materialize
78,(cost=0.00..14.15 rows=3 width=8)
78,Seq Scan on pgbench_branches b
78,(cost=0.00..14.14 rows=3 width=8)
78,"Filter: (bid = ANY ('{1,2,3}'::integer[]))"
78,………………………………………………………………………..
78,"SQL Hash: 1561242727, Plan Hash: -1990695905"
78,c. Disable manual capture of the plan after you capture the execution plan for the desired SQL statement.
78,SET apg_plan_mgmt.capture_plan_baselines = off;
78,"d. View captured query plan for the specific query that you ran previously. The plan_outline column in the table apg_plan_mgmt.dba_plans shows the entire plan for the SQL. For brevity, the plan_outline isn’t shown here. Instead, plan_hash_value from the explain plan preceding is compared with plan_hash from the output of the apg_plan_mgmt.dba_plans query."
78,"SELECT sql_hash,"
78,"plan_hash,"
78,"status,"
78,"estimated_total_cost ""cost"","
78,sql_text
78,FROM apg_plan_mgmt.dba_plans;
78,Output:
78,sql_hash
78,plan_hash
78,status
78,| cost
78,| sql_text
78,------------+-------------+----------+---------+-----------------------------
78,1561242727	-1990695905	 Approved 	 23228.14
78,"select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx);"
78,"e. To instruct the query optimizer to use the approved or preferred captured plans for your managed statements, set the parameter apg_plan_mgmt.use_plan_baselines to true."
78,SET apg_plan_mgmt.use_plan_baselines = true;
78,f. View the explain plan output to see that the QPM approved plan is used by the query optimizer.
78,explain (hashes true)
78,"SELECT Sum(delta),"
78,Sum(bbalance)
78,FROM
78,"pgbench_history h,"
78,pgbench_branches b
78,WHERE
78,b.bid = h.bid
78,"AND b.bid IN ( 1, 2, 3 )"
78,AND mtime BETWEEN (SELECT Min(mtime)
78,FROM
78,pgbench_history mn) AND (SELECT Max(mtime)
78,FROM
78,pgbench_history mx);
78,QUERY PLAN
78,---------------------------------------------------------------------------------------------------
78,Aggregate
78,(cost=33986.68..33986.69 rows=1 width=16)
78,InitPlan 1 (returns $1)
78,Finalize Aggregate
78,(cost=9404.57..9404.58 rows=1 width=8)
78,Gather
78,(cost=9404.35..9404.56 rows=2 width=8)
78,Workers Planned: 2
78,Partial Aggregate
78,(cost=8404.35..8404.36 rows=1 width=8)
78,Parallel Seq Scan on pgbench_history mn
78,(cost=0.00..7703.48 rows=280348 width=8)
78,InitPlan 2 (returns $3)
78,Finalize Aggregate
78,(cost=9404.57..9404.58 rows=1 width=8)
78,Gather
78,(cost=9404.35..9404.56 rows=2 width=8)
78,Workers Planned: 2
78,Partial Aggregate
78,(cost=8404.35..8404.36 rows=1 width=8)
78,Parallel Seq Scan on pgbench_history mx
78,(cost=0.00..7703.48 rows=280348 width=8)
78,Nested Loop
78,(cost=0.27..15177.06 rows=93 width=8)
78,Join Filter: (h.bid = b.bid)
78,Index Scan using pgbench_branches_pkey on pgbench_branches b
78,(cost=0.27..24.73 rows=3 width=8)
78,"Index Cond: (bid = ANY ('{1,2,3}'::integer[]))"
78,Materialize
78,(cost=0.00..15009.36 rows=3364 width=8)
78,Seq Scan on pgbench_history h
78,(cost=0.00..14992.54 rows=3364 width=8)
78,Filter: ((mtime >= $1) AND (mtime <= $3))
78,"SQL Hash: 1561242727, Plan Hash: -1990695905"
78,g. Create a new index on the pgbench_history table column “mtime” to change the planner configuration and force the query optimizer to generate a new plan.
78,create index pgbench_hist_mtime on pgbench_history(mtime);
78,h. View the explain plan output to see that QPM detects a new plan but still uses the approved plan and maintains the plan stability.
78,explain (hashes true)
78,"SELECT Sum(delta),"
78,Sum(bbalance)
78,FROM
78,"pgbench_history h,"
78,pgbench_branches b
78,WHERE
78,b.bid = h.bid
78,"AND b.bid IN ( 1, 2, 3 )"
78,AND mtime BETWEEN (SELECT Min(mtime)
78,FROM
78,pgbench_history mn) AND (SELECT Max(mtime)
78,FROM
78,pgbench_history mx);
78,Output:
78,QUERY PLAN
78,------------------------------------------------------------------------------------------
78,Aggregate
78,(cost=33101.22..33101.23 rows=1 width=16)
78,InitPlan 1 (returns $2)
78,Finalize Aggregate
78,(cost=9224.80..9224.81 rows=1 width=8)
78,Gather
78,(cost=9224.58..9224.79 rows=2 width=8)
78,Workers Planned: 2
78,Partial Aggregate
78,(cost=8224.58..8224.59 rows=1 width=8)
78,Parallel Seq Scan on pgbench_history mn
78,(cost=0.00..7559.67 rows=265967 width=8)
78,InitPlan 2 (returns $5)
78,Finalize Aggregate
78,(cost=9224.80..9224.81 rows=1 width=8)
78,Gather
78,(cost=9224.58..9224.79 rows=2 width=8)
78,Workers Planned: 2
78,Partial Aggregate
78,(cost=8224.58..8224.59 rows=1 width=8)
78,Parallel Seq Scan on pgbench_history mx
78,(cost=0.00..7559.67 rows=265967 width=8)
78,Nested Loop
78,(cost=0.27..14651.15 rows=89 width=8)
78,Join Filter: (h.bid = b.bid)
78,Index Scan using pgbench_branches_pkey on pgbench_branches b
78,(cost=0.27..24.73 rows=3 width=8)
78,"Index Cond: (bid = ANY ('{1,2,3}'::integer[]))"
78,Materialize
78,(cost=0.00..14490.76 rows=3192 width=8)
78,Seq Scan on pgbench_history h
78,(cost=0.00..14474.80 rows=3192 width=8)
78,Filter: ((mtime >= $2) AND (mtime <= $5))
78,Note: An Approved plan was used instead of the minimum cost plan.
78,"SQL Hash: 1561242727, Plan Hash: -1990695905, Minimum Cost Plan Hash: -794604077"
78,(22 rows)
78,"i. Run the following SQL query to view the new plan and status of the plan. To ensure plan stability, QPM stores all the newly generated plans for a managed query in QPM as unapproved plans."
78,"The following output shows that there are two different execution plans stored for the same managed statement, as shown by the two different plan_hash values. Although the new execution plan has the minimum cost (lower than the approved plan), QPM continues to ignore the unapproved plans to maintain plan stability."
78,"The plan_outline column in the table apg_plan_mgmt.dba_plans shows the entire plan for the SQL. For brevity, the plan_outline is not shown here. Instead, plan_hash_value from the explain plan preceding is compared with plan_hash from the output of the apg_plan_mgmt.dba_plans query."
78,"SELECT sql_hash,"
78,"plan_hash,"
78,"status,"
78,"estimated_total_cost ""cost"","
78,sql_text
78,FROM apg_plan_mgmt.dba_plans;
78,Output:
78,sql_hash
78,plan_hash
78,status
78,| cost 	|
78,sql_text
78,------------+-------------+----------+---------+----------------------------
78,1561242727	-1990695905	 Approved 	 23228.14
78,"select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx);"
78,1561242727	-794604077	 UnApproved 	 111.17
78,"select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx);"
78,"The following is an example of plan adaptability with QPM. This example evaluates the unapproved plan based on the minimum speedup factor. It approves any captured unapproved plan that is faster by at least 10 percent than the best approved plan for the statement. For additional details, see Evaluating Plan Performance in the Aurora documentation."
78,"SELECT apg_plan_mgmt.Evolve_plan_baselines (sql_hash, plan_hash, 1.1,'approve')"
78,FROM
78,apg_plan_mgmt.dba_plans
78,WHERE
78,status = 'Unapproved';
78,Output:
78,"NOTICE: [Unapproved] SQL Hash: 1561242727, Plan Hash: -794604077, SELECT sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where ..."
78,NOTICE:
78,Baseline
78,"[Planning time 0.693 ms, Execution time 316.644 ms]"
78,NOTICE:
78,"Baseline+1 [Planning time 0.695 ms, Execution time 213.919 ms]"
78,NOTICE:
78,"Total time benefit: 102.723 ms, Execution time benefit: 102.725 ms, Avg Log Cardinality Error: 3.53418, Cost = 111.16..111.17"
78,NOTICE:
78,Unapproved -> Approved
78,"After QPM evaluates the plan based on the speed factor, the plan status changes from unapproved to approved. At this point, the optimizer can choose the newly approved lower cost plan for that managed statement."
78,"SELECT sql_hash,"
78,"plan_hash,"
78,"status,"
78,"estimated_total_cost ""cost"","
78,sql_text
78,FROM apg_plan_mgmt.dba_plans;
78,Output:
78,sql_hash
78,plan_hash
78,status
78,cost |
78,sql_text
78,------------+-------------+----------+---------+-----------------------------------------
78,1561242727	-1990695905	 Approved 	 23228.14
78,"select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx);"
78,1561242727	-794604077	 Approved 	 111.17
78,"select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx);"
78,j. View the explain plan output to see whether the query is using the newly approved minimum cost plan.
78,explain (hashes true)
78,"SELECT Sum(delta),"
78,Sum(bbalance)
78,FROM
78,"pgbench_history h,"
78,pgbench_branches b
78,WHERE
78,b.bid = h.bid
78,"AND b.bid IN ( 1, 2, 3 )"
78,AND mtime BETWEEN (SELECT Min(mtime)
78,FROM
78,pgbench_history mn) AND (SELECT Max(mtime)
78,FROM
78,pgbench_history mx);
78,Output:
78,QUERY PLAN
78,-----------------------------------------------------------------------------------------------------------------
78,Aggregate
78,(cost=163.22..163.23 rows=1 width=16)
78,InitPlan 2 (returns $1)
78,Result
78,(cost=0.46..0.47 rows=1 width=8)
78,InitPlan 1 (returns $0)
78,Limit
78,(cost=0.42..0.46 rows=1 width=8)
78,Index Only Scan using pgbench_hist_mtime on pgbench_history mn
78,(cost=0.42..23092.45 rows=638320 width=8)
78,Index Cond: (mtime IS NOT NULL)
78,InitPlan 4 (returns $3)
78,Result
78,(cost=0.46..0.47 rows=1 width=8)
78,InitPlan 3 (returns $2)
78,Limit
78,(cost=0.42..0.46 rows=1 width=8)
78,Index Only Scan Backward using pgbench_hist_mtime on pgbench_history mx
78,(cost=0.42..23092.45 rows=638320 width=8)
78,Index Cond: (mtime IS NOT NULL)
78,Hash Join
78,(cost=23.70..161.83 rows=89 width=8)
78,Hash Cond: (h.bid = b.bid)
78,Index Scan using pgbench_hist_mtime on pgbench_history h
78,(cost=0.42..129.85 rows=3192 width=8)
78,Index Cond: ((mtime >= $1) AND (mtime <= $3))
78,Hash
78,(cost=23.23..23.23 rows=3 width=8)
78,Bitmap Heap Scan on pgbench_branches b
78,(cost=12.83..23.23 rows=3 width=8)
78,"Recheck Cond: (bid = ANY ('{1,2,3}'::integer[]))"
78,Bitmap Index Scan on pgbench_branches_pkey
78,(cost=0.00..12.83 rows=3 width=0)
78,"Index Cond: (bid = ANY ('{1,2,3}'::integer[]))"
78,"SQL Hash: 1561242727, Plan Hash: -794604077"
78,3. Fixing plans with QPM using pg_hint_plan
78,"In some cases, the query optimizer doesn’t generate the best execution plan for the query. One approach to fixing this problem is to put query hints into your application code, but this approach is widely discouraged because it makes applications more brittle and harder to maintain, and in some cases, you can’t hint the SQL because it is generated by a 3rd party application. What we will show is how to use hints to control the query optimizer, but then to remove the hints and allow QPM to enforce the desired plan, without adding hints to the application code."
78,"For this purpose, PostgreSQL users can use the pg_hint_plan extension to provide directives such as “scan method”, “join method”, “join order”, or “row number correction” to the optimizer."
78,"The resulting plan will be saved by QPM, along with any GUC parameters you choose to override (such as work_mem). QPM remembers any GUC parameter overrides and uses them when it needs to recreate the plan. To install and learn more about how to use the pg_hint_plan extension, see the pg_hint_plan documentation."
78,QPM steps to fix the plan generated by using hints
78,"Working with pg_hint_plan is incredibly useful for cases where the query can’t be modified to add hints. In this example, you’ll use a sample query to generate an execution plan that you want to modify by adding hints. Then you’ll associate the new execution plan with the original unmodified statement."
78,Here are the detailed steps:
78,a. Check if the plan capture is disabled
78,show apg_plan_mgmt.capture_plan_baselines;
78,Output:
78,apg_plan_mgmt.capture_plan_baselines
78,--------------------------------------
78,off
78,(1 row)
78,b. Run explain plan for the original query without any hints to see the execution plan optimizer generates.
78,The original plan of the query without hints is as follows.
78,EXPLAIN (hashes true)
78,SELECT
78,FROM
78,pgbench_branches b
78,JOIN
78,pgbench_accounts a
78,ON b.bid = a.bid
78,ORDER BY
78,a.aid;
78,Output:
78,QUERY PLAN
78,-----------------------------------------------------------------------------------------
78,Gather Merge
78,(cost=1000.73..2760454.51 rows=10052747 width=465)
78,Workers Planned: 2
78,Nested Loop
78,(cost=0.70..1599118.05 rows=4188645 width=461)
78,Parallel Index Scan using pgbench_accounts_pkey on pgbench_accounts a
78,(cost=0.43..405754.08 rows=4188645 width=97)
78,Index Scan using pgbench_branches_pkey on pgbench_branches b
78,(cost=0.27..0.29 rows=1 width=364)
78,Index Cond: (bid = a.bid)
78,"SQL Hash: 356104612, Plan Hash: 1425407480"
78,c. Enable pg_hint_plan and
78,QPM manual plan capture:
78,SET pg_hint_plan.enable_hint = true;
78,SET apg_plan_mgmt.capture_plan_baselines = manual;
78,"d. Now, EXPLAIN the query with the hints you want to use. In the following example, we are using the HashJoin (a, b) hint, which is a directive for the optimizer to use a hash join algorithm to join table a and table b:"
78,The plan that you want with a hash join is as follows.
78,/*+ HashJoin(a b) */
78,EXPLAIN (hashes true)
78,SELECT
78,FROM
78,pgbench_branches b
78,JOIN
78,pgbench_accounts a
78,ON b.bid = a.bid
78,ORDER BY
78,a.aid;
78,Output:
78,QUERY PLAN
78,--------------------------------------------------------------------------
78,Gather Merge
78,(cost=3345991.72..3443795.01 rows=8377290 width=465)
78,Workers Planned: 2
78,Sort
78,(cost=3344991.70..3355463.31 rows=4188645 width=465)
78,Sort Key: a.aid
78,Hash Join
78,(cost=93.43..221376.54 rows=4188645 width=465)
78,Hash Cond: (a.bid = b.bid)
78,Parallel Seq Scan on pgbench_accounts a
78,(cost=0.00..209856.45 rows=4188645 width=97)
78,Hash
78,(cost=92.08..92.08 rows=108 width=364)
78,Seq Scan on pgbench_branches b
78,(cost=0.00..92.08 rows=108 width=364)
78,"SQL Hash: 356104612, Plan Hash: -58126597"
78,"e. Verify that plan -58126597 was captured, and note the status of the plan."
78,"In your case, query plan Hash might be different. Capture the Plan hash value from output of the previous step (e.g. “-58126597”) to use it later in queries. Also note the SQL hash value (e.g. “356104612”) because you’ll use that later in apg_plan_mgmt.set_plan_status command."
78,"SELECT sql_hash,"
78,"plan_hash,"
78,"status,"
78,"enabled,"
78,sql_text
78,FROM
78,apg_plan_mgmt.dba_plans
78,Where plan_hash=-58126597;
78,Output:
78,sql_hash
78,| plan_hash |
78,status
78,| enabled |
78,sql_text
78,-----------+-----------+----------+---------+---------------------------
78,356104612 | -58126597 | Approved | t
78,| SELECT
78,| FROM
78,pgbench_branches b
78,JOIN
78,pgbench_accounts a +
78,ON b.bid = a.bid
78,| ORDER BY
78,a.aid;
78,"f. If necessary, Approve the plan. In this case, this is the first and only plan saved for the statement with SQL Hash=356104612 (in your case it could be different). So it was saved as an Approved plan. If this statement already had a baseline of approved plans, then this plan would have been saved as an Unapproved plan. In general, to Reject all existing plans for a statement and then Approve one specific plan, you could call apg_plan_mgmt.set_plan_status twice, like this:"
78,"-- please replace ""356104612""-> your SQL Hash value; -58126597 -> your Plan Hash value"
78,"SELECT apg_plan_mgmt.set_plan_status (sql_hash, plan_hash, 'Rejected') from apg_plan_mgmt.dba_plans where sql_hash = 356104612;"
78,"SELECT apg_plan_mgmt.set_plan_status (356104612, -58126597, 'Approved');"
78,"g. Next, remove the optimizer hint from the SQL, set capture_plan_baselines parameter to off to disable plan capturing and"
78,"turn on use_plan_baselines parameter. As you will notice below even after the hint removal, optimizer choose to use the same SQL plan with hash value -58126597 (this might be different in your case)."
78,SET apg_plan_mgmt.capture_plan_baselines = off;
78,SET apg_plan_mgmt.use_plan_baselines = true;
78,EXPLAIN (hashes true)
78,SELECT
78,FROM
78,pgbench_branches b
78,JOIN
78,pgbench_accounts a
78,ON b.bid = a.bid
78,ORDER BY
78,a.aid;
78,Output:
78,QUERY PLAN
78,-----------------------------------------------------------------------------
78,Gather Merge
78,(cost=3345991.72..4323410.46 rows=8377290 width=465)
78,Workers Planned: 2
78,Sort
78,(cost=3344991.70..3355463.31 rows=4188645 width=465)
78,Sort Key: a.aid
78,Hash Join
78,(cost=93.43..221376.54 rows=4188645 width=465)
78,Hash Cond: (a.bid = b.bid)
78,Parallel Seq Scan on pgbench_accounts a
78,(cost=0.00..209856.45 rows=4188645 width=97)
78,Hash
78,(cost=92.08..92.08 rows=108 width=364)
78,Seq Scan on pgbench_branches b
78,(cost=0.00..92.08 rows=108 width=364)
78,Note: An Approved plan was used instead of the minimum cost plan.
78,"SQL Hash: 356104612, Plan Hash: -58126597, Minimum Cost Plan Hash: 1425407480"
78,"As you can observe, the optimizer choose the plan with hash value -58126597 (with hash join between the tables) even though a minimum cost plan with hash value 1425407480 exists. For this specific example, we approved a slightly costly plan with hash join because it returns the results faster compared to an index scan, which you might have do do sometimes in your production environment depending on your use case."
78,4. Deploying QPM-managed plans globally using export and import (no lab)
78,"Large enterprise customers often have applications and databases deployed globally. They also often maintain several environments (Dev, QA, Staging, UAT, Preprod, and Prod) for each application database. However, managing the execution plans manually in each of the database environments can be cumbersome and time-consuming."
78,"QPM provides an option to export and import QPM-managed plans from one database to another database. With this option, you can manage the query execution plans centrally and deploy them to databases globally. This feature is useful for the scenarios where you investigate a set of plans on a preprod database, verify that they perform well, and then load them into a production database."
78,"For additional details, see Exporting and Importing Plans in the Aurora documentation."
78,5. Disabling QPM and deleting plans manually
78,"To disable QPM feature, open your cluster-level parameter group and set the rds.enable_plan_management parameter to 0."
78,Delete all the captured plans by running the following in PSQL:
78,"SELECT SUM(apg_plan_mgmt.delete_plan(sql_hash, plan_hash))"
78,FROM apg_plan_mgmt.dba_plans;
78,Verify that no captured plans exist by running the following:
78,"SELECT sql_hash,plan_hash,enabled, status,estimated_total_cost ""cost"",sql_text FROM apg_plan_mgmt.dba_plans;"
81,Tips for Java Developers |
81,Cloud Foundry Docs
81,Cloud Foundry Documentation
81,Get Involved
81,Doc Index
81,General Information
81,Contributing Documentation
81,Cloud Foundry Concepts
81,Cloud Foundry Overview
81,Security and Networking
81,Cloud Foundry Security
81,Container Security
81,Container-to-Container Networking
81,"Orgs, Spaces, Roles, and Permissions"
81,Planning Orgs and Spaces
81,App Security Groups
81,App SSH Components and Processes
81,High Availability
81,High Availability in Cloud Foundry
81,How Cloud Foundry Maintains High Availability
81,How Cloud Foundry Manages Apps
81,How Apps Are Staged
81,App Container Lifecycle
81,How the Diego Auction Allocates Jobs
81,Cloud Foundry Components
81,Diego Architecture
81,Cloud Foundry Routing Architecture
81,Cloud Controller
81,Cloud Controller Blobstore
81,User Account and Authentication (UAA) Server
81,Garden
81,GrootFS Disk Usage
81,HTTP Routing
81,Cloud Foundry Command Line Interface (cf CLI)
81,Installing the cf CLI
81,Upgrading to cf CLI v7
81,Getting Started with the cf CLI
81,Using the cf CLI with a Proxy Server
81,Using the cf CLI with a Self-Signed Certificate
81,Using cf CLI Plugins
81,Developing cf CLI Plugins
81,cf CLI v6 Reference Guide
81,cf CLI v7 Reference Guide
81,Using Experimental cf CLI Commands
81,Information for Operators
81,Deploying Cloud Foundry
81,Setting Up DNS for Your Environment
81,Deploying Cloud Foundry with cf-deployment
81,Deploying BOSH on AWS
81,Deploying BOSH on GCP
81,Deploying Cloud Foundry
81,Migrating from cf-release to cf-deployment
81,Configuring Your Cloud Foundry for BOSH Backup and Restore
81,Backup and Restore for External Blobstores
81,Additional Configuration
81,High Availability in Cloud Foundry
81,How Cloud Foundry Maintains High Availability
81,Cloud Controller Blobstore Configuration
81,Administering Cloud Foundry
81,Managing the Runtime
81,Stopping and Starting Virtual Machines
81,Creating and Modifying Quota Plans
81,Using Feature Flags
81,Examining GrootFS Disk Usage
81,Using Metadata
81,Managing Custom Buildpacks
81,Using Docker in Cloud Foundry
81,User Accounts and Communications
81,Creating and Managing Users with the cf CLI
81,Creating and Managing Users with the UAA CLI (UAAC)
81,Getting Started with the Notifications Service
81,Routing
81,Enabling IPv6 for Hosted Apps
81,Supporting WebSockets
81,Configuring Load Balancer Health Checks for CF Routers
81,Securing Traffic into CF
81,Enabling TCP Routing
81,Isolation Segments
81,Managing Isolation Segments
81,Routing for Isolation Segments
81,Running and Troubleshooting Cloud Foundry
81,Cloud Foundry Logging
81,Configuring System Logging
81,Configuring Diego for Upgrades
81,Audit Events
81,UAA Audit Requirements
81,Usage Events and Billing
81,Configuring SSH Access for Cloud Foundry
81,Configuring Diego Cell Disk Cleanup Scheduling
81,Configuring Health Monitor Notifications
81,Monitoring and Testing Diego Components
81,Troubleshooting Cloud Foundry
81,UAA Performance
81,UAA Performance Metrics
81,Scaling Cloud Controller
81,Scaling Cloud Controller (cf-for-k8s)
81,Logging and Metrics in Cloud Foundry
81,Loggregator Architecture
81,Installing the Loggregator Plugin for cf CLI
81,Security Event Logging
81,Cloud Foundry Component Metrics
81,Container Metrics
81,Loggregator Guide for CF Operators
81,Overview of Logging and Metrics
81,Deploying a Nozzle to the Loggregator Firehose
81,BOSH Documentation
81,BOSH Backup and Restore
81,Installing BBR
81,Release Notes for BBR
81,Backing Up with BBR
81,Restoring with BBR
81,BBR Logging
81,Experimental Features
81,BBR Developer's Guide
81,Information for Developers
81,Developing and Managing Apps
81,cf push
81,Pushing an App
81,Deploying with App Manifests
81,App Manifest Attribute Reference
81,Deploying an App with Docker
81,Deploying a Large App
81,"Starting, Restarting, and Restaging Apps"
81,Pushing an App with Multiple Processes
81,Running cf push Sub-Step Commands
81,Rolling App Deployments
81,Pushing Apps with Sidecar Processes
81,Using Blue-Green Deployment to Reduce Downtime and Risk
81,Troubleshooting App Deployment and Health
81,SSH for Apps and Services
81,App SSH Overview
81,Accessing Apps with SSH
81,Accessing Services with SSH
81,Routes and Domains
81,Configuring Routes and Domains
81,Configuring CF to Route Traffic to Apps on Custom Ports
81,Managing Services
81,Services Overview
81,Managing Service Instances
81,Sharing Service Instances
81,Delivering Service Credentials to an App
81,Managing Service Keys
81,Configuring Play Framework Service Connections
81,Using an External File System (Volume Services)
81,User-Provided Service Instances
81,Streaming App Logs
81,Streaming App Logs to Log Management Services
81,Service-Specific Instructions for Streaming App Logs
81,Streaming App Logs to Splunk
81,Streaming App Logs with Fluentd
81,Streaming App Logs to Azure OMS Log Analytics
81,Managing Apps with the cf CLI
81,Running Tasks
81,Scaling an App Using cf scale
81,Using App Health Checks
81,Configuring Container-to-Container Networking
81,CF Environment Variables
81,Cloud Controller API Client Libraries
81,Considerations for Designing and Running an App in the Cloud
81,App Revisions
81,Buildpacks
81,About Buildpacks
81,How Buildpacks Work
81,Stack Association
81,Pushing an App with Multiple Buildpacks
81,Using a Proxy
81,Supported Binary Dependencies
81,Production Server Configuration
81,Binary
81,HWC
81,Java
81,Tips for Java Developers
81,Getting Started Deploying Apps
81,Grails
81,Ratpack
81,Spring
81,Configuring Service Connections
81,Grails
81,Play
81,Spring
81,Cloud Foundry Java Client Library
81,.NET Core
81,NGINX Buildpack
81,Node.js
81,Tips for Node.js Developers
81,Environment Variables Defined by the Node Buildpack
81,Configuring Service Connections for Node.js
81,PHP
81,Tips for PHP Developers
81,Getting Started Deploying PHP Apps
81,PHP Buildpack Configuration
81,Composer
81,Sessions
81,New Relic
81,Python
81,Ruby
81,Tips for Ruby Developers
81,Getting Started Deploying Apps
81,Ruby
81,Ruby on Rails
81,Configure Rake Tasks for Deployed Apps
81,Environment Variables Defined by the Ruby Buildpack
81,Configure Service Connections for Ruby
81,Support for Windows Gemfiles
81,Staticfile
81,Customizing and Developing Buildpacks
81,Creating Custom Buildpacks
81,Packaging Dependencies for Offline Buildpacks
81,Merging from Upstream Buildpacks
81,Upgrading Dependency Versions
81,Using CI for Buildpacks
81,Releasing a New Buildpack Version
81,Updating Buildpack-Related Gems
81,Information for Managed Service Authors
81,Services
81,Overview
81,Service Broker API
81,Open Service Broker API
81,Platform Profiles
81,Catalog Metadata
81,Volume Services
81,Release Notes
81,Managing Service Brokers
81,Access Control
81,Binding Credentials
81,CredHub
81,Setting Up and Deploying CredHub with BOSH
81,Configuring a Hardware Security Module
81,Using a Key Management Service with CredHub
81,CredHub Credential Types
81,Backing Up and Restoring CredHub Instances
81,Troubleshooting CredHub
81,Dashboard Single Sign-On
81,Enabling Service Instance Sharing
81,Example Service Brokers
81,App Log Streaming
81,Route Services
81,Supporting Multiple CF Instances
81,API Reference
81,UAA API
81,CAPI API
81,Client Libraries
81,Rate Limit Information Returned by the Cloud Controller API
81,CAPI V2
81,CAPI V3
81,Tips for Java Developers
81,Java Buildpack
81,Design
81,Configuration
81,Java Client Library
81,Grails
81,Groovy
81,Ratpack
81,Raw Groovy
81,Java Main
81,Maven
81,Gradle
81,Play Framework
81,Spring Boot CLI
81,Servlet
81,Maven
81,Gradle
81,Binding to Services
81,Java and Grails Best Practices
81,Provide a JDBC Driver
81,Allocate Sufficient Memory
81,Troubleshoot Out of Memory
81,Troubleshoot Failed Upload
81,Debug Java Apps on Cloud Foundry
81,Slow Starting Java or Grails Apps
81,Extension
81,Environment Variables
81,Page last updated:
81,"Cloud Foundry can deploy a number of different JVM-based artifact types. For a more detailed explanation of what it supports, see Additional Documentation in the Cloud Foundry Java Buildpack repository on GitHub."
81,Java Buildpack
81,"For information about using, configuring, and extending the Cloud Foundry Java buildpack, see the Cloud Foundry Java Buildpack repository on GitHub."
81,Design
81,"The Java buildpack is designed to convert artifacts that run on the JVM into executable apps. It does this by identifying one of the supported artifact types (Grails, Groovy, Java, Play Framework, Spring Boot, and Servlet) and downloading all additional dependencies needed to run. It also analyzes the collection of services bound to the app and downloads any dependencies related to those services."
81,"As an example, pushing a WAR file that is bound to a PostgreSQL database and New Relic for performance monitoring results in the following:"
81,Initialized empty Git repository in /tmp/buildpacks/java-buildpack/.git/
81,--> Java Buildpack source: https://github.com/cloudfoundry/java-buildpack#0928916a2dd78e9faf9469c558046eef09f60e5d
81,--> Downloading Open Jdk JRE 1.7.0_51 from
81,http://.../openjdk/lucid/x86_64/openjdk-1.7.0_51.tar.gz (0.0s)
81,Expanding Open Jdk JRE to .java-buildpack/open_jdk_jre (1.9s)
81,--> Downloading New Relic Agent 3.4.1 from
81,http://.../new-relic/new-relic-3.4.1.jar (0.4s)
81,--> Downloading Postgresql JDBC 9.3.1100 from
81,http://.../postgresql-jdbc/postgresql-jdbc-9.3.1100.jar (0.0s)
81,--> Downloading Spring Auto Reconfiguration 0.8.7 from
81,http://.../auto-reconfiguration/auto-reconfiguration-0.8.7.jar (0.0s)
81,Modifying /WEB-INF/web.xml for Auto Reconfiguration
81,--> Downloading Tomcat 7.0.50 from
81,http://.../tomcat/tomcat-7.0.50.tar.gz (0.0s)
81,Expanding Tomcat to .java-buildpack/tomcat (0.1s)
81,--> Downloading Buildpack Tomcat Support 1.1.1 from
81,http://.../tomcat-buildpack-support/tomcat-buildpack-support-1.1.1.jar (0.1s)
81,--> Uploading droplet (57M)
81,Configuration
81,"In most cases, the buildpack should work without any configuration. If you are new to Cloud Foundry, Cloud Foundry recommends that you make your first attempts without modifying the buildpack configuration. If the buildpack requires some configuration, use a fork of the buildpack. For more information, see Configuration and Extension in the Cloud Foundry Java Buildpack repository on GitHub."
81,Java Client Library
81,"The Cloud Foundry Client Library provides a Java API for interacting with a Cloud Foundry instance. This library, cloudfoundry-client-lib, is used by the Cloud Foundry Maven plugin, the Cloud Foundry Gradle plugin, and other Java-based tools."
81,"For information about using this library, see Java Cloud Foundry Library."
81,Grails
81,"Grails packages apps into WAR files for deployment into a Servlet container. To build the WAR file and deploy it, run:"
81,grails prod war
81,cf push YOUR-APP -p target/YOUR-APP-VERSION.war
81,Where:
81,YOUR-APP is the name of your app.
81,YOUR-APP-VERSION is the name of the WAR file you want to build and deploy.
81,Groovy
81,Cloud Foundry supports Groovy apps based on both Ratpack and a simple collection of files.
81,Ratpack
81,"Ratpack packages apps into two different styles. Cloud Foundry supports the distZip style. To build the ZIP file and deploy it, run:"
81,gradle distZip
81,cf push YOUR-APP -p build/distributions/YOUR-ZIP-FILE.zip
81,Where:
81,YOUR-APP is the name of your app.
81,YOUR-ZIP-FILE is the name of the ZIP file you want to build and deploy.
81,"For more information, see the Ratpack website."
81,Raw Groovy
81,"You can run Groovy apps that are made up of a single entry point and any supporting files without any other work. To deploy them, run:"
81,cf push YOUR-APP
81,Where YOUR-APP is the name of your app.
81,"For more information, see Groovy Container in the Cloud Foundry Java Buildpack repository on GitHub."
81,Java Main
81,"Java apps with a main() method can be run provided that they are packaged as self-executable JARs. For more information, see Java Main Container in the Cloud Foundry Java Buildpack repository on GitHub."
81,"Note: If your app is not web-enabled, you must suppress route creation to avoid a failed to start accepting connections error. To suppress route creation, add no-route: true to the app manifest or use the --no-route flag with the cf push command. For more information about the no-route attribute, see Deploying with App Manifests."
81,Maven
81,"A Maven build can create a self-executable JAR. To build and deploy the JAR, run:"
81,mvn package
81,cf push YOUR-APP -p target/YOUR-APP-VERSION.jar
81,Where:
81,YOUR-APP is the name of your app.
81,YOUR-APP-VERSION is the name of the JAR you want to build and deploy.
81,Gradle
81,"A Gradle build can create a self-executable JAR. To build and deploy the JAR, run:"
81,gradle build
81,cf push YOUR-APP -p build/libs/YOUR-APP-VERSION.jar
81,Where:
81,YOUR-APP is the name of your app.
81,YOUR-APP-VERSION is the name of the JAR you want to build and deploy.
81,Play Framework
81,"The Play Framework packages apps into two different styles. Cloud Foundry supports both the staged and dist styles. To build the dist style and deploy it, run:"
81,play dist
81,cf push YOUR-APP -p target/universal/YOUR-APP-VERSION.zip
81,Where:
81,YOUR-APP is the name of your app.
81,YOUR-APP-VERSION is the name of the dist style ZIP you want to build and deploy.
81,"For more information, see the Play Framework website."
81,Spring Boot CLI
81,"Spring Boot can run apps comprised entirely of POGOs. To deploy them, run:"
81,spring grab *.groovy
81,cf push YOUR-APP
81,Where YOUR-APP is the name of your app.
81,"For more information, see Spring Boot on the Spring website and Spring Boot CLI Container in the Cloud Foundry Java Buildpack repository on GitHub."
81,Servlet
81,Java apps can be packaged as Servlet apps.
81,Maven
81,"A Maven build can create a Servlet WAR. To build and deploy the WAR, run:"
81,mvn package
81,cf push YOUR-APP -p target/YOUR-APP-VERSION.war
81,Where:
81,YOUR-APP is the name of your app.
81,YOUR-APP-VERSION is the name of the WAR you want to build and deploy.
81,Gradle
81,"A Gradle build can create a Servlet WAR. To build and deploy the WAR, run:"
81,gradle build
81,cf push YOUR-APP -p build/libs/YOUR-APP-VERSION.war
81,Where:
81,YOUR-APP is the name of your app.
81,YOUR-APP-VERSION is the name of the WAR you want to build and deploy.
81,Binding to Services
81,"For more information about binding apps to services, see:"
81,Configuring Service Connections for Grails
81,Configuring Service Connections for Play Framework
81,Configuring Service Connections for Spring
81,Java and Grails Best Practices
81,Provide a JDBC Driver
81,"The Java buildpack does not bundle a JDBC driver with your app. If you want your app to access a SQL RDBMS, include the appropriate driver in your app."
81,Allocate Sufficient Memory
81,"If you do not allocate sufficient memory to a Java app when you deploy it, it may fail to start, or Cloud Foundry may terminate it. You must allocate enough memory to allow for:"
81,Java heap
81,"Metaspace, if using Java 8"
81,"PermGen, if using Java 7 or earlier"
81,Stack size per Thread
81,JVM overhead
81,"The config/open_jdk_jre.yml file of the Java buildpack contains default memory size and weighting settings for the JRE. For an explanation of JRE memory sizes and weightings and how the Java buildpack calculates and allocates memory to the JRE for your app, see Open JDK JRE in the Cloud Foundry Java Buildpack on GitHub."
81,"To configure memory-related JRE options for your app, either create a custom buildpack and specify this buildpack in your deployment manifest, or override the default memory settings of your buildpack as described in Configuration and Extension with the properties listed in the Open JDK JRE README in the Cloud Foundry Java Buildpack on GitHub. For more information about configuring custom buildpacks and manifests, see Custom Buildpacks and Deploying with App Manifests."
81,"To see memory utilization when your app is running, run:"
81,cf app YOUR-APP
81,Where YOUR-APP is the name of your app.
81,Troubleshoot Out of Memory
81,A Java app may crash because of insufficient memory on the Garden container or the JVM on which it runs. The sections below provide guidance for help diagnosing and resolving such issues.
81,JVM
81,Error: java.lang.OutOfMemoryError. For example:
81,$ cf logs YOUR-APP --recent
81,2016-06-20T09:18:51.00+0100 [APP/0] OUT java.lang.OutOfMemoryError: Metaspace
81,Where YOUR-APP is the name of your app.
81,"Cause: If the JVM cannot garbage-collect enough space to ensure the allocation of a data-structure, it fails with java.lang.OutOfMemoryError. In the example above, JVM has an under-sized metaspace. You may see failures in other memory pools, such as heap."
81,"Solution: Configure the JVM correctly for your app. For more information, see Allocate Sufficient Memory."
81,Garden Container
81,"Note: The solutions in this section require configuring the memory calculator, which is a sub-project of the Java buildpack that calculates suitable memory settings for Java apps when you push them. For more information, see the java-buildpack-memory-calculator repository on GitHub. If you have questions about the memory calculator, you can ask them in the #java-buildpack channel of the Cloud Foundry Slack organization."
81,Error: The Garden container terminates the Java process with the out of memory event. For example:
81,$ cf events YOUR-APP
81,time
81,event
81,actor
81,description
81,2016-06-20T09:18:51.00+0100
81,app.crash
81,app-name
81,"index: 0, reason: CRASHED, exit_description: out of memory, exit_status: 255"
81,Where YOUR-APP is the name of your app.
81,"This error appears when the JVM allocates more OS-level memory than the quota requested by the app, such as through the manifest."
81,"Cause 1 - Insufficient native memory: This error commonly means that the JVM requires more native memory. In the scope of the Java buildpack and the memory calculator, the term native means the memory required for the JVM to work, along with forms of memory not covered in the other classifications of the memory calculator. This includes the memory footprint of OS-level threads, direct NIO buffers, code cache, program counters, and others."
81,"Solution 1: Determine how much native memory a Java app needs by measuring it with realistic workloads and fine-tuning it accordingly. You can then configure the Java buildpack using the native setting of the memory calculator, as in the example below:"
81,---
81,applications:
81,- name: YOUR-APP
81,memory: 1G
81,env:
81,JBP_CONFIG_OPEN_JDK_JRE: '[memory_calculator: {memory_sizes: {native: 150m}}]'
81,Where YOUR-APP is the name of your app.
81,"This example shows that 150m of the overall available 1G is reserved for anything that is not heap, metaspace, or permgen. In less common cases, this may come from companion processes started by the JVM, such as the Process API."
81,"For more information about measuring how much native memory a Java app needs, see Native Memory Tracking in the Java documentation. For more information about configuring the Java buildpack using the native setting, see OpenJDK JRE in the Cloud Foundry Java Buildpack on GitHub. For more information about the Process API, see Class Process in the Java documentation."
81,"Cause 2 - High thread count: Java threads in the JVM can cause memory errors at the Garden level. When an app is under heavy load, it uses a high number of threads and consumes a significant amount of memory."
81,Solution 2: Set the reserved memory for stack traces to the correct value for your app.
81,"You can use the stack setting of the memory calculator to configure the amount of space the JVM reserves for each Java thread. You must multiply this value by the number of threads your app requires. Specify the number of threads in the stack_threads setting of the memory calculator. For example, if you estimate the max thread count for an app at 800 and the amount of memory needed to represent the deepest stacktrace of a Java thread is 512KB, configure the memory calculator as follows:"
81,---
81,applications:
81,- name: YOUR-APP
81,memory: 1G
81,env:
81,"JBP_CONFIG_OPEN_JDK_JRE: '[memory_calculator: {stack_threads: 800, memory_sizes: {stack: 512k}}]'"
81,Where YOUR-APP is the name of your app.
81,"In this example, the overall memory amount reserved by the JVM for representing the stacks of Java threads is 800 * 512k = 400m."
81,"The correct settings for stack and stack_threads depend on your app code, including the libraries it uses. Your app may technically have no upper limit, such as in the case of cavalier usage of CachedThreadPool executors. However, you still must calculate the depth of the thread stacks and the amount of space the JVM should reserve for each of them. For more information, see Executors.newCachedThreadPool() considered harmful on the Bizo website and the newCachedThreadPool section of the Class Executors topic in the Java documentation."
81,Troubleshoot Failed Upload
81,"If your app fails to upload when you push it to Cloud Foundry, it may be for one of the following reasons:"
81,"WAR is too large: An upload may fail due to the size of the WAR file. Cloud Foundry testing indicates WAR files as large as 250 MB upload successfully. If a WAR file larger than that fails to upload, it may be a result of the file size."
81,"Connection issues: App uploads can fail if you have a slow Internet connection, or if you upload from a location that is very remote from the target Cloud Foundry instance. If an app upload takes a long time, your authorization token can expire before the upload completes. A workaround is to copy the WAR to a server that is closer to the Cloud Foundry instance, and then push it from there."
81,"Out-of-date Cloud Foundry Command-Line Interface (cf CLI) client: Upload of a large WAR is faster and hence less likely to fail if you are using a recent version of the cf CLI. If you are using an older version of the cf CLI client to upload a large WAR, and having problems, try updating to the latest version of the cf CLI."
81,"Incorrect WAR targeting: By default, cf push uploads everything in the current directory. For a Java app, cf push with no option flags uploads source code and other unnecessary files, in addition to the WAR. When you push a Java app, specify the path to the WAR by running:"
81,cf push YOUR-APP -p PATH/TO/WAR-FILE
81,Where:
81,YOUR-APP is the name of your app.
81,PATH/TO/WAR-FILE is the path to the WAR.
81,"You can determine whether or not the path was specified for a previously pushed app by examining the app deployment manifest, manifest.yml. If the path attribute specifies the current directory, the manifest includes a line like:"
81,path: .
81,"To re-push just the WAR, either:"
81,"Delete manifest.yml and run cf push again, specifying the location of the WAR using the -p flag."
81,"Edit the path argument in manifest.yml to point to the WAR, and re-push the app."
81,Debug Java Apps on Cloud Foundry
81,"Because of the way that Cloud Foundry deploys your apps and isolates them, it is not possible to connect to your app with the remote Java debugger. Instead, instruct the app to connect to the Java debugger on your local machine."
81,To set up remote debugging when using BOSH Lite or a Cloud Foundry installation:
81,Open your project in Eclipse.
81,"Right-click on your project, go to Debug as and pick Debug Configurations."
81,Create a new Remote Java Application.
81,"Make sure your project is selected, pick Standard (Socket Listen) from the Connection Type drop down and set a port. Make sure this port is open if you are running a firewall."
81,Click Debug.
81,"The debugger should now be running. If you switch to the Debug perspective, you should see your app listed in the Debug panel, and it should say Waiting for vm to connect at port."
81,"Next, to push your app to Cloud Foundry and instruct Cloud Foundry to connect to the debugger running on your local machine:"
81,"Edit your manifest.yml file. Set the instances count to 1. If you set this greater than one, multiple apps try to connect to your debugger."
81,"Also in manifest.yml, add an env block and create a variable named JAVA_OPTS. For more information about the env block, see Deploying with App Manifests."
81,"Add the remote debugger configuration to the JAVA_OPTS variable: -agentlib:jdwp=transport=dt_socket,address=YOUR-IP-ADDRESS:YOUR-PORT."
81,Save the manifest.yml file.
81,Run:
81,cf push
81,"Upon completion, you should see that your app has started and is now connected to the debugger running in your IDE. You can now add breakpoints and interrogate the app just as you would if it were running locally."
81,Slow Starting Java or Grails Apps
81,"Some Java and Grails apps do not start quickly, and the health check for an app can fail if an app starts too slowly."
81,The current Java buildpack implementation sets the Tomcat bindOnInit property to false. This prevents Tomcat from listening for HTTP requests until an app has fully deployed.
81,"If your app does not start quickly, the health check may fail because it checks the health of the app before the app can accept requests. By default, the health check fails after a timeout threshold of 60 seconds."
81,"To resolve this issue, run cf push with the -t TIMEOUT-THRESHOLD option to increase the timeout threshold. Run:"
81,$ cf push YOUR-APP -t TIMEOUT-THRESHOLD
81,Where:
81,YOUR-APP is the name of your app.
81,TIMEOUT-THRESHOLD is the number of seconds to which you want to increase the timeout threshold.
81,"Note: The timeout threshold cannot exceed 180 seconds. Specifying a timeout threshold greater than 180 seconds results in the following error: Server error, status code: 400, error code: 100001, message: The app is invalid: health_check_timeout maximum_exceeded"
81,Extension
81,"The Java buildpack is also designed to be easily extended. It creates abstractions for three types of components (containers, frameworks, and JREs) in order to allow users to easily add functionality. In addition to these abstractions, there are a number of utility classes for simplifying typical buildpack behaviors."
81,"As an example, the New Relic framework looks like the below:"
81,class NewRelicAgent < JavaBuildpack::Component::VersionedDependencyComponent
81,# @macro base_component_compile
81,def compile
81,FileUtils.mkdir_p logs_dir
81,download_jar
81,@droplet.copy_resources
81,end
81,# @macro base_component_release
81,def release
81,@droplet.java_opts
81,.add_javaagent(@droplet.sandbox + jar_name)
81,".add_system_property('newrelic.home', @droplet.sandbox)"
81,".add_system_property('newrelic.config.license_key', license_key)"
81,".add_system_property('newrelic.config.app_name', ""'#{application_name}'"")"
81,".add_system_property('newrelic.config.log_file_path', logs_dir)"
81,end
81,protected
81,# @macro versioned_dependency_component_supports
81,def supports?
81,"@application.services.one_service? FILTER, 'licenseKey'"
81,end
81,private
81,FILTER = /newrelic/.freeze
81,def application_name
81,@application.details['application_name']
81,end
81,def license_key
81,@application.services.find_service(FILTER)['credentials']['licenseKey']
81,end
81,def logs_dir
81,@droplet.sandbox + 'logs'
81,end
81,end
81,"For more information, see Design, Extending, and Configuration and Extension in the Cloud Foundry Java Buildpack repository on GitHub."
81,Environment Variables
81,You can access environments variable programmatically.
81,"For example, you can obtain VCAP_SERVICES by running:"
81,"System.getenv(""VCAP_SERVICES"");"
81,"For more information, see CF Environment Variables."
81,Create a pull request or raise an issue on the source for this page in GitHub
81,Cloud Foundry Documentation
81,© 2021 Cloud Foundry Foundation. All Rights Reserved.
81,Get Involved
87,FME Community
87,LoadingÃ—Sorry to interruptCSS ErrorRefresh
88,Top 3 Snowflake Performance Tuning Tactics — Analytics.Today
88,Skip to Content
88,Analytics.Today
88,Current Page:
88,Blog
88,About
88,Contact
88,Open Menu
88,Close Menu
88,Open Menu
88,Close Menu
88,Analytics.Today
88,Current Page:
88,Blog
88,About
88,Contact
88,Current Page:
88,Blog
88,About
88,Contact
88,Top 3 Snowflake Performance Tuning Tactics
88,Jul 17
88,Written By John Ryan
88,Image by Gino Crescoli from Pixabay
88,"The Snowflake Data Cloud has an excellent reputation as an analytics platform for blisteringly fast query performance, but without indexes. So, how can you tune the Snowflake database to maximize query performance? This article explains the top three techniques to tune your system to maximum throughput, including data ingestion, data transformation, and end-user queries.Snowflake Query PerformanceOne of my favourite phrases is: What problem are we trying to solve? As techies, we often launch into solutions before we even understand the true nature of the problem. The performance issues on any analytics platform generally fall into one of three categories: 1.Data Load Speed: The ability to load massive volumes of data as quickly as possible.2.Data Transformation: The ability to maximize throughput, and rapidly transform the raw data into a form suitable for queries.3.Data Query Speed: Which aims to minimize the latency of each query and deliver results to business intelligence users as fast as possible.1. Snowflake Data LoadingAvoid Scanning FilesThe diagram below illustrates the most common method of bulk loading data into Snowflake, which involves transferring the data from the on-premise system to cloud storage, and then using the COPY command to load to Snowflake."
88,"Before copying data, Snowflake checks the file has not already been loaded, and this leads the first and easiest way to maximize load performance by partitioning staged data files to avoid scanning terabytes of files that have already been loaded. The code snippet below shows a COPY using a range of options:-- Simple method:"
88,Scan entire stage
88,copy into sales_table
88,from @landing_data
88,pattern='.*[.]csv';
88,-- Most Flexible method:
88,Limit within directory
88,copy into sales_table
88,from @landing_data/sales/transactions/2020/05
88,pattern='.*[.]csv';
88,-- Fastest method:
88,A named file
88,copy into sales_table
88,from @landing_data/sales/transactions/2020/05/sales_050.csv;
88,"While the absolute fastest method is to name a specific file, using pattern matching to identify the files is the most flexible. The alternative option is to remove the files immediately after loading.Choose a sensible virtual warehouse sizeThe diagram below illustrates a common mistake made by designers when loading large data files into Snowflake, which involves scaling up to a bigger virtual warehouse to speed the load process. In reality, scaling up the warehouse has no performance benefit in this case."
88,Snowflake:
88,Loading large data files
88,"The above COPY statement will open the 10Gb data file and sequentially load the data using a single thread on one node, leaving the remaining servers idle. Benchmark tests demonstrate a load rate of around 9 Gb per minute, which is fast but could be improved. Unless you have other parallel loads using the same virtual warehouse, the above solution is also remarkably inefficient, as you will pay for four servers, while using only one.The diagram below illustrates a much better approach, which involves breaking up the single 10Gb file into 100 x 100Mb files to make use of Snowflake’s automatic parallel execution."
88,Snowflake: Fast parallel loading multiple data files
88,"Using the above method, Snowflake spreads the load across all servers to maximize throughput and achieves a load rate of around 31 Gb per minute. Furthermore, once the task has completed, the server can auto-suspend, which reduces spend2. Snowflake Transformation PerformanceLatency Vs. ThroughputWhile tuning the SQL is often a very effective way to reduce the elapsed time for long running queries, designers often miss an opportunity. As Abraham Maslow said, ""If the only tool you have is a hammer, you tend to see every problem as a nail"".In addition to reducing the latency (I.E. maximising performance) of individual queries, it’s also important to maximize throughput – to achieve the greatest amount of work in the shortest possible time. Of course, it depends upon ""the problem you're trying to solve"". As I have indicated before, by increasing virtual warehouse size, it is possible to reduce elapsed time from seven hours to just four minutes. But that's not the solution for every use-case.The diagram below illustrates a typical data transformation pattern that involves executing a sequence of batch tasks on a given virtual warehouse. As each task completes, the next task is started:"
88,Snowflake:
88,Serial processing
88,"One solution to improve throughput is to scale up to a bigger virtual warehouse to complete the work faster, but even this technique will eventually reach a limit (I'll explain why in another article).Furthermore, while it might improve query performance, there's also a greater chance of inefficient use of resources on a larger warehouse. Let's face it, if you're running an X-SMALL warehouse at 60% of its capacity you are wasting 40% of the machine resources. If however you're running a monster X4-LARGE warehouse at 60% capacity, you are wasting around 128 times the resources."
88,Snowflake: Parallel scale-out processing
88,"In the above example, Apache Airflow is used to execute multiple parallel tasks (each with a different connection to Snowflake), and each task uses the same virtual warehouse. As the workload increases, jobs begin to queue as there are insufficient resources available. However, the Snowflake multi-cluster feature can be configured to automatically create another same-size virtual warehouse, and this continues to take up the load.As tasks complete, the above solution automatically scales back down to a single cluster, and once the last task finishes, the last running cluster will suspend. This is by far the most efficient method of completing batch parallel tasks, and we still have the option of scaling up.The SQL snippet below illustrates the command needed to create a multi-cluster warehouse, which will automatically suspend after 60 seconds idle time, but use the ECONOMY scaling policy to favour throughput and saving credits over individual query latency.-- Create a multi-cluster warehouse for batch processing"
88,create or replace warehouse batch_vwh with
88,warehouse_size
88,= SMALL
88,min_cluster_count
88,= 1
88,max_cluster_count
88,= 10
88,scaling_policy
88,= economy
88,auto_suspend
88,= 60
88,initially_suspended = true;
88,"3. Tuning Snowflake Query PerformanceSelect Only Required ColumnsLike many other data analytic platforms, Snowflake uses a columnar data store. As Turning Medal award winnerProfessor Michael Stonebraker indicates, in a seminal YouTube lecture on databases, ""....a column store is over a hundred times faster than than a row-store"". He demonstrated this fact with the C-Store Database which he demonstrated was 164 times faster than a commercially available database. For the record, Oracle, Exadata, SQLServer and PostgreSQL are all row store databases, designed for OLTP systems.One feature of column-store databases, is they achieve the remarkable performance gains by physically organising the data in columns rather than rows. Storing data in columns makes it much more efficient to retrieve a small sub-set of columns from the entire table."
88,Snowflake:
88,Column optimised database storage
88,"In the above diagram, the query fetches just two columns, and on a table with 100 columns, this will be 98% faster than a traditional row-store, which needs to read all the data from disk.This leads to a simple best practice (in production systems). Avoid selecting all the columns from a table or view using a select * from. While it's OK for ad-hoc queries, you'll find it much faster to indicate the specific columns you need.Maximize Cache UsageThe diagram below illustrates a vital component of the Snowflake internal architecture that it caches data in both the virtual warehouse and the cloud services layer. As I have indicated before, taking steps to maximise cache usage is a simple method to improve overall query performance on Snowflake."
88,Snowflake:
88,Cache Layers
88,"Business intelligence dashboards frequently re-execute the same query to refresh the screen showing changed values. Snowflake automatically optimizes these queries by returning results from the Results Cache with results available for 24 hours after each query execution.Data is also cached within the virtual warehouse on fast SSD, but unlike the Results Cache, the virtual warehouse holds raw data which is aged out on a least recently used basis. While it’s not possible to directly adjust the virtual warehouse cache, it is possible to optimize usage with the following steps:Fetch required attributes:  Avoid using SELECT * in queries as this fetches all data attributes from Database Storage to the Warehouse Cache.  Not only is this slow, but it potentially fills the warehouse cache with data that is not needed.Scale Up:  While you should never scale up to tune a specific query, it may be sensible to resize the warehouse to improve overall query performance.  As scaling up adds additional servers, it spreads the workload and effectively increases the overall warehouse cache size.Consider Data Clustering:  For tables over a terabyte in size, consider creating a cluster key to maximize partition elimination.  This solution both maximizes query performance for individual queries and returns fewer micro-partitions making the best use of the Warehouse Cache.-- Identify slow performing SQL statements"
88,select query_id
88,as query_id
88,round(bytes_scanned/1024/1024)
88,as mb_scanned
88,total_elapsed_time / 1000
88,as elapsed_seconds
88,(partitions_scanned /
88,"nullif(partitions_total,0)) * 100 as pct_table_scan"
88,percent_scanned_from_cache * 100
88,as pct_from cache
88,bytes_spilled_to_local_storage
88,as spill_to_local
88,bytes_spilled_to_remote_storage
88,as spill_to_remote
88,from
88,snowflake.account_usage.query_history
88,where (bytes_spilled_to_local_storage > 1024 * 1024 or
88,bytes_spilled_to_remote_storage > 1024 * 1024 or
88,percentage_scanned_from_cache < 0.1)
88,and
88,elapsed_seconds > 120
88,and
88,bytes_scanned > 1024 * 1024
88,order by elapsed_seconds desc;
88,"The SQL snippet above can help identify potential query performance issues on queries that run for more than 2 minutes and scan over a megabyte of data. In particular, look out for:Table Scans:  A high value of PCT_TABLE_SCAN and a large number of MB_SCANNED indicates potential poor query selectivity on large tables.  Check the query WHERE clause and consider using a cluster key if appropriate.Spilling:  Any value in SPILL_TO_LOCAL or SPILL_TO_REMOTE indicates a potentially large sort of operation on a small virtual warehouse.  Consider moving the query to a bigger warehouse or scaling up the existing warehouse if appropriate.ConclusionA common misconception about Snowflake is the only solution available to improve query performance is to scale up to a bigger warehouse, but this is a potentially poor strategy. In reality, the best approach depends upon the problem area, which is most often in ingestion, transformation, or end-user queries, and often the most effective solutions are based upon a design approach rather than pure query tuning.By all means, consider scaling up to a large warehouse to improve query performance, but first identify and focus on the actual problem. You may find there are more effective and efficient solutions available.Notice Anything Missing?No annoying pop-ups or adverts."
88,"No bull, just facts, insights and opinions."
88,Sign up below and I will ping you a mail when new content is available.
88,"I will never spam you or abuse your trust. Alternatively, you can leave a comment below.Disclaimer: The opinions expressed on this site are entirely my own, and will not necessarily reflect those of my employer."
88,Email
88,Thank you!
88,John Ryan
88,Previous
88,Previous
88,What is a Snowflake Virtual Warehouse?
88,Next
88,Next
88,When should I use Data Vault?
88,Designed by me and hosted on Squarespace.(c) Copyright John Ryan 2020.
88,All Rights Reserved.
90,"Optimizing Drupal to load faster (Server, MySQL, caching, theming, HTML) | Managing site performance and scalability | Drupal Wiki guide on Drupal.org"
90,Skip to main content
90,Skip to search
90,Drupal.org home
90,Why Drupal?For developers
90,For marketers
90,For agencies
90,Case studies
90,About Drupal
90,Drupal 9
90,In the news
90,BuildDownload & Extend
90,Browse Repository
90,Documentation
90,Modules
90,Themes
90,Distributions
90,Issue queues
90,SolutionsBy industry
90,By feature
90,Case studies
90,For hosting
90,ServicesMarketplace
90,Hosting
90,Training
90,CommunityPortal
90,Contributor guide
90,Organizations
90,Forum
90,Promote Drupal
90,Community Case Studies
90,Drupal Swag
90,Core Development & Strategic Initiatives
90,ResourcesUser guide
90,Documentation
90,Support
90,Security
90,Jobs
90,Events
90,Newsletter
90,Project News
90,Partner Press
90,Drupal 9
90,"Diversity, Equity, and Inclusion Resources"
90,GiveDrupal Association
90,Supporters
90,Promote Drupal
90,Join us
90,Contributor guide
90,About Drupal.org
90,EventsDrupalCon North America
90,Community Events
90,Drupical
90,DrupalFest
90,Try DrupalDemo online
90,Download
90,Return to content
90,Search form
90,Search
90,Log in
90,Create account
90,Documentation
90,Search
90,Drupal WikiDrupal 7Managing site performance and scalability
90,Gain visibility and targeted engagement opportunities with the best and brightest talent and thought leaders in the Drupal ecosystem.
90,Connect with community as a DrupalCon sponsor
90,Advertising sustains the DA. Ads are hidden for members. Join today
90,On this page
90,Basic settings
90,Theme optimization
90,Coding standard and proper use of already existing core API
90,Secure codes
90,DB Query optimization in codes
90,DB table optimization
90,Disable unnecessary modules
90,Remove unnecessary contents and others
90,Cache modules
90,Make changes according to Google Pagespeed and yahoo YSlow suggestions
90,MySQL Settings
90,Apache settings
90,Also we can check this options :
90,1) Turn Page Caching On
90,2) Turn Views caching on
90,Managing site performance and scalability
90,Planning for Performance
90,Caching to improve performance
90,Changing PHP memory limits
90,Content Delivery Network [CDN]
90,Design for Low Bandwidth
90,Increase upload size in your php.ini
90,"Optimizing Drupal to load faster (Server, MySQL, caching, theming, HTML)"
90,Optimizing MySQL
90,Randomizing MySQL Users For Exceeded max_questions Error
90,Server tuning considerations
90,Tuning php.ini for Drupal
90,"Optimizing Drupal to load faster (Server, MySQL, caching, theming, HTML)"
90,Last updated on
90,25 March 2021
90,Basic settings
90,Configure cron job (for Drupal 6 http://drupal.org/project/poormanscron)
90,Make sure all cache tables are clearing properly especially cache_form
90,Enable cache options in performance page
90,"(For Drupal 6, http://drupal.org/project/advagg )"
90,Theme optimization
90,Manually Remove blankspaces and comments from .tpl
90,No indentation in .tpl
90,Turn on CSS and JS aggregation in the performance page
90,Manually reduce css file size by removing duplicate and combine similar together
90,Move codes to functions which should be in a custom common module. Use functions for similar problems instead of coding separately. Refer core API
90,Coding standard and proper use of already existing core API
90,http://drupal.org/coding-standards
90,https://drupalize.me/videos/understanding-drupal-coding-standards?p=2012
90,Secure codes
90,http://drupal.org/writing-secure-code
90,DB Query optimization in codes
90,Join db queries whenever possible
90,"For Db update and insert, use core API"
90,Use drupal standard http://drupal.org/coding-standards
90,DB table optimization
90,http://drupal.org/project/db_maintenance
90,Disable unnecessary modules
90,Devel
90,Statistics
90,Update status
90,Use syslog instead of Database logging
90,Remove unnecessary contents and others
90,Cache modules
90,"Make use of object caches to reduce database overhead, e.g. Memcache, Redis or APC"
90,https://drupal.org/project/authcache
90,Some module may help improve
90,http://drupal.org/project/ajaxblocks (not available with Drupal 8 & 9 )
90,Make changes according to Google Pagespeed and yahoo YSlow suggestions
90,MySQL Settings
90,Cache Size say 32MB in MySQL
90,Use https://github.com/initlabopen/mysqlconfigurer for fully automated MySQL performance tuning
90,Apache settings
90,DNS lookup : OFF
90,Set FollowSymLinks everywhere and never set SymLinksIfOwnerMatch
90,Avoid content negotiation. Or use type-map files rather than Options MultiViews directive
90,"KeepAlive on, and KeepAliveTimeout very low (1 or 2 sec)"
90,Disable or comment access.log settings
90,Enable mod_deflate or mod_gzip
90,Install APC server with higher memory limit apc.shm_size = 64
90,Also we can check this options :
90,1) Turn Page Caching On
90,"What page caching does is that instead of using a bunch of database queries to get the data used in making a typical web page, the rendered contents of the web page are stored in a separate database cache table so that it can be recalled quicker. If you have 10 people visiting the site from different computers, Drupal first looks into the database cache table to see if the page is there, if it is, it just gives them the page. Think of saving the output of 50 separate queries so that is accessible with a single query. You obviously are reducing the SQL queries required by a lot. What the page cache table actually stores is HTML content."
90,"Page Caching is that it only works to optimize the page load time for Anonymous users. This is because when you are logged in, you might have blocks that show up on the page that are customized for you, if it served everybody the same page, they would see your customized information (think of a My Recent Posts block), so Drupal does not use the Page Cache for Authenticated users automatically. This allows you to turn Page Caching on and still get the benefit for Anonymous user page load times, but does not break the site for Authenticated users. There are other caching options that will help with Authenticated user page performance, we will talk about those later."
90,"To enable Page Caching, you go to Configuration | Development and select the checkbox next to ""Cache pages for anonymous users""."
90,2) Turn Views caching on
90,"As mentioned when talking about Page Caching only working for anonymous users above, there are other caching options for helping with Authenticated user page performance. One of those options is to turn on caching for blocks and pages that you create using the Views module. This allows you to cache the output of the query used to generate the view, or the end HTML output of your View, and you can tune the cache for them separately. And realize too that this means you can cache portions of a page if you are using one or several Views blocks in the page, it will just cache that block in the page, not the whole page."
90,See more @ https://www.lullabot.com/articles/a-beginners-guide-to-caching-data-in-d...
90,Help improve this page
90,Page status:
90,No known problems
90,You can:
90,"Log in, click Edit, and edit this page"
90,"Log in, click Discuss, update the Page status value, and suggest an improvement"
90,Log in and create a Documentation issue with your suggestion
90,"Drupal’s online documentation is © 2000-2021 by the individual contributors and can be used in accordance with the Creative Commons License, Attribution-ShareAlike 2.0. PHP code is distributed under the GNU General Public License."
90,Thank you to these Drupal contributors
90,Top Drupal contributor Acquia would like to thank their partners for their contributions to Drupal.
90,Infrastructure management for Drupal.org provided by
90,News itemsNews
90,Planet Drupal
90,Social media
90,Sign up for Drupal news
90,Security advisories
90,Jobs
90,Our communityCommunity
90,"Services, Training & Hosting"
90,Contributor guide
90,Groups & meetups
90,DrupalCon
90,Code of conduct
90,DocumentationDocumentation
90,Drupal Guide
90,Drupal User Guide
90,Developer docs
90,API.Drupal.org
90,Drupal code baseDownload & Extend
90,Drupal core
90,Modules
90,Themes
90,Distributions
90,Governance of communityAbout
90,Web accessibility
90,Drupal Association
90,About Drupal.org
90,Terms of service
90,Privacy policy
90,Drupal is a registered trademark of Dries Buytaert.
91,OnGres | EXPLAIN ANALYZE may be lying to you
91,Resources
91,Blog
91,Services
91,Professional Services
91,Consulting
91,Training
91,Products
91,StackGres
91,PostgresqlCO.NF
91,About
91,Contact
91,Post
91,EXPLAIN ANALYZE may be lying to you
91,Álvaro Hernández
91,"May 26, 2020 ·"
91,10 min read
91,postgresql
91,performance
91,Share this post
91,Álvaro Hernández
91,Founder and CEO
91,The Observer effect
91,"In physics, the observer effect is the theory that the mere observation of a"
91,phenomenon inevitably changes that phenomenon. This is often the result of
91,"instruments that, by necessity, alter the state of what they measure in some"
91,manner.
91,"Observer effect, Wikipedia"
91,(edit; this previously referred to the
91,Uncertainty principle)
91,"In layman’s terms, what the Observer effect states is that by measuring a"
91,"property of a system, you may be altering that system itself: your observation"
91,becomes a distorted version of the reality.
91,"In most cases, this distortion is negligible and we can simply ignore it. If"
91,"we use a thermometer to measure someone’s temperature, some heat will be"
91,"transferred from the person to the termometer, effectively lowering the person’s"
91,"temperature. But it should not be noticeable, and well below the error margin of"
91,the thermometer.
91,"But what happens when the measurement may not just affect, but rather completely"
91,ruin the measurement?
91,Where the potential lie is
91,You are probably resorting a lot to use Postgres'
91,EXPLAIN ANALYZE
91,command when you want to optimize a query’s performance. You probably look at
91,"the query nodes, see which ones have the highest execution time and then try to"
91,"optimize them. The costlier the node is, the biggest return of investment you"
91,"get if you can optimize it. Obviously, a query optimization may change the query"
91,"plan altogether, but you get the point: you want to know where most of the query"
91,execution time is going.
91,Now grab your favorite Postgres and run the following commands:
91,"create table i1 as select i from generate_series(1,1000*1000) as i;"
91,"create table i2 as select i from generate_series(1,1000) as i;"
91,analyze;
91,explain analyze select sum(i1.i * i2.i) from i1 inner join i2 using (i);
91,Note that the first analyze command is not related at all with the explain analyze command that follows it.
91,Run the query. Note the time reported by explain analyze. Now run the query
91,"again and note the execution time without explain analyze. You can do this,"
91,"for example, by:"
91,Running from psql and using both \timing and \o /dev/null.
91,Using pg_stat_statements.
91,"The latter is a better method, as the former includes client round-trip time and processing. But"
91,this overhead should be negligible for this case.
91,Do you see anything wrong? The execution time as reported by EXPLAIN ANALYZE
91,is substantially higher than that of the real query execution time. On my
91,"system, running 20 times after another 20 times of warmup:"
91,query
91,calls
91,total
91,mean
91,min
91,max
91,stddev
91,explain analyze select sum(i1.i * i2.i) from i1 inner join i2 using (i)
91,917.20
91,45.86
91,45.32
91,49.24
91,0.84
91,select sum(i1.i * i2.i) from i1 inner join i2 using (i)
91,615.73
91,30.79
91,30.06
91,34.48
91,0.92
91,"That’s about a 50% overhead! As we can see here, the measurement is"
91,"significantly altering the observed fact. But it can get much worse. For instance,"
91,on a virtual instance running on a non
91,Nitro EC2 instance (r4.large):
91,query
91,calls
91,total
91,mean
91,min
91,max
91,stddev
91,explain analyze select sum(i1.i * i2.i) from i1 inner join i2 using (i)
91,21080.18
91,1054.01
91,1053.36
91,1055.96
91,0.55
91,select sum(i1.i * i2.i) from i1 inner join i2 using (i)
91,2596.85
91,129.84
91,129.33
91,130.45
91,0.28
91,"Here EXPLAIN ANALYZE got 8 times slower, a 700% overhead!"
91,Astute readers may realize that this effect is related to the system clock.
91,"Non Nitro instances are virtualized with Xen, which exposes a xen virtualized"
91,clock to the VMs. On Nitro instances and other virtualized environments where
91,"KVM is used, clock is as fast as the hypervisor’s, and results are similar to"
91,the first ones shown here. We may also mitigate this effort on r4.large by
91,switching to the tsc time source:
91,echo tsc | sudo tee -a /sys/devices/system/clocksource/clocksource0/current_clocksource
91,query
91,calls
91,total
91,mean
91,min
91,max
91,stddev
91,explain analyze select sum(i1.i * i2.i) from i1 inner join i2 using (i)
91,3747.07
91,187.37
91,187.12
91,187.56
91,0.12
91,select sum(i1.i * i2.i) from i1 inner join i2 using (i)
91,2602.45
91,130.12
91,129.88
91,130.77
91,0.21
91,Also note that results will change if you configure differently
91,max_parallel_workers_per_gather as these results are affected by the level of
91,parallelism used.
91,The good news
91,"You shouldn’t be surprised, however. This behavior is known and documented. As"
91,"usual, Postgres documentation is as complete as it can be:"
91,"The measurement overhead added by EXPLAIN ANALYZE can be significant,"
91,especially on machines with slow gettimeofday() operating-system calls. You
91,can use the pg_test_timing tool to measure the overhead of timing on your
91,system.
91,EXPLAIN caveats
91,"However, I have found that many users and DBAs are either unaware of this effect"
91,or not aware of how significant it may be. This post is my humble contribution
91,to make this effect more widely unserstood.
91,The Volcano
91,"Why is this happening, after all? Postgres, like other OLTP databases, follows a"
91,query execution model named
91,the Volcano model.
91,"Under this model, also known as one-row-at-a-time, each node of the query"
91,execution tree contains code to process rows one by one. Instead of every node
91,"gathering all the rows belonging to it before combining with the next node, as"
91,"soon as a row is gathered at one node, it is processed through the rest of the"
91,tree. This makes sense –gathering all the rows at a given node may require to
91,"hold all that data in memory, which could be impossible–, but it introduced the"
91,EXPLAIN ANALYZE problem described here.
91,The executor processes a tree of “plan nodes”. The plan tree is essentially
91,"a demand-pull pipeline of tuple processing operations. Each node, when"
91,"called, will produce the next tuple in its output sequence, or NULL if no"
91,more tuples are available. If the node is not a primitive relation-scanning
91,"node, it will have child node(s) that it calls in turn to obtain input"
91,tuples.
91,src/backend/executor/README
91,So we can already explain exactly where the overhead comes from: in order to
91,"measure the execution time of a given node, as shown by explain analyze, you"
91,"need to measure the execution time on a per-row basis, and then aggregate them"
91,"per node, to obtain the total execution time per node."
91,Since rows are not executed one after the other (since a row may be processed by
91,"other nodes first), you basically need to get the system time before and after"
91,processing every row. In other words: you are calling the system twice per row.
91,"On a node that processes millions of rows, you are then calling the system time"
91,millions of times.
91,"But how cheap (or expensive) is it to call the system clock? In Postgres, this"
91,is implemented in the
91,elapsed_time
91,"function, which in turn relies on the"
91,INSTR_TIME macros defined in
91,instr_time.h.
91,"Which calls the system call clock_gettime, a fast system call on most systems."
91,"In particular, on Linux, is typically implemented as a"
91,"VDSO, meaning that there’s no context"
91,"switch between user and kernel space, making the call significantly faster."
91,"But again, how fast is “fast”, if we might be calling this millions of times?"
91,"Again, Postgres"
91,documentation comes to
91,"the rescue, as there’s a binary included in Postgres to precisely do this,"
91,"pg_test_timing. Indeed, it has a documentation"
91,section
91,explaining how to use it to measure the EXPLAIN ANALYZE overhead.
91,"On one of the systems used for the measurements above, it reports:"
91,Testing timing overhead for 3 seconds.
91,"Per loop time including overhead: 4620,82 ns"
91,Histogram of timing durations:
91,< us
91,% of total
91,count
91,"0,00000"
91,"0,00000"
91,"0,00000"
91,"99,85491"
91,648295
91,"0,01586"
91,103
91,"0,12060"
91,783
91,"0,00863"
91,"Basically, the overhead is for most cases around 5 micro seconds. That time"
91,multiplied by millions means seconds or dozens of seconds of overhead. I
91,recommend you to read Clock sources in
91,Linux if you
91,want to dive deeper into the topic.
91,The not-that-good news
91,Let’s go back to our goal of using the execution timing information to see how
91,"we can optimize a query. If profiling overhead is substantial, but it is"
91,"proportional to real execution time, it wouldn’t matter much –as all query"
91,"execution times would be scaled alike, and the slowest node would remain the"
91,slowest node. But the problem is that they aren’t: some nodes suffer
91,"significantly higher overhead, and may appear to be slower than others,"
91,"while it’s not the case in reality. Unfortunately, this means that you cannot"
91,trust EXPLAIN ANALYZE to optimize your queries.
91,It completely depends on the query and its execution nodes. We can use ltrace
91,to count the number of times the clock_gettime is called:
91,sudo ltrace -p $postgres_backend_pid -c -e clock_gettime
91,query
91,clock_gettime calls
91,parallel
91,explain analyze select sum(i1.i * i2.i) from i1 inner join i2 using (i)
91,2004028
91,off
91,select sum(i1.i * i2.i) from i1 inner join i2 using (i)
91,off
91,explain analyze select sum(i2.i) from i2
91,2016
91,off
91,explain analyze select sum(i1.i * i2.i) from i1 inner join i2 using (i)
91,38656
91,"Here are some examples. For the join query, we can observe that the clock is"
91,"called 2N+4M+K times, where N is the number of rows of i1, M the number of"
91,"rows of i2 and K is a constant factor; 28 in this case, 16 in the case"
91,of the summing query (the third one).
91,It is a very interesting the case when parallel mode is activated. Here the time
91,"is reported in blocks to some degree, significantly lowering the number of times"
91,the clock is called.
91,The overhead EXPLAIN ANALYZE introduces is not proportional to the real
91,"duration of the query plan, but rather proportional to the number of rows"
91,"processed by the node. While they may be aligned, more rows processed does not"
91,"always lead to higher execution times, and counting on this assumption may lead to"
91,believing a node is slower when it is in fact faster than another one. In turn
91,leading to a bad query optimization strategy.
91,I have worked deeper on the topic and tried the following:
91,"Take some queries and the number of calls to clock_gettime, as in the"
91,"previous table, and measure the EXPLAIN ANALYZE execution times (without"
91,the additional overhead introduced by ltrace). Then solve the equation with
91,"the clock time as an unknown. However, the results vary significantly from"
91,"query execution to query execution, and are not comparable. I have obtained"
91,results diverging up to one order of magnitude. Not even a linear regression
91,helps here with such disparate results.
91,Try measuring the clock_gettime overhead with the fastest and most advanced
91,perf profiler available:
91,eBPF.
91,"However, even then, BPF’s overhead is higher than that of the"
91,"clock_gettime, making it uneffective."
91,How to become a bit more truthful
91,I guess it’s not easy. The fact that parallel mode appears to call the system
91,"time in blocks, at certain cases, could be a good way to move forward."
91,Another alternative would be to provide a “correction mechanism”. If the clock
91,"time can be measured precisely, and the number of times the clock is called is"
91,"known –Postgres certainly could keep track of it–, its countribution could be"
91,"substracted from the total measured time. While probably not 100% exact, it"
91,would be much better than what it is as of today.
91,Extra thoughts
91,"In reality, EXPLAIN ANALYZE is a query execution profiler. Being aware of"
91,"this, we all know that profilers introduce more or less profiling overhead."
91,"This is the key takeaway: EXPLAIN ANALYZE is a profiler, and its overhead"
91,ranges from high to very/extremely high on systems with slow virtualized clocks.
91,"Why EXPLAIN and EXPLAIN ANALYZE share the same “command”? They are, in"
91,"reality, two very different things: the former gives you the query execution"
91,plan; the latter profiles the query.
91,While the output is similar –but only
91,"that, similar– they do very different things. I’d rename the latter to"
91,PROFILE SELECT....
91,Pretty much the same story with VACUUM and VACUUM FULL. The latter should
91,"be renamed to DEFRAG or REWRITE TABLE –or VACUUM FULL YES I REALLY KNOW WHAT I AM DOING PLEASE DO LOCK MY TABLE, for that matter."
91,Comments
91,Please enable JavaScript to view the comments powered by Disqus.
91,comments powered by Disqus
91,More Posts
91,You may also likethis related content
91,Boost your User-Defined Functions in PostgreSQL
91,Emanuel Calvo
91,Anthony Sotolongo
91,"Feb 5, 2021 ·"
91,9 min read
91,"Introduction Using the RDBMS only to store data is restricting the full potential of the database systems, which were designed for server-side processing and provide other options besides being a data container."
91,Read post
91,63-Node EKS Cluster running on a Single Instance with Firecracker
91,Álvaro Hernández
91,"Jan 13, 2021 ·"
91,7 min read
91,63-Node EKS Cluster running on a Single Instance with Firecracker This blog post is a part of a series of posts devoted to Firecracker automation. Currently it consists of the following posts:
91,Read post
91,"Repository, Tuning Guide and API for your postgresql.conf"
91,Álvaro Hernández
91,"Dec 18, 2020 ·"
91,3 min read
91,"Repository, Tuning Guide and API for your postgresql.conf postgresqlco.nf (aka postgresqlCO.NF, or simply &ldquo;CONF&rdquo;) was born a little bit more than two years ago. CONF&rsquo;s main goal was to help Postgres users find more and easier help to understand and tune their postgresql."
91,Read post
91,Free 1GB Postgres Database on AWS CloudShell
91,Álvaro Hernández
91,"Dec 17, 2020 ·"
91,5 min read
91,"Free 1GB Postgres Database on AWS CloudShell TL;DR AWS CloudShell is a CLI embedded in the AWS Web Console. It is meant to make it easier to run the AWS CLI, SDK and other scripts from your web browser, without having to install anything locally or having to deal with local credential and profiles management."
91,Read post
91,About OnGres
91,"We like open source, we develop open source software, and we are very active and well known at Postgres community. We build very innovative projects in Postgres ecosystem and are the founders of Fundación PostgreSQL."
91,Contact us
91,We are currently working on more awesome stuff
91,Subscribe to our newsletter to be up to date!
91,-None-
91,Newsletter
91,Contact Form
91,Careers
91,StackGres
91,I accept the OnGres Privacy Policy and agree to receive news and promotions every now and then
91,Resources
91,Blog
91,Services
91,Professional Services
91,Consulting
91,Training
91,Products
91,StackGres
91,PostgreSQLCO.NF
91,Company
91,Team
91,Careers
91,Contact
91,//Language
91,English
91,© 2021 OnGres Inc.
91,Cookies Policy
91,Privacy Policy
91,"By continuing to browse the site, you agree to our use of cookies"
92,WAL in PostgreSQL: 4. Setup and Tuning / Postgres Professional corporate blog / Habr
92,How to become an author
92,All streams
92,Development
92,Administrating
92,Design
92,Management
92,Marketing
92,PopSci
92,Log in
92,Sign up
92,135.40
92,Rating
92,Postgres Professional
92,Разработчик СУБД Postgres Pro
92,erogov
92,"April 16, 2020 at 03:05 PM"
92,WAL in PostgreSQL: 4. Setup and Tuning
92,Original author: Egor Rogov
92,"Postgres Professional corporate blog,"
92,"PostgreSQL,"
92,SQL
92,Translation
92,"So, we got acquainted with the structure of the buffer cache and in this context concluded that if all the RAM contents got lost due to failure, the write-ahead log (WAL) was required to recover. The size of the necessary WAL files and the recovery time are limited thanks to the checkpoint performed from time to time."
92,"In the previous articles we already reviewed quite a few important settings that anyway relate to WAL. In this article (being the last in this series) we will discuss problems of WAL setup that are unaddressed yet: WAL levels and their purpose, as well as the reliability and performance of write-ahead logging."
92,WAL levels
92,"The main WAL task is to ensure recovery after a failure. But once we have to maintain the log anyway, we can also adapt it to other tasks by adding some more information to it. There are several logging levels. The wal_level parameter specifies the level, and each next level includes everything that gets into WAL of the preceding level plus something new."
92,Minimal
92,"The minimum possible level is set by the value of wal_level = minimal and ensures only recovery after a failure. To save space, the operations related to bulk data processing (such as CREATE TABLE AS SELECT or CREATE INDEX) are not WAL-logged. Instead, the data needed are immediately written to disk, and a new object is added to the system catalog and becomes visible at the transaction commit. If a failure occurs while the operation is performed, the data that are already written remain invisible and do not violate the consistency rules. And if a failure occurs after completion of the operation, everything needed is already on disk and does not need logging."
92,"Let's take a look. First we'll set the necessary level (to this end, we will also need to change another parameter — max_wal_senders)."
92,=> ALTER SYSTEM SET wal_level = minimal;
92,=> ALTER SYSTEM SET max_wal_senders = 0;
92,student$ sudo pg_ctlcluster 11 main restart
92,Note that the change of the level requires restarting the server.
92,Let's remember the current WAL location:
92,=> SELECT pg_current_wal_insert_lsn();
92,pg_current_wal_insert_lsn
92,---------------------------
92,0/353927BC
92,(1 row)
92,"Now let's perform creation of a table (CREATE TABLE AS SELECT) and remember the WAL location again. The amount of data retrieved by the SELECT operator does not matter at all in this case, so one row is enough."
92,=> CREATE TABLE wallevel AS
92,SELECT 1 AS n;
92,=> SELECT pg_current_wal_insert_lsn();
92,pg_current_wal_insert_lsn
92,---------------------------
92,0/353A7DFC
92,(1 row)
92,Let's look at WAL records using the familiar pg_waldump utility.
92,postgres$ /usr/lib/postgresql/11/bin/pg_waldump -p /var/lib/postgresql/11/main/pg_wal -s 0/353927BC -e 0/353A7DFC
92,"Certainly, some details can differ from one launch to another, but in this case we get the following. The record of the Heap2 manager relates to vacuuming, here it is in-page vacuum of a table from the system catalog (system objects are easily distinguished with a naked eye by a small number in rel):"
92,rmgr: Heap2
92,len (rec/tot):
92,59/
92,"7587, tx:"
92,"0, lsn: 0/353927BC, prev 0/35392788, desc: CLEAN remxid 101126, blkref #0: rel 1663/16386/1247 blk 8 FPW"
92,The record of getting the next OID for the table to be created follows:
92,rmgr: XLOG
92,len (rec/tot):
92,30/
92,"30, tx:"
92,"0, lsn: 0/35394574, prev 0/353927BC, desc: NEXTOID 82295"
92,And this is pure creation of the table:
92,rmgr: Storage
92,len (rec/tot):
92,42/
92,"42, tx:"
92,"0, lsn: 0/35394594, prev 0/35394574, desc: CREATE base/16386/74103"
92,But the insert of data into the table is not WAL-logged. Multiple records follow on row inserts into different tables and indexes — this way PostgreSQL writes the information on the created table to the system catalog (provided in a shorthand form):
92,rmgr: Heap
92,len (rec/tot):
92,203/
92,"203, tx:"
92,"101127, lsn: 0/353945C0, prev 0/35394594, desc: INSERT off 71, blkref #0: rel 1663/16386/1247 blk 8"
92,rmgr: Btree
92,len (rec/tot):
92,53/
92,"685, tx:"
92,"101127, lsn: 0/3539468C, prev 0/353945C0, desc: INSERT_LEAF off 37, blkref #0: rel 1663/16386/2703 blk 2 FPW"
92,...
92,rmgr: Btree
92,len (rec/tot):
92,53/
92,"2393, tx:"
92,"101127, lsn: 0/353A747C, prev 0/353A6788, desc: INSERT_LEAF off 10, blkref #0: rel 1664/0/1233 blk 1 FPW"
92,And finally the transaction commit:
92,rmgr: Transaction len (rec/tot):
92,34/
92,"34, tx:"
92,"101127, lsn: 0/353A7DD8, prev 0/353A747C, desc: COMMIT 2019-07-23 18:59:34.923124 MSK"
92,Replica
92,"When we restore a PostgreSQL instance from backup, we start with some state of the file system and gradually bring the data to the target point of the recovery by playing back the archived WAL records. The number of such records can be pretty large (for example, records for several days), that is, the recovery period will span many checkpoints rather than one. So, it is clear that the minimum logging level is insufficient — if an operation is not logged, we will be unaware of whether we need to redo it. To support restoring from backup, all the operations must be WAL-logged."
92,The same is true for the replication: everything that is not logged will not be sent to the replica and will not be replayed. And a wish to run queries on a replica complicates the situation even more.
92,"First, we need information on exclusive advisory locks that occur on the main server since they can conflict the queries on the replica. Such locks are WAL-logged and then the startup process applies them on the replica."
92,"Second, we need to create data snapshots, and to do this, as we remember, information on the transactions being executed is needed. In the case of a replica, not only local transactions are meant, but also transactions on the main server. The only way to provide this information is to WAL-log it from time to time (this happens once every 15 seconds)."
92,"The level of WAL that ensures both restoring from backup and a possibility of physical replication is set by the value of wal_level = replica. (Before version 9.6, two separate levels were available — archive and hot_standby — but later they were combined.)"
92,It's this level that is used by default starting with PostgreSQL 10 (while earlier it was minimal). So let's just restore the parameters to their default values:
92,=> ALTER SYSTEM RESET wal_level;
92,=> ALTER SYSTEM RESET max_wal_senders;
92,student$ sudo pg_ctlcluster 11 main restart
92,Deleting the table and redoing exactly the same sequence of steps as last time:
92,=> DROP TABLE wallevel;
92,=> SELECT pg_current_wal_insert_lsn();
92,pg_current_wal_insert_lsn
92,---------------------------
92,0/353AF21C
92,(1 row)
92,=> CREATE TABLE wallevel AS
92,SELECT 1 AS n;
92,=> SELECT pg_current_wal_insert_lsn();
92,pg_current_wal_insert_lsn
92,---------------------------
92,0/353BE51C
92,(1 row)
92,Now let's check WAL records.
92,postgres$ /usr/lib/postgresql/11/bin/pg_waldump -p /var/lib/postgresql/11/main/pg_wal -s 0/353AF21C -e 0/353BE51C
92,"Vacuuming, getting the OID, creation of the table and registration in the system catalog — same as before so far:"
92,rmgr: Heap2
92,len (rec/tot):
92,58/
92,"58, tx:"
92,"0, lsn: 0/353AF21C, prev 0/353AF044, desc: CLEAN remxid 101128, blkref #0: rel 1663/16386/1247 blk 8"
92,rmgr: XLOG
92,len (rec/tot):
92,30/
92,"30, tx:"
92,"0, lsn: 0/353AF258, prev 0/353AF21C, desc: NEXTOID 82298"
92,rmgr: Storage
92,len (rec/tot):
92,42/
92,"42, tx:"
92,"0, lsn: 0/353AF278, prev 0/353AF258, desc: CREATE base/16386/74106"
92,rmgr: Heap
92,len (rec/tot):
92,203/
92,"203, tx:"
92,"101129, lsn: 0/353AF2A4, prev 0/353AF278, desc: INSERT off 73, blkref #0: rel 1663/16386/1247 blk 8"
92,rmgr: Btree
92,len (rec/tot):
92,53/
92,"717, tx:"
92,"101129, lsn: 0/353AF370, prev 0/353AF2A4, …"
92,rmgr: Btree
92,len (rec/tot):
92,53/
92,"2413, tx:"
92,"101129, lsn: 0/353BD954, prev 0/353BCC44, desc: INSERT_LEAF off 10, blkref #0: rel 1664/0/1233 blk 1 FPW"
92,"And this is something new. The record of the exclusive lock, related to the Standby manager — here it is the lock on the transaction ID (we will discuss why it is needed in the next series of articles):"
92,rmgr: Standby
92,len (rec/tot):
92,42/
92,"42, tx:"
92,"101129, lsn: 0/353BE2D8, prev 0/353BD954, desc: LOCK xid 101129 db 16386 rel 74106"
92,And this is the record of row inserts in our table (compare the file number rel with the one in the CREATE record):
92,rmgr: Heap
92,len (rec/tot):
92,59/
92,"59, tx:"
92,"101129, lsn: 0/353BE304, prev 0/353BE2D8, desc: INSERT+INIT off 1, blkref #0: rel 1663/16386/74106 blk 0"
92,This is the commit record:
92,rmgr: Transaction len (rec/tot):
92,421/
92,"421, tx:"
92,"101129, lsn: 0/353BE340, prev 0/353BE304, desc: COMMIT 2019-07-23 18:59:37.870333 MSK; inval msgs: catcache 74 catcache 73 catcache 74 catcache 73 catcache 50 catcache 49 catcache 7 catcache 6 catcache 7 catcache 6 catcache 7 catcache 6 catcache 7 catcache 6 catcache 7 catcache 6 catcache 7 catcache 6 catcache 7 catcache 6 snapshot 2608 relcache 74106 snapshot 1214"
92,"And there is one more record, which occurs from time to time and is not tied to the completed transaction, relates to the Standby manager and informs of the transactions being executed at this point in time:"
92,rmgr: Standby
92,len (rec/tot):
92,50/
92,"50, tx:"
92,"0, lsn: 0/353BE4E8, prev 0/353BE340, desc: RUNNING_XACTS nextXid 101130 latestCompletedXid 101129 oldestRunningXid 101130"
92,Logical
92,"And finally, the last level is specified by the value of wal_level = logical and provides for logical decoding and logical replication. It must be turned on for the publishing server."
92,"From the perspective of WAL records, this level is virtually the same as replica: records are added that relate to replication origins, as well as arbitrary logical records that applications can add to WAL. But logical decoding mainly depends on the information on the transactions being executed since it is needed to create the data snapshot to track changes to the system catalog."
92,We will not go into details of backup and replication now since this is a topic for a separate series of articles.
92,Reliability of writing
92,"It's clear that a logging technique must be reliable and ensure recovery whatever the circumstances (certainly, not related to corruption of the data storage media). Many things affect reliability, of which we will discuss caching, corruption of data and atomicity of writing."
92,Caching
92,Multiple caches stand in the way of data to a nonvolatile storage (such as a hard disk drive platter).
92,"If a program (any, but PostgreSQL in this case) asks the operating system (OS) to write something on disk, the OS transfers the data to its RAM cache. Writing actually happens asynchronously, depending on the settings of I/O scheduler of the OS."
92,"When the OS decides to write the data, they get into the cache of the storage (hard disk). Electronics of the storage can also postpone writing, for example, by grouping data that are more efficient to be written together. And if a RAID controller is used, one more caching level is added between the OS and disk."
92,"So, without taking special measures, it is absolutely unclear when the data is actually saved in a reliable manner. And usually it makes no difference, but there are critical areas where PostgreSQL must be sure that the data are written with due reliability. This is, primarily, logging (if a WAL record did not reach disk, it will be lost along with the rest of the RAM contents) and a checkpoint (we must be sure that dirty pages are really written to disk). But there are other situations, such as performing unlogged operations at the level of minimal and so on."
92,"The OS provides capabilities to ensure immediate writing of the data to nonvolatile memory. There are a few options, but they reduce to the two main: either after a write, the synchronization call is performed (fsync, fdatasync) or after opening a file (or writing to it) a special flag is set to indicate a need for synchronization or even for a direct write bypassing the OS cache."
92,"As for WAL, the pg_test_fsync utility allows us to choose a method that best suits a particular OS and a particular file system, and this method is specified in the wal_sync_method parameter. Normal files are synchronized using fsync."
92,"A subtle point is that to choose the method, we need to take into account hardware characteristics. For example: if a controller employing a battery-backup unit (BBU) is used, there is no reason why we should avoid using the cache of the controller since the BBU enables saving the data in case of power outage."
92,The documentation provides a lot of context on this.
92,"In any case, synchronization is expensive and performed at most as often as absolutely necessary (we will get back to this a bit later, when we discuss the performance)."
92,"In general, you can turn off the synchronization (the fsync parameter is responsible for this), but in this case, you have to forget about the reliability of storage. By turning fsync off you agree that the data can be irreversibly lost at any time. Probably, the only reasonable use case for this parameter is a temporary increase of the performance when the data can be easily restored from a different source (like at the initial migration)."
92,Data corruption
92,"Hardware is imperfect and the data can be corrupted in the storage when transmitted over interface cables, and so on. Some of such errors are handled at the hardware level, but the others are not."
92,"For quick detection of an issue, checksums are provided in WAL records."
92,"Data pages can also be protected by checksums. Earlier this could be done only at the cluster initialization, but in PostgreSQL 12 it is possible to turn the checksums on and off by means of the pg_checksums utility (but only when the server is shut down rather than «on the fly» so far)."
92,"In a production environment, checksums must be obligatory turned on regardless of the overhead costs of computing and verifying them. This reduces the probability of not detecting the corruption duly."
92,"It reduces, but not eliminates the probability."
92,"First, checksums are verified only when the page is accessed; therefore, the corruption may escape detection until the moment when it gets into all backups. It's for this reason that pg_probackup verifies the checksums of all the cluster pages during the data backup."
92,"Second, a page filled with zeros is regarded as correct, so if the file system mistakenly «nullifies» a file, this can escape detection."
92,"Third, checksums protect only the main fork of the data. The other forks and the rest of files (for instance, transaction statuses XACT) are not protected at all."
92,Alas.
92,Let's see how it works. First we make sure that checksums are turned on (note that in a package installed on Debian-like systems this is not the case by default):
92,=> SHOW data_checksums;
92,data_checksums
92,----------------
92,(1 row)
92,The data_checksums parameter is read-only.
92,This is the file where our table is located:
92,=> SELECT pg_relation_filepath('wallevel');
92,pg_relation_filepath
92,----------------------
92,base/16386/24890
92,(1 row)
92,"Let's shut down the server and change a few bytes on the zero page, for example: erase LSN of the last WAL record from the header."
92,student$ sudo pg_ctlcluster 11 main stop
92,postgres$ dd if=/dev/zero of=/var/lib/postgresql/11/main/base/16386/24890 oflag=dsync conv=notrunc bs=1 count=8
92,8+0 records in
92,8+0 records out
92,"8 bytes copied, 0,0083022 s, 1,0 kB/s"
92,"In general, it was not needed to shut down the server. It is sufficient that the page is flushed to disk and evicted from the cache (otherwise, the server would continue working with the page in the cache). But this scenario is more complicated to reproduce."
92,Now we start the server and try to read the table.
92,student$ sudo pg_ctlcluster 11 main start
92,=> SELECT * FROM wallevel;
92,WARNING:
92,"page verification failed, calculated checksum 23222 but expected 50884"
92,ERROR:
92,invalid page in block 0 of relation base/16386/24890
92,"But what shall we do if it is impossible to restore the data from backup? The ignore_checksum_failure parameter enables trying to read the table, of course, with a risk of getting corrupted data."
92,=> SET ignore_checksum_failure = on;
92,=> SELECT * FROM wallevel;
92,WARNING:
92,"page verification failed, calculated checksum 23222 but expected 50884"
92,---
92,(1 row)
92,"Of course, everything is fine in this case since we hurt only the header rather than pure data."
92,"And there is one more point to note. When checksums are turned on, hint bits are WAL-logged (we discussed them earlier) since a change to any, even inessential, bit results in a change to the checksum. When checksums are turned off, the wal_log_hints parameter is responsible for WAL-logging hint bits."
92,"Changes to hint bits are always logged as FPI (full page image), which pretty much increases the WAL size. In this case, it makes sense to use the wal_compression parameter to turn on compression of FPIs (this parameter was added in version 9.5). We will look at specific figures a bit later."
92,Atomicity of writing
92,"And finally, there is an issue with the atomicity of writing. A database page occupies not less than 8 KB (it may be 16 or 32 KB), and at a low level, a write is done in blocks, which usually have smaller sizes (usually 512 bytes or 4 KB). Therefore, in case of a power outage, a data page can be written partially. It's clear that during a recovery, it makes no sense to apply usual WAL records to such a page."
92,"To prevent this, PostgreSQL enables WAL-logging a full page image at the first change of the page since the beginning of a checkpoint cycle (the same image is also logged when hint bits change). The full_page_writes parameter controls this, and it is turned on by default."
92,"If a recovery process comes across an FPI in WAL, it writes the image to disk unconditionally (without LSN checking): the FPI is more trustworthy since it is protected by a checksum, like each WAL record. And it is this reliably correct image, which normal WAL records are applied to."
92,"Although in PostgreSQL, an FPI does not include free space (we discussed the block structure earlier), FPIs considerably increase the amount of WAL records generated. As already mentioned, the situation can be improved by compression of FPIs (using the wal_compression parameter)."
92,"To get an insight into changing the WAL size, let's conduct a simple experiment using the pgbench utility. Performing the initialization:"
92,student$ pgbench -i test
92,dropping old tables...
92,creating tables...
92,generating data...
92,"100000 of 100000 tuples (100%) done (elapsed 0.15 s, remaining 0.00 s)"
92,vacuuming...
92,creating primary keys...
92,done.
92,The full_page_writes parameter is turned on:
92,=> SHOW full_page_writes;
92,full_page_writes
92,------------------
92,(1 row)
92,Let's perform a checkpoint and immediately run a test for 30 seconds.
92,=> CHECKPOINT;
92,=> SELECT pg_current_wal_insert_lsn();
92,pg_current_wal_insert_lsn
92,---------------------------
92,0/38E04A08
92,(1 row)
92,student$ pgbench -T 30 test
92,starting vacuum...end.
92,transaction type: TPC-B (sort of)
92,scaling factor: 1
92,query mode: simple
92,number of clients: 1
92,number of threads: 1
92,duration: 30 s
92,number of transactions actually processed: 26851
92,latency average = 1.117 ms
92,tps = 895.006720 (including connections establishing)
92,tps = 895.095229 (excluding connections establishing)
92,=> SELECT pg_current_wal_insert_lsn();
92,pg_current_wal_insert_lsn
92,---------------------------
92,0/3A69C478
92,(1 row)
92,Getting the size of WAL records:
92,=> SELECT pg_size_pretty('0/3A69C478'::pg_lsn - '0/38E04A08'::pg_lsn);
92,pg_size_pretty
92,----------------
92,25 MB
92,(1 row)
92,Now let's turn off the full_page_writes parameter:
92,=> ALTER SYSTEM SET full_page_writes = off;
92,=> SELECT pg_reload_conf();
92,And we repeat the experiment.
92,=> CHECKPOINT;
92,=> SELECT pg_current_wal_insert_lsn();
92,pg_current_wal_insert_lsn
92,---------------------------
92,0/3A69C530
92,(1 row)
92,student$ pgbench -T 30 test
92,starting vacuum...end.
92,transaction type: TPC-B (sort of)
92,scaling factor: 1
92,query mode: simple
92,number of clients: 1
92,number of threads: 1
92,duration: 30 s
92,number of transactions actually processed: 27234
92,latency average = 1.102 ms
92,tps = 907.783080 (including connections establishing)
92,tps = 907.895326 (excluding connections establishing)
92,=> SELECT pg_current_wal_insert_lsn();
92,pg_current_wal_insert_lsn
92,---------------------------
92,0/3BE87658
92,(1 row)
92,Getting the size of WAL records:
92,=> SELECT pg_size_pretty('0/3BE87658'::pg_lsn - '0/3A69C530'::pg_lsn);
92,pg_size_pretty
92,----------------
92,24 MB
92,(1 row)
92,"Yes, the size decreased, but not so much as we could expect."
92,"The thing is that the cluster was initialized with the checksums on data pages and therefore, FPIs have to be WAL-logged anyway when hint bits change. These data (in the situation above) make up about half of the whole amount, which you can make sure of by looking at the statistics:"
92,postgres$ /usr/lib/postgresql/11/bin/pg_waldump --stats -p /var/lib/postgresql/11/main/pg_wal -s 0/3A69C530 -e 0/3BE87658
92,Type
92,(%)
92,Record size
92,(%)
92,FPI size
92,(%)
92,----
92,---
92,-----------
92,---
92,--------
92,---
92,XLOG
92,1721 (
92,"1,03)"
92,84329 (
92,"0,77)"
92,"13916104 (100,00)"
92,Transaction
92,"27235 ( 16,32)"
92,926070 (
92,"8,46)"
92,0 (
92,"0,00)"
92,Storage
92,1 (
92,"0,00)"
92,42 (
92,"0,00)"
92,0 (
92,"0,00)"
92,CLOG
92,1 (
92,"0,00)"
92,30 (
92,"0,00)"
92,0 (
92,"0,00)"
92,Standby
92,4 (
92,"0,00)"
92,240 (
92,"0,00)"
92,0 (
92,"0,00)"
92,Heap2
92,"27522 ( 16,49)"
92,"1726352 ( 15,76)"
92,0 (
92,"0,00)"
92,Heap
92,"109691 ( 65,71)"
92,"8169121 ( 74,59)"
92,0 (
92,"0,00)"
92,Btree
92,756 (
92,"0,45)"
92,45380 (
92,"0,41)"
92,0 (
92,"0,00)"
92,--------
92,--------
92,--------
92,Total
92,166931
92,"10951564 [44,04%]"
92,"13916104 [55,96%]"
92,Zero rows are removed to make the table more compact. Pay attention to the summary row (Total) and compare the size of full images (FPI size) with he size of normal records (Record size).
92,"The full_page_writes parameter can be turned off only if the file system and hardware themselves ensure the automicity of writes. But, as we can see, there isn't much sense in it (provided checksums are turned on)."
92,Now let's see how compression can help.
92,=> ALTER SYSTEM SET full_page_writes = on;
92,=> ALTER SYSTEM SET wal_compression = on;
92,=> SELECT pg_reload_conf();
92,Repeating the same experiment.
92,=> CHECKPOINT;
92,=> SELECT pg_current_wal_insert_lsn();
92,pg_current_wal_insert_lsn
92,---------------------------
92,0/3BE87710
92,(1 row)
92,student$ pgbench -T 30 test
92,starting vacuum...end.
92,transaction type: TPC-B (sort of)
92,scaling factor: 1
92,query mode: simple
92,number of clients: 1
92,number of threads: 1
92,duration: 30 s
92,number of transactions actually processed: 26833
92,latency average = 1.118 ms
92,tps = 894.405027 (including connections establishing)
92,tps = 894.516845 (excluding connections establishing)
92,=> SELECT pg_current_wal_insert_lsn();
92,pg_current_wal_insert_lsn
92,---------------------------
92,0/3CBD3EA8
92,(1 row)
92,Getting the size of WAL records:
92,=> SELECT pg_size_pretty('0/3CBD3EA8'::pg_lsn - '0/3BE87710'::pg_lsn);
92,pg_size_pretty
92,----------------
92,13 MB
92,(1 row)
92,"Conclusion: if there are many FPIs (due to checksums or full_page_writes, that is, almost always), most likely it makes sense to use compression although it loads the processor."
92,Performance
92,"During regular work of a server, WAL files are continuously written one by one. Because of no random access, even HDD disks do the job fine. But this kind of load is pretty much different from the one when data files are accessed."
92,"So, it is usually beneficial to store WAL on a separate physical disk (or disk array) mounted to the file system of the server. Instead of the $PGDATA/pg_wal directory, a symbolic link to the appropriate directory must be created."
92,"There are a couple of situations where WAL files need to be not only written but read. The first one is a clear case of recovery after a failure. The second one is less trivial. It occurs if stream replication is used and a replica is late to receive WAL records while they are still in the OS buffers of the main server. In this case, the walsender process has to read the necessary data from disk. We will discuss this in more detail when we reach replication."
92,WAL is written in either of the two modes:
92,"Synchronous — at a transaction commit, the work cannot be continued until all WAL records of this transaction get on disk."
92,"Asynchronous — a transaction completes immediately, and WAL is written in the background."
92,"The synchronous_commit parameter, turned on by default, sets the synchronous mode."
92,"Because synchronization is connected with the actual (that is, slow) input/output, it is beneficial to do it as infrequently as possible. To this end, a backend process that completes a transaction and writes WAL makes a short pause, defined by the commit_delay parameter. But this happens only if the system has not less than commit_siblings active transactions. This behavior relies on the expectation that during the waiting time some transactions will be completed and it will be possible to synchronize them in one go. This is similar to how you hold the doors of an elevator so that someone has time to jump into the car."
92,"By default, commit_siblings = 5 and commit_delay = 0, so actually there is no wait. It makes sense to change the value of commit_delay only for systems that execute a great number of OLTP transactions."
92,Then the process flushes part of WAL up to the LSN needed (or a little more if during the waiting time new records were added). After that the transaction is considered completed.
92,"Synchronous writing ensures durability (the letter D in the ACID acronym): if a transaction is committed, all its WAL records are already on disk and won't be lost. But a drawback is that synchronous writing increases the response time (the COMMIT command does not return control until the end of the synchronization) and reduces the system performance."
92,You can make writing asynchronous by setting synchronous_commit = off (or local).
92,"When writing is asynchronous, WAL records are flushed by the wal writer process, which alternates work and waits (the waiting time is specified by the wal_writer_delay parameter with the default value of 200 ms)."
92,"When the process wakes up after a wait, it checks whether completely filled WAL pages appeared since last time. If they did appear, the process ignores the current page, not filled to the end, and writes only completely filled pages. (However, not all at once: writing stops when it reaches the end of the cache and proceeds from the beginning of the cache next time.)"
92,"But if none of the pages are filled, the process writes the current WAL page (not filled to the end) — otherwise, what did it wake up for?"
92,"This algorithm aims to avoid synchronization of the same page several times wherever possible, which is critical for a large stream of updates."
92,"Asynchronous writing is more efficient than the synchronous since commits of the changes do not wait for writes of WAL pages. But the reliability decreases: committed data can be lost in case of failure if less than 3 × wal_writer_delay units of time elapsed between the commit and failure (with the default settings, this is a little longer than half a second)."
92,Not an easy choice between efficiency and reliability is up to the system administrator.
92,"Note that: unlike turning off synchronization (fsync = off), asynchronous mode does not make recovery impossible. In case of failure, the system will restore the consistent state, but maybe, some of the last transactions will not be present there."
92,"You can set the synchronous_commit parameter for separate transactions. This enables increasing the performance by sacrificing the reliability only of some transactions. Say, financial transactions must be committed synchronously, while chat messages can be compromised."
92,"Actually both modes work together. Even with a synchronous commit, WAL records of a long transaction will be written asynchronously in order to free WAL buffers. And if during a flush of a page from the buffer cache it appears that the corresponding WAL record is not on disk yet, it will be immediately flushed in the synchronous mode."
92,"To get an insight into the gains of an asynchronous commit, let's try to repeat the pgbench test in this mode."
92,=> ALTER SYSTEM SET synchronous_commit = off;
92,=> SELECT pg_reload_conf();
92,student$ pgbench -T 30 test
92,starting vacuum...end.
92,transaction type: TPC-B (sort of)
92,scaling factor: 1
92,query mode: simple
92,number of clients: 1
92,number of threads: 1
92,duration: 30 s
92,number of transactions actually processed: 45439
92,latency average = 0.660 ms
92,tps = 1514.561710 (including connections establishing)
92,tps = 1514.710558 (excluding connections establishing)
92,"With synchronous commits, we got approximately 900 transactions per second (tps) and 1500 tps with asynchronous commits. It goes without saying that in a real-life system under the actual load, the proportion will be different, but it is clear that for short transactions the effect can be pretty considerable."
92,"Here the series of articles on WAL logging comes to an end. If anything critical is unaddressed, I would appreciate if you provide comments. Thank you all!"
92,"And next, amazing adventures are awaiting us in the world of locks, but that's another story."
92,Tags:
92,postgresql
92,wal
92,write-ahead log
92,Add tags
92,Hubs:
92,Postgres Professional corporate blog
92,PostgreSQL
92,SQL
92,Specify the reason of the downvote so the author could improve the post
92,Send anonymously
92,Mark this post with your tags
92,"You should divide tags with commas. E.g.: programming, algorithms"
92,Save
92,3.1k
92,Comment
92,Share
92,Copy link
92,Facebook
92,Twitter
92,Telegram
92,Pocket
92,Violation
92,Describe nature of the violation
92,Send
92,Postgres Professional
92,Разработчик СУБД Postgres Pro
92,140.5
92,Karma
92,0.0
92,Rating
92,Егор Рогов erogov
92,Пользователь
92,Payment system
92,Facebook
92,Twitter
92,Google+
92,LiveJournal
92,Similar posts
92,"April 3, 2020 at 02:37 PM"
92,WAL in PostgreSQL: 3. Checkpoint
92,2.7k
92,"March 26, 2020 at 05:50 PM"
92,WAL in PostgreSQL: 2. Write-Ahead Log
92,2.3k
92,"March 10, 2020 at 02:12 PM"
92,WAL in PostgreSQL: 1. Buffer Cache
92,3.4k
92,Comments
92,"Only users with full accounts can post comments. Log in, please."
92,Information
92,Foundation date
92,"January 27, 2015"
92,Website
92,postgrespro.ru
92,Number of employees
92,51–100 employees
92,Registration date
92,"September 30, 2015"
92,Representative
92,Иван Панченко
92,Links
92,Конференция разработчиков и пользователей PostgreSQL PGConf.Russia
92,pgconf.ru
92,Blog on Habr
92,Locks in PostgreSQL: 4. Locks in memory
92,2.9k
92,Locks in PostgreSQL: 3. Other locks
92,3.9k
92,Locks in PostgreSQL: 2. Row-level locks
92,3.8k
92,Locks in PostgreSQL: 1. Relation-level locks
92,3.1k
92,Parallelism in PostgreSQL: treatment of trees and conscience
92,1.7k
92,JSONPath in PostgreSQL: committing patches and selecting apartments
92,3.6k
92,What is Baked in the Baker's Dozen?
92,916
92,WAL in PostgreSQL: 4. Setup and Tuning
92,3.1k
92,WAL in PostgreSQL: 3. Checkpoint
92,2.7k
92,WAL in PostgreSQL: 2. Write-Ahead Log
92,2.3k
92,WAL in PostgreSQL: 1. Buffer Cache
92,3.4k
92,On recursive queries
92,5.6k
92,MVCC in PostgreSQL-8. Freezing
92,1.8k
92,MVCC in PostgreSQL-7. Autovacuum
92,1.1k
92,MVCC in PostgreSQL-6. Vacuum
92,1.4k
92,MVCC in PostgreSQL-5. In-page vacuum and HOT updates
92,1.6k
92,MVCC in PostgreSQL-4. Snapshots
92,2.6k
92,MVCC in PostgreSQL-3. Row Versions
92,2.9k
92,"MVCC in PostgreSQL-2. Forks, files, pages"
92,2.6k
92,MVCC in PostgreSQL-1. Isolation
92,4.9k
92,Top posts
92,Day
92,Week
92,Month
92,Various things in MetaPost
92,&plus14
92,8.4k
92,Compilation of math functions into Linq.Expression
92,&plus4
92,2.6k
92,"Audio over Bluetooth: most detailed information about profiles, codecs, and devices"
92,&plus22
92,175k
92,Algorithms in Go: Bit Manipulation
92,&plus2
92,294
92,11 Kubernetes implementation mistakes – and how to avoid them
92,&plus10
92,934
92,Go Quiz
92,&plus3
92,679
92,"Pitfalls in String Pool, or Another Reason to Think Twice Before Interning Instances of String Class in C#"
92,&plus1
92,327
92,Build (CI/CD) of non-JVM projects using gradle/kotlin
92,&plus1
92,315
92,Multiple violations of policies in RMS open letter
92,&plus16
92,2.7k
92,Compilation of math functions into Linq.Expression
92,&plus4
92,2.6k
92,"High-Quality Text-to-Speech Made Accessible, Simple and Fast"
92,&plus5
92,1.6k
92,How to Start Reverse Engineering in 2021
92,&plus3
92,1.6k
92,Your account
92,Log in
92,Sign up
92,Sections
92,Posts
92,Hubs
92,Companies
92,Users
92,Sandbox
92,Info
92,How it works
92,For Authors
92,For Companies
92,Documents
92,Agreement
92,Terms of service
92,Services
92,Ads
92,Subscription plans
92,Content
92,Seminars
92,Megaprojects
92,© 2006 – 2021 «Habr»
92,Language settings
92,About
92,Support
92,Mobile version
92,Language settings
92,Interface
92,Русский
92,English
92,Content
92,Russian
92,English
92,Save settings
93,Troubleshoot Performance Issues Using Flight Recorder
93,Previous
93,Next
93,JavaScript must be enabled to correctly display this content
93,Troubleshooting Guide
93,General Java Troubleshooting
93,Troubleshoot Performance Issues Using Flight Recorder
93,4 Troubleshoot Performance Issues Using Flight Recorder
93,Identify performance issues with a Java application and debug these issues using flight recordings.
93,"To learn more about creating a recording with Flight Recorder in JMC, see Use JMC to Start a Flight Recording."
93,The data provided by Flight Recorder helps you investigate performance issues. No other tool gives as much profiling data without skewing the results with its own performance overhead. This chapter provides information about performance issues that you can identify and debug using data from Flight Recorder.
93,This chapter contains the following sections:
93,Flight Recorder Overhead
93,Find Bottlenecks
93,Garbage Collection Performance
93,Synchronization Performance
93,I/O Performance
93,Code Execution Performance
93,Flight Recorder Overhead
93,"When you measure performance, it is important to consider any performance overhead added by Flight Recorder. The overhead will differ depending on the application. If you have any performance tests set up, you can measure if there is any noticeable overhead on your application."
93,The overhead for recording a standard time fixed recording (profiling recording) using the default settings is less than two percent for most applications. Running with a standard continuous recording generally has no measurable performance effect.
93,"Using Heap Statistics event, which is disabled by default, can cause significant performance overhead. This is because enabling Heap Statistics triggers an old garbage collection at the beginning and the at end of the test run. These old GCs give some extra pause times to the application, so if you are measuring latency or if your environment is sensitive to pause times, do not run with Heap Statistics enabled. Heap Statistics are useful when debugging memory leaks or when investigating the live set of the application. For more information, see Use JDK Mission Control to Debug Memory Leak."
93,"Note:For performance profiling use cases, heap statistics may not be necessary."
93,Find Bottlenecks
93,"Different applications have different bottlenecks. Waiting for I/O or networking, synchronization between threads, CPU usage or garbage collection times can cause bottlenecks in an application. It is possible that an application has more than one bottleneck."
93,Topics:
93,Use JDK Mission Control to Find Bottlenecks
93,Use the jfr Tool to Find Bottlenecks
93,Use JDK Mission Control to Find Bottlenecks
93,You can use JMC to find application bottlenecks.
93,One way to find out the application bottlenecks is to analyze the Automated Analysis Results
93,page. This page provides comprehensive automatic analysis of flight recording data.
93,Open the Threads page in the Java Application page. The Threads page contains the following information:
93,A graph that plots live thread usage by the application over time.
93,A table with all live threads used by the application.
93,Stack traces for selected threads.
93,"Here is a sample figure of a recording, which shows a graph with thread details."
93,"Figure 4-1 Bottlenecks - Threads - GraphDescription of ""Figure 4-1 Bottlenecks - Threads - Graph"""
93,"In the graph, each row is a thread, and each thread can have several lines. In the figure, each thread has a line, which represents the Java Application events that were enabled for this recording. The selected Java Application events all have the important property that they are all thread-stalling events. Thread stalling indicates that the thread was not running your application during the event, and they are all duration events. The duration event measures the duration the application was not running."
93,"In the graph, each color represents a different type of event. For example:"
93,"Yellow represents Java Monitor Wait events. The yellow part is when threads are waiting for an object. This often means that the thread is idle, perhaps waiting for a task."
93,"Salmon represents the Java Monitor Blocked events or synchronization events. If your Java application's important threads spend a lot of time being blocked, then that means that a critical section of the application is single threaded, which is a bottleneck."
93,"Red represents the Socket Reads and Socket Writes events. Again, if the Java application spends a lot of time waiting for sockets, then the main bottleneck may be in the network or with the other machines that the application communicates."
93,"Green represents parts that don't have any events. This part means that the thread is not sleeping, waiting, reading to or from a socket, or not being blocked. In general, this is where the application code is run. If your Java application's important threads are spending a lot of time without generating any application events, then the bottleneck in the application is the time spent executing code or the CPU itself."
93,"Note:For most Java Application event types, only events longer than 20 ms are recorded. (This threshold can be modified when starting the flight recording.) The areas may not have recorded events because the application is doing a lot of short tasks, such as writing to a file (a small part at a time) or spending time in synchronization for very short amounts of time."
93,"The Automated Analysis Results page also shows information about garbage collections. To see if garbage collections may be a bottleneck, see the next topic about garbage collection performance."
93,Use the jfr Tool to Find Bottlenecks
93,"Different applications have different bottlenecks. For some applications, a bottleneck may be waiting for I/O or networking, it may be synchronization between threads, or it may be actual CPU usage. For others, a bottleneck may be garbage collection times. It is possible that an application has more than one bottleneck."
93,One way to find the application bottlenecks is to look at the following events in your flight recording. Make sure that all of these events are enabled in the recording template that you are using:
93,jdk.FileRead
93,jdk.FileWrite
93,jdk.SocketRead
93,jdk.SocketWrite
93,jdk.JavaErrorThrow
93,jdk.JavaExceptionThrow
93,jdk.JavaMonitorEnter
93,jdk.JavaMonitorWait
93,jdk.ThreadStart
93,jdk.ThreadEnd
93,jdk.ThreadSleep
93,jdk.ThreadPark
93,"The selected Java Application events all have the important property that they are all thread-stalling events. Thread stalling indicates that the thread was not running your application during the event, and they are all duration events. The duration event measures the duration the application was not running."
93,Use the jfr tool to print the events that were recorded and look for the following information:
93,jdk.JavaMonitorWait events show how much time a thread spends waiting for a monitor.
93,jdk.ThreadSleep and jdk.ThreadPark events show when a thread is sleeping or parked.
93,Read and write events show how much time is spent in I/O.
93,"If your Java application's important threads spend a lot of time being blocked, then that means that a critical section of the application is single threaded, which is a bottleneck. If the Java application spends a lot of time waiting for sockets, then the main bottleneck may be in the network or with the other machines that the application communicates with. If your Java application's important threads are spending a lot of time without generating any application events, then the bottleneck in the application is the time spent executing code or the CPU itself. Each of these bottlenecks can be further investigated within the flight recording."
93,"Note:For most Java Application event types, only events longer than 20 ms are recorded. (This threshold can be modified when starting the flight recording.) To summarize, the areas may not have recorded events because the application is doing a lot of short tasks, such as writing to a file (a small part at a time) or spending time in synchronization for very short amounts of time."
93,Garbage Collection Performance
93,Flight recordings can help you diagnose garbage collection issues in Java application.
93,Topics:
93,Use JDK Mission Control to Debug Garbage Collection Issues
93,Use the jfr Tool to Debug Garbage Collection Issues
93,Use JDK Mission Control to Debug Garbage Collection Issues
93,You can use JMC to debug garbage collections (GC) issues.
93,Tuning the HotSpot Garbage Collector can have a big effect on performance. See Garbage Collection Tuning Guide for general information.
93,"Take a profiling flight recording of your running application. Do not include the heap statistics, as that will trigger additional old garbage collections. To get a good sample, take a longer recording, for example one hour."
93,"Open the recording in JMC. Look at the Garbage Collections section in the Automated Analysis Results page. Here is a sample figure of a recording, which provides a snapshot of garbage collection performance during runtime."
93,"Figure 4-2 Automated Analysis Results - Garbage Collections Description of ""Figure 4-2 Automated Analysis Results - Garbage Collections """
93,You can observe from the figure that there is a Full GC event. This is indicative of the fact that application needs more memory than what you have allocated.
93,"For further analysis, open the Garbage Collections page under the JVM Internals page to investigate the overall performance impact of the GC. Here is a sample figure of a recording, which shows a graph with GC pauses."
93,"Figure 4-3 Garbage Collection Performance - GC PausesDescription of ""Figure 4-3 Garbage Collection Performance - GC Pauses"""
93,"From the graph look at the Sum of Pauses from the recording. The Sum of Pauses is the total amount of time that the application was paused during a GC. Many GCs do most of their work in the background. In those cases, the length of the GC does not matter and what matters is how long the application actually had to stop. Therefore, the Sum of Pauses is a good measure for the GC effect."
93,"The main performance problems with garbage collections are usually either that individual GCs take too long, or that too much time is spent in paused GCs (total GC pauses)."
93,"When an individual GC takes too long, you may need to change the GC strategy. Different GCs have different trade-offs when it comes to pause times verses throughput performance. See Behavior-Based Tuning."
93,"For example, you may also need to fix your application so that it makes less use of finalizers or semireferences."
93,"If the application spends too much time paused, you can look into different ways to overcome this. One way is to increase the Java heap size. Look at the GC Configuration page to estimate the heap size used by the application, and change the initial heap size and maximum heap size to a higher value. The bigger the heap, the longer time it is between GCs. Watch out for any memory leaks in the Java application, because that may cause more frequent GCs until an OutOfMemoryError is thrown. For more information, see Use JDK Mission Control to Debug Memory Leak. Another way to reduce the GC cycles is to allocate fewer temporary objects. In the TLAB Allocations page, look at how much memory is allocated over the course of the recording. Small objects are allocated inside TLABs, and large objects are allocated outside TLABs. Often, the majority of allocations happen inside TLABs. Lastly, to reduce the need of GCs, decrease the allocation rate. Select the TLAB Allocations page and then look at the allocation sites that have the most memory pressure. You can either view it per class or thread to see which one consumes the most allocation."
93,"Some other settings may also increase GC performance of the Java application. See Garbage Collection Tuning Guide in the Java Platform, Standard Edition HotSpot Virtual Machine Garbage Collection Tuning Guide to discuss GC performance."
93,Use the jfr Tool to Debug Garbage Collection Issues
93,Recordings from Flight Recorder can help diagnose Java application issues with garbage collections.
93,"Tuning the HotSpot Garbage Collector can have a big effect on performance. See Introduction to Garbage Collection Tuning in the Java Platform, Standard Edition HotSpot Virtual Machine Garbage Collection Tuning Guide for information."
93,"To investigate garbage collection issues, take a profiling flight recording of your application while it is running. Do not include the heap statistics, because that triggers extra old collections. To get a good sample, take a longer recording, for example, 1 hour."
93,Use the jfr tool to print the jdk.GCPhasePause events that were recorded. The following example shows the information contained in the event:
93,c:\Program Files\Java\jdk-15\bin>jfr print --events jdk.GCPhasePause \
93,gctest.jfr
93,jdk.GCPhasePause {
93,startTime = 11:19:13.779
93,duration = 3.419 ms
93,gcId = 1
93,"name = ""GC Pause"""
93,"eventThread = ""VM Thread"" (osThreadId = 17528)"
93,"}Using the information from the jdk.GCPhasePause events, you can calculate the average sum of pauses for each GC, the maximum sum of pauses, and the total pause time. The sum of pauses is the total amount of time that the application was paused during a GC. Many GCs do most of their work in the background. In those cases, the length of the GC does not matter and what matters is how long the application actually had to stop. Therefore, the sum of pauses is a good measure for the GC effect."
93,"The main performance problems with garbage collections are usually either that individual GCs take too long, or that too much time is spent in paused GCs (total GC pauses)."
93,"When an individual GC takes too long, you may need to change the GC strategy. Different GCs have different trade-offs when it comes to pause times verses throughput performance. For example, you may also need to fix your application so that it makes less use of finalizers or semireferences. See Behavior-Based Tuning in the Java Platform, Standard Edition HotSpot Virtual Machine Garbage Collection Tuning Guide."
93,"When the application spends too much time paused, there are different ways to work around that:"
93,"Increase the Java heap size. The bigger the Java heap, the longer time it is between GCs. Watch out for any memory leaks in the Java application, because that may cause more and more frequent GCs until an OutOfMemoryError is thrown. For more information, see Use JDK Mission Control to Debug Memory Leak."
93,"To reduce the number of GCs, allocate fewer temporary objects. Small objects are allocated inside TLABs, and large objects are allocated outside TLABs. Often, the majority of allocations happen inside TLABs. The jdk.ObjectAllocationInNewTLAB and jdk.ObjectAllocationOutsideTLAB events provide information about the allocation of temporary objects."
93,"To reduce the need of GCs, decrease the allocation rate. The jdk.ThreadAllocationStatistics event provides information about the allocations per thread."
93,"Some other settings may also increase GC performance of the Java application. See Garbage-First Garbage Collection in the Java Platform, Standard Edition HotSpot Virtual Machine Garbage Collection Tuning Guide for more information about GC performance."
93,Synchronization Performance
93,Java applications encounter synchronization issues when the application threads spend a lot of time waiting to enter a monitor.
93,Topics:
93,Use JDK Mission Control to Debug Synchronization Issues
93,Use jdk.JavaMonitorWait Events to Debug Synchronization Issues
93,Use JDK Mission Control to Debug Synchronization Issues
93,You can use JMC to debug Java Application synchronization issues.
93,"Open the flight recording in JMC and look at the Automated Analysis Results page. Here is a sample figure of a recording, which shows threads that are blocked on locks."
93,"Figure 4-4 Synchronization Issue - Automated Analysis Results PageDescription of ""Figure 4-4 Synchronization Issue - Automated Analysis Results Page"""
93,"Focus on the Lock Instances section of the page, which is highlighted in red. This is indicative of a potential problem. You can observe that there are threads that are blocked on locks."
93,"For further analysis, open the Lock Instances"
93,"page. Here is a sample figure of a recording, which shows the thread that is blocked on locks the most and the stack trace of the thread waiting to acquire the lock."
93,"Figure 4-5 Synchronization Issue - Lock InstanceDescription of ""Figure 4-5 Synchronization Issue - Lock Instance"""
93,"You can notice that threads in the application were blocked on locks for a total time of 3 hours. The most common monitor class in contention was Logger, which was blocked 2972 times."
93,"Typically, logging is an area that can be a bottleneck in applications. In this scenario, the blocking events all seem to be due to calls to the log method. You can review and make required code changes to fix this issue."
93,Use jdk.JavaMonitorWait Events to Debug Synchronization Issues
93,"To debug Java Application synchronization issues, which is where the application threads spend a lot of time waiting to enter a monitor, look at the jdk.JavaMonitorWait events in a recording from Flight Recorder."
93,"Look at the locks that are contended the most and the stack trace of the threads waiting to acquire the lock. Typically, look for contention that you did not think would be an issue. Logging is a common area that can be an unexpected bottleneck in some applications."
93,"When you see performance degradation after a program update or at any specific times in the Java application, take a flight recording when things are good, and take another one when things are bad to look for a synchronization site that increases a lot."
93,"Note:By default, contention events with a duration longer than 20 ms are recorded. This threshold can be modified when starting the flight recording. Shorter thresholds give more events and also potentially more overhead. If you believe contention is an issue, then you could take a shorter recording with a very low threshold of only a few milliseconds. When this is done on a live application, make sure to start with a very short recording, and monitor the performance overhead."
93,I/O Performance
93,Topics:
93,Use JDK Mission Control to Debug I/O Issues
93,Use the Socket Read and Write Events to Debug I/O Issues
93,Use JDK Mission Control to Debug I/O Issues
93,You can diagnose I/O issues in an application by monitoring the Socket I/O
93,or the
93,File I/O pages in JMC.
93,"When a Java application spends a lot of time either in Socket Read, Socket Write, File Read, or File Write, then I/O or networking can cause bottleneck. To diagnose I/O issues in applications, open the Socket I/O page under the Java Application page in the Automated Analysis Results page. Here is a sample figure of a recording, which shows Socket I/O details."
93,"Figure 4-6 Socket I/O - Java ApplicationDescription of ""Figure 4-6 Socket I/O - Java Application"""
93,The figure shows that for the application the longest recorded socket write took 349.745 ms to write 81 B to the host.
93,"File or networking I/O issues are diagnosed in a similar fashion. Look at the files read from or written to the most, then see each file read/write and the time spent on I/O."
93,"By default, the Socket I/O page lists events with a duration longer than 10 ms. When starting a flight recording, you can lower the File I/O Threshold or Socket I/O Threshold to gather more data, but this could potentially have a higher performance overhead."
93,Use the Socket Read and Write Events to Debug I/O Issues
93,"When a Java application spends a lot of time reading or writing sockets or files, then I/O or networking may be the bottleneck. Recordings from Flight Recorder can help identify problem areas."
93,"To diagnose I/O issues in applications, look at the following events in your flight recording. Make sure that all of these events are enabled in the recording template that you are using:"
93,jdk.SocketWrite
93,jdk.SocketRead
93,jdk.FileWrite
93,jdk.FileRead
93,"Use the socket read and write information in your flight recording to calculate the number of reads from a specific remote address, the total number of bytes read, and the total time spent waiting. Look at each event to analyze the time spent and data read."
93,"File or networking I/O issues are diagnosed in a similar fashion. Look at the files read to or written to the most, then see each file read/write and the time spent on I/O."
93,"By default, only events with a duration longer than 20 ms are recorded. When starting a flight recording, you can lower the file I/O threshold or the socket I/O threshold to gather more data, potentially with a higher performance effect."
93,Code Execution Performance
93,Topics:
93,Use JDK Mission Control to Monitor Code Execution Performance
93,Use jdk.CPULoad and jdk.ThreadCPULoad Events to Monitor Code Execution Performance
93,Use JDK Mission Control to Monitor Code Execution Performance
93,You can use JMC to monitor the code execution performance.
93,"When there are not a lot of Java Application events, it could be that the main bottleneck of your application is the running code. In such scenarios, look at the Method Profiling section of the Automated Analysis Results page. Here is a sample figure of a recording, which indicates that there is value in optimizing certain methods."
93,"Figure 4-7 Code Execution Performance - Automated Analysis Results PageDescription of ""Figure 4-7 Code Execution Performance - Automated Analysis Results Page"""
93,"Now, open the Java Application page. Here is a sample figure of a recording, which shows the Method Profiling graph and the stack traces."
93,"Figure 4-8 Code Execution Performance - Java ApplicationDescription of ""Figure 4-8 Code Execution Performance - Java Application"""
93,"You can observe that the stack trace view shows the aggregated stack traces of any selection in the editor and also the stack traces for the profiling samples. In the figure, you can notice that one of these methods has a lot more samples than the others. This means that the JVM has spent more time executing that method relative to the other methods."
93,"To identify which method would be the one to optimize to improve the performance of the application, open the Method Profiling page. Here is a sample figure of a recording, which shows the method that needs to be optimized."
93,"Figure 4-9 Code Execution Performance - Method ProfilingDescription of ""Figure 4-9 Code Execution Performance - Method Profiling"""
93,"As you can observe, in the stack trace view, the most sampled method was"
93,HolderOfUniqueValues.countIntersection(). You can review and make required code changes to optimize this method to effectively improve the performance of the application.
93,Use jdk.CPULoad and jdk.ThreadCPULoad Events to Monitor Code Execution Performance
93,"When there are not a lot of Java Application events, it could be that the main bottleneck of your application is the running code. Recordings from Flight Recorder can help identify problem areas."
93,"Look at the jdk.CPULoad events and review the CPU usage over time. This shows the CPU usage of the JVM being recorded and the total CPU usage on the machine. If the JVM CPU usage is low, but the CPU usage of the machine is high, then some other application is likely taking a lot of CPU. In that case, look at the other applications running on the system using OS tools such as Top or the task manager to find out which processes are using a lot of CPU."
93,"In case your application is using a lot of CPU time, look at jdk.ThreadCPULoad events and identify the threads that use the most CPU time. This information is based on method sampling, so it may not be 100% accurate if the sample count is low. When a recording is running, the JVM samples the threads. By default, a continuous recording does only some method sampling, while a profiling recording does as much as possible. The method sampling gathers data from only those threads running code. The threads waiting for I/O, sleeping, waiting for locks, and so on are not sampled. Therefore, threads with a lot of method samples are the ones using the most CPU time; however, how much CPU is used by each thread is not known."
93,"The Hot Methods tab in the Code tab group helps find out where your application spends most of the execution time. This tab shows all the samples grouped by top method in the stack. Use the Call Tree tab to start with the lowest method in the stack traces and then move upward. starts with Thread.run, and then looks at the calls that have been most sampled."
94,ManageIQ
94,ManageIQ
94,Download
94,Documentation
94,Community
94,Blog
94,Get Started
94,User Reference
94,Automation Book
94,API Docs
94,Developer Guides
94,Latest
94,Installation
94,Amazon Web Services (AWS)
94,Kubernetes
94,IBM Cloud
94,Microsoft Azure
94,Google Compute Engine
94,VMware vSphere
94,Red Hat Enterprise OpenStack Platform
94,Red Hat Enterprise Virtualization
94,Microsoft System Center Virtual Machine Manager (SCVMM)
94,Configuration
94,Deployment Planning Guide
94,General Configuration
94,High Availability Guide
94,Appliance Hardening Guide
94,Administration
94,"Monitoring, Alerts, and Reporting"
94,Policies and Profiles Guide
94,Managing Infrastructure and Inventory
94,Managing Providers
94,Provisioning Virtual Machines and Hosts
94,Scripting Actions in ManageIQ
94,Authentication
94,Active Directory
94,2-Factor-Authentication with IPA
94,IPA/AD Trust
94,LDAP
94,SAML
94,OpenID-Connect
94,Integration
94,AWS CloudFormation and OpenStack Heat
94,ServiceNow
94,Reference
94,Capabilities Matrix
94,ManageIQ REST API
94,Methods Available for Automation
94,Versions:
94,Latest
94,Kasparov
94,Jansa
94,Installing on VMware vSphere
94,Installing ManageIQ
94,Obtaining the appliance
94,Uploading the Appliance on VMware vSphere
94,Configuring ManageIQ
94,Accessing the Appliance Console
94,Configuring a Database
94,Configuring an Internal Database
94,Configuring an External Database
94,Configuring a Worker Appliance
94,Additional Configuration for Appliances on VMware vSphere
94,Installing VMware VDDK on ManageIQ
94,Tuning Appliance Performance
94,Logging In After Installing ManageIQ
94,Changing the Default Login Password
94,Appendix
94,Appliance Console Command-Line Interface (CLI)
94,Database Configuration Options
94,v2_key Options
94,IPA Server Options
94,Certificate Options
94,Other Options
94,Installing on VMware vSphere
94,Installing ManageIQ
94,Installing ManageIQ consists of the following steps:
94,Downloading the appliance for your environment as a virtual machine
94,image template.
94,Setting up a virtual machine based on the appliance.
94,Configuring the ManageIQ appliance.
94,"After you have completed all the procedures in this guide, you will have"
94,a working environment on which additional customizations and
94,configurations can be performed.
94,Obtaining the appliance
94,Uploading the Appliance on VMware vSphere
94,Uploading the ManageIQ appliance file onto VMware vSphere systems
94,has the following requirements:
94,44 GB of space on the chosen vSphere datastore.
94,12 GB RAM.
94,4 VCPUs.
94,Administrator access to the vSphere Client.
94,"Depending on your infrastructure, allow time for the upload."
94,Note:
94,These are the procedural steps as of the time of writing. For more
94,"information, consult the VMware documentation."
94,Use the following procedure to upload the ManageIQ appliance OVF template from your local file system using the vSphere Client.
94,"In the vSphere Client, select menu:File[Deploy OVF Template]. The"
94,Deploy OVF Template wizard appears.
94,Specify the source location and click Next.
94,Select Deploy from File to browse your file system for the
94,"OVF template, for example manageiq-vsphere-ivanchuk-4.ova."
94,Select Deploy from URL to specify a URL to an OVF template
94,located on the internet.
94,View the OVF Template Details page and click Next.
94,Select the deployment configuration from the drop-down menu and
94,click Next. The option selected typically controls the memory
94,"settings, number of CPUs and reservations, and application-level"
94,configuration parameters.
94,Select the host or cluster on which you want to deploy the OVF
94,template and click Next.
94,Select the host on which you want to run the run the ManageIQ
94,"appliance, and click Next."
94,"Navigate to, and select the resource pool where you want to run the"
94,ManageIQ appliance and click Next.
94,"Select a datastore to store the deployed ManageIQ appliance,"
94,and click Next. Ensure to select a datastore large enough to
94,accommodate the virtual machine and all of its virtual disk files.
94,"Select the disk format to store the virtual machine virtual disks,"
94,and click Next.
94,Select Thin Provisioned if the storage is allocated on
94,demand as data is written to the virtual disks.
94,Select Thick Provisioned if all storage is immediately
94,allocated.
94,"For each network specified in the OVF template, select a network by"
94,right-clicking the Destination Network column in your
94,infrastructure to set up the network mapping and click Next.
94,The IP Allocation page does not require any configuration
94,changes. Leave the default settings in the IP Allocation page
94,and click Next.
94,Set the user-configurable properties and click Next. The
94,properties to enter depend on the selected IP allocation scheme. For
94,"example, you are prompted for IP related information for the"
94,deployed virtual machines only in the case of a fixed IP allocation
94,scheme.
94,Review your settings and click Finish.
94,The progress of the import task appears in the vSphere Client Status
94,panel.
94,Configuring ManageIQ
94,After installing ManageIQ and running it for the first
94,"time, you must perform some basic configuration. To configure"
94,"ManageIQ, you must at a minimum:"
94,Add a disk to the infrastructure hosting your appliance.
94,Configure the database.
94,Configure the ManageIQ appliance using the internal
94,appliance console.
94,Accessing the Appliance Console
94,Start the appliance and open a terminal console.
94,Enter the appliance_console command. The ManageIQ appliance
94,summary screen displays.
94,Press Enter to manually configure settings.
94,"Press the number for the item you want to change, and press Enter."
94,The options for your selection are displayed.
94,Follow the prompts to make the changes.
94,Press Enter to accept a setting where applicable.
94,Note:
94,The ManageIQ appliance console automatically logs out
94,after five minutes of inactivity.
94,Configuring a Database
94,ManageIQ uses a database to store information about the
94,"environment. Before using ManageIQ, configure the database"
94,options for it; ManageIQ provides the following two
94,options for database configuration:
94,Install an internal PostgreSQL database to the appliance
94,Configure the appliance to use an external PostgreSQL database
94,Configuring an Internal Database
94,"Before installing an internal database, add a disk to the infrastructure"
94,hosting your appliance. See the documentation specific to your
94,infrastructure for instructions for adding a disk. As a storage disk
94,"usually cannot be added while a virtual machine is running, Red Hat"
94,recommends adding the disk before starting the appliance.
94,ManageIQ only supports installing of an internal VMDB on blank
94,disks; installation will fail if the disks are not blank.
94,Start the appliance and open a terminal console.
94,Enter the appliance_console command. The ManageIQ appliance
94,summary screen displays.
94,Press Enter to manually configure settings.
94,Select Configure Application from the menu.
94,You are prompted to create or fetch an encryption key.
94,"If this is the first ManageIQ appliance, choose Create"
94,key.
94,"If this is not the first ManageIQ appliance, choose"
94,Fetch key from remote machine to fetch the key from the
94,"first appliance. For worker and multi-region setups, use this"
94,option to copy key from another appliance.
94,Note:
94,All ManageIQ appliances in a multi-region
94,deployment must use the same key.
94,Choose Create Internal Database for the database location.
94,Choose a disk for the database. This can be either a disk you
94,"attached previously, or a partition on the current disk."
94,Red Hat recommends using a separate disk for the database.
94,"If there is an unpartitioned disk attached to the virtual machine,"
94,the dialog will show options similar to the following:
94,1) /dev/vdb: 20480
94,2) Don't partition the disk
94,Enter 1 to choose /dev/vdb for the database location. This
94,option creates a logical volume using this device and mounts the
94,volume to the appliance in a location appropriate for storing
94,"the database. The default location is /var/lib/pgsql, which"
94,can be found in the environment variable
94,$APPLIANCE_PG_MOUNT_POINT.
94,Enter 2 to continue without partitioning the disk. A second
94,prompt will confirm this choice. Selecting this option results
94,in using the root filesystem for the data directory (not advised
94,in most cases).
94,Enter Y or N for Should this appliance run as a standalone
94,database server?
94,Select Y to configure the appliance as a database-only
94,"appliance. As a result, the appliance is configured as a basic"
94,"PostgreSQL server, without a user interface."
94,Select N to configure the appliance with the full
94,administrative user interface.
94,"When prompted, enter a unique number to create a new region."
94,Creating a new region destroys any existing data on the chosen
94,database.
94,Create and confirm a password for the database.
94,ManageIQ then configures the internal database. This takes a few
94,"minutes. After the database is created and initialized, you can log in"
94,to ManageIQ.
94,Configuring an External Database
94,"Based on your setup, you will choose to configure the appliance to use"
94,"an external PostgreSQL database. For example, we can only have one"
94,"database in a single region. However, a region can be segmented into"
94,"multiple zones, such as database zone, user interface zone, and"
94,"reporting zone, where each zone provides a specific function. The"
94,appliances in these zones must be configured to use an external
94,database.
94,The postgresql.conf file used with ManageIQ databases requires
94,"specific settings for correct operation. For example, it must correctly"
94,"reclaim table space, control session timeouts, and format the PostgreSQL"
94,"server log for improved system support. Due to these requirements, Red"
94,Hat recommends that external ManageIQ databases use a
94,postgresql.conf file based on the standard file used by the
94,ManageIQ appliance.
94,Ensure you configure the settings in the postgresql.conf to suit your
94,"system. For example, customize the shared_buffers setting according to"
94,the amount of real storage available in the external system hosting the
94,"PostgreSQL instance. In addition, depending on the aggregate number of"
94,"appliances expected to connect to the PostgreSQL instance, it may be"
94,necessary to alter the max_connections setting.
94,Note:
94,ManageIQ requires PostgreSQL version 9.5.
94,Because the postgresql.conf file controls the operation of all
94,"databases managed by a single instance of PostgreSQL, do not mix"
94,ManageIQ databases with other types of databases in a single
94,PostgreSQL instance.
94,Start the appliance and open a terminal console.
94,Enter the appliance_console command. The ManageIQ appliance
94,summary screen displays.
94,Press Enter to manually configure settings.
94,Select Configure Application from the menu.
94,You are prompted to create or fetch a security key.
94,"If this is the first ManageIQ appliance, choose Create"
94,key.
94,"If this is not the first ManageIQ appliance, choose"
94,Fetch key from remote machine to fetch the key from the
94,first appliance.
94,Note:
94,All ManageIQ appliances in a multi-region
94,deployment must use the same key.
94,Choose Create Region in External Database for the database location.
94,Enter the database hostname or IP address when prompted.
94,Enter the database name or leave blank for the default
94,(vmdb_production).
94,Enter the database username or leave blank for the default (root).
94,Enter the chosen database user’s password.
94,Confirm the configuration if prompted.
94,ManageIQ will then configure the external database.
94,Configuring a Worker Appliance
94,"You can use multiple appliances to facilitate horizontal scaling, as"
94,"well as for dividing up work by roles. Accordingly, configure an"
94,"appliance to handle work for one or many roles, with workers within the"
94,appliance carrying out the duties for which they are configured. You can
94,configure a worker appliance through the terminal. The following steps
94,demonstrate how to join a worker appliance to an appliance that already
94,has a region configured with a database.
94,Start the appliance and open a terminal console.
94,Enter the appliance_console command. The ManageIQ appliance
94,summary screen displays.
94,Press Enter to manually configure settings.
94,Select Configure Application from the menu.
94,You are prompted to create or fetch a security key. Since this is
94,"not the first ManageIQ appliance, choose 2) Fetch key from"
94,"remote machine. For worker and multi-region setups, use this"
94,option to copy the security key from another appliance.
94,Note:
94,All ManageIQ appliances in a multi-region deployment
94,must use the same key.
94,Choose Join Region in External Database for the database location.
94,Enter the database hostname or IP address when prompted.
94,Enter the port number or leave blank for the default (5432).
94,Enter the database name or leave blank for the default
94,(vmdb_production).
94,Enter the database username or leave blank for the default (root).
94,Enter the chosen database user’s password.
94,Confirm the configuration if prompted.
94,Additional Configuration for Appliances on VMware vSphere
94,Installing VMware VDDK on ManageIQ
94,Execution of SmartState Analysis on virtual machines within a VMware
94,environment requires the Virtual Disk Development Kit (VDDK). This
94,"version of ManageIQ supports VDDK versions 6.0, 6.5, and"
94,6.7.
94,To install VMware VDDK:
94,Download the required VDDK version
94,(VMware-vix-disklib-[version].x86_64.tar.gz) from the VMware
94,website.
94,Note:
94,"If you do not already have a login ID to VMware, then you will"
94,"need to create one. At the time of this writing, the file can be"
94,found by navigating to menu:Downloads[vSphere]. Select the
94,"version from the drop-down list, then click the Drivers &"
94,"Tools tab. Expand Automation Tools and SDKs, and click"
94,Go to Downloads next to the VMware vSphere Virtual Disk
94,"Development Kit version. Alternatively, find the file by"
94,searching for it using the Search on the VMware site.
94,See VMware documentation for information about their policy
94,concerning backward and forward compatibility for VDDK.
94,Download and copy the VMware-vix-disklib-[version].x86_64.tar.gz
94,file to the /root directory of the appliance.
94,Start an SSH session into the appliance.
94,Extract and install the VMware-vix-disklib-[version].x86_64.tar.gz
94,file using the following commands:
94,# cd /root
94,# tar -xvf VMware-vix-disklib-[version].x86_64.tar.gz
94,# cp vmware-vix-disklib-distrib/ -rf /usr/lib/vmware-vix-disklib/
94,# ln -s /usr/lib/vmware-vix-disklib/lib64/libvixDiskLib.so /usr/lib/libvixDiskLib.so
94,# ln -s /usr/lib/vmware-vix-disklib/lib64/libvixDiskLib.so.6 /usr/lib/libvixDiskLib.so.6
94,# ln -s /usr/lib/vmware-vix-disklib/lib64/libvixDiskLib.so.6.7.0 /usr/lib/libvixDiskLib.so.6.7.0
94,Run ldconfig to instruct ManageIQ to find the newly
94,installed VDDK library.
94,Note:
94,Use the following command to verify the VDDK files are listed and
94,accessible to the appliance:
94,# ldconfig -p | grep vix
94,Restart the ManageIQ appliance.
94,The VDDK is now installed on the ManageIQ appliance. This
94,enables use of the SmartState Analysis server role on the appliance.
94,Tuning Appliance Performance
94,"By default, the ManageIQ appliance uses the tuned"
94,service and its virtual-guest profile to optimize performance. In most
94,"cases, this profile provides the best performance for the appliance."
94,"However on some VMware setups (for example, with a large vCenter"
94,"database), the following additional tuning may further improve appliance"
94,performance:
94,"When using the virtual-guest profile in tuned, edit the"
94,vm.swappiness setting to 1 in the tuned.conf file from the
94,default of vm.swappiness = 30.
94,Use the noop scheduler instead. See the VMware
94,documentation for more
94,details on the best scheduler for your environment. See Setting the
94,Default I/O
94,Scheduler
94,in the Red Hat Enterprise Linux Performance Tuning Guide for
94,instructions on changing the default I/O scheduler.
94,Logging In After Installing ManageIQ
94,"Once ManageIQ is installed, you can log in and perform"
94,administration tasks.
94,Log in to ManageIQ for the first time after installing by:
94,Navigate to the URL for the login screen. (https://xx.xx.xx.xx on
94,the virtual machine instance)
94,Enter the default credentials (Username: admin | Password:
94,smartvm) for the initial login.
94,Click Login.
94,Changing the Default Login Password
94,Change your password to ensure more private and secure access to
94,ManageIQ.
94,Navigate to the URL for the login screen. (https://xx.xx.xx.xx on
94,the virtual machine instance)
94,Click Update Password beneath the Username and Password
94,text fields.
94,Enter your current Username and Password in the text fields.
94,Input a new password in the New Password field.
94,Repeat your new password in the Verify Password field.
94,Click Login.
94,Appendix
94,Appliance Console Command-Line Interface (CLI)
94,"Currently, the appliance_console_cli feature is a subset of the full functionality of the appliance_console itself, and covers functions most likely to be scripted by using the command-line interface (CLI)."
94,"After starting the ManageIQ appliance, log in with a user name of root and the default password of smartvm. This displays the Bash prompt for the root user."
94,"Enter the appliance_console_cli or appliance_console_cli --help command to see a list of options available with the command, or simply enter appliance_console_cli --option <argument> directly to use a specific option."
94,Database Configuration Options
94,Option
94,Description
94,–region (-r)
94,region number (create a new region in the database - requires database credentials passed)
94,–internal (-i)
94,internal database (create a database on the current appliance)
94,–dbdisk
94,database disk device path (for configuring an internal database)
94,–hostname (-h)
94,database hostname
94,–port
94,database port (defaults to 5432)
94,–username (-U)
94,database username (defaults to root)
94,–password (-p)
94,database password
94,–dbname (-d)
94,database name (defaults to vmdb_production)
94,v2_key Options
94,Option
94,Description
94,–key (-k)
94,create a new v2_key
94,–fetch-key (-K)
94,fetch the v2_key from the given host
94,–force-key (-f)
94,create or fetch the key even if one exists
94,–sshlogin
94,ssh username for fetching the v2_key (defaults to root)
94,–sshpassword
94,ssh password for fetching the v2_key
94,IPA Server Options
94,Option
94,Description
94,–host (-H)
94,set the appliance hostname to the given name
94,–ipaserver (-e)
94,IPA server FQDN
94,–ipaprincipal (-n)
94,IPA server principal (default: admin)
94,–ipapassword (-w)
94,IPA server password
94,–ipadomain (-o)
94,IPA server domain (optional). Will be based on the appliance domain name if not specified.
94,–iparealm (-l)
94,IPA server realm (optional). Will be based on the domain name of the ipaserver if not specified.
94,–uninstall-ipa (-u)
94,uninstall IPA client
94,Note:
94,"In order to configure authentication through an IPA server, in addition to using Configure External Authentication (httpd) in the appliance_console, external authentication can be optionally configured via the appliance_console_cli (command-line interface)."
94,"Specifying –host will update the hostname of the appliance. If this step was already performed via the appliance_console and the necessary updates that are made to /etc/hosts if DNS is not properly configured, the –host option can be omitted."
94,Certificate Options
94,Option
94,Description
94,–ca (-c)
94,CA name used for certmonger (default: ipa)
94,–postgres-client-cert (-g)
94,install certs for postgres client
94,–postgres-server-cert
94,install certs for postgres server
94,–http-cert
94,install certs for http server (to create certs/httpd* values for a unique key)
94,–extauth-opts (-x)
94,external authentication options
94,"Note: The certificate options augment the functionality of the certmonger tool and enable creating a certificate signing request (CSR), and specifying certmonger the directories to store the keys."
94,Other Options
94,Option
94,Description
94,–logdisk (-l)
94,log disk path
94,–tmpdisk
94,initialize the given device for temp storage (volume mounted at /var/www/miq_tmp)
94,–verbose (-v)
94,print more debugging info
94,Example Usage.
94,$ ssh root@appliance.test.company.com
94,To create a new database locally on the server by using /dev/sdb:
94,# appliance_console_cli --internal --dbdisk /dev/sdb --region 0 --password smartvm
94,To copy the v2_key from a host some.example.com to local machine:
94,# appliance_console_cli --fetch-key some.example.com --sshlogin root --sshpassword smartvm
94,You could combine the two to join a region where db.example.com is the appliance hosting the database:
94,# appliance_console_cli --fetch-key db.example.com --sshlogin root --sshpassword smartvm --hostname db.example.com --password mydatabasepassword
94,To configure external authentication:
94,# appliance_console_cli --host appliance.test.company.com
94,--ipaserver ipaserver.test.company.com
94,--ipadomain test.company.com
94,--iparealm TEST.COMPANY.COM
94,--ipaprincipal admin
94,--ipapassword smartvm1
94,To uninstall external authentication:
94,# appliance_console_cli
94,--uninstall-ipa
94,Download
94,Documentation
94,Community
94,Blog
94,Github
94,Created with Sketch.
94,Gitter
94,Forum
94,© 2021 ManageIQ.
94,"Sponsored by Red Hat, Inc."
94,Logo
94,Security
94,Legal & Privacy
96,Deep Dive into the New Features of Apache Spark 3.0 - Databricks
96,SAIS 2020
96,Agenda
96,Speakers
96,Training
96,Sponsors
96,Special Events
96,Women at Summit
96,Financial Services
96,Government and Education
96,Healthcare and Life Sciences
96,Media and Entertainment
96,Retail and Consumer Goods
96,Job Board
96,FAQ
96,WATCH KEYNOTES
96,SAIS 2020
96,Agenda
96,Speakers
96,Training
96,Sponsors
96,Special Events
96,Women at Summit
96,Financial Services
96,Government and Education
96,Healthcare and Life Sciences
96,Media and Entertainment
96,Retail and Consumer Goods
96,Job Board
96,FAQ
96,WATCH KEYNOTES
96,Deep Dive into the New Features of Apache Spark 3.0Download Slides
96,"Continuing with the objectives to make Spark faster, easier, and smarter, Apache Spark 3.0 extends its scope with more than 3000 resolved JIRAs. We will talk about the exciting new developments in the Spark 3.0 as well as some other major initiatives that are coming in the future. In this talk, we want to share with the community many of the more important changes with the examples and demos."
96,"The following features are covered: accelerator-aware scheduling, adaptive query execution, dynamic partition pruning, join hints, new query explain, better ANSI compliance, observable metrics, new UI for structured streaming, new UDAF and built-in functions, new unified interface for Pandas UDF, and various enhancements in the built-in data sources [e.g., parquet, ORC and JDBC]."
96,Watch more Spark + AI sessions here
96,Try Databricks for free
96,Video Transcript
96,About us and our Open Source contributions
96,"Hello, everyone. Today, Wenchen and I are glad to share with you the latest"
96,"updates about the upcoming release, Spark 3.0."
96,So I’m Xiao Li. Both Wenchen and I are
96,working for Databricks. We focus on the open-source developments. Both of us are Spark
96,committers and PMC members.
96,About Databricks
96,Databricks provides a unified data analytics platform to accelerate
96,"your data-driven animation. We are a global company with more than 5,000 customers across"
96,"various industries, and we have more than 450 partners worldwide. And most of you might"
96,"have heard of Databricks as original creator of Spark, Delta Lake, MLflow, and Koalas."
96,These are open-source projects that are leading innovation in the fields of data and machine
96,learnings. We continue to contribute and nurture this open-source community.
96,Spark 3.0 Highlights
96,"In Spark 3.0, the whole community resolved more than 3,400 JIRAs. Spark SQL"
96,"and the Core are the new core module, and all the other components are built on Spark"
96,"SQL and the Core. Today, the pull requests for Spark SQL and the core constitute"
96,"more than 60% of Spark 3.0. In the last few releases, the percentage keeps going up. Today,"
96,we will focus on the key features in both Spark SQL and the Core.
96,This release delivered
96,"many new capabilities, performance gains, and extended compatibility for the Spark ecosystem."
96,This is a combination of the tremendous contributions from the open-source community. It is impossible
96,"to discuss the new features within 16 minutes. We resolved more than 3,400 JIRAs. Even in"
96,"this light, I did my best, but I only can put 24 new Spark 3.0 features."
96,"Today, we would like to present some of them. First, let us talk about the"
96,performance-related features.
96,Spark 3.0 Enhanced Performance
96,High performance is one of the major advantages when people
96,select Spark as their computation engine. This release keeps enhancing the performance
96,"for interactive, batch, streaming, and [inaudible] workloads. Here, I will first cover four of"
96,"the performance features in SQL query compilers. Later, Wenchen will talk about the performance enhancement for building data sources."
96,The four major features in query compilers include
96,a new framework for adaptive query execution and a new runtime filtering for dynamic partition
96,"pruning. And also, we greatly reduce the overhead of our query compiler by more than a half,"
96,especially on the optimizer overhead and the SQL cache synchronization. Supporting a complete
96,set of join hints is another useful features many people are waiting for.
96,Adaptive query
96,"execution was available at the previous releases. However, the previous framework has a few"
96,"major drawbacks. Very few companies are using it in the production systems. In this release,"
96,Databricks and the [inaudible] work together and redesigned the new framework and resolved
96,all the known issues.
96,Let us talk about what we did in this release.
96,Spark Catalyst Optimizer
96,Michael Armbrust.
96,"is the creator of Spark SQL and also, Catalyst Optimizer. In the initial release of Spark"
96,"SQL, all the optimizer rules are heuristic-based. To generate good query plans, the query optimizer"
96,"needs to understand the data characteristics. Then in Spark 2.x, we introduced a cost-based"
96,"optimizer. However, in most cases, data statistics are commonly absent, especially when statistics collection is even more expensive than the data processing in the [search?]. Even if"
96,"the statistics are available, the statistics are likely out of date. Based on the storage and the compute separation in Spark, the characteristics of data [rival?] is unpredictable. The costs"
96,are often misestimated due to the different deployment environment and the black box user-defined
96,"functions. We are unable to estimate the cost for the UDF. Basically, in many cases, Spark"
96,optimizer is enabled to generate the best plan due to this limitation.
96,Adaptive Query Execution
96,For all these
96,"reasons, runtime adaptivity becomes more critical for Spark than the traditional systems."
96,So this release introduced a new adaptive query execution framework called
96,AQE. The basic idea of adaptive planning is simple. We optimize the execution plan using
96,the existing rules of the optimizer and the planner after we collect more accurate statistics
96,from the finished plans.
96,The red line shows the new logics we added in this release. Instead
96,"of directly optimizing the execution plans, we send back the unfinished plan segments"
96,and then use an existing optimizer and planner to optimize them at the end and build a new
96,execution plan. This release includes three adaptive features. We can convert the soft
96,"merge join to broadcast hash join, based on the runtime statistics. We can shrink the"
96,number of reducers after over-partitioning. We can also handle the skew join at runtime.
96,"If you want to know more details, please read the blog post I posted here."
96,"Today, I will briefly explain them one by one."
96,Maybe most of you already
96,"learn many performance tuning tips. For example, to make your join faster, you might guide"
96,your optimizer to choose a broadcast hash join instead of the sort merge join. You can
96,"increase the spark.sql.autobroadcastjointhreshold or use a broadcast join hint. However, it"
96,is hard to tune it. You might hit out of memory exceptions and even get worse performance.
96,"Even if it works now, it is hard to maintain over time because it is sensitive to your"
96,data workloads.
96,You might be wondering why Spark is unable to make the wise choice by
96,itself. I can easily list multiple reasons.
96,the statistics might
96,be missing or out of date.
96,the file is compressed.
96,"the file format is column-based, so the file"
96,size does not represent the actual data volume.
96,the filters could be compressed
96,(the filters) might also contain the black box UDFs.
96,"The whole query fragments might be large,"
96,"complex, and it is hard to estimate the actual data volume for Spark to make the best choice."
96,Convert Sort Merge Join to Broadcast Hash Join
96,So this is an example to show how AQE converts a sort merge join to
96,"a broadcast hash join at runtime. First, execute the leave stages. Query the statistics from"
96,the shuffle operators which materialize the query fragments. You can see the actual size
96,of stage two is much smaller than the estimated size reduced from 30 megabytes to 8 megabytes
96,so we can optimize the remaining plan and change the join algorithm from sort merge
96,join to broadcast hash join.
96,Another popular performance tuning tip is to tune the configuration
96,"spark.sql.shuffle.partitions. The default value is a magic number, 200. Previously,"
96,"the original default is 8. Later, it was increased to 200. I believe no one knows the reason"
96,"why it become 200 instead of 50, 400, or 2,000."
96,"It is very hard to tune it, to be"
96,"honest. Because it is a global configuration, it is almost impossible to decide the best"
96,"value for every query’s fragment using a single configuration, especially when your query"
96,plan is huge and complex.
96,"If you set it to very small values, the partition will be huge,"
96,and the aggregation and the sort might need to spew the data to the disk. If the configuration
96,"values are too big, the partition will be small. But the number of partitions is big."
96,It will cause inefficient IO and the performance bottleneck could be the task scheduler. Then
96,"it will slow down everybody. Also, it is very hard to maintain over time."
96,Dynamically Coalesce Shuffle Partitions
96,Until you can
96,"solve it in a smart way, we can first increase our initial partition number to a big one."
96,"After we execute the leave query stage, we can know the actual size of each partition."
96,Then we can automatically correlate the nearby partitions and automatically reduce the number
96,of partitions to a smaller number. This example shows how we reduce the number of partitions
96,from 50 to 5 at runtime. And we added the actual coalesce at runtime.
96,Data Skew
96,One more popular performance tuning tip is about data skew. Data skew is
96,very annoying. You could see some long-running or frozen task and a lot of disks spinning
96,and a very low resource authorization rate in most nodes and even out of memory. Our
96,Spark community might tell you many different ways to solve such a typical performance problem.
96,You can find the skew value and the right queries to handle the skew value separately.
96,"And also, you can add the actual skew keys that can remove the data skew, either new"
96,"columns or some existing columns. Anyway, you have to manually rewrite your queries,"
96,"and this is annoying and sensitive to your workloads, too, which could be changed over"
96,time.
96,"This is an example without the skew optimization. Because of data skew,"
96,"after the shuffle, the shuffle partition, A0, will be very large. If we do a join on"
96,"these two tables, the whole performance bottleneck is to join the values for this specific partition,"
96,"A0. For this partition, A0, the cost of shuffle, sort, and merge are much bigger than the other"
96,partitions. Everyone is waiting for the partition 0 to complete and slow down the execution
96,of the whole query.
96,Our adaptive query execution can handle it very well in the data skew case.
96,"After executing the leaf stages (stages one and stage two), we can optimize our queries"
96,"with a skew shuffle reader. Basically, it will split the skew partitions into smaller"
96,subpartitions after we realize some shuffle partitions are too big.
96,Let us use same example to show how to resolve it using adaptive query
96,"execution. After realizing partitions are too large, AQE will add a skew reader to automatically"
96,"split table A’s partition part 0 to three segments: split 0, split 1, and split 2. Then"
96,it will also duplicate another side for table B. Then we will have three copies for table
96,B’s part 0.
96,"After this step, we can parallelize the shuffle reading, sorting, merging for"
96,this split partition A0. We can avoid generating very big partition for the sort merge join.
96,"Overall, it will be much faster."
96,"Based on a terabyte of TPC-DS benchmark, without statistics, Spark 3.0 can make Q7 eight times faster and also achieve two times fast"
96,and speed up for Q5 and more than 1.1 speed up for another 26 queries. So this is just
96,"the beginning. In the future releases, we will continue to improve the compiler and"
96,introduce more new adaptive rules.
96,Dynamic Partition Pruning
96,The second performance features I want to highlight is dynamic partition pruning.
96,"So this is another runtime optimization rule. Basically, dynamic partition pruning is to"
96,avoid partition scanning based on the queried results of the other query fragments. It is
96,important for star schema queries. We can achieve a significant speed up in TPC-DS
96,queries.
96,"So this is a number, in a TPC-DS benchmark, 60 out of 102 queries show a significant"
96,speed up between 2 times and 18 times. It is to prune the partitions that joins read
96,from the fact table T1 by identifying those partitions that result from filtering the
96,"dimension table, T2."
96,"Let us explain it step by step. First, we will do the filter push down"
96,"in the left side. And on the right side, we can generate a new filter for the partition"
96,column PP because join P is a partition column. Then we get the query results of the left
96,"side. We can reuse our query results and generate the lists of constant values, EPP, and filter"
96,"result. Now, we can push down the in filter in the right side. This will avoid scanning"
96,"all the partitions of the huge fact table, T1. For this example, we can avoid scanning"
96,"90% of partitioning. With this dynamic partition pruning, we can achieve 33 times speed up."
96,JOIN Optimizer Hints
96,So the last performance feature is join hints. Join hints are very common
96,"optimizer hints. It can influence the optimizer to choose an expected join strategies. Previously,"
96,"we already have a broadcast hash join. In this release, we also add the hints for the"
96,"other three join strategies: sort merge join, shuffle hash join, and the shuffle nested"
96,loop join.
96,"Please remember, this should be used very carefully. It is difficult to manage"
96,over time because it is sensitive to your workloads. If your workloads’ patterns are
96,"not stable, the hint could even make your query much slower."
96,Here are examples how to
96,use these hints in the SQL queries. You also can do the same thing in the DataFrame API.
96,"When we decide the join strategies, [our leads are different here?]."
96,So a broadcast
96,"hash join requires one side to be small, no shuffle, no sort, so it performs very fast."
96,"For the shuffle hash join, it needs to shuffle the data but no sort is needed. So it can"
96,handle the large tables but will still hit out of memory if the data is skewed.
96,Sort
96,merge join is much more robust. It can handle any data size. It needs to shuffle and salt
96,data slower in most cases when the table size is small compared with a broadcast hash join.
96,"And also, shuffle nested loop join, it doesn’t require the join keys, unlike the other three"
96,join strategies.
96,Richer APIs: new features and simplify development
96,"To enable new use cases and simplify the Spark application development,"
96,this release delivers a new capability and enhanced interesting features.
96,Pandas UDF
96,"Let’s, first,"
96,talk about Pandas UDF. This is a pretty popular performance features for the PySpark users.
96,So let us talk about the history of UDF support in PySpark. In the first release of Python
96,"support, 2013, we already support Python lambda functions for RDD API. Then in 2014, users"
96,"can register Python UDF for Spark SQL. Starting from Spark 2.0, Python UDF registration is"
96,"session-based. And then next year, users can register the use of Java UDF in Python API."
96,"In 2018, we introduced Pandas UDF. In this release, we redesigned the interface for Pandas"
96,UDF by using the Python tab hints and added more tabs for the Pandas UDFs.
96,To adjust our compatibility with the old Pandas UDFs from Apache Spark 2.0
96,"with the Python 2.6 and above, Python [inaudible] such as pandas.Series, Pandas DataFrame, cube"
96,"hole, and the iterator can be used to impress new Pandas UDF types. For example, in Spark"
96,"2.3, we have a Scala UDF. The input is a pandas.Series and its output is also pandas.Series. In Spark"
96,"2.0, we do not require users to remember any UDF types. You just need to specify the input"
96,"and the output types. In Spark 2.3, we also have a Grouped Map Pandas UDF, so input is"
96,"a Pandas DataFrame, and the output is also Pandas DataFrames."
96,Old vs New Pandas UDF interface
96,This slide shows the difference between the old and the new interface. The
96,same here. The new interface can also be used for the existing Grouped Aggregate Pandas
96,"UDFs. In addition, the old Pandas UDF was split into two API categories: Pandas UDFs"
96,and Pandas function APIs. You can treat Pandas UDFs in the same way that you use the other
96,PySpark column instance.
96,"For example, here, calculate the values. You are calling the"
96,Pandas UDF calculate. We do support the new Pandas UDF types from iterators of series
96,to iterator other series and from iterators of multiple series to iterator of series.
96,So this is useful for [inaudible] state initialization of your Pandas UDFs and also useful for Pandas
96,UDF parquet.
96,"However, you can now use Pandas function APIs with this column instance. Here"
96,"are these two examples: map Pandas function API and the core group, the map Pandas UDF,"
96,the APIs. These APIs are newly added in these units.
96,Back to Wenchen
96,"So next, Wenchen will go over the remaining"
96,features and provide a deep dive into accumulator with Scalar. Please welcome Wenchen.
96,"Thanks, Xiao, for the first half of the talk. Now, let me take over from"
96,here and introduce the remaining Spark 3.0 features.
96,Accelerator-aware Scheduling
96,"I will start with, straight away,"
96,"our scheduler. In 2018 Spark Summit, we already announced the new project [inaudible]. As"
96,"you’re now aware, our scheduler is part of this project. It can be widely used for executing"
96,"special workloads. In this release, we support standalone, YARN, and Kubernetes scheduler"
96,"scheduler backend. So far, users need to specify the require resources using a [inaudible]"
96,configs.
96,"In the future, we will support the job, stage, and task levels. To further understand"
96,"this feature, let’s look at the workflow. Ideally, the cost manager should be able to"
96,"automatically discover resources, like GPUs. When the user submits an application with"
96,"resource request, Spark should pass the resources request to a cluster manager and then the cluster"
96,manager cooperates to allocate and launch executors with the required resources. After
96,"Spark job is submitted, Spark should schedule tasks on available executors, and the cluster"
96,manager should track the results usage and perform dynamic resource allocation.
96,"For example,"
96,"when there are too many pending tasks, the cluster manager should allocate more executors"
96,"to run more tasks at the same time. When a task is running, the user shall be able to"
96,"retrieve the assigned resources and use them in their code. In the meanwhile, cluster manager"
96,shall monitor and recover failed executions.
96,"Now, let’s look at how can a cluster manager discover resources and how"
96,can users request resources.
96,"As an admin of the cluster, I can specify a script to auto discover executors. The discovery script can be specified separately on Java as executors."
96,We also provided an example to auto discover Nvidia GPU resources. You can adjust
96,"this example script for other kinds of resources. Then as a user of Spark, I can request resources"
96,at the application level. I can use the config spark.executor.resource.{resourceName}.amount
96,and the corresponding config for Java to specify the executors amount on the Java and executors.
96,"Also, I can use the config spark.task.resource.{resourceName}.amount to specify the executors required by each"
96,"task. As I mentioned earlier, we will support more time-proven labor later, like the job"
96,or stage labor. Please stay tuned.
96,Retrieve Assigned Accelerators
96,"Next, we’ll see how you can leverage the assigned executors to actually"
96,"execute your workloads, which is probably the most important part to the users. So as"
96,"a user of Spark, I can retrieve the assigned executors from the task content. Here is an"
96,example in PySpark. The contents of resources returns a map from the resource name to resource
96,"info. In the example, we request for GPUs, and we can take the GPU address from the resource"
96,map. Then we launch the TensorFlow to train my model within GPUs. Spark will take care
96,"of the resource allocation and acceleration and also monitor the executors here for failure recovery, which makes my life much easier."
96,Cluster Manager Support
96,"As I mentioned earlier, the executor aware"
96,"of scheduling support has been added to standalone, YARN, and Kubernetes cost manager. You can"
96,"check the Spark JIRA tickets to see more details. Unfortunately, the Mesos support is still"
96,not available. We’d really appreciate it if any Mesos expert has interest and is willing
96,to help the Spark community to add the Mesos support. Please leave a comment in the [inaudible]
96,if you want to work on it. Thanks in advance.
96,Improved Spark Web UI for Accelerators
96,"Last but not the least, we also improved the Spark Web UI to show all"
96,"the discovery resources on the executor page. In this page, we can see that there are GPUs"
96,available on the executor one. You can check the Web UI to see how many executors
96,"are available in the cluster, so you can better schedule your jobs."
96,32 New Built-in Functions
96,"In this release, we also"
96,introduced 32 new built-in functions and add high auto functions in the Scalar API. The
96,Spark community pays a lot of attention to compatibility. We have investigated many other
96,"ecosystems, like the PostgreSQL, and implemented many commonly used functions in Spark."
96,"Hopefully,"
96,these new built-in functions can make it faster to build your queries as you don’t need to
96,waste time to learn a lot of UDFs.
96,"Due to the time limitations, I can’t go over all"
96,"the functions here, so let me just introduce some map type functions as an example."
96,"When you deal with map type values, it’s common to get the keys and values"
96,"for the map as an array. There are two functions, map keys and map values can do"
96,this for you. The example is from the Databricks runtime notebook. Or you may want to do
96,"something more complicated, like creating a new map by transforming the original map"
96,"where it’s a keys and a map values functions. So if there are two functions, transform keys"
96,"and transform values can do this for you, and you just need to write a handler function"
96,to specify the transformation logic.
96,"As I mentioned earlier, the functions"
96,also have Scalar APIs rather than the SQL API. Here is an example about how to do the
96,"same thing, but it’s a Scalar API. You can just write a normal Scala function, which"
96,takes the [kernel?] objects as the input to have the same effect as the SQL API.
96,Monitoring and Debuggability
96,This release also includes many enhancements and makes the monitoring
96,more comprehensive and stable. We can make it easier to close out and get back to your
96,Spark applications.
96,Structured Streaming UI
96,The first feature I will talk to you about is the new UI for the Spark
96,"streaming. Here, the drive to show it– Spark streaming was initially introduced in Spark"
96,2.0. This release has the dedicated– it was Spark web UI for inspection of these streaming
96,"jobs. This UI offers two sets of statistics: one, abbreviate information of [completed?]"
96,"streaming queries and two, detailed statistics information about the streaming query including"
96,"the input rate, processor rate, input loads, [inaudible], operation duration and others."
96,"More specifically, the input rate and processor rate means how many records per second the"
96,streaming software produces and the Spark streaming engine processes. It can give you
96,a sense about if the streaming engine is fast enough to process the continuous input data.
96,"Similarly, you can tell it from the past duration as well. If many batch takes more time than"
96,"the micro-batch [inaudible], it means the engine is not fast enough to process your"
96,"data, and you may need to enable the [inaudible] feature to make the source produce the data"
96,slower.
96,And so operating time is also a very useful matrix. It tells you the time spent
96,on each operator so that you can know where is the bottleneck in your query.
96,DDL and DML enhancements
96,We also have many different enhancements in DDL and DML commands. Let
96,me talk about the improvements in the EXPLAIN command as an example. This is a typical output
96,of the EXPLAIN command. You have many operators in the query plan tree and some operators
96,have other additional information. Reading plans is critical for understanding and attuning
96,"queries. The existing solution looks [inaudible], and, as a stream of each operator, can be"
96,very wide or even truncated. And it becomes wider and wider each release as we add more
96,and more information in the operator to help debugging.
96,"This release, we enhance the EXPLAIN command with a new formatted mode and also provided a capability to dump the plans to"
96,the files. You can see it becomes much easier to read and understand. So here is a very
96,simple plan tree at the beginning. Then follows a detailed section for each operator. This
96,makes it very easy to get an overview of the query by looking at the plan tree. It also
96,makes it very easy to see the details of each operator as the information is now stacked
96,"vertically. And in the end, there is a section to show all the subqueries. In the future"
96,"releases, we will add more and more useful information for each operator."
96,"This release, we also introduced a new API to define your own metrics to observe data quality. Data quality is very important to many applications. It’s usually easy to"
96,"define metrics for data quality by some [other?] function, for example, but it’s also hard"
96,"to calculate the metrics, especially for streaming queries."
96,"For example, you want to keep monitoring"
96,"the data quality of your streaming source. You can simply define the metrics as the percentage of the error records. Then you can do two things. Make it a habit. One, code observe"
96,method of the streaming error rate to define your metrics with a name and the start
96,"of stream. So this example, the name is data quality and the matrix, it just will count"
96,the error record and see how many percent of it in the total lookups.
96,"Two, you add a"
96,"listener to watch the streaming process events, and in the case of your matrix, the name,"
96,"do whatever you want to do, such as sending an email if there are more than 5% error data."
96,SQL Compatibility
96,"Now, let’s move to the next topic. SQL compatibility is also super critical"
96,"for workloads mapped from the other database systems through Spark SQL. In this release,"
96,we introduced the ANSI store assignment policy for table insertion. We added runtime overall
96,checking with respect to ANSI results keywords into the parser. We also switched the calendar
96,to the widely-used calendar which is the ISO and SQL standard.
96,Let’s look at how the first
96,two features can help you to enforce data quality. I say more about the assignment.
96,It’s something like assigning a value to a variable in programming language. In the SQL
96,"world, it is table insertion or upsert, which is kind of assigning values to a table column."
96,"Now, let’s see an example."
96,"Assume there is a table with two columns, I and J, which are type int and"
96,"type string. If we write a int value to the string column, it’s totally okay. It’s totally"
96,"safe. However, if we write a string value to the int column, it’s risky. The string"
96,"value is very likely to not be in integer form, and Spark will fail and worry about"
96,it.
96,"If you do believe your string values are safe to be inserted into an int column, you"
96,can add a cast manually to bypass the type check in Spark.
96,We can also write long type
96,"of values to the int column, and Spark will do the overflow check at runtime. If your"
96,"input data is invalid, Spark will be show exception at runtime to tell you about it."
96,"In this example, the integer one is okay, but the larger value below can’t fit the integer"
96,"type, and you’ll receive this error if you run this table insertion command which tells"
96,you about the overflow problem.
96,Built-in Data Source Enhancements
96,"Also, this release enhances built-in data sources. For example, to"
96,"populate data source, we can’t do nested column and filter pushdown. Also, we support"
96,[inaudible] for CSV files. This release also introduced a new [inaudible] resource and
96,also a new [inaudible] resource for testing and benchmarking. Let me further introduce
96,the origin of nested columns in Parquet and ORC resource.
96,The first one is a kind of [baloney?].
96,"[inaudible] like [inaudible] and [ORC?], we can skip reading some [inaudible] in the blocks"
96,if they don’t contain the columns we need. This [technique?] can be applied to nested
96,columns as well in Spark 2.0. To check if your query– in Spark 3.0. To check if your
96,"query can benefit from this [inaudible] or not, you can run the EXPLAIN command and see"
96,if the read schema over the file scan note strips the [inaudible] nested columns. In
96,"this example, only the nested column is inserted, so the read schema only contains A."
96,"[inaudible] is also a very popular technical [inaudible]. Similarly,"
96,you can also check the expand result and see if the [pushed?] filters over the file scan
96,"note contains the name of column filters. In this example, we do have a filter where"
96,"it’s nested column A, and it does appear in the [pushed?] filter, which makes this version"
96,happen in this query.
96,Catalog plugin API
96,This release also expands other efforts on the extensibility and ecosystem
96,"like the v2 API enhancements, Java 11, Hadoop, and [inaudible] support. [inaudible]"
96,API. This release extends the [inaudible] to API by adding the Catalog plugin. The Catalog plug-in API allows users to reject their own [inaudible] and take over the [inaudible]
96,data operations from Spark. This can give end users a more seamless experience to assist
96,"external tables. Now, end users [inaudible] reject [inaudible] and manipulate the tables"
96,"[inaudible], where before, end users have to reject each table. For example, let’s say"
96,you have rejected a MySQL connector [inaudible] named MySQL. You can use SELECT to get data
96,from existing MySQL table. We can also INSERT into a MySQL table with Spark’s [inaudible].
96,"You can also create an outer tables in MySQL with Spark, which was just not possible before,"
96,"because before, we don’t have the Catalog plug-in. Now this example will be available"
96,in Spark 3.1 when we finish [inaudible].
96,When to use Data Source V2 API?
96,Some people may have a question. Now Spark has both – it has a V1 and a V2
96,"APIs – which one should I use? In general, we want everyone to move to V2 [inaudible]."
96,But the V2 API is not ready yet as we need more feedback to polish the API. Here are
96,"some tips about when to pick the V2 API. So if you want [inaudible], the catalogue function"
96,"it is, so it has to be the V2 because V1 API doesn’t have this ability. If you want to"
96,"support both versions streaming in your data source, then you should use V2 because in"
96,"V1, the streaming and the [inaudible] are different APIs which makes it harder to reuse"
96,"the code. And if you are sensitive to the scan performance, then you can try the V2"
96,"API because it allows you to report the data provisioning to [inaudible] in Spark, and"
96,also it allows you to implement [inaudible] reader for better performance.
96,If you don’t
96,"care about this stuff and just want to [inaudible] source once and you change it, please use"
96,the V1 as V2 is not very stable.
96,Extensibility and Ecosystem
96,"The ecosystem also evolves very fast. In this release, Spark"
96,can be better integrated into the ecosystem by supporting the newer version of these common
96,"components like Java 11, Hadoop 3, Hadoop 3 [inaudible], and Hadoop 2.3 [inaudible]."
96,I want to mention some breaking changes here.
96,"Starting from this release, we’re only building Spark with Scala 2.12, so Scala 2.11 is no longer [inaudible]. And we deprecated Python 2 too because it is end of life. In the download image, we put a build of Spark with different"
96,"[inaudible] and Hadoop combinations. By default, it will be Hadoop 2.7 and it would have 2.3"
96,[exclusion?]. There are another two [companies?] of previews available. One is Hadoop 2.7 and
96,"Hadoop 1.2 execution, which is for people who can’t upgrade their end forms. The other"
96,"is Hadoop 3.2 and Hadoop 2.3 execution, which is for people who want to try Hadoop"
96,3. We also extend the support for different Hadoop and Hive versions from 0.12 to 3.1.
96,Documentation Improvements
96,Documentation improvements is the last existing news I want to share
96,with everyone. How to read on a standard the web UI is a common question to many new Spark
96,users. This is especially true for Spark SQL users and Spark streaming users. They are
96,"using the [inaudible]. They usually don’t know what it is, and what our jobs [inaudible]."
96,"Also, the [inaudible] are using many queries and matrix names, which are not very clear"
96,"to many users. Starting from this release, we add a new section for [inaudible] reading"
96,the web UI. It includes the [inaudible] job page and [inaudible] and also SQL streaming
96,"[inaudible]. This is just a start. We will continue to enhance it, then SQL reference."
96,"Finally, this release already has a SQL reference for Spark SQL. Spark SQL"
96,"is the most popular and important component in Spark. However, we did not have our own"
96,SQL reference to define the SQL [semantic?] and detailed behaviors. Let me quickly go
96,over the major chapters in SQL reference. So we have a page to explain the ANSI components
96,"of Spark. So as I mentioned before, we have SQL compatibility, but to avoid [correcting?]"
96,"the [effecting?] queries, we make it optional. So you can only enable the ANSI compatibility"
96,by enabling this flag.
96,"You also have a page to explain the detailed semantic of each [inaudible],"
96,so you can know what it means and what’s the behavior of them. You also have a page to
96,explain the data and partner strings used for formatting and parsing functions [inaudible].
96,There’s also a page to give the document for each function in Spark. We also have a page
96,"to explain the syntax, how to define the table or function [inaudible]. Also, there’s a page"
96,to explain the syntax and the semantics of each [inaudible] in Spark SQL.
96,"Also, there’s"
96,a page to explain the null semantic. The null is a very special value in Spark SQL and other
96,ecosystems. So there must be a page to either explain what’s the meaning of null in the
96,"null queries. Also, we have a page to explain the syntax for all the commands, like DDL"
96,"and DML commands, and also insert is also included in the document. In fact, SELECT has so many features, so we want to have a page to explain all of them. Yeah, there are"
96,also a page for other special commands like SHOW TABLES.
96,"Finally, [inaudible]. This is another critical enhancements in Spark"
96,"3.0 document. In this release, all the components have [inaudible] guides. When you upgrade"
96,"your Spark version, you can read them carefully, and, in fact, [inaudible]. You might be wondering"
96,why it is much longer than the previous version. It’s because we try to document all the important
96,"looking changes you want to hear. If you upgrade into some errors, that’s another wordy document"
96,"or slightly confusing error message, please open a ticket, and we will try and fix it"
96,in subsequent releases.
96,"Now, that Spark is almost 10 years old now. The Spark community"
96,is very serious about making change. And we try our best to avoid [inaudible] changing.
96,"If you upgrade to Spark 3.0 at this time, you may see explicit error messages about"
96,changing. So the error message also provides config names for you to either go back to existing behavior or go with the new behavior.
96,"this talk, we talked about many exciting features and improvements in"
96,"Spark 3.0. Due to the lack of time, there are still many other nice features not being"
96,covered by this talk. Please download Spark 3.0 and try yourself.
96,You can also try the
96,[inaudible] Databricks [inaudible] 10.0 beta. All the new features are already available.
96,"The Community Edition is for free. Without the contributions by the whole community,"
96,it is impossible to deliver such a successful release. It’s thanks to all the Spark committers
96,"all over the world. Thank you. Thank you, everyone."
96,Watch more Spark + AI sessions here
96,Try Databricks for free
96,« back
96,About Xiao Li
96,Databricks
96,"Xiao Li is an engineering manager, Apache Spark Committer and PMC member at Databricks. His main interests are on Spark SQL, data replication and data integration. Previously, he was an IBM master inventor and an expert on asynchronous database replication and consistency verification. He received his Ph.D. from University of Florida in 2011."
96,About Wenchen Fan
96,Databricks
96,"Wenchen Fan is a software engineer at Databricks, working on Spark Core and Spark SQL. He mainly focuses on the Apache Spark open source community, leading the discussion and reviews of many features/fixes in Spark. He is a Spark committer and a Spark PMC member."
96,Video Archive
96,Terms of Use
96,Privacy Policy
96,Event Policy
96,Looking for a talk from a past event? Check the Video Archive
96,Organized by Databricks
96,"If you have questions, or would like information on sponsoring a Spark + AI Summit, please contact organizers@spark-summit.org."
96,"Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation."
96,The Apache Software Foundation has no affiliation with and does not endorse the materials provided at this event.
97,Spark tips. DataFrame API - Blog | luminousmen
97,Spark tips. DataFrame API
97,Last updated
97,Thu May 07 2020
97,"There are many different tools in the world, each of which solves a range of problems. Many of them are judged by how well and correct they solve this or that problem, but there are tools that you just like, you want to use them. They are properly designed and fit well in your hand, you do not need to dig into the documentation and understand how to do this or that simple action. About one of these tools for me I will be writing this series of posts."
97,I will describe the optimization methods and tips that help me solve certain technical problems and achieve high efficiency using Apache Spark. This is my updated collection.
97,"Many of the optimizations that I will describe will not affect the JVM languages ​​so much, but without these methods, many Python applications may simply not work."
97,Whole series:
97,Spark tips. DataFrame API
97,Spark Tips. Don't collect data on driver
97,The 5-minute guide to using bucketing in Pyspark
97,Spark Tips. Partition Tuning
97,Use DataFrame API
97,We know that RDD is a fault-tolerant collection of elements that can be processed in parallel. But RDDs actually kind of black box of data — we know that it holds some data but we do not know the type of the data or any other properties of the data. Hence it's data cannot be optimized as well as the operations on it.
97,"Spark 1.3 introduced a new abstraction — a DataFrame, in Spark 1.6 the Project Tungsten was introduced, an initiative which seeks to improve the performance and scalability of Spark. DataFrame data is organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood."
97,Using DataFrame API you get:
97,Efficient serialization/deserialization and memory usage
97,"In RDDs Spark uses Java serialization when it is necessary to distribute data across a cluster. The serialization of individual Scala and Java objects is expensive. In Pyspark, it has become more expensive when all data is double-serialized/deserialized to Java/Scala and then to Python (using cloudpickle) and back."
97,"The cost of double serialization is the most expensive part when working with Pyspark. For distributed systems such as Spark, most of our time is spent only on serialization of data. Actual computing is often not a big blocking part — most of it is just serializing things and shuffling them around. We want to avoid this double cost of serialization."
97,"In RDD we have this double serialization, and we pay for it on every operation — wherever we need to distribute data into a cluster or call the python function. It also requires both data and structure to be transferred between nodes."
97,"With the DataFrame API, everything is a bit different. Since Spark understands data structure and column types if they are represented as DataFrame, he can understand how to store and manage them more efficiently(Java objects have a large inherent memory overhead as well as JVM GC)."
97,"The DataFrame API does two things that help to do this (through the Tungsten project). First, using off-heap storage for data in binary format. Second, generating encoder code on the fly to work with this binary format for your specific objects."
97,"In simple words, Spark says:"
97,"I'm going to generate these efficient encoders so that I can pull the data right out in this compact format and process it in that format right through the transformation chain. I'm going to minimize the amount of work I'm required to do in JVM (no, I'm not going to do anything in Python) and maximize the amount of data I can process in my compact format stored in off-heap memory. It turns out that you don't pay a penalty for serialization/deserialization — you work directly on this compact format, and using off-heap memory is a great way to reduce GC pauses because it is not in the scope of GC."
97,"So it's great that we can store our data inside the JVM, but we can still write code in Python using a very clear API and it will still be very efficient!"
97,Schema Projection
97,The RDD API explicitly uses schematic projection. Therefore the user needs to define the schema manually.
97,"There's no need to specify the schema explicitly in DataFrame. As a rule, Spark can detect the schema automatically(inferSchema option). But the schema's resolution depends mainly on the data sources. If the source should contain structured data (i.e. relational database), the schema is extracted directly without any guesswork."
97,"More complex operations apply to semi-structured data, such as JSON files. In these cases the schema is guesswork."
97,Optimizations
97,"RDD cannot be optimized by Spark — they are fully lambda driven. RDD is rather a ""black box"" of data, which cannot be optimized because Spark can' t look inside what the data actually consists of. So Spark can't do any optimizations on your behalf."
97,This feels more acute in non-JVM languages such as Python.
97,"DataFrame has additional metadata due to its column format, which allows Spark to perform certain optimizations on a completed request. Before your query is run, a logical plan is created using Catalyst Optimizer and then it's executed using the Tungsten execution engine."
97,What is Catalyst?
97,Catalyst Optimizer — the name of the integrated query optimizer and execution scheduler for Spark Datasets/DataFrame.
97,"Catalyst Optimizer is the place where most of the ""magic"" tends to improve the speed of your code execution by logically improving it. But in any complex system, unfortunately, ""magic"" is not enough to always guarantee optimal performance. As with relational databases, it is useful to learn a little bit about how the optimizer works to understand how it plans and customizes your applications."
97,"In particular, Catalyst Optimizer can perform refactoring of complex queries. However, almost all of its optimizations are qualitative and rule-based rather than quantitative and statistical. For example, Spark knows how and when to do things like combine filters or move filters before joining. Spark 2.0 even allows you to define, add, and test your own additional optimization rules at runtime."
97,Catalyst Optimizer supports both rule-based and cost-based optimization.
97,"Cost-Based Optimizer(CBO): If an SQL query can be executed in two different ways (e.g. #1 and #2 for the same original query), then CBO essentially calculates the cost of each path and analyzes which path is cheaper, and then executes that path to improve the query execution."
97,"Rule-Based optimizer(RBO): follows different optimization rules that apply depending on the query. These include constant folding, predicate pushdown, projection pruning, null propagation, Boolean simplification, and other rules."
97,"In fact, there is no separation inside the Spark, the two approaches work together, cost-based optimization is performed by generating multiple plans using rules, and then computing their costs."
97,"As an example, imagine that we have users (they come from the database) and their transactions (I will generate some random values, but it could be a database as well):"
97,transactions = spark.range(160000)\
97,".select(F.col('id').alias('key'), F.rand(12).alias('value'))"
97,users = spark.read.jdbc(
97,"table='users', url='jdbc:postgresql://localhost:5432/postgres')"
97,"users.join(transactions, 'key')\"
97,.filter(F.col('key') > 100)\
97,.explain()
97,"We see here that our developer in a programming rush wrote user filtering after join. It would be more reasonable to do the filter first and then join because the shuffle data will be reduced. But if we see the physical plan, we see that Catalyst Optimizer did it for us. What's more, it did a predicate pushdown of our filter to the database (Spark will try to move the filtering data as close to the source as possible to avoid loading unnecessary data into memory, see PushedFilters)."
97,== Physical Plan ==
97,"*(5) SortMergeJoin [cast(key#6 as bigint)], [key#2L], Inner"
97,":- *(2) Sort [cast(key#6 as bigint) ASC NULLS FIRST], false, 0"
97,"+- Exchange hashpartitioning(cast(key#6 as bigint), 200)"
97,+- *(1) Filter (cast(key#6 as int) > 100)
97,"+- *(1) Scan JDBCRelation(users) [numPartitions=1] [key#6,name#7] PushedFilters: [*IsNotNull(key)], ReadSchema: struct<key:string,name:string>"
97,"+- *(4) Sort [key#2L ASC NULLS FIRST], false, 0"
97,"+- Exchange hashpartitioning(key#2L, 200)"
97,"+- *(3) Project [id#0L AS key#2L, rand(12) AS value#3]"
97,"+- *(3) Range (0, 160000, step=1, splits=1)"
97,"Spark does not ""own"" any storage, so it does not build indexes, B-trees, etc. on the disk. (although support for parquet files, if used well, may give you some related features). Spark has been optimized for the volume, variety, etc. of big data — so, traditionally, it is not designed to maintain and use statistics. about the dataset. For example, in cases where the DBMS may know that a particular filter will remove most records and apply it early in the query, Spark does not know this fact and will not perform such optimization."
97,transactions = spark.range(160000) \
97,".select(F.col('id').alias('key'), F.rand(12).alias('value'))"
97,users = spark.read.jdbc(
97,"table='users', url='jdbc:postgresql://localhost:5432/postgres')"
97,"users.join(transactions, 'key') \"
97,.drop_duplicates(['key']) \
97,.explain()
97,This gives us one more Exchange(shuffle) but not a predicate pushdown of distinct operation into the database. We need to rewrite it ourselves to do distinct first.
97,== Physical Plan ==
97,"*(6) HashAggregate(keys=[key#6], functions=[first(key#2L, false), first(value#3, false)])"
97,"+- Exchange hashpartitioning(key#6, 200)"
97,"+- *(5) HashAggregate(keys=[key#6], functions=[partial_first(key#2L, false), partial_first(value#3, false)])"
97,"+- *(5) SortMergeJoin [cast(key#6 as bigint)], [key#2L], Inner"
97,":- *(2) Sort [cast(key#6 as bigint) ASC NULLS FIRST], false, 0"
97,"+- Exchange hashpartitioning(cast(key#6 as bigint), 200)"
97,"+- *(1) Scan JDBCRelation(users) [numPartitions=1] [key#6] PushedFilters: [*IsNotNull(key)], ReadSchema: struct<key:string>"
97,"+- *(4) Sort [key#2L ASC NULLS FIRST], false, 0"
97,"+- Exchange hashpartitioning(key#2L, 200)"
97,"+- *(3) Project [id#0L AS key#2L, rand(12) AS value#3]"
97,"+- *(3) Range (0, 160000, step=1, splits=1)"
97,"To sum up, use Spark DataFrames, release the power of [Catalyst Optimizer] (https://databricks.com/glossary/catalyst-optimizer) and [Tungsten execution engine] (https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html). You do not have to waste time tuning your RDDs, you can focus on the business problem."
97,Understandable code
97,The code in RDD expresses more how of a solution better than what. It is a bit difficult to read the code based on RDD and understand what is going on.
97,"Using DataFrame API it's easy to go one level up — to business logic. Since DataFrame API looks like SQL and there is a schema, it's easy to see the real work with data. So, using DataFrame API, you are closer to what you are trying to achieve and not to how you are doing it."
97,"You don't have to worry about how the DBMS find out if you need to perform a table scan or which indexes to use — all you need is a result. You express what you want, and you let Spark under the cover find the most effective way to do it."
97,DataFrame-based API is the primary API for MLlib
97,"As of Spark 2.0, the RDD-based APIs in the spark.mllib package have entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package."
97,Recommended books
97,Spark: The Definitive Guide
97,Learning Spark: Lightning-Fast Data Analytics
97,#big_data
97,#spark
97,#python
97,Buy me a coffee
97,"More? Well, there you go:"
97,Azure Blob Storage with Pyspark
97,Data Challenges in Big Data
97,The ultimate Python style guidelines
97,Tags
97,About
97,License
97,Subscribe
97,Support author
97,Telegram channel
97,RSS
97,Feedly
97,© Copyright luminousmen.com All Rights Reserved
98,Server tuning — Nextcloud latest Administration Manual latest documentation
98,Introduction
98,Release notes
98,Maintenance and release schedule
98,Installation and server configuration
98,System requirements
98,Deployment recommendations
98,Installation on Linux
98,Installation wizard
98,Installing from command line
98,Supported apps
98,SELinux configuration
98,Nginx configuration
98,Hardening and security guidance
98,Server tuning
98,Using cron to perform background jobs
98,Reducing system load
98,Caching
98,Using MariaDB/MySQL instead of SQLite
98,Using Redis-based transactional file locking
98,TLS / encryption app
98,Enable HTTP/2 for faster loading
98,Tune PHP-FPM
98,Enable PHP OPcache
98,Example installation on Ubuntu 20.04 LTS
98,Example installation on CentOS 8
98,Example installation on OpenBSD
98,Nextcloud configuration
98,Apps management
98,User management
98,File sharing and management
98,File workflows
98,Groupware
98,Database configuration
98,Mimetypes management
98,Maintenance
98,Issues and troubleshooting
98,GDPR
98,Nextcloud latest Administration Manual
98,Installation and server configuration »
98,Server tuning
98,Edit on GitHub
98,Server tuning¶
98,Using cron to perform background jobs¶
98,See Background jobs for a description and the
98,benefits.
98,Reducing system load¶
98,High system load will slow down Nextcloud and might also lead to other unwanted
98,side effects. To reduce load you should first identify the source of the problem.
98,"Tools such as htop, iotop, netdata or"
98,glances
98,will help to identify the process or the drive that slows down your system. First
98,you should make sure that you installed/assigned enough RAM. Swap usage should be
98,"prevented by all means. If you run your database inside a VM, you should not"
98,store it inside a VM image file. Better put it on a dedicated block device to
98,reduce latency due to multiple abstraction layers.
98,Caching¶
98,"Caching improves performance by storing data, code, and other objects in memory."
98,Memory cache configuration for the Nextcloud server must be installed and configured.
98,See Memory caching.
98,Using MariaDB/MySQL instead of SQLite¶
98,MySQL or MariaDB are preferred because of the performance limitations of
98,"SQLite with highly concurrent applications, like Nextcloud."
98,See the section Database configuration for how to
98,configure Nextcloud for MySQL or MariaDB. If your installation is already running on
98,SQLite then it is possible to convert to MySQL or MariaDB using the steps provided
98,in Converting database type.
98,"For more details and help tuning your database, check this article at MariaDB."
98,Using Redis-based transactional file locking¶
98,"File locking is enabled by default, using the database locking backend. This"
98,places a significant load on your database. See the section
98,Transactional file locking for how to
98,configure Nextcloud to use Redis-based Transactional File Locking.
98,TLS / encryption app¶
98,TLS (HTTPS) and file encryption/decryption can be offloaded to a processor’s
98,AES-NI extension. This can both speed up these operations while lowering
98,processing overhead. This requires a processor with the AES-NI instruction set.
98,Here are some examples how to check if your CPU / environment supports the
98,AES-NI extension:
98,For each CPU core present: grep flags /proc/cpuinfo or as a summary for
98,all cores: grep -m 1 '^flags' /proc/cpuinfo If the result contains any
98,"aes, the extension is present."
98,Search eg. on the Intel web if the processor used supports the extension
98,Intel Processor Feature Filter You may set a filter by
98,"""AES New Instructions"" to get a reduced result set."
98,"For versions of openssl >= 1.0.1, AES-NI does not work via an engine and"
98,will not show up in the openssl engine command. It is active by default
98,on the supported hardware. You can check the openssl version via openssl
98,version -a
98,If your processor supports AES-NI but it does not show up eg via grep or
98,"coreinfo, it is maybe disabled in the BIOS."
98,"If your environment runs virtualized, check the virtualization vendor for"
98,support.
98,Enable HTTP/2 for faster loading¶
98,HTTP/2 has huge speed improvements over HTTP with multiple request. Most browsers already support HTTP/2 over TLS (HTTPS). Refer to your web server manual for guides on how to enable HTTP/2.
98,Tune PHP-FPM¶
98,If you are using a default installation of PHP-FPM you might have noticed
98,excessive load times on the web interface or even sync issues. This is due
98,to the fact that each simultaneous request of an element is handled by a
98,separate PHP-FPM process. So even on a small installation you should allow
98,more processes to run in parallel to handle the requests.
98,This link can help you calculate the good values for your system.
98,Enable PHP OPcache¶
98,The OPcache improves the performance of PHP applications by caching precompiled bytecode. We recommend at least the following settings:
98,opcache.enable = 1
98,opcache.interned_strings_buffer = 8
98,opcache.max_accelerated_files = 10000
98,opcache.memory_consumption = 128
98,opcache.save_comments = 1
98,opcache.revalidate_freq = 1
98,For more details check out the official documentation or this blog post about some recommended settings.
98,Next
98,Previous
98,© Copyright 2021 Nextcloud GmbH.
98,Read the Docs
98,v: latest
98,Versions
98,stable
98,latest
98,Downloads
98,On Read the Docs
98,Project Home
98,Builds
99,VMware Knowledge Base
99,LoadingÃ—Sorry to interruptCSS ErrorRefresh
