data_directory	Server Configuration data_directory  ( string )  Specifies the directory to use for data storage.  If you wish to keep the configuration files elsewhere than the data directory, the  postgres   -D command-line option or  PGDATA  environment variable must point to the directory containing the configuration files, and the  data_directory  parameter.  Notice that data_directory  overrides  -D  and  PGDATA  for the location of the data directory, but not for the location of the configuration files.  If all three parameters plus  data_directory  are explicitly set, then it is not necessary to specify  -D  or  PGDATA .  Notes This command can't be used to set  data_directory , nor parameters that are not allowed in postgresql
hba_file	  pg_hba_file_rules .   pg_hba_file_rules  Columns .  hba_file  ( string )  Specifies the configuration file for host-based authentication (customarily called pg_hba.  If you wish, you can specify the configuration file names and locations individually using the parameters  config_file ,  hba_file  and/or  ident_file .  It is possible to place the authentication configuration file elsewhere, however; see the hba_file  configuration parameter.  The system view  pg_hba_file_rules  can be helpful for pre-testing changes to the pg_hba.   pg_hba_file_rules .   pg_hba_file_rules The view  pg_hba_file_rules  provides a summary of the contents of the client authentication configuration file,  pg_hba.  By default, the  pg_hba_file_rules  view can be read only by superusers.   pg_hba_file_rules  Columns Name Type Description line_number integer Line number of this rule in pg_hba.  Ł Fix rare  ﬁlost saved point in indexﬂ  errors in scans of multicolumn GIN indexes (Tom Lane) Ł Fix unportable use of  getnameinfo()  in  pg_hba_file_rules  view (Tom Lane) On FreeBSD 11, and possibly other platforms, the view's  address  and  netm.  Ł Add view  pg_hba_file_rules  to display the contents of  pg_hba. conf,  568 pg_hba_file_rules,  1973 pg_ident
ident_file	 ident_file  ( string )  Specifies the configuration file for user name mapping (customarily called  pg_ident.  If you wish, you can specify the configuration file names and locations individually using the parameters  config_file ,  hba_file  and/or  ident_file .  (It is possible to place the map file elsewhere, however; see the ident_file  configuration parameter. Index I icount,  2373 ICU,  458 ,  603 ,  1422 ident,  580 identifier length,  31 syntax of,  31 IDENTIFY_SYSTEM,  2008 ident_file configuration parameter,  509 idle_in_transaction_session_timeout configuration parameter,  553 idx,  2373 IFNULL,  282
external_pid_file	 external_pid_file  ( string )  Specifies the name of an additional process-ID (PID) file that the server should create for use by server administration programs
port	 Bug Reporting Guidelines .  What to Report .  Where to Report Bugs .  Enum Support Functions .  psql Support .  Supported Platforms .  Error Reporting and Logging .  Locale Support .  Collation Support .  Character Set Support .  Progress Reporting .  SSL Support .  Interface Support Functions .  Synchronous Replication Support for Logical Decoding .  Reporting Errors Within the Server .  Native Language Support .  Sampling Method Support Functions .  Date/Time Support .  Supported Features .  Unsupported Features .  Enum Support Functions .  B-tree Support Functions .  Hash Support Functions .  GiST Support Functions .  SP-GiST Support Functions .  GIN Support Functions .  BRIN Support Functions .  Event Trigger Support by Command Tag .  Procedure and Support Numbers for Minmax Operator Classes .  Procedure and Support Numbers for Inclusion Operator Classes .  Supported Algorithms for  crypt() .  It describes all the functionality that the current version of PostgreSQL officially supports.  This part supports the other parts with structured information sorted by command or program.  It supports a large part of the SQL standard and offers many modern features: Ł complex queries Ł foreign keys Ł triggers Ł updatable views Ł transactional integrity Ł multiversion concurrency control Also, PostgreSQL can be extended by the user in .  Version 3 appeared in 1991 and added support for multiple storage managers, an improved query executor, and a rewritten rule system.  For the most part, subsequent releases until Postgres95 (see below) focused on portability and reliability.  It became increasingly obvious that maintenance of the prototype code and support was taking up large amounts of time that should have been devoted to database research.  In an effort to reduce this support burden, the Berkeley POSTGRES project officially ended with Version 4. ) Subqueries were not supported until PostgreSQL (see below), but they could be imitated in Postgres95 with user-defined SQL functions.  Support for the  GROUP BY  query clause was also added.  Ł A new front-end library,  libpgtcl , supported Tcl-based clients.  As such, it depends on the user community for ongoing support.  Bug Reporting Guidelines When you find a bug in PostgreSQL we want to hear about it.  Your bug reports play an important part in making PostgreSQL more reliable because even the utmost care cannot guarantee that every part of PostgreSQL will work on every platform under every circumstance.  The following suggestions are intended to assist you in forming bug reports that can be handled in an effective fashion.  Or perhaps it is simply too hard and there are more important things on the agenda.  If you need help immediately, consider obtaining a commercial support contract.  Identifying Bugs Before you report a bug, please read and re-read the documentation to verify that you can really do whatever it is you are trying.  If it is not clear from the documentation whether you can do something or not, please report that too; it is a bug in the documentation.  Ł PostgreSQL fails to compile, build, or install according to the instructions on supported platforms.  If you cannot decode the information on the TODO list, report your problem.  What to Report The most important thing to remember about bug reporting is to state all the facts and only facts.  Reporting the bare facts is relatively straightforward (you can probably copy and paste them from the screen) but all too often important details are left out because someone thought it does not matter or the report would be understood anyway.  The following items should be contained in every bug report: Ł The exact sequence of steps  from program start-up  necessary to reproduce the problem.  Note If you are reporting an error message, please obtain the most verbose form of the message.  Note In case of fatal errors, the error message reported by the client might not contain all the information available.  Ł The output you expected is very important to state.  Most executable programs also support a  --version  option; at least  postgres --version  and  psql --version  should work.  We can only provide limited support for sites using older releases of PostgreSQL; if you require more than we can provide, consider acquiring a commercial support contract.  In most cases it is sufficient to report the vendor and version, but do not assume everyone knows what exactly  ﬁDebianﬂ  contains or that everyone runs on x86_64.  Do not be afraid if your bug report becomes rather lengthy.  It is better to report everything the first time than us having to squeeze the facts out of you.  Here is an article 9  that outlines some more tips on reporting bugs.  When writing a bug report, please avoid confusing terminology.  Where to Report Bugs In general, send bug reports to the bug report mailing list at <pgsql-bugs@lists.  Another method is to fill in the bug report web-form available at the project's  web site 10 .  Entering a bug report this way causes it to be mailed to the  <pgsql-bugs@lists. Preface If your bug report has security implications and you'd prefer that it not become immediately visible in public archives, don't send it to  pgsql-bugs .  Security issues can be reported privately to <security@postgresql.  Do not send bug reports to any of the user mailing lists, such as <pgsql-sql@lists.  These mailing lists are for answering user questions, and their subscribers normally do not wish to receive bug reports.  More importantly, they are unlikely to fix them.  Also, please do  not  send reports to the developers' mailing list <pgsql-hackers@lists.  This list is for discussing the development of PostgreSQL, and it would be nice if we could keep the bug reports separate.  We might choose to take up a discussion about your bug report on  pgsql-hackers , if the problem needs more review.  If you have a problem with the documentation, the best place to report it is the documentation mailing list  <pgsql-docs@lists.  If your bug is a portability problem on a non-supported platform, send mail to <pgsql-hackers@lists. org> , so we (and you) can work on porting PostgreSQL to your platform.  This part is mainly intended to give you some hands-on experience with important aspects of the PostgreSQL system.  Ł Using an existing graphical frontend tool like pgAdmin or an office suite with ODBC or JDBC support to create and manipulate a database.  For the purposes of this tutorial that is not important.  Whereas columns have a fixed order in each row, it is important to remember that SQL does not guarantee the order of the rows within the table in any way (although they can be explicitly sorted for display). ) PostgreSQL supports the standard SQL types  int ,  smallint ,  real ,  double precision , char( N ) ,  varchar( N ) ,  date ,  time ,  timestamp , and  interval , as well as other types of general utility and a rich set of geometric types.  Consequently, type names are not key words in the syntax, except where required to support special cases in the SQL standard. The SQL Language Like most other relational database products, PostgreSQL supports  aggregate functions .  It is important to understand the interaction between aggregates and SQL's  WHERE  and  HAVING clauses. 00     WHERE name = (SELECT branch_name FROM accounts WHERE name =  'Bob'); The details of these commands are not important here; the important point is that there are several separate updates involved to accomplish this rather simple operation. , on disk) before the transaction is reported complete.  Another important property of transactional databases is closely related to the notion of atomic updates: when multiple transactions are running concurrently, each one should not be able to see the incomplete changes made by others.  We already saw that  ORDER BY  can be omitted if the ordering of rows is not important.  There is another important concept associated with window functions: for each row, there is a set of rows within its partition called its  window frame .  Many of the commands that we have already discussed Š  SELECT ,  UPDATE , and  DELETE  Š support this  ONLY  notation.  The rest treats several aspects that are important for tuning a database for optimal performance.  Enum Support Functions .  psql Support .  Note that dollar signs are not allowed in identifiers according to the letter of the SQL standard, so their use might render applications less portable.  If you want to write portable applications you are advised to always quote a particular name or never quote it.  String Constants with Unicode Escapes PostgreSQL also supports another type of escape syntax for strings that allows specifying arbitrary Unicode characters by code point.  However there are corner cases in which a query might change behavior without any parsing error being reported.  Some client libraries also support specifying data values separately from the SQL command string, in which case parameters are used to refer to the out-of-line data values. ) An important special case is extracting a field from a table column that is of a composite type: (compositecol).  Window Function Calls A  window function call  represents the application of an aggregate-like function over some portion of the rows selected by a query.  Obviously, this is not something that a portable application should rely on.  PostgreSQL also supports  mixed  notation, which combines positional and named notation.  The remaining details of this function definition are not important here (see  Chapter 37  for more information).  Note PostgreSQL does not support  CHECK  constraints that reference table data other than the new or updated row being checked.  The  NULL  constraint is not present in the SQL standard and should not be used in portable applications.  So be careful when developing applications that are intended to be portable.  For complete information on the different types of privileges supported by PostgreSQL, refer to the GRANT  reference page.  In some contexts it is important to be sure that row security is not being applied.  If there is no match in the search path, an error is reported, even if matching table names exist in other schemas in the database.  There are a few usage patterns easily supported by the default configuration: Ł Constrain ordinary users to user-private schemas. Data Definition system that implements only the basic schema support specified in the standard.  Of course, some SQL database systems might not implement schemas at all, or provide namespace support by allowing (possibly limited) cross-database access.  If you need to work with those systems, then maximum portability would be achieved by not using schemas at all.  Many of the commands that we have already discussed Š  SELECT ,  UPDATE  and  DELETE  Š support the  ONLY  keyword.  However, this syntax is still supported for compatibility with older releases where the default could be changed.  If a foreign table is part of an inheritance hierarchy then any operations not supported by the foreign table are not supported on the whole hierarchy either.  RENAME ) typically default to including child tables and support the  ONLY  notation to exclude them. ,  REINDEX ,  VACUUM ) typically only work on individual, physical tables and do not support recursing over inheritance hierarchies.  Table Partitioning PostgreSQL supports basic table partitioning. Data Definition PostgreSQL offers built-in support for the following forms of partitioning: Range Partitioning The table is partitioned into  ﬁrangesﬂ  defined by a key column or set of columns, with no overlap between the ranges of values assigned t.  Currently supported partitioning methods include range and list, where each partition is assigned a range of keys and a list of keys, respectively.  Ł Using  ONLY  to add or drop a constraint on only the partitioned table is supported when there are no partitions.  Once partitions exist, using  ONLY  will result in an error as adding or dropping constraints on only the partitioned table, when partitions exist, is not supported.  One of the most important advantages of partitioning is precisely that it allows this otherwise painful task to be executed nearly instantaneously by manipulating the partition structure, rather than physically moving large amounts of data around.  It might also be a useful time to aggregate data into smaller formats, perform other data manipulations, or run reports.  Ł Since primary keys are not supported on partitioned tables, foreign keys referencing partitioned tables are not supported, nor are foreign key references from a partitioned table to some other table.  There is no support for enforcing uniqueness (or an exclusion constraint) across an entire partitioning hierarchy.  Partitioning can be implemented using table inheritance, which allows for several features which are not supported by declarative partitioning, such as: Ł Partitioning enforces a rule that all partitions must have exactly the same set of columns as .  Ł Declarative partitioning only supports list and range partitioning, whereas table inheritance allows data to be divided in a manner of the user's choosing.  When choosing how to partition your table, it's also important to consider what changes may occur in the future.  It is also important to consider the overhead of partitioning during query planning and execution.  With either of these two types of workload, it is important to make the right decisions early, as re-partitioning large quantities of data can be painfully slow.  Foreign Data PostgreSQL implements portions of the SQL/MED specification, allowing you to access data that resides outside PostgreSQL using regular SQL queries. Data Definition To access foreign data, you need to create a  foreign server  object, which defines how to connect to a particular external data source according to the set of options used by its supporting foreign data wrapper. Data Definition Almost all  DROP  commands in PostgreSQL support specifying  CASCADE .  The INSERT ,  UPDATE , and  DELETE  commands all have an optional  RETURNING  clause that supports this.  However, it is supported for compatibility with older releases.  To support this, the table function can be declared as returning the pseudo- type  record  with no  OUT  parameters.  The  JOIN  syntax in the  FROM  clause is probably not as portable to other SQL database management systems, even though it is in the SQL standard.   LIMIT  and  OFFSET LIMIT  and  OFFSET  allow you to retrieve just a portion of the rows that are generated by the rest of the query: SELECT  select_list     FROM  table_expression     [ ORDER BY .  When using  LIMIT , it is important to use an  ORDER BY  clause that constrains the result rows into a unique order. sub_part   ) SELECT sub_part, SUM(quantity) as total_quantity FROM included_parts GROUP BY sub_part When working with recursive queries it is important to be sure that the recursive part of the query will eventually return no tuples, or else the quer.  The number of affected rows reported to the client would only include rows removed from  bar .  Trying to update the same row twice in a single statement is not supported.  If you're concerned about portability, always specify the precision and scale explicitly.  In practice, these types are usually implementations of IEEE Standard 754 for Binary Floating-Point Arithmetic (single and double precision, respectively), to the extent that the underlying processor, operating system, and compiler support it.  Ł If you want to do complicated calculations with these types for anything important, especially if you rely on certain behavior in boundary cases (infinity, underflow), you should evaluate the implementation carefully.  With the default value of  0 , the output is the same on every platform supported by PostgreSQL.  Increasing it will produce output that more accurately represents the stored value, but may be unportable.  PostgreSQL also supports the SQL-standard notations  float  and  float( p )  for specifying inexact numeric types.  The data types  smallserial ,  serial  and  bigserial  are not true types, but merely a notational convenience for creating unique identifier columns (similar to the  AUTO_INCREMENT  property supported by some other databases).  The database character set determines the character set used to store textual values; for more information on character set support, refer to Section 23.  The  bytea  type supports two formats for input and output:  ﬁhexﬂ  format and PostgreSQL's historical ﬁescapeﬂ  format.  Date/Time Types PostgreSQL supports the full set of SQL date and time types, shown in  Table 8.  For some formats, ordering of day, month, and year in date input is ambiguous and there is support for specifying the expected ordering of these fields. Data Types January 8 04:05:06 1999 PST is supported.  Special Values PostgreSQL supports several special date/time input values for convenience, as shown in  Table 8.  We do  not  recommend using the type  time with time zone  (though it is supported by PostgreSQL for legacy applications and for compliance with the SQL standard).  They are equivalent to the  enum  types supported in a number of programming languages.  All standard comparison operators and related aggregate functions are supported for enums.  Although enum types are primarily intended for static sets of values, there is support for adding new values to an existing enum type, and for renaming values (see  ALTER TYPE ).  Polygons are very similar to closed paths, but are stored differently and have their own set of support routines.  If the  /y  portion is missing, the netmask is 32 for IPv4 and 128 for IPv6, so the value represents just a single host.  On display, the  /y  portion is suppressed if the netmask specifies a single host.  To convert a traditional 48 bit MAC address in EUI-48 format to modified EUI-64 format to be included as the host portion of an IPv6 address, use macaddr8_set7bit  as shown: SELECT macaddr8_set7bit('08:00:2b:01:02:03');     macaddr8_set7bit      ---.  Text Search Types PostgreSQL provides two data types that are designed to support full text search, which is the activity of searching through a collection of natural-language  documents  to locate those that best match a query .  It is important to understand that the  tsvector  type itself does not perform any word normalization; it assumes the words it is given are normalized appropriately for the application.  Its advantage over storing XML data in a  text field is that it checks the input values for well-formedness, and there are support functions to perform type-safe operations on it; see  Section 9.  There is also currently no built-in support for validating against other XML schema languages such as XML Schema.  In this case, an encoding declaration in the XML data will be observed, and if it is absent, the data will be assumed to be in UTF-8 (as required by the XML standard; note that PostgreSQL does not support UTF-16).  The necessary preprocessing support is, however, not yet available in the PostgreSQL distribution.   jsonb  also supports indexing, which can be a significant advantage.   jsonb  Containment and Existence Testing  containment  is an important capability of  jsonb .  The default GIN operator class for  jsonb  supports queries with top-level key-exists operators  ? ,  ?& and  ?|  operators and path/value-exists operator  @> . ) An example of creating an index with this operator class is: CREATE INDEX idxgin ON api USING GIN (jdoc); The non-default GIN operator class  jsonb_path_ops  supports indexing the  @>  operator only.  While the simple-index approach is far more flexible (since it supports queries about any key), targeted expression indexes are likely to be smaller and faster to search than a simple index.  Although the  jsonb_path_ops  operator class supports only queries with the  @>  operator, it has notable performance advantages over the default operator class  jsonb_ops .  jsonb  also supports  btree  and  hash  indexes.  These are usually useful only if it's important to check equality of complete JSON documents.  Arrays of domains are not yet supported.  The first two only support one-dimensional arrays, but  array_cat  supports multidimensional arrays.  Note however an important restriction of the current implementation: since no constraints are associated with a composite type, the constraints shown in the table definition  do not apply  to values of the composite type outside the table.  Range types' B-tree and hash support is primarily meant to allow sorting and hashing internally in queries, rather than creation of actual indexes.  The  pg_lsn  type supports the standard comparison operators, like  =  and  > .  Some also support polymorphic functions using the types  anyelement ,  anyarray ,  anynonarray ,  anyenum , and anyrange .  To preserve the type safety of this restriction it is important to follow this coding rule: do not create any function that is declared to return internal  unless it has at least one  internal  argument.  If you are concerned about portability then note that most of the functions and operators described in this chapter, with the exception of the most trivial arithmetic and comparison operators and some explicitly marked functions, are not specified b.  Supported formats are:  base64 , hex ,  escape .  Currently the only supported flag is a minus sign ( - ) which will cause the format specifier's output to be left-justified.  The following types are supported: Ł s  formats the argument value as a simple string.  Supported formats are:  base64 , hex ,  escape .  In addition to these facilities borrowed from  LIKE ,  SIMILAR TO  supports these pattern-matching metacharacters borrowed from POSIX regular expressions: Ł |  denotes alternation (either of two alternatives).  The text matching the portion of the pattern between these markers is returned.  It returns null if there is no match, otherwise the portion of the text that matched the pattern.  But if the pattern contains any parentheses, the portion of the text that matched the first parenthesized subexpression (the one whose left parenthesis comes first) is returned.  Supported flags (though not  g ) are described in  Table 9.  Supported flags are described in  Table 9.   regexp_split_to_table  supports the flags described in  Table 9.  PostgreSQL supports both forms, and also implements some extensions that are not in the POSIX standard, but have become widely used due to their availability in programming languages such as Perl and Tcl.  Regular Expression Atoms Atom Description ( re ) (where  re  is any regular expression) matches a match for  re , with the match noted for possible reporting (?: re ) as above, but the match is not noted for reporting (a  ﬁnon-capturingﬂ  set of par.  Ranges are very collating- sequence-dependent, so portable programs should avoid relying on them.  Note PostgreSQL currently does not support multi-character collating elements. 2, and should be used with caution in software intended to be portable to other systems.  However, programs intended to be highly portable should not employ REs longer than 256 bytes, as a POSIX-compliant implementation can refuse to accept such REs. Functions and Operators Pattern Description OF time-zone offset from UTC (only supported in to_char ) Modifiers can be applied to any template pattern to alter its behavior.   to_char  and  to_number  do not support the use of  V  combined with a decimal point (e. 4 to_timestamp( double precision ) timestamp with time zone Convert Unix epoch (seconds since 1970-01-01 00:00:00+00) to timestamp to_timestamp(1284352323) 2010-09-13 04:32:03+00  In addition to these functions, the SQL  OVERLAPS  operator is support.  Enum Support Functions For enum types (described in  Section 8.  Enum Support Functions Function Description Example Example Result enum_first(anyenum) Returns the first value of the input enum type enum_first(null::rainbow) red enum_last(anyenum) Returns the last value of the input enum type enum_last(null::rain.  Geometric Functions and Operators The geometric types  point ,  box ,  lseg ,  line ,  path ,  polygon , and  circle  have a large set of native support functions and operators, shown in  Table 9.   macaddr  Functions Function Return Type Description Example Result trunc( macaddr ) macaddr set last 3 bytes to zero trunc(macaddr '12:34:56:78:90:ab') 12:34:56:00:00:00 The  macaddr  type also supports the standard relational operators ( > ,  <= ,.  This function is equivalent to the  XMLEXISTS  predicate, except that it also offers support for a namespace mapping argument.  A default namespace specification is not currently supported.  If the  column_expression  for a  NOT NULL  column does not match anything and there is no  DEFAULT  or the  default_expression  also evaluates to null, an error is reported.  They can be thought of as XML export functionality: table_to_xml(tbl regclass, nulls boolean, tableforest boolean,  targetns text) query_to_xml(query text, nulls boolean, tableforest boolean,  targetns text) cursor_to_xml(cursor refcursor, count int.  The first format is a proper XML document, which will be important in many applications.  The field/element/path extraction operators that accept integer JSON array subscripts all support negative subscripting from the end of arrays.  To get late-binding behavior, force the constant to be stored as a  text  constant instead of regclass : nextval('foo'::text)       foo  is looked up at runtime Note that late binding was the only behavior supported in PostgreSQL releases before 8.  Important To avoid blocking concurrent transactions that obtain numbers from the same sequence, a  nextval  operation is never rolled back; that is, once a value has been fetched it is considered used and will not be returned again.  (An error is reported if  nextval  has never been called for this sequence in this session.  The value reported by currval  is also set to the specified value.  Furthermore, the value reported by  currval  is not changed in this case.  Important Because sequences are non-transactional, changes made by  setval  are not undone if the transaction rolls back. 2  for more details about which operators support indexed operations.  Aggregate functions which support  Partial Mode  are eligible to participate in various optimizations, such as parallel aggregation.  A query like: SELECT count(*) FROM sometable; will require effort proportional to the size of the table: PostgreSQL will need to scan either the entire table or the entirety of an index which includes all rows in the table.  For example: =>   SELECT * FROM items_sold;  make  | model | sales -------+-------+-------  Foo   | GT    |  10  Foo   | Tour  |  20  Bar   | City  |  15  Bar   | Sport |  5 (4 rows) =>   SELECT make, model, GROUPING(make,model), sum(sales) FROM  it.  Likewise, the standard's  FROM FIRST  or  FROM LAST  option for  nth_value  is not implemented: only the default  FROM FIRST  behavior is supported.  Since the result depends only on whether any rows are returned, and not on the contents of those rows, the output list of the subquery is normally unimportant.  To support matching of rows which include elements without a default B-tree operator class, the following operators are defined for composite type comparison:  *= ,  *<> ,  *< ,  *<= ,  *> , and  *>= . ] inet_client_addr  returns the IP address of the current client, and  inet_client_port returns the port number.   inet_server_addr  returns the IP address on which the server accepted the current connection, and  inet_server_port  returns the port number. Functions and Operators Name Description index_scan Does the index support plain (non-bitmap) scans? bitmap_scan Does the index support bitmap scans? backward_scan Can the scan direction be changed in mid-scan (to support  FETCH BACKWARD  on a cursor.  Index Access Method Properties Name Description can_order Does the access method support  ASC ,  DESC  and related keywords in  CREATE INDEX ? can_unique Does the access method support unique indexes? can_multi_col Does the access method support ind. 69  provide server transaction information in an exportable form. Functions and Operators Name Return Type Description txid_visible_in_snapshot( bigint , txid_snapshot ) boolean is transaction ID visible in snapshot? (do not use with subtransaction ids) txid_status( bigint ) text report the status of the given tran.  However, these functions export a 64-bit format that is extended with an  ﬁepochﬂ  counter so it will not wrap around during the life of an installation.  txid_status(bigint)  reports the commit status of a recent transaction.  The status of a transaction will be reported as either  in progress ,  committed , or  aborted , provided that the transaction is recent enough that the system retains the commit status of that transaction.  Note that prepared transactions are reported as  in progress ; applications must check  pg_prepared_xacts  if they need to determine whether the txid is a prepared transaction.  To solve this problem, PostgreSQL allows a transaction to  export  the snapshot it is using.  As long as the exporting transaction remains open, other transactions can  import  its snapshot, and thereby be guaranteed that they see exactly the same view of the database that the first transaction sees.  Snapshots are exported with the  pg_export_snapshot  function, shown in  Table 9. 82 , and imported with the  SET TRANSACTION  command.  Snapshot Synchronization Functions Name Return Type Description pg_export_snapshot() text Save the current snapshot and return its identifier The function  pg_export_snapshot  saves the current snapshot and returns a  text  string identifying the sn.  This string must be passed (outside the database) to clients that want to import the snapshot.  The snapshot is available for import only until the end of the transaction that exported it.  A transaction can export more than one snapshot, if needed.  Once a transaction has exported any snapshots, it cannot be prepared with  PREPARE TRANSACTION .  See  SET TRANSACTION  for details of how to use an exported snapshot.  Collation Management Functions Name Return Type Description pg_collation_actual_version( oid ) text Return actual version of collation from operating system pg_import_system_collations( schema regnamespace ) integer Import operating system collation.  pg_import_system_collations  adds collations to the system catalog  pg_collation based on all the locales it finds in the operating system.  If the lock was not held, it will return  false , and in addition, an SQL warning will be reported by the server. 0000 3  The reason for this step is to support function-style cast specifications in cases where there is not an actual cast function.  However, if your database does not use the C locale you will need to create the index with a special operator class to support indexing of pattern-matching queries; see  Section 11.  As an example, the standard distribution of PostgreSQL includes GiST operator classes for several two-dimensional geometric data types, which support indexed queries using these operators: << &< &> >> <<| &<| |&> |>> @> <@ ~= 355 .     SP-GiST indexes, like GiST indexes, offer an infrastructure that supports various kinds of searches.  As an example, the standard distribution of PostgreSQL includes SP-GiST operator classes for two-dimensional points, which support indexed queries using these operators: << >> ~= <@ <^ >^ (See  Section 9.  Like GiST and SP-GiST, GIN can support many different user-defined indexing strategies, and the particular operators with which a GIN index can be used vary depending on the indexing strategy.  As an example, the standard distribution of PostgreSQL includes a GIN operator class for arrays, which supports indexed queries using these operators: <@ @> = && (See  Section 9.  Like GiST, SP-GiST and GIN, BRIN can support many different indexing strategies, and the particular operators with which a BRIN index can be used vary depending on the indexing strategy.  This supports indexed queries using these operators: < <= = >= > 356 . : CREATE INDEX test2_mm_idx ON test2 (major, minor); Currently, only the B-tree, GiST, GIN, and BRIN index types support multicolumn indexes.  The exact rule is that equality constraints on leading columns, plus any inequality constraints on the first column that does not have an equality constraint, will be used to limit the portion of the index that is scanned.  Constraints on columns to the right of these columns are checked in the index, so they save visits to the table proper, but they do not reduce the portion of the index that has to be scanned.  Conditions on additional columns restrict the entries returned by the index, but the condition on the first column is the most important one for determining how much of the index needs to be scanned.  Of the index types currently supported by PostgreSQL, only B-tree can produce sorted output Š the other index types return matching rows in an unspecified, implementation-dependent order.  An important special case is  ORDER BY  in combination with  LIMIT   n : an explicit sort will have to process all the data to identify the first  n rows, but if there is an index matching the  ORDER BY , the first  n  rows can be retrieved directly.  To combine multiple indexes, the system scans each needed index and prepares a  bitmap  in memory giving the locations of table rows that are reported as matching that index's conditions.  Thus, indexes on expressions are useful when retrieval speed is more important than insertion and update speed.  PostgreSQL supports partial indexes with arbitrary predicates, so long as only columns of the table being indexed are involved. , it avoids them when retrieving common values, so the earlier example really only saves index size, it is not required to avoid index usage), and grossly incorrect plan choices are cause for a bug report.  There are also some built-in operator classes besides the default ones: Ł The operator classes  text_pattern_ops ,  varchar_pattern_ops , and bpchar_pattern_ops  support B-tree indexes on the types  text ,  varchar , and  char respectively.  Indexes and Collations An index can support only one collation per index column.  So if queries of the form, say, SELECT * FROM test1c WHERE content >  constant  COLLATE "y"; are also of interest, an additional index could be created that supports the  "y"  collation, like this: CREATE INDEX test1c_content_y_index ON test1c (cont.  The heap- access portion of an index scan thus involves a lot of random access into the heap, which can be slow, particularly on traditional rotating media. ) To solve this performance problem, PostgreSQL supports  index-only scans , which can answer queries from an index alone without any heap access.  The index type must support index-only scans.  GiST and SP-GiST indexes support index-only scans for some operator classes but not others.  Other index types have no support.  As a counterexample, GIN indexes cannot support index-only scans because each index entry typically holds only part of the original data value.  Note it's important that the index be declared on  (x, y)  not  (y, x) , as for most index types (particularly B-trees) searches that do not constrain the leading index columns are not very efficient.  Examining Index Usage Although indexes in PostgreSQL do not need maintenance or tuning, it is still important to check which indexes are actually used by the real-life query workload.  PostgreSQL has  ~ ,  ~* ,  LIKE , and ILIKE  operators for textual data types, but they lack many essential properties required by modern information systems: Ł There is no linguistic support, even for English.  Ł They tend to be slow because there is no index support, so they must process all documents for every search. 13 ), the most important of which is the match operator  @@ , which we introduce in  Section 12.  However, retrieving files from outside the database requires superuser permissions or special function support, so this is usually less convenient than keeping all the data inside PostgreSQL.  The  @@  operator also supports  text  input, allowing explicit conversion of a text string to  tsvector or  tsquery  to be skipped in simple cases.  (This is more important when using a GiST index than a GIN index; see Section 12.  It's also important to be able to display the results nicely.  PostgreSQL provides support for all of these functions. 0} Typically weights are used to mark words from special areas of the document, like the title or an initial abstract, so they can be treated with more or less importance than words in the document body.  It is important to note that the ranking functions do not use any global information, so it is impossible to produce a fair normalization to 1% or 100% as sometimes desired. body,'')), 'D');   return new; end $$ LANGUAGE plpgsql; CREATE TRIGGER tsvectorupdate BEFORE INSERT OR UPDATE     ON messages FOR EACH ROW EXECUTE PROCEDURE messages_trigger(); Keep in mind that it is important to specify the configuration name expli.  Words containing only the basic ASCII letters are reported as a separate token type, since it is sometimes useful to distinguish them.  email  does not support all valid email characters as defined by RFC 5322.  Specifically, the only non-alphanumeric characters supported for email user names are period, dash, and underscore.  As an example, a hyphenated word will be reported both as the entire word and as each component: SELECT alias, description, token FROM ts_debug('foo-bar-beta1');       alias      |               description                |      token      ---------.  Alternatively, the dictionary can be configured to report non-stop-words as unrecognized, allowing them to be passed on to the next dictionary in the list.  Phrases are not supported (use the thesaurus template ( Section 12.  PostgreSQL's current implementation of the thesaurus dictionary is an extension of the synonym dictionary with added  phrase  support.  An error is reported if the subdictionary fails to recognize a word.  Ispell Dictionary The Ispell dictionary template supports  morphological dictionaries , which can normalize many different linguistic forms of a word into the same lexeme.  Also, some more modern dictionary file formats are supported Š  MySpell 2  (OO < 2.  Ispell dictionaries support splitting compound words; a useful feature. dict  file of Ispell: larder/M lardy/RT large/RSPMYT largehearted Note MySpell does not support compound words.  Hunspell has sophisticated support for compound words.  psql Support Information about text search configuration objects can be obtained in psql using a set of commands: \dF{d,p,t}[+] [PATTERN] An optional  +  produces more details.  Important Some PostgreSQL data types and functions have special rules regarding transactional behavior.  When relying on Serializable transactions to prevent anomalies, it is important that any data read from a permanent user table not be considered valid until the transaction which read it has successfully committed.  It is important that an environment which uses this technique have a generalized way of handling serialization failures (which always return with a SQLSTATE value of '40001'), because it will be very hard to predict exactly which transactions might .  If some Serializable transactions insert new keys directly without following this protocol, unique constraints violations might be reported even in cases where they could not occur in a serial execution of the concurrent transactions.  This is always an important performance consideration, but it can be particularly important in a busy system using Serializable transactions.  Please see  [ports12]  for detailed information. ) This should be taken into account when porting applications to PostgreSQL from other environments.  Support for the Serializable transaction isolation level has not yet been added to Hot Standby replication targets (described in  Section 26.  The strictest isolation level currently supported in hot standby mode is Repeatable Read.  It's important to understand that the cost of an upper-level node includes the cost of all its child nodes.  It's also important to realize that the cost only reflects things that the planner cares about.  In particular, the cost does not consider the time spent transmitting result rows to the client, which could be an important factor in the real elapsed time; but the planner ignores it because it cannot change it by altering the plan.  The thing that's usually most important to look for is whether the estimated row counts are reasonably close to reality.  In such cases, the loops  value reports the total number of executions of the node, and the actual time and rows values shown are averages per-execution.  Also, if the outer (first) child contains rows with duplicate key values, the inner (second) child is backed up and rescanned for the portion of its rows matching that key value.  When there are many outer duplicates, the reported actual row count for the inner child plan node can be significantly larger than the number of rows that are actually in the inner relation.  BitmapAnd and BitmapOr nodes always report their actual row counts as zero, due to implementation limitations.  The following subsections describe the kinds of extended statistics that are currently supported. ) The important point is that these different join possibilities give semantically equivalent results but might have hugely different execution costs. 33  rows=1 width=97)          Filter: (filler ~~ '%x%'::text) (4 rows) In all cases, the  Gather  or  Gather Merge  node will have exactly one child plan, which is the portion of the plan that will be executed in parallel.  If it is somewhere else in the plan tree, then only the portion of the plan below it will run in parallel.  Every background worker process which is successfully started for a given parallel query will execute the parallel portion of the plan.  The leader will also execute that portion of the plan, but it has an additional responsibility: it must also read all of the tuples generated by the workers.  When the parallel portion of the plan generates only a small number of tuples, the leader will often behave very much like an additional worker, speeding up query execution.  Conversely, when the parallel portion of the plan generates a large number of tuples, the leader may be almost entirely occupied with reading the tuples generated by the workers and performing any further processing steps which are required by plan .  In such cases, the leader will do very little of the work of executing the parallel portion of the plan. Parallel Query When the node at the top of the parallel portion of the plan is  Gather Merge  rather than  Gather , it indicates that each process executing the parallel portion of the plan is producing tuples in sorted order, and that the leader is .  If this occurs, the leader will execute the portion of the plan below the  Gather  node entirely by itself, almost as if the Gather  node were not present.  Parallel Plans Because each worker executes the parallel portion of the plan to completion, it is not possible to simply take an ordinary query plan and run it using multiple workers.  Instead, the parallel portion of the plan must be what is known internally to the query optimizer as a  partial plan ; that is, it must be constructed so that each process which executes the plan will generate only a subset of the output rows in suc.  Parallel Scans The following types of parallel-aware table scans are currently supported.  Currently, parallel index scans are supported only for btree indexes.  Other scan types, such as scans of non-btree indexes, may support parallel scans in the future.  The inner side of the join may be any kind of non-parallel plan that is otherwise supported by the planner provided that it is safe to run within a parallel worker.  Parallel Aggregation PostgreSQL supports parallel aggregation by aggregating in two stages.  First, each process participating in the parallel portion of the query performs an aggregation step, producing a partial result for each group of which that process is aware.  Parallel aggregation is not supported in all situations.  Parallel aggregation is not supported if any aggregate function call contains  DISTINCT  or  ORDER BY  clause and is also not supported for ordered set aggregates or when the query involves  GROUPING SETS .  It can only be used when all joins involved in the query are also part of the parallel portion of the plan.  If you write a function which does this, and this behavior difference is important to you, mark such functions as  PARALLEL RESTRICTED  to ensure that they execute only in the leader.  So, for example, if a  WHERE  clause applied to a particular table is parallel restricted, the query planner will not consider performing a scan of that table in the parallel portion of a plan.  In some cases, it would be possible (and perhaps even efficient) to include the scan of that table in the parallel portion of the query and defer the evaluation of the  WHERE  clause so that it happens above the  Gather  node.  Supported Platforms .  Error Reporting and Logging .  Locale Support .  Collation Support .  Character Set Support .  Supported Character Sets .  Progress Reporting .  VACUUM Progress Reporting .  Using this option disables support for compressed archives in pg_dump and pg_restore.  Python 3 is supported if it's version 3.  Ł To enable Native Language Support (NLS), that is, the ability to display a program's messages in a language other than English, you need an implementation of the Gettext API.  Ł You need OpenSSL, if you want to support encrypted client connections.  Ł You need Kerberos, OpenLDAP, and/or PAM, if you want to support authentication using those services.  --enable-nls[= LANGUAGES ] Enables Native Language Support (NLS), that is, the ability to display a program's messages in a language other than English.   LANGUAGES  is an optional space-separated list of codes of the languages that you want supported, for example  --enable-nls='de fr' .  --with-pgport= NUMBER Set  NUMBER  as the default port number for server and clients.  The port can always be changed later on, but if you specify it here then both server and clients will have the same default compiled in, which can be very convenient.  --with-gssapi Build with support for GSSAPI authentication.  --with-icu Build with support for the ICU  library.  This is supported for ICU4C version 4. ) --with-openssl   Build with support for SSL (encrypted) connections.  --with-pam Build with PAM  (Pluggable Authentication Modules) support.  --with-bsd-auth Build with BSD Authentication support. ) --with-ldap Build with LDAP  support for authentication and connection parameter lookup (see Section 33.  --with-systemd Build with support for systemd  service notifications.  --with-bonjour Build with Bonjour support.  This requires Bonjour support in your operating system.  --with-libxml Build with libxml2, enabling SQL/XML support.  The default segment size, 1 gigabyte, is safe on all supported platforms.  If your operating system has  ﬁlargefileﬂ  support (which most do, nowadays), you can use a larger segment size.  But be careful not to select a value larger than is supported by your platform and the file systems you intend to use.  --disable-spinlocks Allow the build to succeed even if PostgreSQL has no CPU spinlock support for the platform.  The lack of spinlock support will result in poor performance; therefore, this option should only be used if the build aborts and informs you that the platform lacks spinlock support.  If this option is required to build PostgreSQL on your platform, please report the problem to the PostgreSQL developers.  --disable-strong-random Allow the build to succeed even if PostgreSQL has no support for strong random numbers on the platform.  This disables support for compressed archives in pg_dump and pg_restore.  --enable-dtrace  Compiles PostgreSQL with support for the dynamic tracing tool DTrace.  On Solaris, to include DTrace support in a 64-bit binary, you must specify  DTRACEFLAGS="-64"  to configure.  An important example is that gcc's  -Werror  option cannot be included in the CFLAGS  passed to  configure , because it will break many of  configure 's built-in tests.  For example, you could do make COPT='-Werror' or export COPT='-Werror' make Note When developing code inside the server, it is recommended to use the configure options --enable-cassert  (which turns on many run-time error checks) and  --enable- debu.  If using GCC, it is best to build with an optimization level of at least  -O1 , because using no optimization ( -O0 ) disables some important compiler warnings (such as the use of uninitialized variables).  If you built with debugging support, stripping will effectively remove the debugging support, so it should only be done if debugging is no longer needed. bash_profile  (or  /etc/ profile , if you want it to affect all users): PATH=/usr/local/pgsql/bin:$PATH export PATH If you are using  csh  or  tcsh , then use this command: set path = ( /usr/local/pgsql/bin $path )  To enable your system to find the .  Supported Platforms A platform (that is, a CPU architecture and operating system combination) is considered supported by the PostgreSQL development community if the code contains provisions to work on that platform and it has recently been verified .  Code support exists for M68K, M32R, and VAX, but these architectures are not known to have been tested recently.  It is often possible to build on an unsupported CPU type by configuring with  --disable- spinlocks , but performance will be poor.  In most cases, all CPU architectures supported by a given operating system will work.  If you have installation problems on a platform that is known to be supported according to recent build farm results, please report it to  <pgsql-bugs@lists.  If you are interested in porting PostgreSQL to a new platform,  <pgsql-hackers@lists. 1 are considered supported.  The minimum recommended fix levels for supported AIX versions are: AIX 4.  The problem was reported to IBM, and is recorded as bug report PMR29657.  One user reports: When implementing PostgreSQL version 8. Installation from Source Code Warning This is really a workaround for problems relating to immaturity of IPv6 support, which improved visibly during the course of AIX 5.  It has been reported that this workaround is not only unnecessary, but causes problems on AIX 6. 1, where IPv6 support has become more mature. ) If you omit the export of OBJECT_MODE , your build may fail with linker errors.  Ł The  adduser  command is not supported; use the appropriate user management application on Windows NT, 2000, or XP.  Ł The  su  command is not supported; use ssh to simulate su on Windows NT, 2000, or XP.  Ł OpenSSL is not supported.  Ł Start  cygserver  for shared memory support.  To fix this, set the locale to C by doing  export LANG=C. 20, and we have reports of successful installations on HP-UX 11.  See HP's support sites such as  ftp://us-ffs. 1 machines you will need to specify  +DAportable  in  CFLAGS .  The native Windows port requires a 32 or 64-bit version of Windows 2000 or later.  Solaris PostgreSQL is well-supported on Solaris.  We have heard reports of problems when using GCC 2.  Native builds of psql don't support command line editing.  The Cygwin build does support command line editing, so it should be used where psql is needed for interactive use on Windows.  64-bit PostgreSQL builds are supported with Microsoft Windows SDK version 6.  Compilation is supported down to Windows XP and Windows Server 2003 when building with Visual Studio 2005 to Visual Studio 2013.  Building with Visual Studio 2015 is supported down to Windows Vista and Windows Server 2008.  Building with Visual Studio 2017 and Visual Studio 2019 is supported down to Windows 7 SP1 and Windows Server 2008 R2 SP1.  Microsoft Windows SDK If your build environment doesn't ship with a supported version of the Microsoft Windows SDK it is recommended that you upgrade to the latest version (currently version 10), available for download from  https://www.  Gettext Gettext is required to build with NLS support, and can be downloaded from  http:// gnuwin32.  MIT Kerberos Required for GSSAPI authentication support.  libxml2 and libxslt Required for XML support.  openssl Required for SSL support.  ossp-uuid Required for UUID-OSSP support (contrib only).  zlib Required for compression support in pg_dump and pg_restore.  Special Considerations for 64-bit Windows PostgreSQL will only build for the x64 architecture on 64-bit Windows, there is no support for Itanium processors. Installation from Source Code on Windows Mixing 32- and 64-bit versions in the same build tree is not supported.  For this reason, it is important to start the correct command prompt before building.  There is no support for loading a 32-bit library in a 64-bit server.  Several of the third party libraries that PostgreSQL supports may only be available in 32-bit versions, in which case they cannot be used with 64-bit PostgreSQL.  Therefore, it is important to make this choice correctly the first time.  For this, use the usual Unix shell syntax: $  postgres -D /usr/local/pgsql/data >logfile 2>&1 & It is important to store the server's stdout and stderr output somewhere, as shown above. Server Setup and Operation HINT:  Is another postmaster already running on port 5432? If not,  wait a few seconds and retry.  FATAL:  could not create any TCP/IP sockets This usually means just what it suggests: you tried to start another server on the same port where one is already running.  For example, trying to start a server on a reserved port number might draw something like: $  postgres -p 666 LOG:  could not bind IPv4 address "127. 1": Permission denied HINT:  Is another postmaster already running on port 666? If not,  wait a few seconds and retry.  Or it could mean that you do not have System-V-style shared memory support configured into your kernel at all.  If you get an  ﬁillegal system callﬂ  error, it is likely that shared memory or semaphores are not supported in your kernel at all. com" and  accepting         TCP/IP connections on port 5432? This is the generic  ﬁI couldn't find a server to talk toﬂ  failure.  (It is important to realize that  Connection refused  in this context does  not  mean that the server got your connection request and rejected it. 0 and later the IPC cleanup code does not properly detect processes in other jails, preventing the running of postmasters on the same port in different jails.  Of particular importance are limits on the number of processes per user, the number of open files per process, and the amount of memory available to each process.  Ł On Linux  /proc/sys/fs/file-max  determines the maximum number of open files that the kernel will support.  If you do this, you should also set these environment variables in the startup script before invoking the postmaster: export PG_OOM_ADJUST_FILE=/proc/self/oom_score_adj export PG_OOM_ADJUST_VALUE=0 These settings will cause postmaster child processe. 4 kernels are reported to have early versions of the 2. c ) to verify what is supported in your kernel before you try this in a 2. pid` Important It is best not to use SIGKILL to shut down the server.  The least downtime can be achieved by installing the new server in a different directory and running both the old and the new servers in parallel, on different ports.  This is possible because Slony supports replication between different major versions of PostgreSQL.  Many other operating systems support this functionality, including Windows. Server Setup and Operation PostgreSQL has native support for using SSL connections to encrypt client/server communications for increased security.  This requires that OpenSSL is installed on both client and server systems and that support in PostgreSQL is enabled at build time (see  Chapter 16 ).  With SSL support compiled in, the PostgreSQL server can be started with SSL enabled by setting the parameter  ssl  to  on  in  postgresql.  The server will listen for both normal and SSL connections on the same TCP port, and will negotiate with any connecting client on whether to use SSL. cnf  and is located in the directory reported by  openssl version -d .  OpenSSL supports a wide range of ciphers and authentication algorithms, of varying strength.  In all these cases, the error condition is reported in the server log.  A secure tunnel listens on a local port and forwards all traffic to a port on the remote machine.  Traffic sent to the remote port can arrive on its  localhost  address, or different bind address if desired; it does not appear as coming from your local machine. com The first number in the  -L  argument, 63333, is the local port number of the tunnel; it can be any unused port.  (IANA reserves ports 49152 through 65535 for private use. , the port number your database server is using.  In order to connect to the database server using this tunnel, you connect to port 63333 on the local machine: psql -h localhost -p 63333 postgres To the database server it will then look as though you are user  joe  on host  foo.  You could also have set up port forwarding as ssh -L 63333:foo.  Include files or directories can be used to logically separate portions of the database configuration, rather than having a single large  postgresql.  This is important because only the last setting encountered for a particular parameter while the server is reading configuration files will be used.  port  ( integer )  The TCP port the server listens on; 5432 by default.  Note that the same port number is used for all IP addresses the server listens on.  The default is typically 100 connections, but might be less if your kernel settings will not support it (as determined during initdb).  nnnn  where  nnnn  is the server's port number, an ordinary file named  .  This parameter is ignored if the server was not compiled with Bonjour support.  This parameter is supported only on systems that support  TCP_KEEPIDLE  or an equivalent socket option, and on Windows; on other systems, it must be zero.  This parameter is supported only on systems that support  TCP_KEEPINTVL  or an equivalent socket option, and on Windows; on other systems, it must be zero.  This parameter is supported only on 511 . Server Configuration systems that support  TCP_KEEPCNT  or an equivalent socket option; on other systems, it must be zero.  Note This parameter is not supported on Windows, and must be zero.  See the ciphers manual page in the OpenSSL package for the syntax of this setting and a list of supported values.  It needs to be supported by all clients that connect.  Note that older clients might lack support for the SCRAM authentication mechanism, and hence not work with passwords encrypted with SCRAM-SHA-256.  The default is typically 128 megabytes ( 128MB ), but might be less if your kernel settings will not support it (as determined during initdb).  At present, this feature is supported only on Linux.  Not all values are supported on all platforms; the first supported option is the default for that platform.  But on some platforms (notably, most BSD systems), the kernel will allow individual processes to open many more files than the system can actually support if many processes all try to open that many files.  There are many situations where it is not important that maintenance commands like  VACUUM  and  ANALYZE  finish quickly; however, it is usually very important that these commands do not significantly interfere with the ability of the system to perf.  (If  BLCKSZ  is not 8kB, the default and maximum values scale proportionally to it.  The default is 1 on supported systems, otherwise 0. Server Configuration max_worker_processes  ( integer )  Sets the maximum number of background processes that the system can support.  max_parallel_workers  ( integer )  Sets the maximum number of workers that the system can support for parallel queries.  (If  BLCKSZ  is not 8kB, the maximum value scales proportionally to it.  The default value is replica , which writes enough data to support WAL archiving and replication, including running read-only queries on a standby server.  Finally,  logical  adds information necessary to support logical decoding.  In  off mode, there is no waiting, so there can be a delay between when success is reported to the client and when the transaction is later guaranteed to be safe against a server crash.  So, turning  synchronous_commit  off can be a useful alternative when performance is more important than exact certainty about the durability of a transaction.  The default is the first method in the above list that is supported by the platform, except that  fdatasync  is the default on Linux.  (If  BLCKSZ  is not 8kB, the default and maximum values scale proportionally to it.  It is important for the command to return a zero exit status only if it succeeds. 6 ) that the server can support.  synchronous_standby_names  ( string )  Specifies a list of standby servers that can support  synchronous replication , as described in Section 26. 6 and is still supported.  You may wish to set this to a non-zero value on a primary server that is supporting hot standby servers, as described in  Section 26.  The standby will report the last write-ahead log location it has written, the last position it has flushed to disk, and the last position it has applied.  This parameter's value is the maximum interval, in seconds, between reports.  You can raise or lower both values together to change the importance of disk I/O costs relative to CPU costs, which are described by the following parameters.  When setting this parameter you should consider both PostgreSQL's shared buffers and the portion of the kernel's disk cache that will be used for PostgreSQL data files, though some data might exist in both places.  Error Reporting and Logging 19.  Where To Log log_destination  ( string )  PostgreSQL supports several methods for logging server messages, including stderr, csvlog and syslog.  On Windows, eventlog is also supported. ) The supported  % -escapes are similar to those listed in the Open Group's  strftime  1  specification.  NOTICE WARNING ERROR Reports an error that caused the current command to abort.  WARNING ERROR LOG Reports information of interest to administrators, e.  INFO INFORMATION FATAL Reports an error that caused the current session to abort. Server Configuration Severity Usage syslog eventlog PANIC Reports an error that caused all database sessions to abort.  Escape Effect Session only %a Application name yes %u User name yes %d Database name yes %r Remote host name or IP address, and remote port yes %h Remote host name or IP address yes %p Process ID no 543 .  Unlike  TimeZone , this value is cluster-wide, so that all sessions will report timestamps consistently.  Using CSV-Format Log Output Including  csvlog  in the  log_destination  list provides a convenient way to import log files into a database table. Server Configuration these columns: time stamp with milliseconds, user name, database name, process ID, client host:port number, session ID, per-session line number, command tag, session start time, virtual transaction ID, regular transaction ID, err.  There are a few things you need to do to simplify importing CSV log files: 1.  This lets you predict what the file name will be and know when an individual log file is complete and therefore ready to be imported.  This is useful to protect against accidentally importing the same information twice.  The  COPY  command commits all of the data it imports at one time, so any error will cause the entire import to fail.  If you import a partial log file and later import the file again when it is complete, the primary key violation will cause the import 546 .  Wait until the log is complete and closed before importing.  This procedure will also protect against accidentally importing a partial line that hasn't been completely written, which would also cause  COPY  to fail.  Note that even when enabled, this information is not visible to all users, only to superusers and the user owning the session being reported on, so it should not represent a security risk.  log_statement_stats  reports total statement statistics, while the others report per-module statistics.  Note that the value is distributed proportionally among the running autovacuum workers, if there is more than one, so that the sum of the limits for each worker does not exceed the value of this variable.  An error is reported if the search path is empty.  This setting supports shared use of a database (where no users have private schemas, and all share use of  public ), private per-user schemas, and combinations of these.  Both methods support all possible values, although the hex encoding will be somewhat larger than the base64 encoding.  The character sets supported by the PostgreSQL server are described in  Section 23.  Every PostgreSQL-supported library has a  ﬁmagic blockﬂ  that is checked to guarantee compatibility.  Increasing this value reduces the amount of time wasted in needless deadlock checks, but slows down reporting of real deadlock errors. 2 did not support null values in arrays, and therefore would treat  NULL  as specifying a normal array element with the string value  ﬁNULLﬂ .  For example,  lo_import()  and lo_export()  need superuser privileges regardless of this setting. ' ) is supported.  In some cases it might have been entirely forgotten, making it unsafe to retry; the second attempt may be reported as successful, when in fact the data has been lost.  In these circumstances, the only way to avoid data loss is to recover from the WAL after any failure is reported, preferably after investigating the root cause of the failure and replacing any faulty hardware.  If set to true, PostgreSQL will instead report an error but continue to run so that the data flushing operation can be retried in a later checkpoint. Server Configuration options report various aspects of PostgreSQL behavior that might be of interest to certain applications, particularly administrative front-ends.  block_size  ( integer )  Reports the size of a disk block.  data_checksums  ( boolean )  Reports whether data checksums are enabled for this cluster.  debug_assertions  ( boolean )  Reports whether PostgreSQL has been built with assertions enabled.  integer_datetimes  ( boolean )  Reports whether PostgreSQL was built with support for 64-bit-integer dates and times.  lc_collate  ( string )  Reports the locale in which sorting of textual data is done.  lc_ctype  ( string )  Reports the locale that determines character classifications.  max_function_args  ( integer )  Reports the maximum number of function arguments.  max_identifier_length  ( integer )  Reports the maximum identifier length.  max_index_keys  ( integer )  Reports the maximum number of index keys.  segment_size  ( integer )  Reports the number of blocks (pages) that can be stored within a file segment.  server_encoding  ( string )    Reports the database encoding (character set). Server Configuration server_version  ( string )  Reports the version number of the server.  server_version_num  ( integer )  Reports the version number of the server as an integer.  wal_block_size  ( integer )  Reports the size of a WAL disk block.  wal_segment_size  ( integer )  Reports the number of blocks (pages) in a WAL segment file.  This is intended to give developers an opportunity to attach to the server process with a debugger.  This is intended to give developers an opportunity to attach to the server process with a debugger to trace down misbehavior in authentication.  Currently, the supported resource managers are  heap ,  heap2 ,  btree ,  hash ,  gin ,  gist ,  sequence ,  spgist ,  brin , and  generic . Server Configuration Detection of a checksum failure during a read normally causes PostgreSQL to report an error, aborting the current transaction.  Setting  ignore_checksum_failure  to on causes the system to ignore the failure (but still report a warning), and continue processing.  If the header is corrupt an error will be reported even if this option is enabled.  zero_damaged_pages  ( boolean )  Detection of a damaged page header normally causes PostgreSQL to report an error, aborting the current transaction.  Setting  zero_damaged_pages  to on causes the system to instead report a warning, zero out the damaged page in memory, and continue processing.  To make use of this option the server must be built with SSL support.  Note that entries in IPv6 format will be rejected if the system's C library does not have support for IPv6 addresses. x to  connect # to database "postgres" as the same user name that ident reports  for # the connection (typically the operating system user name).  # # Require SCRAM authentication for most users, but make an  exception # for user 'mike', who uses an older client that doesn't support  SCRAM # authentication.  # # TYPE  DATABASE        USER            ADDRESS                  METHOD local   sameuser        all                                     md5 local   all             @admins                                 md5 local   all             +support       .  This option is supported for all authentication methods that receive external user names.  It is a challenge-response scheme that prevents password sniffing on untrusted connections and supports storing passwords on the server in a cryptographically hashed form that is thought to be secure.  This is the most secure of the currently provided methods, but it is not supported by older client libraries.  (Previous PostgreSQL releases supported storing the password on the server in plain text.  To upgrade an existing installation from  md5  to  scram-sha-256 , after having ensured that all client libraries in use are new enough to support SCRAM, set  password_encryption = 'scram-sha-256'  in  postgresql.  PostgreSQL supports GSSAPI with Kerberos authentication according to RFC 1964.  GSSAPI provides automatic authentication (single sign-on) for systems that support it.  GSSAPI support has to be enabled when PostgreSQL is built; see  Chapter 16  for more information.  PostgreSQL also supports a parameter to strip the realm from the principal.  This method is supported for backwards compatibility and is strongly discouraged as it is then impossible to distinguish different users with the same user name but coming from different realms.  The following configuration options are supported for GSSAPI: include_realm If set to 0, the realm name from the authenticated user principal is stripped off before being passed through the user name mapping ( Section 20.  The following configuration options are supported for SSPI: include_realm If set to 0, the realm name from the authenticated user principal is stripped off before being passed through the user name mapping ( Section 20.  This is only supported on TCP/IP connections.  The following configuration options are supported for ident: map Allows for mapping between system and database user names.  Virtually every Unix-like operating system ships with an ident server that listens on TCP port 113 by default.  The basic functionality of an ident server is to answer questions like  ﬁWhat user initiated the connection that goes out of your port  X and connects to my port  Y ?ﬂ .  The drawback of this procedure is that it depends on the integrity of the client: if the client machine is untrusted or compromised, an attacker could run just about any program on port 113 and return any user name they choose.  This method is only supported on local connections.  The following configuration options are supported for peer: map Allows for mapping between system and database user names. Client Authentication ldapport Port number on LDAP server to connect to.  If no port is specified, the LDAP library's default port setting will be used.  The format is ldap:// host [: port ]/ basedn [?[ attribute ][?[ scope ]]] scope  must be one of  base ,  one ,  sub , typically the latter.  Only one attribute is used, and some other components of standard LDAP URLs such as filters and extensions are not supported.  The  ldaps  URL scheme (direct SSL connection) is not supported.  LDAP URLs are currently only supported with OpenLDAP, not on Windows. net/dc=example,dc=net? uid?sub" Some other software that supports authentication against LDAP uses the same URL format, so it will be easier to share the configuration.  There is no support for RADIUS accounting.  The following configuration options are supported for RADIUS: radiusservers The DNS names or IP addresses of the RADIUS servers to connect to.  Note The encryption vector used will only be cryptographically strong if PostgreSQL is built with support for OpenSSL.  radiusports The port numbers to connect to on the RADIUS servers.  If no port is specified, the default RADIUS port ( 1812 ) will be used.  The following configuration options are supported for SSL certificate authentication: map Allows for mapping between system and database user names.  The following configuration options are supported for PAM: pamservice PAM service name.  Tip The server log might contain more information about an authentication failure than is reported to the client. ) Since the role identity determines the set of privileges available to a connected client, it is important to carefully configure privileges when setting up a multiuser environment.  It is important to understand, however, that this is not (yet) intended as a general-purpose  ﬁ COPY DATABASE ﬂ  facility.  This means that tablespaces can be used  only  on systems that support symbolic links.  PostgreSQL supports two localization facilities: Ł Using the locale features of the operating system to provide locale-specific collation order, number formatting, translated messages, and other aspects.  Ł Providing a number of different character sets to support storing text in all kinds of languages, and providing character set translation between client and server.  Locale Support Locale  support refers to an application respecting cultural preferences regarding alphabets, sorting, number formatting, etc.  Overview Locale support is automatically initialized when a database cluster is created using  initdb .  To support that, a set of locale subcategories exist that control only certain aspects of the localization rules: LC_COLLATE String sort order LC_CTYPE Character classification (What is a letter? Its upper-case equivalent?) LC_MESSAGES Language of m.  If you want the system to behave as if it had no locale support, use the special locale name  C , or equivalently  POSIX .  All other locale support is built in automatically.  Problems If locale support doesn't work according to the explanation above, check that the locale support in your operating system is correctly configured.  The directory  src/test/locale  in the source distribution contains a test suite for PostgreSQL's locale support.  Collation Support The collation feature allows specifying the sort order and character classification behavior of data per- column, or even per-operation.  ICU locales can only be used if support for ICU was configured when PostgreSQL was built.  ICU does not support separate  ﬁcollateﬂ  and  ﬁctypeﬂ  settings, so they are always the same.  Additional collations may be available depending on operating system support.  Predefined Collations If the operating system provides support for using multiple locales within a single program ( newlocale  and related functions), or if support for ICU is configured, then when a database cluster is initialized,  initdb  populat.  New operating system locales can also be imported en masse using the  pg_import_system_collations()  function.  Some (less frequently used) encodings are not supported by ICU. 3 ) or if the operating system has been upgraded to provide new locale definitions (in which case see also pg_import_system_collations() ).  The first style is preferred going forward, but it is not supported by older ICU versions.  Thus, there will be no direct feedback if a collation specification is composed using features that the given ICU installation does not actually support.  Character Set Support The character set support in PostgreSQL allows you to store text in a variety of character sets (also called encodings), including single-byte character sets such as the ISO 8859 series and multiple-byte character sets such as .  All supported character sets can be used transparently by clients, but a few are not supported for use within the server (that is, as a server-side encoding).  An important restriction, however, is that each database's character set must be compatible with the database's  LC_CTYPE  (character classification) and  LC_COLLATE  (string sort order) locale settings. org/reports/tr35/tr35-collation. ) If you have ICU support configured, ICU-provided locales can be used with most but not all server-side encodings.  Supported Character Sets Table 23.  For example, the PostgreSQL JDBC driver does not support  MULE_INTERNAL ,  LATIN6 ,  LATIN8 , and  LATIN10 . UTF8  |  {=c/hlinnaka,hlinnaka=CTc/hlinnaka} (7 rows) Important On most modern operating systems, PostgreSQL can determine which character set is implied by the  LC_CTYPE  setting, and it will enforce that only the matching database encoding is used.  Automatic Character Set Conversion Between Server and Client PostgreSQL supports automatic character set conversion between server and client for certain character set combinations.  Client/Server Character Set Conversions Server Character Set Available Client Character Sets BIG5 not supported as a server encoding EUC_CN EUC_CN ,  MULE_INTERNAL ,  UTF8 EUC_JP EUC_JP ,  MULE_INTERNAL ,  SJIS ,  UTF8 EUC_JIS_2004 EUC_JIS_2004 ,  S. Localization Server Character Set Available Client Character Sets JOHAB not supported as a server encoding KOI8R KOI8R ,  ISO_8859_5 ,  MULE_INTERNAL , UTF8 ,  WIN866 ,  WIN1251 KOI8U KOI8U ,  UTF8 LATIN1 LATIN1 ,  MULE_INTERNAL ,  UTF8 LATIN2 LATIN2. ) If the conversion of a particular character is not possible Š suppose you chose  EUC_JP  for the server and  LATIN1  for the client, and some Japanese characters are returned that do not have a representation in  LATIN1  Š an error is reported.  check_postgres 1  is available for monitoring database health and reporting unusual conditions.  It is important to have reasonably accurate statistics, otherwise poor choices of plans might degrade database performance.  For tables that are regularly vacuumed for space reclamation purposes, this is of little importance.  Multixacts and Wraparound Multixact IDs  are used to support row locking by multiple transactions.  Index types that support  CREATE INDEX  with the  CONCURRENTLY  option can instead be recreated that way.   check_postgres 3  provides Nagios alerts when important messages appear in the log files, as well as detection of many other extraordinary conditions.  While the procedure is essentially simple, it is important to have a clear understanding of the underlying techniques and assumptions.  (If you do not have sufficient privileges to back up the entire database, you can still back up portions of the database to which you do have access using options such as  -n  schema  or  -t  table . ) To specify which database server pg_dump should contact, use the command line options  -h  host and  -p  port .  Similarly, the default port is indicated by the  PGPORT  environment variable or, failing that, by the compiled-in default.  An important advantage of pg_dump over the other backup methods described later is that pg_dump's output can generally be re-loaded into newer versions of PostgreSQL, whereas file-level backups and continuous archiving are both extremely server-vers.  psql supports options similar to pg_dump for specifying the database server to connect to and the user name to use.  The ability of pg_dump and psql to write to or read from pipes makes it possible to dump a database directly from one server to another, for example: pg_dump -h  host1   dbname  | psql -h  host2   dbname Important The dumps produced by pg_dump are r.  To support convenient dumping of the entire contents of a database cluster, the  pg_dumpall  program is provided.  Parallel dumps are only supported for the "directory" archive format.  An alternative file-system backup approach is to make a  ﬁconsistent snapshotﬂ  of the data directory, if the file system supports that functionality (and you are willing to trust that it is implemented correctly).  Thus, this technique supports  point-in-time recovery : it is possible to restore the database to its state at any time since your base backup was taken.  As with the plain file-system-backup technique, this method can only support restoration of an entire database cluster, not a subset.  It is important that the archive command return zero exit status if and only if it succeeds.  This is an important safety feature to preserve the integrity of your archive in case of administrator error (such as sending the output of two different servers to the same archive directory).  You should ensure that any error condition or request to a human operator is reported appropriately so that the situation can be resolved reasonably quickly. ) The speed of the archiving command is unimportant as long as it can keep up with the average rate at which your server generates WAL data.  It is very important that these steps are executed in sequence, and that the success of a step is verified before proceeding to the next step.  It is important that the command return nonzero exit status on failure.  These page snapshots are designed to support crash recovery, since we might need to fix partially-written disk pages.  Slony-I is an example of this type of replication, with per-table granularity, and support for multiple standby servers.  However, no formal support for that is offered and you are advised to keep primary and standby servers at the same release level as much as possible. 50 port=5432 user=foo  password=foopass' 643 .  On systems that support the keepalive socket option, setting  tcp_keepalives_idle , tcp_keepalives_interval  and  tcp_keepalives_count  helps the primary promptly notice a broken connection.  Authentication It is very important that the access privileges for replication be set up so that only trusted users can read the WAL stream, because it is easy to extract privileged information from it. 100/32        md5 The host name and port number of the primary, connection user name, and password are specified in the  recovery. 50 , port  5432 , the account name for replication is  foo , and the password is  foopass , the administrator can add the following line to the  recovery. 50 # and port 5432 as the user "foo" whose password is "foopass". 50 port=5432 user=foo  password=foopass' 26.  Monitoring An important health indicator of streaming replication is the amount of WAL records generated in the primary, but not yet applied in the standby. 50 port=5432 user=foo  password=foopass' primary_slot_name = 'node_a_slot' 26.  The amount of data loss is proportional to the replication delay at the time of failover.  Check the respective documentation for details on synchronous replication support.  Setting  synchronous_commit  to  remote_apply  will cause each commit to wait until the current synchronous standbys report that they have replayed the transaction, making it visible to user queries.  Multiple Synchronous Standbys Synchronous replication supports one or more synchronous standby servers; transactions will wait until all the standby servers which are considered as synchronous confirm receipt of their data.  For example, an application workload might consist of: 10% of changes are important customer details, while 90% of changes are less important data that the business can more easily survive if it is lost, such as chat messages between users.  With synchronous replication options specified at the application level (on the primary) we can offer synchronous replication for the most important changes, without slowing down the bulk of the total workload.  Application level options are an important and practical tool for allowing the benefits of synchronous replication for high performance applications.  If you're setting up the reporting servers that are only used to offload read-only queries from the primary, not for high availability purposes, you don't need to promote it.  Normal recovery processing would request a file from the WAL archive, reporting failure if the file was unavailable.  It can also be extended as needed to support specific configurations and environments.  The method for triggering failover is an important part of planning and design. High Availability, Load Balancing, and Replication Ł max_connections Ł max_prepared_transactions Ł max_locks_per_transaction Ł max_worker_processes It is important that the administrator select appropriate settings for  max_standby_archive_delay  and.  If the standby server is tasked as an additional server for decision support queries then it might be acceptable to set the maximum delay values to many hours, or even -1 which means wait forever for queries to complete.  The check_postgres monitoring script will also work, though some reported values could give different or confusing results.  That is the earliest file that must be kept to allow a restore to be restartable, so this information can be used to truncate the archive to just the minimum required to support restarting from the current restore.  It is important for the command to return a zero exit status only if it succeeds.  This information can be used to truncate the archive to just the minimum required to support restart from the current restore.  The connection string should specify the host name (or address) of the primary server, as well as the port number if it is not the same as the standby server's default.  It may be useful to have a time-delayed copy of the data, offering opportunities to correct data loss errors.  For example, if you set this parameter to  5min , the standby will replay each transaction commit only when the system time on the standby is at least five minutes past the commit time reported by the master.  Standard Unix Tools On most Unix platforms, PostgreSQL modifies its command title as reported by  ps , so that individual server processes can readily be identified.  The Statistics Collector PostgreSQL's  statistics collector  is a subsystem that supports collection and reporting of information about server activity.  PostgreSQL also supports reporting dynamic information about exactly what is going on in the system right now, such as the exact command currently being executed by other server processes, and which other connections exist in the system.  When using the statistics to monitor collected data, it is important to realize that the information does not update instantaneously.  Also, the collector itself emits a new report at most once per  PGSTAT_STAT_INTERVAL milliseconds (500 ms unless altered while building the server).  Another important point is that when a server process is asked to display any of these statistics, it first fetches the most recent report emitted by the collector process and then continues to use this snapshot for all statistical views and functio.  client_hostname text Host name of the connected client, as reported by a reverse DNS lookup of  client_addr .  client_port integer TCP port number that the client is using for communication with this backend, or  -1  if a Unix socket is used backend_start timestamp with time zone Time when this process was started.  Ł disabled : This state is reported if  track_activities  is disabled in this backend.  client_hostname text Host name of the connected client, as reported by a reverse DNS lookup of  client_addr .  client_port integer TCP port number that the client is using for communication with this WAL sender, or  -1  if a Unix socket is used backend_start timestamp with time zone Time when this process was started, i. , when the client connected to this WAL sender backend_xmin xid This standby's  xmin  horizon reported by hot_standby_feedback .  The lag times reported in the  pg_stat_replication  view are measurements of the time taken for recent WAL to be written, flushed and replayed and for the sender to know about it.  Note The reported lag times are not predictions of how long it will take for the standby to catch up with the sending server assuming the current rate of replay.  In particular, when the standby has caught up completely,  pg_stat_replication  shows the time taken to write, flush and replay the most recent reported WAL location rather than zero as some users might expect. ) The access functions for per-database statistics take a database OID as an argument to identify which database to report on.  Progress Reporting PostgreSQL has the ability to report the progress of certain commands during command execution.  Currently, the only command which supports progress reporting is  VACUUM .  VACUUM Progress Reporting Whenever  VACUUM  is running, the  pg_stat_progress_vacuum  view will contain one row for each backend (including autovacuum worker processes) that is currently vacuuming.  The tables below describe the information that will be reported and provide information about how to interpret it.  Progress reporting is not currently supported for  VACUUM FULL  and backends running  VACUUM FULL  will not be listed in this view.  This number is reported as of the beginning of the scan; blocks added later will not be (and need not be) visited by this  VACUUM .  During this phase,  VACUUM  will vacuum the free space map, update statistics in  pg_class , and report statistics to the statistics collector.  Dynamic Tracing PostgreSQL provides facilities to support dynamic tracing of the database server.  Currently, the  DTrace 1  utility is supported, which, at the time of this writing, is available on Solaris, macOS, FreeBSD, NetBSD, and Oracle Linux.  Supporting other dynamic tracing utilities is theoretically possible by changing the definitions for the macros in  src/include/utils/probes.  To include DTrace support specify  --enable-dtrace  to configure.  clog-checkpoint-start (bool) Probe that fires when the CLOG portion of a checkpoint is started.  clog-checkpoint-done (bool) Probe that fires when the CLOG portion of a checkpoint is complete.  subtrans-checkpoint- start (bool) Probe that fires when the SUBTRANS portion of a checkpoint is started.  subtrans-checkpoint- done (bool) Probe that fires when the SUBTRANS portion of a checkpoint is complete.  multixact-checkpoint- start (bool) Probe that fires when the MultiXact portion of a checkpoint is started.  multixact-checkpoint- done (bool) Probe that fires when the MultiXact portion of a checkpoint is complete.  buffer-checkpoint- start (int) Probe that fires when the buffer- writing portion of a checkpoint is started.  twophase-checkpoint- start () Probe that fires when the two- phase portion of a checkpoint is started.  twophase-checkpoint- done () Probe that fires when the two- phase portion of a checkpoint is complete.  This is usually not worth worrying about if you are just reporting the values of a few local variables.  Disk Full Failure The most important disk monitoring task of a database administrator is to make sure the disk doesn't become full.  If your system supports per-user disk quotas, then the database will naturally be subject to whatever quota is placed on the user the server runs as.  Reliability Reliability is an important property of any serious database system, and PostgreSQL does everything possible to guarantee reliable operation.  Recent SATA drives (those following ATAPI-6 or later) offer a drive cache flush command ( FLUSH CACHE EXT ), while SCSI drives have long supported a similar command  SYNCHRONIZE CACHE .  WAL also makes it possible to support on-line backup and point-in-time recovery, as described in Section 25.  By archiving the WAL data we can support reverting to any time instant covered by the available WAL data: we simply install a prior physical backup of the database, and replay the WAL log just as far as the desired time.  The client is therefore guaranteed that a transaction reported to be committed will be preserved, even in the event of a server crash immediately after.  There is a short time window between the report of transaction completion to the client and the time that the transaction is truly committed (that is, it is guaranteed not to be lost if the server crashes).  The commands supporting two-phase commit, such as PREPARE TRANSACTION , are also always synchronous.  It disables all logic within PostgreSQL that attempts to synchronize writes to different portions of the database, and therefore a system crash (that is, a hardware or operating system crash, not a failure of PostgreSQL itself) could result in arbit.  A value of half of the average time the program reports it takes to flush after a single 8kB write operation is often the most effective setting for  commit_delay , so this value is recommended as the starting point to use when optimizing for a part.  Enabling the  wal_debug  configuration parameter (provided that PostgreSQL has been compiled with support for it) will result in each  XLogInsertRecord  and  XLogFlush  WAL call being logged to the server log.  The aim of WAL is to ensure that the log is written before database records are altered, but this can be subverted by disk drives  that falsely report a successful write to the kernel, when in fact they have only cached the data and not yet stored i.  Administrators should try to ensure that disks holding PostgreSQL's WAL log files do not make such false reports.  To deal with the case where  pg_control  is corrupt, we should support the possibility of scanning existing log segments in reverse order Š newest to oldest Š in order to find the latest checkpoint.   pg_control  is small enough (less than one disk page) that it is not subject to partial-write problems, and as of this writing there have been no reports of database failures due solely to the inability to read  pg_control  itself.  PostgreSQL supports both mechanisms concurrently, see  Chapter 26 .  Replication to differently-named tables on the subscriber is not supported. Regression Tests make installcheck or for a parallel test: make installcheck-parallel The tests will expect to contact the server at the local host and the default port number, unless directed otherwise by  PGHOST  and  PGPORT  environment variables. utf8  test only works when support for ICU was built.  When a test is reported as  ﬁfailedﬂ , always examine the differences between expected and actual results; you might find that the differences are not significant.  Nonetheless, we still strive to maintain accurate reference files across all supported platforms, so it can be expected that all tests pass. ) If for some reason a particular platform generates a  ﬁfailureﬂ  for a given test, but inspection of the output convinces you that the result is valid, you can add a new comparison file to silence the failure report in future test runs.  In general, it is advisable to try to run the regression tests in the locale setup that is wanted for production use, as this will exercise the locale- and encoding-related code portions that will actually be used in production.  However, please report it anyway, so that we can add an  ORDER BY  to that particular query to eliminate the bogus  ﬁfailureﬂ  in future releases.  On platforms supporting  getrlimit() , the server should automatically choose a safe value of max_stack_depth ; so unless you've manually overridden this setting, a failure of this kind is a reportable bug.  For example: some systems interpret very small floating-point values as zero, rather than reporting an underflow error.  If any such file is an exact match, the test is considered to pass; otherwise, the one that generates the shortest diff is used to create the failure report.  This is currently supported when compiling with GCC and requires the  gcov  and  lcov  programs.  Event Support Functions .  SSL Support .  Importing a Large Object .  Exporting a Large Object .  So some aspects of libpq's behavior will be important to you if you use one of those packages.  PGconn *PQsetdbLogin(const char *pghost,                      const char *pgport,                      const char *pgoptions,                      const char *pgtty,                      const char *dbName,                      const char *login,   . libpq - C Library PGconn *PQsetdb(char *pghost,                 char *pgport,                 char *pgoptions,                 char *pgtty,                 char *dbName); This is a macro that calls  PQsetdbLogin  with null pointers for the  login  an.  PQpingParams PQpingParams  reports the status of the server.  This might indicate that the server is not running, or that there is something wrong with the given connection parameters (for example, wrong port number), or that there is a network connectivity problem (for example, a firewall blocking the connect.  PQping PQping  reports the status of the server.  Example: host=localhost port=5432 dbname=mydb connect_timeout=10 The recognized parameter key words are listed in  Section 33.  Connection URIs The general form for a connection URI is: postgresql://[user[:password]@][netloc][:port][,.  For example: postgresql:///mydb?host=localhost&port=5433 Percent-encoding may be used to include symbols with special meaning in any of the URI parts, e.  A URI of the form  postgresql://host1:port1,host2:port2,host3:port3/ 743 . libpq - C Library is equivalent to a connection string of the form  host=host1,host2,host3 port=port1,port2,port3 .  In the Keyword/Value format, the  host ,  hostaddr , and  port  options accept a comma-separated list of values.  As an exception, if only one  port  is specified, it applies to all the hosts.  In the connection URI format, you can list multiple  host:port  pairs separated by commas, in the host  component of the URI.  If your machine supports IPv6, you can also use those addresses.  Using  hostaddr  instead of  host  allows the application to avoid a host name look-up, which might be important in applications with time constraints.  port Port number to connect to at the server host, or socket file name extension for Unix-domain connections.   If multiple hosts were given in the  host  or  hostaddr  parameters, this parameter may specify a comma-separated list of ports of the same length as the host list, or it may specify a single port number to be used for all hosts.  An empty string, or an empty item in a comma- separated list, specifies the default port number established when PostgreSQL was built.  (No error is reported if this file does not exist.  It is only supported on systems where  TCP_KEEPIDLE  or an equivalent socket option is available, and on Windows; on other systems, it has no effect.  It is only supported on systems where  TCP_KEEPINTVL  or an equivalent socket option is available, and on Windows; on other systems, it has no effect.  It is only supported on systems where  TCP_KEEPCNT  or an equivalent socket option is available; on other systems, it has no effect.  If PostgreSQL is compiled without SSL support, using options  require ,  verify-ca , or  verify-full  will cause an error, while options  allow  and  prefer  will be accepted but libpq will not actually attempt an SSL connection.  This option is only available if PostgreSQL is compiled with SSL support.  This parameter is ignored if a connection without SSL is made, or if the version of OpenSSL used does not support it. ) This option is only supported on platforms for which the  peer  authentication method is implemented; see  Section 20.  Currently this is disregarded except on Windows builds that include both GSSAPI and SSPI support.  If a multi-host connection string is used, the values of  PQhost ,  PQport , and  PQpass  can change if a new connection is established using the same  PGconn  object.  PQport   Returns the port of the active connection. libpq - C Library char *PQport(const PGconn *conn); If multiple ports were specified in the connection parameters,  PQport  returns the port actually connected to.  PQport  returns  NULL  if the  conn  argument is  NULL .  Otherwise, if there is an error producing the port information (perhaps if the connection has not been fully established or there was an error), it returns an empty string.  If multiple ports were specified in the connection parameters, it is not possible to rely on the result of  PQport  until the connection is established.   PQTRANS_UNKNOWN  is reported if the connection is bad.  PQTRANS_ACTIVE  is reported only when a query has been sent to the server and not yet completed. libpq - C Library const char *PQparameterStatus(const PGconn *conn, const char  *paramName); Certain parameter values are reported by the server automatically at connection startup or whenever their values change.  Parameters reported as of the current release include  server_version , server_encoding ,  client_encoding ,  application_name ,  is_superuser , session_authorization ,  DateStyle ,  IntervalStyle ,  TimeZone , integer_datetimes , and  standard_conf.  ( server_encoding , TimeZone , and  integer_datetimes  were not reported by releases before 8. 0;  standard_conforming_strings  was not reported by releases before 8. 1; IntervalStyle  was not reported by releases before 8. 4;  application_name  was not reported by releases before 9. 0-protocol servers do not report parameter settings, but libpq includes logic to obtain values for  server_version  and  client_encoding  anyway.  If no value for  standard_conforming_strings  is reported, applications can assume it is  off , that is, backslashes are treated as escapes in string literals.  int PQprotocolVersion(const PGconn *conn); Applications might wish to use this function to determine whether certain features are supported. 4 servers support only protocol 2. 0 is obsolete and not supported by libpq.   PQexecParams  is supported only in protocol 3.   PQprepare  is supported only in protocol 3.  PQexecPrepared  is supported only in protocol 3.   PQdescribePrepared  is supported only in protocol 3.  PQdescribePortal   Submits a request to obtain information about the specified portal, and waits for completion.  PGresult *PQdescribePortal(PGconn *conn, const char  *portalName); PQdescribePortal  allows an application to obtain information about a previously created portal.  (libpq does not provide any direct access to portals, but you can use this function to inspect the properties of a cursor created with a  DECLARE CURSOR  SQL command. ) PQdescribePortal  is supported only in protocol 3.  portalName  can be  ""  or  NULL  to reference the unnamed portal, otherwise it must be the name of an existing portal.  The functions  PQnfields ,  PQfname ,  PQftype , etc can be applied to the  PGresult  to obtain information about the result columns (if any) of the portal.  If the  PGresult is not an error result,  ﬁPGresult is not an error resultﬂ  is reported instead.  PQresultErrorField Returns an individual field of an error report.  This is present only in reports generated by PostgreSQL versions 9. ) PG_DIAG_SOURCE_FILE The file name of the source-code location where the error was reported.  PG_DIAG_SOURCE_LINE The line number of the source-code location where the error was reported.  PG_DIAG_SOURCE_FUNCTION The name of the source-code function reporting the error.  Tip It is especially important to do proper escaping when handling strings that were received from an untrustworthy source.  Also, it has no way to report error conditions.  It has a few deficiencies, however, that can be of importance to some users: Ł PQexec  waits for the command to be completed.  PQsendDescribePortal   Submits a request to obtain information about the specified portal, without waiting for completion.  int PQsendDescribePortal(PGconn *conn, const char *portalName); This is an asynchronous version of  PQdescribePortal : it returns 1 if it was able to dispatch the request, and 0 if not.  Ordinarily, libpq discards any such rows and reports only the 773 .  The per-column format codes will always be zero when the overall copy format is textual, but the binary format can support both text and binary columns.  Although they still work, they are deprecated due to poor error handling, inconvenient methods of detecting end-of-data, and lack of support for binary or nonblocking transfers.  It is particularly important that this function, rather than  free() , be used on Microsoft Windows.  Currently supported algorithms are  md5  and  scram-sha-256  ( on  and  off  are also accepted as aliases for  md5 , for compatibility with older server versions).  Note that support for  scram- sha-256  was introduced in PostgreSQL version 10, and will not work correctly with older server versions.  It is exported because some applications find it useful to generate result objects (particularly objects with error status) themselves.  Event Support Functions PQregisterEventProc   Registers an event callback procedure with libpq. libpq - C Library Ł   PGPORT  behaves the same as the  port  connection parameter.  This file should contain lines of the following format: hostname : port : database : username : password (You can add a reminder comment to the file by copying the line above and preceding it with  # .  For example: # comment [mydb] host=somehost port=5433 user=admin 794 .  LDAP Lookup of Connection Parameters If libpq has been compiled with LDAP support (option  --with-ldap  for  configure ) it is possible to retrieve connection options like  host  or  dbname  via LDAP from a central server.  The URL must conform to RFC 1959 and be of the form ldap://[ hostname [: port ]]/ search_base ? attribute ? search_scope ? filter where  hostname  defaults to  localhost  and  port  defaults to 389. com description:port=5439 description:dbname=mydb description:user=mydb_user description:sslmode=require might be queried with the following LDAP URL: ldap://ldap. conf  would be: # only host and port are stored in LDAP, specify dbname and user  explicitly [customerdb] dbname=customer user=appuser ldap://ldap.  SSL Support 795 . libpq - C Library PostgreSQL has native support for using SSL connections to encrypt client/server communications for increased security. cnf  and is located in the directory reported by  openssl version -d .  prefer Maybe No I don't care about encryption, but I wish to pay the overhead of encryption if the server supports it.  SSL Library Initialization If your application initializes  libssl  and/or  libcrypto  libraries and libpq is built with SSL support, you should call  PQinitOpenSSL  to tell libpq that the  libssl  and/or  libcrypto libraries have been initialized b.  When SSL support is not compiled in, this function is present but does nothing. ) For maximum portability, put the  -L  option before the  -lpq  option. libpq - C Library     PQclear(res);     /* close the portal .   */     res = PQexec(conn, "CLOSE myportal");     PQclear(res);     /* end the transaction */     res = PQexec(conn, "END");     PQclear(res);     /* close the connection to the database and cleanup */     PQfinish(conn);     return 0; } Example 33.  We use the libpq C library for the examples in this chapter, but most programming interfaces native to PostgreSQL support equivalent functionality.  Other interfaces might use the large object interface internally to provide generic support for large values.  PostgreSQL also supports a storage system called  ﬁTOASTﬂ , which automatically stores values larger than a single database page into a secondary storage area per table.  Also, reading and updating portions of a large object can be done efficiently, while most operations on a TOASTed field will read or write the whole value as a unit.  Importing a Large Object  To import an operating system file as a large object, call Oid lo_import(PGconn *conn, const char *filename); filename  specifies the operating system name of the file to be imported as a large object.   The function Oid lo_import_with_oid(PGconn *conn, const char *filename, Oid  lobjId); also imports a new large object.  If  lobjId  is  InvalidOid  (zero) then lo_import_with_oid  assigns an unused OID (this is the same behavior as  lo_import ).  lo_import_with_oid  is new as of PostgreSQL 8.  Exporting a Large Object  To export a large object into an operating system file, call 813 . Large Objects int lo_export(PGconn *conn, Oid lobjId, const char *filename); The  lobjId  argument specifies the OID of the large object to export and the  filename  argument specifies the operating system name of the file.  The ones just as convenient to call via SQL commands are  lo_creat ,  lo_create , lo_unlink ,  lo_import , and  lo_export . raster, '/tmp/motd') FROM image     WHERE name = 'beautiful image'; The server-side  lo_import  and  lo_export  functions behave considerably differently from their client-side analogs.  In contrast, the client-side import and export functions read and write files in the client's file system, using the permissions of the client program. h" #define BUFSIZE         1024 /*  * importFile -  *    import file "in_filename" into database as large object  "lobjOid"  *  */ static Oid importFile(PGconn *conn, char *filename) {     Oid         lobjId;     int         lobj_fd;     char        . Large Objects         {             fprintf(stderr, "\nWRITE FAILED!\n");             break;         }     }     free(buf);     fprintf(stderr, "\n");     lo_close(conn, lobj_fd); } /*  * exportFile -  *    export large object "lobjOid" to file "out_. Large Objects     res = PQexec(conn, "begin");     PQclear(res);     printf("importing file \"%s\" . \n", in_filename); /*  lobjOid = importFile(conn, in_filename); */     lobjOid = lo_import(conn, in_filename);     if (lobjOid == 0)         fprintf(stderr, "%s\n", PQerrorMessage(conn));     else     {         printf("\tas large object %u. \n",  out_filename); /*      exportFile(conn, lobjOid, out_filename); */         if (lo_export(conn, lobjOid, out_filename) < 0)             fprintf(stderr, "%s\n", PQerrorMessage(conn));     }     res = PQexec(conn, "end");     PQclear(res);     PQf.  Third, embedded SQL in C is specified in the SQL standard and supported by many other SQL database systems.  The PostgreSQL implementation is designed to match this standard as much as possible, and it is usually possible to port embedded SQL programs written for other SQL databases to PostgreSQL with relative ease.  Connecting to the Database Server One connects to a database using the following statement: EXEC SQL CONNECT TO  target  [AS  connection-name ] [USER  user-name ]; The  target  can be specified in the following ways: Ł dbname [@ hostname ][: port ] . ECPG - Embedded SQL in C Ł unix:postgresql:// hostname [: port ][/ dbname ][? options ] Ł an SQL string literal containing one of the above forms Ł a reference to a character variable containing one of the above forms (see examples) Ł DEFAULT If you .  So if you want to develop portable applications, you might want to use something based on the last example above to encapsulate the connection target string somewhere.  The embedded SQL interface also supports autocommit of transactions (similar to psql's default behavior) via the  -t  command-line option to  ecpg  (see  ecpg ) or via the  EXEC SQL SET AUTOCOMMIT TO ON  statement.  In particular, it has implemented support for the  numeric ,  decimal ,  date , timestamp , and  interval  types.  Arrays Multi-dimensional SQL-level arrays are not directly supported in ECPG.  Composite Types Composite types are not directly supported in ECPG, but an easy workaround is possible.  User-defined Base Types New user-defined base types are not directly supported by ECPG.  (This is important only on Windows, where memory allocation and release sometimes need to be done by the same library.  At the moment ECPG always parses the complete string and so it currently does not support to store the address of the first invalid character in  *endptr .  At the moment ECPG always parses the complete string and so it currently does not support to store the address of the first invalid character in  *endptr .  At the moment ECPG always parses the complete string and so it currently does not support to store the address of the first invalid character in  *endptr .  See  PGTYPESInvalidTimestamp  for important notes on this value.  Note that time zones are not supported by ECPG.  At the moment ECPG always parses the complete string and so it currently does not support to store the address of the first invalid character in  *endptr .  The statement to build a named SQL Descriptor Area is below: EXEC SQL SET DESCRIPTOR  name  VALUE  num   field  = : hostvar ; PostgreSQL supports retrieving more that one record in one  FETCH  statement and storing the data in host variables in this.  The definitions are similar at the core, but if you want to write portable applications, then you should investigate the different implementations carefully.  The PostgreSQL server natively supports SQLSTATE  error codes; therefore a high degree of consistency can be achieved by using this error code scheme throughout all applications.  Therefore, this scheme can only achieve poor portability and does not have a hierarchical code assignment.  Remember that these are not portable to other SQL implementations.  To simplify the porting of applications to the  SQLSTATE  scheme, the corresponding  SQLSTATE  is also listed.  Note that this makes your code unportable.  Large Objects Large objects are not directly supported by ECPG, but ECPG application can manipulate large objects through the libpq large object functions, obtaining the necessary  PGconn  object by calling the ECPGget_PGconn()  function.  C++ Applications ECPG has some limited support for C++ applications.  [  database_name  ] [  @ host  ] [  : port  ] Connect over TCP/IP unix:postgresql:// host  [  : port  ]  /  [  database_name  ] [ ? connection_option  ] Connect over Unix-domain sockets tcp:postgresql:// host  [  : port  ]  /  [  database_name  ] [ .  Only  COUNT , to get the number of columns in the result set, is currently supported. 1  for a list of supported items.  Only  COUNT , to set the number of descriptor items, is currently supported. 1  for a list of supported items.  Besides the previously explained syntactic sugar, the Informix compatibility mode ports some functions for input, output and transformation of data as well as embedded SQL statements known from E/SQL to ECPG.  For example, PostgreSQL's datetime and interval types do not know about ranges like for example  YEAR TO MINUTE  so you won't find support in ECPG for that either.  Additional Types The Informix-special "string" pseudo-type for storing right-trimmed character string data is now supported in Informix-mode without using  typedef .  Informix-compatible SQLDA Descriptor Areas Informix-compatible mode supports a different structure than the one described in  Section 35.  The most important function in the library is  ECPGdo , which takes care of executing most commands.  The information schema is defined in the SQL standard and can therefore be expected to be portable and remain stable Š unlike the system catalogs, which are specific to PostgreSQL and are modeled after implementation concerns.  Since PostgreSQL does not support multiple character sets within one database, this view only shows one, which is the database encoding.  (not all supported by PostgreSQL).   sql_features The table  sql_features  contains information about which formal features defined in the SQL standard are supported by PostgreSQL.   sql_languages The table  sql_languages  contains one row for each SQL language binding that is supported by PostgreSQL.  PostgreSQL supports direct SQL and embedded SQL in C; that is all you will learn from this table.  PostgreSQL only supports the language C.   sql_packages The table  sql_packages  contains information about which feature packages defined in the SQL standard are supported by PostgreSQL.   sql_packages  Columns Name Data Type Description feature_id character_data Identifier string of the package feature_name character_data Descriptive name of the package is_supported yes_or_no YES  if the package is fully supported by the current ver.   sql_parts The table  sql_parts  contains information about which of the several parts of the SQL standard are supported by PostgreSQL.   sql_parts  Columns Name Data Type Description feature_id character_data An identifier string containing the number of the part feature_name character_data Descriptive name of the part is_supported yes_or_no YES  if the part is fully supported by th.   sql_sizing  Columns Name Data Type Description sizing_id cardinal_number Identifier of the sizing item sizing_name character_data Descriptive name of the sizing item supported_value cardinal_number Value of the sizing item, or 0 if the size is unli.  PostgreSQL does not support this.  In PostgreSQL, sequences also support  SELECT  and  UPDATE  privileges in addition to the  USAGE privilege.  Support Functions for Aggregates .  Index Method Support Routines .  Supported Argument and Result Data Types .  Reporting Errors and Messages .  Interface Support Functions .  Exported Snapshots .  Synchronous Replication Support for Logical Decoding .  Note two important things about defining the function: Ł The select list order in the query must be exactly the same as that in which the columns appear in the table associated with the composite type.  This notation is specified in recent versions of the SQL standard, and thus may be more portable than using  SETOF .  If this rule is violated, the behavior is not portable.  Another important example is that the  current_timestamp  family of functions qualify as STABLE , since their values do not change within a transaction.  For functions written in SQL or in any of the standard procedural languages, there is a second important property determined by the volatility category, namely the visibility of any data changes that have been made by the SQL command that is calling.  Support for that calling convention is indicated by writing a  PG_FUNCTION_INFO_V1()  macro call for the function, as illustrated below.  Another important point is to avoid leaving any uninitialized bits within data type values; for example, take care to zero out any alignment padding bytes that might be present in structs.  Without this, it's difficult to support hash indexes or hash joins, as you must pick out only the significant bits of your data structure to compute a hash.  For portability reasons it's best to include  postgres. sl ) can be omitted from the  CREATE FUNCTION  command, and normally should be omitted for best portability.  (If it is not, you can report an error along the lines of  ﬁfunction returning record called in context that cannot accept type recordﬂ .  When using ValuePerCall mode, it is important to remember that the query is not guaranteed to be run to completion; that is, due to options such as  LIMIT , the executor might stop making calls to the set-returning function before all rows have been.  To use the ValuePerCall support macros described here, include  funcapi. Extending SQL         /* Build a tuple descriptor for our result type */         if (get_call_result_type(fcinfo, NULL, &tupdesc) !=  TYPEFUNC_COMPOSITE)             ereport(ERROR,                     (errcode(ERRCODE_FEATURE_NOT_SUPPORTED),         .  These options are implemented behind the scenes and are not the concern of the aggregate's support functions.  Moving-Aggregate Mode Aggregate functions can optionally support  moving-aggregate mode , which allows substantially faster execution of aggregate functions within windows with moving frame starting points.  Without an inverse transition function, the window function mechanism must recalculate the aggregate from scratch each time the frame starting point moves, resulting in run time proportional to the number of input rows times the average frame length.  With an inverse transition function, the run time is only proportional to the number of input rows.  As an example, we could extend the  sum  aggregate given above to support moving-aggregate mode like this: CREATE AGGREGATE sum (complex) (     sfunc = complex_add,     stype = complex,     initcond = '(0,0)',     msfunc = complex_add,     minvfunc .  When writing moving-aggregate support functions, it is important to be sure that the inverse transition function can reconstruct the correct state value exactly.  The most common case is where the aggregate support functions are to be written in C and the state type should be declared as  internal  because there is no SQL-level equivalent for it.  PostgreSQL also supports ordered-set aggregates , which differ from normal aggregates in two key ways.  Unlike the case for normal aggregates, the sorting of input rows for an ordered-set aggregate is  not done behind the scenes, but is the responsibility of the aggregate's support functions.  While normal aggregates can often be implemented with support functions written in PL/pgSQL or another PL language, ordered-set aggregates generally have to be written in C, since their state values aren't definable as any SQL data type.  Currently, ordered-set aggregates cannot be used as window functions, and therefore there is no need for them to support moving-aggregate mode.  Partial Aggregation Optionally, an aggregate function can support  partial aggregation .  This mode can be used for parallel aggregation by having different worker processes scan different portions of a table. ) To support partial aggregation, the aggregate definition must provide a  combine function , which takes two values of the aggregate's state type (representing the results of aggregating over two subsets of the input rows) and produces a new value o.  As simple examples,  MAX  and  MIN  aggregates can be made to support partial aggregation by specifying the combine function as the same greater-of-two or lesser-of-two comparison function that is used as their transition function.  The parallel-safety markings on its support functions are not consulted.  Support Functions for Aggregates A function written in C can detect that it is being called as an aggregate support function by calling AggCheckCallContext , for example: if (AggCheckCallContext(fcinfo, NULL)) One reason for checking this is that wh. ) Another support routine available to aggregate functions written in C is  AggGetAggref , which returns the  Aggref  parse node that defines the aggregate call. 2 , PostgreSQL can be extended to support new data types.  Binary I/O is normally faster but less portable than textual I/O.  Operators can then be defined atop the functions, and if needed, operator classes can be created to support indexing of the data type.  To support TOAST storage, the C functions operating on the data type must always be careful to unpack any toasted values they are handed by using  PG_DETOAST_DATUM .  If data alignment is unimportant (either just for a specific function or because the data type specifies byte alignment anyway) then it's possible to avoid some of the overhead of PG_DETOAST_DATUM .  If the alignment is important you must go through the regular  PG_DETOAST_DATUM  interface.  Another feature that's enabled by TOAST support is the possibility of having an  expanded  in-memory data representation that is more convenient to work with than the format that is stored on disk.  Therefore, existing functions that work with the flat varlena format will continue to work, though slightly inefficiently, with expanded inputs; they need not be converted until and unless better performance is important.  PostgreSQL supports left unary, right unary, and binary operators.  In most cases it is only practical to support hashing for operators that take the same data type on both sides.  Also, it is a good idea (but not strictly required) for a hash operator family that supports multiple data types to provide equality operators for every combination of the data types; this allows better optimization.  Also, it is a good idea (but not strictly required) for a  btree  operator family that supports multiple data types to provide equality operators for every combination of the data types; this allows better optimization.  Support for regular access to tables is built into PostgreSQL, but all index methods are described 1037 .  An operator class can also specify some  support procedures  that are needed by the internal operations of the index method, but do not directly correspond to any  WHERE - clause operator that can be used with the index.  B-tree Strategies Operation Strategy Number less than 1 less than or equal 2 equal 3 greater than or equal 4 greater than 5 Hash indexes support only equality comparisons, and so they use only one strategy, shown in Table 37.  Instead, the ﬁconsistencyﬂ  support routine of each particular GiST operator class interprets the strategy numbers however it likes.  Instead the support routines of each operator class interpret the strategy numbers according to the operator class's definition.  Instead the support routines of each operator class interpret the strategy numbers according to the operator class's definition.  Instead the support routines of each operator class interpret the strategy numbers 1039 .  (Some index access methods also support  ordering operators , which typically don't return Boolean values; that feature is discussed in  Section 37.  Index Method Support Routines Strategies aren't usually enough information for the system to figure out how to use an index.  In practice, the index methods require additional support routines in order to work.  The index method defines the set of functions it needs, and the operator class identifies the correct functions to use by assigning them to the  ﬁsupport function numbersﬂ  specified by the index method.  B-trees require a single support function, and allow a second one to be supplied at the operator class author's option, as shown in  Table 37.  B-tree Support Functions Function Support Number Compare two keys and return an integer less than zero, zero, or greater than zero, indicating whether the first key is less than, equal to, or greater than the second 1 Return the addresses of C-calla. h  (optional) 2 Hash indexes require one support function, shown in  Table 37.  Hash Support Functions Function Support Number Compute the hash value for a key 1 GiST indexes have nine support functions, two of which are optional, as shown in  Table 37.  GiST Support Functions Function Description Support Number consistent determine whether key satisfies the query qualifier 1 union compute union of a set of keys 2 compress compute a compressed representation of a key or value to be indexed 3 decompr.  SP-GiST Support Functions Function Description Support Number config provide basic information about the operator class 1 choose determine how to insert a new value into an inner tuple 2 picksplit determine how to partition a set of values 3 inner_c.  GIN Support Functions Function Description Support Number compare compare two keys and return an integer less than zero, zero, or greater than zero, indicating 1 1041 . Extending SQL Function Description Support Number whether the first key is less than, equal to, or greater than the second extractValue extract keys from a value to be indexed 2 extractQuery extract keys from a query condition 3 consistent determine . 13 ; those basic functions may require additional support functions to be provided.  BRIN Support Functions Function Description Support Number opcInfo return internal information describing the indexed columns' summary data 1 add_value add a new value to an existing summary index tuple 2 consistent determine whether value matches q.  The number and types of the arguments to each support function are likewise dependent on the index method.  For B-tree and hash the comparison and hashing support functions take the same input data types as do the operators included in the operator class, but this is not the case for most GiST, SP-GiST, GIN, and BRIN support functions.  The next step is the registration of the support routine required by B-trees.  This is how we declare the function: CREATE FUNCTION complex_abs_cmp(complex, complex)     RETURNS integer     AS ' filename '     LANGUAGE C IMMUTABLE STRICT; Now that we have the required operators and support routine, we can finally create the op.  An operator family contains one or more operator classes, and can also contain indexable operators and corresponding support functions that belong to the family as a whole but not to any single class within the family.  The reason for defining operator classes is that they specify how much of the family is needed to support any particular index.  The instances that have both input types equal to an operator class's input type are the primary operators and support functions for that operator class, and in most cases should be declared as part of the operator class rather than as loose members.  In a B-tree operator family, all the operators in the family must sort compatibly, meaning that the transitive laws hold across all the data types supported by the family:  ﬁif A = B and B = C, then A = 1046 .  For each operator in the family there must be a support function having the same two input data types as the operator.  Each operator class should include just the non-cross-type operators and support function for its data type.  To build a multiple-data-type hash operator family, compatible hash support functions must be created for each data type supported by the family.  Notice that there is only one support function per data type, not one per equality operator.  Each operator class should include just the non-cross-type equality operator and the support function for its data type.  The set of operators supported is just whatever the primary support functions for a given operator class can handle.  But since that kind of operator class only provides equality, in practice it is only enough to support array equality.  Another important point is that an operator that appears in a hash operator family is a candidate for hash joins, hash aggregation, and related optimizations.  Ordering Operators Some index access methods (currently, only GiST) support the concept of  ordering operators .  The reason for defining ordering operators that way is that it supports nearest-neighbor searches, if the operator is one that measures distance.  The index methods that support lossy searches (currently, GiST, SP-GiST and GIN) allow the support functions of individual operator classes to set the recheck flag, and so this is essentially an operator-class feature.          STORAGE box; At present, only the GiST, GIN and BRIN index methods support a  STORAGE  type that's different from the column data type.  The GiST  compress  and  decompress  support routines must deal with data- type conversion when  STORAGE  is used.  The GIN  extractValue  and extractQuery  support routines are responsible for extracting keys from indexed values.  BRIN is similar to GIN: the  STORAGE  type identifies the type of the stored summary values, and operator classes' support procedures are responsible for interpreting the summary values correctly.  PostgreSQL does not currently support extension scripts issuing  CREATE POLICY  or  SECURITY LABEL  statements.  Another important point is that schemas can belong to extensions, but not vice versa: an extension as such has an unqualified name and does not exist  ﬁwithinﬂ  any schema.  There are three supported levels of relocatability: Ł A fully relocatable extension can be moved into another schema at any time, even after it's been loaded into a database.  Ł If the extension does not support relocation at all, set  relocatable = false  in its control file, and also set  schema  to the name of the intended target schema.  As the foreign key relationships are set up at CREATE EXTENSION time (prior to data being loaded into the tables) circular dependencies are not supported.  The update mechanism can be used to solve an important special case: converting a  ﬁlooseﬂ  collection of objects into an extension.   CREATE EXTENSION  supports this case with its  FROM   old_version  option, which causes it to not run the normal installation script for the target version, but instead the update script named  extension -- old_version -- target_version .  The command  make install will install the control and script files into the correct directory as reported by pg_config.  Extension Building Infrastructure If you are thinking about distributing your PostgreSQL extension modules, setting up a portable build system for them can be fairly difficult.  Note that trying to run a test that is missing its expected file will be reported as  ﬁtroubleﬂ , so make sure you have all expected files.  Tip The easiest way to create the expected files is to create empty files, then do a test run (which will of course report differences).   INSTEAD OF  triggers do not support  WHEN  conditions.  Each programming language that supports triggers has its own method for making the trigger input data available to the trigger function. ) The function  trigf  reports the number of rows in the table  ttest  and skips the actual operation if the command attempts to insert a null value into the column  x .  Like regular triggers, event triggers can be written in any procedural language that includes event trigger support, or in C, but not in plain SQL.  Currently, the only supported events are  ddl_command_start ,  ddl_command_end , table_rewrite  and  sql_drop .  Support for additional events may be added in future releases.  The event trigger mechanism does not support these object types.  For a complete list of commands supported by the event trigger mechanism, see  Section 39. 1  lists all commands for which event triggers are supported.  Event Trigger Support by Command Tag Command Tag ddl_command_start ddl_command_end sql_drop table_rewrite Notes ALTER AGGREGATE X X - -   ALTER COLLATION X X - -   ALTER CONVERSION X X - -   ALTER DOMAIN X X - -   ALTER DEFAULT PRIVILEGES X X - -   . 9 | m       |        90 (8 rows) This is the simplest  SELECT  you can do on our views, so we take this opportunity to explain the basics of view rules.  There are several ways in which PostgreSQL can support the appearance of updating a view, however.  Notice we are also exploiting the ability to put an index on the materialized view, whereas  file_fdw  does not support indexes; this advantage might not apply for other sorts of foreign data access.  Also, there are some cases that are not supported by these types of rules at all, notably including WITH  clauses in the original query and multiple-assignment sub- SELECT s in the  SET  list of UPDATE  queries.  (Recursive expansion of a rule will be detected and reported as an error.  They need to support this same functionality anyway for  INSERT .  Here we can see why it is important that the original query tree is executed last. sl_name; If you want to support  RETURNING  queries on the view, you need to make the rules include RETURNING  clauses that compute the view rows. un_name); Note that this one rule supports both  INSERT  and  INSERT RETURNING  queries on the view Š the RETURNING  clause is simply ignored for  INSERT .  It is important to understand that even a view created with the  security_barrier  option is intended to be secure only in the limited sense that the contents of the invisible tuples will not be passed to possibly-insecure functions.  This might reduce the number of rows it processes, and if so the reported status will be affected. Procedural Languages support is configured in, the handlers for PL/Tcl and PL/TclU are built and installed in the library directory, but the language itself is not installed in any database by default.  Likewise, the PL/Perl and PL/PerlU handlers are built and installed if Perl support is configured, and the PL/PythonU handler is installed if Python support is configured, but these languages are not installed by default.  It's portable and easy to learn.  Supported Argument and Result Data Types Functions written in PL/pgSQL can accept as arguments any scalar or array data type supported by the server, and they can return a result of any of these types.  It is important not to confuse the use of  BEGIN / END  for grouping statements in PL/pgSQL with the similarly-named SQL commands for transaction control. user_id%TYPE; By using  %TYPE  you don't need to know the data type of the structure you are referencing, and most importantly, if the data type of the referenced item changes in the future (for instance: you change the type of  user_id  from  intege.  But the form with  %ROWTYPE  is more portable.  Normally these details are not important to a PL/pgSQL user, but they are useful to know when trying to diagnose a problem.  For  INSERT / UPDATE / DELETE  with  RETURNING , PL/pgSQL reports an error for more than one returned row, even when  STRICT  is not specified. PL/pgSQL - SQL Procedural Language If the  STRICT  option is given, an error is reported unless the query produces exactly one row.  The important difference is that  EXECUTE  will re-plan the command on each execution, generating a plan that is specific to the current parameter values; whereas PL/pgSQL may otherwise create a generic plan and cache it for re- use.  SELECT INTO  is not currently supported within  EXECUTE ; instead, execute a plain  SELECT command and specify  INTO  as part of the  EXECUTE  itself.  Note The PL/pgSQL  EXECUTE  statement is not related to the  EXECUTE  SQL statement supported by the PostgreSQL server.  Control Structures Control structures are probably the most useful (and important) part of PL/pgSQL. 5 , retrieves information about current execution state (whereas the  GET STACKED DIAGNOSTICS  command discussed above reports information about the execution state as of a previous error).  (Internally, a refcursor  value is simply the string name of a so-called portal containing the active query for the cursor.  This name can be passed around, assigned to other  refcursor  variables, and so on, without disturbing the portal. ) All portals are implicitly closed at transaction end.  cursor  must be the name of a  refcursor  variable that references an open cursor portal.   CLOSE CLOSE  cursor ; CLOSE  closes the portal underlying an open cursor.  To do this, the function opens the cursor and returns the cursor name to the caller (or simply opens the cursor using a portal name specified by or otherwise known to the caller).  The portal name used for a cursor can be specified by the programmer or automatically generated.  To specify a portal name, simply assign a string to the  refcursor  variable before opening it.  The string value of the  refcursor  variable will be used by  OPEN  as the name of the underlying portal.  However, if the  refcursor  variable is null,  OPEN  automatically generates a name that does not conflict with any existing portal, and assigns it to the  refcursor  variable.  Note A bound cursor variable is initialized to the string value representing its name, so that the portal name is the same as the cursor variable name, unless the programmer overrides it by 1141 .  Reporting Errors and Messages Use the  RAISE  statement to report messages and raise errors.  Whether messages of a particular priority are reported to the client, written to the server log, or both is controlled by the  log_min_messages  and  client_min_messages  configuration variables.  The format string specifies the error message text to be reported. PL/pgSQL - SQL Procedural Language You can attach additional information to the error report by writing  USING  followed by  option  = expression  items.  ERRCODE Specifies the error code (SQLSTATE) to report, either by condition name, as shown in Appendix A , or directly as a five-character SQLSTATE code.  (If an error occurs while evaluating the  condition , it is reported as a normal error.  Note that  ASSERT  is meant for detecting program bugs, not for reporting ordinary error conditions.  For  INSERT  and  UPDATE operations, the return value should be  NEW , which the trigger function may modify to support  INSERT RETURNING  and  UPDATE RETURNING  (this will also affect the row value passed to any subsequent triggers, or passed to a .  A PL/pgSQL Event Trigger Procedure This example trigger simply raises a  NOTICE  message each time a supported command is executed.  PL/pgSQL Under the Hood This section discusses some implementation details that are frequently important for PL/pgSQL users to know.  By default, PL/pgSQL will report an error if a name in a SQL statement could refer to either a variable or a table column.  Porting from Oracle PL/SQL This section explains differences between PostgreSQL's PL/pgSQL language and Oracle's PL/SQL language, to help developers who port applications from Oracle ®  to PostgreSQL.  The main differences you should keep in mind when porting from PL/SQL to PL/pgSQL are: Ł If a name used in a SQL command could be either a column name of a table or a reference to a variable of the function, PL/SQL treats it as a column name.  It's often best to avoid such ambiguities in the first place, but if you have to port a large amount of code that depends on this behavior, setting  variable_conflict may be the best solution.  Ł Integer  FOR  loops with  REVERSE  work differently: PL/SQL counts down from the second number to the first, while PL/pgSQL counts down from the first number to the second, requiring the loop bounds to be swapped when porting. 9  shows how to port a simple function from PL/SQL to PL/pgSQL.  Ł The  show errors  command does not exist in PostgreSQL, and is not needed since errors are reported automatically.  This is how this function would look when ported to PostgreSQL: CREATE OR REPLACE FUNCTION cs_fmt_browser_version(v_name varchar,                                                   v_version  varchar) RETURNS varchar AS $$ BEGIN     IF v_version IS N. 10  shows how to port a function that creates another function and how to handle the ensuing quoting problems. 11  shows how to port a function with  OUT  parameters and string manipulation. 3  there is a PL/pgSQL implementation of  instr  that you can use to make your porting easier. 12  shows how to port a procedure that uses numerous features that are specific to Oracle.  This is how we could port this procedure to PL/pgSQL: CREATE OR REPLACE FUNCTION cs_create_job(v_job_id integer) RETURNS  void AS $$ DECLARE     a_running_job_count integer; BEGIN     LOCK TABLE cs_jobs IN EXCLUSIVE MODE;     SELECT count(*) INTO a_.  2 The exception names supported by PL/pgSQL are different from Oracle's.  Other Things to Watch For This section explains a few other things to watch for when porting Oracle PL/SQL functions to PostgreSQL.  Appendix This section contains the code for a set of Oracle-compatible  instr  functions that you can use to simplify your porting efforts.  The shared object code for the PL/Tcl and PL/TclU call handlers is automatically built and installed in the PostgreSQL library directory if Tcl support is specified in the configuration step of the installation procedure.  Whether messages of a particular priority are reported to the client, written to the server log, or both is controlled by the  log_min_messages  and  client_min_messages  configuration variables.  Here's a little example event trigger procedure that simply raises a  NOTICE  message each time a supported command is executed: CREATE OR REPLACE FUNCTION tclsnitch() RETURNS event_trigger AS $$   elog NOTICE "tclsnitch: $TG_event $TG_tag" $$ LANGU.  If an error is not caught but is allowed to propagate out to the top level of execution of the PL/Tcl function, it is reported as a SQL error in the function's calling query.  Conversely, SQL errors that occur within PL/Tcl's  spi_exec ,  spi_prepare , and  spi_execp commands are reported as Tcl errors, so they are catchable by Tcl's  catch  command.  The contents are in Tcl list format, and the first word identifies the subsystem or library reporting the error; beyond that the contents are left to the individual subsystem or library.  For database errors reported by PL/Tcl commands, the first word is  POSTGRES , the second word is the PostgreSQL version number, and additional words are field name/value pairs providing detailed information about the error.  The subtransaction  command does not trap errors, it only assures that all database operations executed inside its scope will be rolled back together when an error is reported.  A rollback of an explicit subtransaction occurs on any error reported by the contained Tcl code, not only errors originating from database access.  PL/Perl also supports anonymous code blocks called with the  DO  statement: DO $$     # PL/Perl code $$ LANGUAGE plperl; An anonymous code block receives no arguments, and whatever value it might return is discarded.  The optional second parameter to  spi_exec_prepared  is a hash reference of attributes; the only attribute currently supported is  limit , which sets the maximum number of rows returned by a query.  Whether messages of a particular priority are reported to the client, written to the server log, or both is controlled by the  log_min_messages and  client_min_messages  configuration variables.  Note Perl cannot support multiple interpreters within one process unless it was built with the appropriate flags, namely either  usemultiplicity  or  useithreads .  Python 3 PL/Python supports both the Python 2 and Python 3 language variants.  (The PostgreSQL installation instructions might contain more precise information about the exact supported minor versions of Python. ) Because the Python 2 and Python 3 language variants are incompatible in some important aspects, the following naming and transitioning scheme is used by PL/Python to avoid mixing them: Ł The PostgreSQL language named  plpython2u  implements PL/Pyth.  This will continue to work into the very distant future, until Python 2 support might be completely dropped by PostgreSQL. 0 3  for more information about porting to Python 3.  This type is imported from the cdecimal  package if that is available. 6 and below, when multi-dimensional arrays were not supported. value = value   return nv $$ LANGUAGE plpythonu; Functions with  OUT  parameters are also supported.  Anonymous Code Blocks PL/Python also supports anonymous code blocks called with the  DO  statement: DO $$ 1202 .  Database Access The PL/Python language module automatically imports a Python module called  plpy .  This separation makes it easier to handle specific errors, for instance: CREATE FUNCTION insert_fraction(numerator int, denominator int)  RETURNS text AS $$ from plpy import spiexceptions try:     plan = plpy. execute(plan, [result]) $$ LANGUAGE plpythonu; If the second  UPDATE  statement results in an exception being raised, this function will report the error, but the result of the first  UPDATE  will nevertheless be committed. execute("UPDATE accounts SET balance = balance + 100  WHERE account_name = 'mary'")     except:         import sys         subxact.  Whether messages of a particular priority are reported to the client, written to the server log, or both is controlled by the  log_min_messages  and  client_min_messages  configuration variables.  In that case, the string representation of the tuple of positional arguments becomes the message reported to the client.  The following keyword-only arguments are accepted: detail hint sqlstate schema_name table_name column_name datatype_name constraint_name The string representation of the objects passed as keyword-only arguments is used to enrich the messages reporte.  It also supports use of dynamic parameter sets via hook functions specified in  ParamListInfo .  Second, a portal can outlive the current procedure (it can, in fact, live to the end of the current transaction).  Returning the portal name to the procedure's caller provides a way of returning a row set as result.  The passed-in parameter data will be copied into the cursor's portal, so it can be freed while the cursor still exists.  Arguments const char *  name name for portal, or  NULL  to let the system select a name SPIPlanPtr  plan prepared statement (returned by  SPI_prepare ) Datum *  values An array of actual parameter values.  bool  read_only true  for read-only execution Return Value Pointer to portal containing the cursor.  Note there is no error return convention; any error will be reported via  elog .  The passed-in parameter data will be copied into the cursor's portal, so it can be freed while the cursor still exists.  Arguments const char *  name name for portal, or  NULL  to let the system select a name const char *  command command string int  nargs number of input parameters ( $1 ,  $2 , etc. Server Programming Interface bool  read_only true  for read-only execution int  cursorOptions integer bit mask of cursor options; zero produces default behavior Return Value Pointer to portal containing the cursor.  Note there is no error return convention; any error will be reported via  elog .  It also supports use of dynamic parameter sets via hook functions specified in  ParamListInfo .  The passed-in parameter data will be copied into the cursor's portal, so it can be freed while the cursor still exists.  Arguments const char *  name name for portal, or  NULL  to let the system select a name SPIPlanPtr  plan prepared statement (returned by  SPI_prepare ) ParamListInfo  params data structure containing parameter types and values; NULL if none bool  re.  Note there is no error return convention; any error will be reported via  elog . Server Programming Interface SPI_cursor_find SPI_cursor_find  Š  find an existing cursor by name Synopsis Portal SPI_cursor_find(const char *  name ) Description SPI_cursor_find  finds an existing portal by name.  Arguments const char *  name name of the portal Return Value pointer to the portal with the specified name, or  NULL  if none was found 1235 . Server Programming Interface SPI_cursor_fetch SPI_cursor_fetch  Š  fetch some rows from a cursor Synopsis void SPI_cursor_fetch(Portal  portal , bool  forward , long  count ) Description SPI_cursor_fetch  fetches some rows from a cursor.  Arguments Portal  portal portal containing the cursor bool  forward true for fetch forward, false for fetch backward long  count maximum number of rows to fetch Return Value SPI_processed  and  SPI_tuptable  are set as in  SPI_execute  if successful. Server Programming Interface SPI_cursor_move SPI_cursor_move  Š  move a cursor Synopsis void SPI_cursor_move(Portal  portal , bool  forward , long  count ) Description SPI_cursor_move  skips over some number of rows in a cursor.  Arguments Portal  portal portal containing the cursor bool  forward true for move forward, false for move backward long  count maximum number of rows to move Notes Moving backward may fail if the cursor's plan was not created with the  CURSOR_OPT_SC. Server Programming Interface SPI_scroll_cursor_fetch SPI_scroll_cursor_fetch  Š  fetch some rows from a cursor Synopsis void SPI_scroll_cursor_fetch(Portal  portal ,  FetchDirection  direction ,                              long  count ) Description .  Arguments Portal  portal portal containing the cursor FetchDirection  direction one of  FETCH_FORWARD ,  FETCH_BACKWARD ,  FETCH_ABSOLUTE  or  FETCH_RELATIVE long  count number of rows to fetch for  FETCH_FORWARD  or  FETCH_BACKWARD ; absolute row n. Server Programming Interface SPI_scroll_cursor_move SPI_scroll_cursor_move  Š  move a cursor Synopsis void SPI_scroll_cursor_move(Portal  portal ,  FetchDirection  direction ,                             long  count ) Description SPI_scroll_cursor_mo.  Arguments Portal  portal portal containing the cursor FetchDirection  direction one of  FETCH_FORWARD ,  FETCH_BACKWARD ,  FETCH_ABSOLUTE  or  FETCH_RELATIVE long  count number of rows to move for  FETCH_FORWARD  or  FETCH_BACKWARD ; absolute row nu. Server Programming Interface SPI_cursor_close SPI_cursor_close  Š  close a cursor Synopsis void SPI_cursor_close(Portal  portal ) Description SPI_cursor_close  closes a previously created cursor and releases its portal storage.  Arguments Portal  portal portal containing the cursor 1240 .  Interface Support Functions The functions described here provide an interface for extracting information from result sets returned by  SPI_execute  and other SPI functions.  Exported Snapshots When a new replication slot is created using the streaming replication interface (see CREATE_REPLICATION_SLOT ), a snapshot is exported (see  Section 9.  Applications that do not require snapshot export may suppress it with the  NOEXPORT_SNAPSHOT option. 8 ) is only supported on replication slots used over the streaming replication interface.  The function interface and additional, non-core interfaces do not support synchronous replication.  Synchronous Replication Support for Logical Decoding Logical decoding can be used to build  synchronous replication  solutions with the same user interface as synchronous replication for  streaming replication .  The OID is used only to avoid having to store the long version in situations where space efficiency is important. SQL Commands This part contains reference information for the SQL commands supported by PostgreSQL.  The first form of the command changes the support functions or the generic options of the foreign-data wrapper (at least one clause is required).  Currently only  CHECK  constraints are supported.  Unless the table's foreign-data wrapper supports OIDs, this column will simply read as zeroes.  This form also supports  OWNED BY , which will only move indexes owned by the roles specified.  ] ALTER OPERATOR FAMILY  name  USING  index_method  DROP   {  OPERATOR  strategy_number  (  op_type  [ ,  op_type  ] )    | FUNCTION  support_number  (  op_type  [ ,  op_type  ] )   } [, .  You can add operators and support functions to the family, remove them from the family, or change the family's name or owner.  When operators and support functions are added to a family with  ALTER OPERATOR FAMILY , they are not part of any specific operator class within the family, but are just  ﬁlooseﬂ  within the family.  Typically, single-data-type operators and functions are part of operator classes because they are needed to support an index on that specific data type, while cross-data-type operators and functions are made loose members of the family.  In an  ADD FUNCTION  clause, the operand data type(s) the function is intended to support, if different from the input data type(s) of the function.  For B-tree sort support functions and all functions in GiST, SP-GiST and GIN operator classes, it is necessary to specify the operand data type(s) the function is to be used with.  In a  DROP FUNCTION  clause, the operand data type(s) the function is intended to support must be specified.  support_number The index method's support procedure number for a function associated with the operator family.  function_name The name (optionally schema-qualified) of a function that is an index method support procedure for the operator family. ALTER OPERATOR FAMILY Notes Notice that the  DROP  syntax only specifies the  ﬁslotﬂ  in the operator family, by strategy or support number and input data type(s).  Also, for  DROP FUNCTION  the type(s) to specify are the input data type(s) the function is intended to support; for GiST, SP-GiST and GIN indexes this might have nothing to do with the actual input argument types of the function.  This is no longer supported because whether an index operator is  ﬁlossyﬂ  is now determined on-the-fly at run time.  Examples The following example command adds cross-data-type operators and support functions to an operator family that already contains B-tree operator classes for data types  int4  and  int2 .  The supported options are: refresh  ( boolean ) When false, the command will not try to refresh table information.  The supported options are: copy_data  ( boolean ) Specifies whether the existing data in the publications that are being subscribed to should be copied once the replication starts.   sequence_option is an option supported by  ALTER SEQUENCE  such as  INCREMENT BY .   EXTENDED  is the default for most data types that support non- PLAIN storage.  This form also supports  OWNED BY , which will only move tables owned by the roles specified.  Currently, the only supported functionality is to change the parser's name.  Currently, the only supported functionality is to change the template's name.  Currently supported options are: check_option  ( string ) Changes the check option of the view.  Not all foreign data wrappers support ANALYZE .  If the table's wrapper does not support  ANALYZE , the command prints a warning and does nothing.  One or both of these can be omitted if  ANALYZE  deems them uninteresting (for example, in a unique-key column, there are no common values) or if the column data type does not support the appropriate operators.  Increasing the target causes a proportional increase in the time and space needed to do  ANALYZE .  If any of the child tables are foreign tables whose foreign data wrappers do not support  ANALYZE , those child tables are ignored while gathering inheritance statistics.  You are advised to be careful about the transaction semantics when porting database applications.  VERBOSE Prints a progress report as each table is clustered.  Notes In cases where you are accessing single rows randomly within a table, the actual order of the data in the table is unimportant.  The syntax CLUSTER  index_name  ON  table_name is also supported for compatibility with pre-8.  Currently,  COPY FROM  is not supported for tables with row-level security.  To ensure portability to other PostgreSQL installations that might use non-default  DateStyle  settings,  DateStyle  should be set to  ISO before using  COPY TO .  CSV Format This format option is used for importing and exporting the Comma Separated Value ( CSV ) file format used by many other programs, such as spreadsheets.  This can cause errors if you import data from a system that pads  CSV  lines with white space out to some fixed width. COPY such a situation arises you might need to preprocess the  CSV  file to remove the trailing white space, before importing the data into PostgreSQL.  Thus you might encounter some files that cannot be imported using this mechanism, and  COPY  might produce files that other programs cannot process.  It is somewhat faster than the text and  CSV  formats, but a binary-format file is less portable across machine architectures and PostgreSQL versions. ) Flags field 32-bit integer bit mask to denote important aspects of the file format.  This design allows for both backwards-compatible header additions (add header extension chunks, or set low-order flag bits) and non-backwards-compatible changes (set high-order flag bits to signal such changes, and add supporting data to the extensi.  A reader should report an error if a field-count word is neither -1 nor the expected number of columns. 0 and is still supported: COPY  table_name  [ (  column_name  [, . 3 and is still supported: COPY [ BINARY ]  table_name  [ WITH OIDS ]     FROM { ' filename ' | STDIN }     [ [USING] DELIMITERS ' delimiter_character ' ]     [ WITH NULL AS ' null_string ' ] COPY [ BINARY ]  table_name  [ WITH OIDS ]     TO { ' filen.  Only  INDEX  is supported at present.  An aggregate can optionally support  moving-aggregate mode , as described in  Section 37.  An aggregate can optionally support  partial aggregation , as described in  Section 37.  To be able to create an aggregate function, you must have  USAGE  privilege on the argument types, the state type(s), and the return type, as well as  EXECUTE  privilege on the supporting functions.  (Aggregate functions do not support  OUT arguments. CREATE AGGREGATE combinefunc The  combinefunc  function may optionally be specified to allow the aggregate function to support partial aggregation.  Note that the parallel-safety markings of the aggregate's support functions are not consulted by the planner, only the marking of the aggregate itself.  Notes In parameters that specify support function names, you can write a schema name if needed, for example SFUNC = public.  Do not write argument types there, however Š the argument types of the support functions are determined from other parameters.  If an aggregate supports moving-aggregate mode, it will improve calculation efficiency when the aggregate is used as a window function for a window with moving frame start (that is, a frame start mode other than  UNBOUNDED PRECEDING ).  Note that whether or not the aggregate supports moving-aggregate mode, PostgreSQL can handle a moving frame end without recalculation; this is done by continuing to add new values to the aggregate's state.  Currently, ordered-set aggregates do not need to support moving-aggregate mode, since they cannot be used as window functions.  Partial (including parallel) aggregation is currently not supported for ordered-set aggregates.  Also, it will never be used for aggregate calls that include  DISTINCT  or  ORDER BY  clauses, since those semantics cannot be supported during partial aggregation.  When a cast has different source and target types and a function that takes more than one argument, it supports converting from one type to another and applying a length coercion in a single step. CREATE CAST conversion functions are not named to support this convention then you will have surprised users.  The character sets supported by the PostgreSQL server are described in  Section 23. iso885915'     ENCODING LATIN9     TEMPLATE template0; The specified locale and encoding settings must match, or an error will be reported.  If the expression produces a FALSE result, an error is reported and the value is not allowed to be converted to the domain type.  This makes it possible to restrict the firing of the trigger to a subset of the cases in which it is supported.  Currently the only supported filter_variable  is  TAG .  The script will typically create new SQL objects such as functions, data types, operators and index support methods.  Notes Before you can use  CREATE EXTENSION  to load an extension into a database, the extension's supporting files must be installed.  The return type is ignored; the function should report invalid options using the  ereport(ERROR)  function.  For more information on the data types supported by PostgreSQL, refer to  Chapter 8 .  Although foreign tables can be specified as partitions, routing of tuples to foreign-table partitions is not supported.  Particularly important in this regard is the temporary-table schema, which is searched first by default, and is normally writable by anyone.  The attributes are not portable, neither are the different available languages.  Multiple fields can be specified if the index method supports multicolumn indexes.  A partial index is an index that contains entries for only a portion of a table, usually a portion that is more useful for indexing than the rest of the table.  PostgreSQL supports building indexes without locking out writes.  The psql  \d  command will report such an index as  INVALID : postgres=# \d tab        Table "public.  However, since  REINDEX  does not support concurrent builds, this option is unlikely to seem attractive. CREATE INDEX violations could be reported in other queries prior to the index becoming available for use, or even in cases where the index build eventually fails.  Concurrent builds of expression indexes and partial indexes are supported.  Currently, only the B-tree, GiST, GIN, and BRIN index methods support multicolumn indexes. ) Only B-tree currently supports unique indexes.  For index methods that support ordered scans (currently, only B-tree), the optional clauses  ASC ,  DESC , NULLS FIRST , and/or  NULLS LAST  can be specified to modify the sort ordering of the index.  The  NULLS  options are useful if you need to support  ﬁnulls sort lowﬂ  behavior, rather than the default  ﬁnulls sort highﬂ , in queries that depend on indexes to avoid sorting steps.  This behavior simplifies loading of old dump files, which are likely to contain out-of-date information about language support functions.  If no  inline_handler  function is specified, the language does not support anonymous code blocks.  To signal an error, the validator function should use the  ereport()  function.  The  TRUSTED  option and the support function name(s) are ignored if the server has an entry for the specified language name in  pg_pltemplate .  To support loading of old dump files, CREATE LANGUAGE  will accept a function declared as returning  opaque , but it will issue a notice and change the function's declared return type to  language_handler .  A materialized view has many of the same properties as a table, but there is no support for temporary materialized views or automatic generation of OIDs.  All parameters supported for  CREATE TABLE  are also supported for  CREATE MATERIALIZED VIEW  with the exception of  OIDS .  HASHES Indicates this operator can support a hash join.  MERGES Indicates this operator can support a merge join.  The operator class also specifies the support procedures to be used by the index method when the operator class is selected for an index column.  In a  FUNCTION  clause, the operand data type(s) the function is intended to support, if different from the input data type(s) of the function (for B-tree comparison functions and hash functions) or the class's data type (for B-tree sort support fun.  These defaults are correct, and so  op_type  need not be specified in FUNCTION  clauses, except for the case of a B-tree sort support function that is meant to support cross-data-type comparisons.  support_number The index method's support procedure number for a function associated with the operator class.  function_name The name (optionally schema-qualified) of a function that is an index method support procedure for the operator class.  This is no longer supported because whether an index operator is  ﬁlossyﬂ  is now determined on-the-fly at run time.  An operator family defines a collection of related operator classes, and perhaps some additional operators and support functions that are compatible with these operator classes but not essential for the functioning of any individual index.  It should be populated by issuing subsequent  CREATE OPERATOR CLASS  commands to add contained operator classes, and optionally  ALTER OPERATOR FAMILY  commands to add  ﬁlooseﬂ  operators and their corresponding support functions.  Such rows are silently suppressed; no error is reported.  The following parameters are supported: publish  ( string ) This parameter determines which DML operations will be published by the new publication to the subscribers.  It is important to realize that a rule is really a command transformation mechanism, or command macro.  If you want to support  INSERT RETURNING  and so on, then be sure to put a suitable  RETURNING  clause into each of these rules.  (This method does not currently work to support  RETURNING  queries, however. CREATE RULE It is very important to take care to avoid circular rules.  For example, though each of the following two rule definitions are accepted by PostgreSQL, the  SELECT  command would cause PostgreSQL to report an error because of recursive expansion of a rule: CREATE RULE "_RETURN" AS     ON SELECT TO t1     DO I. CREATE SERVER Examples Create a server  myserver  that uses the foreign-data wrapper  postgres_fdw : CREATE SERVER myserver FOREIGN DATA WRAPPER postgres_fdw OPTIONS  (host 'foo', dbname 'foodb', port '5432'); See  postgres_fdw  for more details.  Currently supported kinds are ndistinct , which enables n-distinct statistics, and  dependencies , which enables functional dependency statistics.  If this clause is omitted, all supported statistics kinds are included in the statistics object.  The following parameters are supported: copy_data  ( boolean ) Specifies whether the existing data in the publications that are being subscribed to should be copied once the replication starts.  The logical replication workers report the positions of writes and flushes to the publisher, and when using synchronous replication, the publisher will wait for the actual flush. 50 port=5432 user=foo  dbname=foodb'         PUBLICATION mypublication, insert_only; Create a subscription to a remote server that replicates tables in the  insert_only  publication and does not start replicating until enabled at a later time. 50 port=5432 user=foo  dbname=foodb'         PUBLICATION insert_only                WITH (enabled = false); Compatibility CREATE SUBSCRIPTION  is a PostgreSQL extension.  For more information on the data types supported by PostgreSQL, refer to  Chapter 8 .  If the same column name exists in more than one parent table, an error is reported unless the data types of the columns match in each of the parent tables.  Otherwise, any parents that specify default values for the column must all specify the same default, or an error will be reported.  If there is none, an error will be reported.  If no existing partition matches the values in the new row, an error will be reported.  Partitioned tables do not support  UNIQUE ,  PRIMARY KEY ,  EXCLUDE , or  FOREIGN KEY constraints; however, you can define these constraints on individual partitions.  The access method must support  amgettuple  (see  Chapter 60 ); at present this means GIN cannot be used.  Specifying these parameters for partitioned tables is not supported, but you may specify them for individual leaf partitions.  Since PostgreSQL does not support SQL modules, this distinction is not relevant in PostgreSQL.  SQL:1999-style inheritance is not yet supported by PostgreSQL.  Note that the  INSERT  command supports only one override clause that applies to the entire statement, so having multiple identity columns with different behaviors is not well supported.  PostgreSQL does not support these self-referencing columns explicitly, but the same effect can be had using the OID feature. CREATE TABLESPACE Notes Tablespaces are only supported on systems that support symbolic links.  That usage is not supported by PostgreSQL. CREATE TRIGGER INSTEAD OF  triggers do not support  WHEN  conditions.  To support loading of old dump files,  CREATE TRIGGER  will accept a function declared as returning  opaque , but it will issue a notice and change the function's declared return type to  trigger .  The support functions  input_function  and  output_function  are required, while the functions  receive_function ,  send_function ,  type_modifier_input_function , type_modifier_output_function  and  analyze_function  are optional.  (This case is mainly meant to support domain input functions, which might need to reject NULL inputs.  The binary representation should be chosen to be cheap to convert to internal form, while being reasonably portable.  (This case is mainly meant to support domain receive functions, which might need to reject NULL inputs.  The optional  type_modifier_input_function  and type_modifier_output_function  are needed if the type supports modifiers, that is optional constraints attached to a type declaration, such as  char(5)  or  numeric(30,2) .  To support loading of old dump files,  CREATE TYPE  will accept I/O functions declared using  opaque , but it will issue a notice and change the function declarations to use the correct types.  ] ) This clause specifies optional parameters for a view; the following parameters are supported: check_option  ( string ) This parameter may be either  local  or  cascaded , and is equivalent to specifying  WITH [ CASCADED | LOCAL ] CHECK OPTION  (.  The following check options are supported: LOCAL New rows are only checked against the conditions defined directly in the view itself.  Note that the  CHECK OPTION  is only supported on views that are automatically updatable, and do not have  INSTEAD OF  triggers or  INSTEAD  rules.   INSERT statements that have an  ON CONFLICT UPDATE  clause are fully supported. kind = 'Comedy'; This view will support  INSERT ,  UPDATE  and  DELETE .  Therefore PostgreSQL reports an error if such a command is used outside a transaction block.  However, for compatibility with earlier versions, PostgreSQL will allow backward fetches without  SCROLL , if the cursor's query plan is simple enough that no extra overhead is needed to support it.  However, ECPG, the embedded SQL preprocessor for PostgreSQL, supports the standard SQL cursor conventions, including those involving  DECLARE  and  OPEN  statements.  Only one index name can be specified, and the  CASCADE  option is not supported.  (Thus, an index that supports a  UNIQUE  or PRIMARY KEY  constraint cannot be dropped this way.  Important Keep in mind that the statement is actually executed when the  ANALYZE  option is used. EXPLAIN the only one supported.  It is expected that all new options will be supported only in the parenthesized syntax.  This functionality is currently supported only for tables, sequences, and functions (but note that  ALL TABLES  is considered to include views and foreign tables).  The SQL standard does not support setting the privileges on more than one object per command.  According to the SQL standard, grant options can be granted to  PUBLIC ; PostgreSQL only supports granting grant options to roles.  PostgreSQL only supports it when granting role membership, and even then only superusers may use it in nontrivial ways. IMPORT FOREIGN SCHEMA IMPORT FOREIGN SCHEMA  Š  import table definitions from a foreign server Synopsis IMPORT FOREIGN SCHEMA  remote_schema     [ { LIMIT TO | EXCEPT } (  table_name  [, .  By default, all tables and views existing in a particular schema on the foreign server are imported.  Parameters remote_schema The remote schema to import from. ] ) Import only foreign tables matching one of the given table names. ] ) Exclude specified foreign tables from the import.  All tables existing in the foreign schema will be imported except the ones listed here.  server_name The foreign server to import from.  local_schema The schema in which the imported foreign tables will be created. ] ) Options to be used during the import.  Examples Import table definitions from a remote schema  foreign_films  on server  film_server , creating the foreign tables in local schema  films : 1631 . IMPORT FOREIGN SCHEMA IMPORT FOREIGN SCHEMA foreign_films     FROM SERVER film_server INTO films; As above, but import only the two tables  actors  and  directors  (if they exist): IMPORT FOREIGN SCHEMA foreign_films LIMIT TO (actors, directors)     .  Note that exclusion constraints are not supported as arbiters with  ON CONFLICT DO UPDATE .  In all cases, only  NOT DEFERRABLE  constraints and unique indexes are supported as arbiters.  Therefore PostgreSQL reports an error if  LOCK  is used outside a transaction block.  PostgreSQL supports that too; see  SET TRANSACTION  for details.  NOTIFY  interacts with SQL transactions in some important ways.  VERBOSE Prints a progress report as each index is reindexed.  In this case it's important for the system to not have used any of the suspect indexes itself.  Examples Rebuild a single index: REINDEX INDEX my_index; Rebuild all the indexes on the table  my_table : REINDEX TABLE my_table; Rebuild all indexes in a particular database, without trusting the system indexes to be valid already: $  export PGOPTI.  Also, SQL has an optional clause  AND [ NO ] CHAIN  which is not currently supported by PostgreSQL.  Recursive data-modifying statements are not supported, but you can use the results of a recursive  SELECT  query in a data-modifying statement.  For example: SELECT DISTINCT ON (location) location, time, report     FROM weather_reports     ORDER BY location, time DESC; retrieves the most recent weather report for each location.  But if we had not used  ORDER BY  to force descending order of time values for each location, we'd have gotten a report from an unpredictable time for each location.  SQL:2008 introduced a different syntax to achieve the same result, which PostgreSQL also supports.  With  NOWAIT , the statement reports an error, rather than waiting, if a selected row cannot be locked immediately.  (These points apply equally to all SQL commands supporting the  ONLY  option.  PostgreSQL currently supports only the options listed above.  For example, after  SET TIME ZONE -7 ,  SHOW TIME ZONE  would report  <-07>+07 .  This mode is well suited for long-running reports or backups.  The pre-existing transaction must have exported its snapshot with the  pg_export_snapshot  function (see  Section 9.  That function returns a snapshot identifier, which must be given to  SET TRANSACTION SNAPSHOT  to specify which snapshot is to be imported.  If the importing transaction uses  SERIALIZABLE  isolation level, then the transaction that exported the snapshot must also use that isolation level.  Also, a non-read-only serializable transaction cannot import a snapshot from a read-only transaction.  Examples To begin a new transaction with the same snapshot as an already existing transaction, first export the snapshot from the existing transaction.  That will return the snapshot identifier, for example: BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ; SELECT pg_export_snapshot();  pg_export_snapshot ---------------------  00000003-0000001B-1 (1 row) Then give the snapshot identifier in a  SET.  TRUNCATE  is not currently supported for foreign tables.  Be careful when porting applications that use this extension.  VERBOSE Prints a detailed vacuum activity report for each table. VACUUM Examples To clean a single table  onek , analyze it for the optimizer and print a detailed vacuum activity report: VACUUM (VERBOSE, ANALYZE) onek; Compatibility There is no  VACUUM  statement in the SQL standard.  -p  port --port= port Specifies the TCP port or local Unix domain socket file extension on which the server is listening for connections. clusterdb This utility, like most other PostgreSQL utilities, also uses the environment variables supported by libpq (see  Section 33.  The character sets supported by the PostgreSQL server are described in  Section 23.  -p  port --port= port Specifies the TCP port or the local Unix domain socket file extension on which the server is listening for connections.  This utility, like most other PostgreSQL utilities, also uses the environment variables supported by libpq (see  Section 33.  Examples To create the database  demo  using the default database server: $  createdb demo To create the database  demo  using the server on host  eden , port 5000, using the  template0 template database, here is the command-line command and the und.  -p  port --port= port Specifies the TCP port or local Unix domain socket file extension on which the server is listening for connections.  Environment PGHOST PGPORT PGUSER Default connection parameters This utility, like most other PostgreSQL utilities, also uses the environment variables supported by libpq (see  Section 33. dropdb -p  port --port= port Specifies the TCP port or local Unix domain socket file extension on which the server is listening for connections.  Environment PGHOST PGPORT PGUSER Default connection parameters This utility, like most other PostgreSQL utilities, also uses the environment variables supported by libpq (see  Section 33.  Examples To destroy the database  demo  on the default database server: $  dropdb demo To destroy the database  demo  using the server on host  eden , port 5000, with verification and a peek at the underlying command: 1739 . dropuser -p  port --port= port Specifies the TCP port or local Unix domain socket file extension on which the server is listening for connections.  Environment PGHOST PGPORT PGUSER Default connection parameters This utility, like most other PostgreSQL utilities, also uses the environment variables supported by libpq (see  Section 33.  Examples To remove user  joe  from the default database server: $  dropuser joe To remove user  joe  using the server on host  eden , port 5000, with verification and a peek at the underlying command: $  dropuser -p 5000 -h eden -i -e joe Role "joe".  If this option is not specified and the server supports temporary replication slots (version 10 and later), then a temporary replication slot is automatically used for WAL streaming. pg_basebackup --no-slot This option prevents the creation of a temporary replication slot during the backup even if it's supported by the server.  The following methods for collecting the write-ahead logs are supported: n none Don't include write-ahead log in the backup.  -P --progress Enables progress reporting.  Turning this on will deliver an approximate progress report during the backup.  Will output some extra steps during startup and shutdown, as well as show the exact file name that is currently being processed if progress reporting is also enabled.  -p  port --port= port Specifies the TCP port or local Unix domain socket file extension on which the server is listening for connections.  Environment This utility, like most other PostgreSQL utilities, uses the environment variables supported by libpq (see  Section 33. 296346 (excluding connections establishing) The first six lines report some of the most important parameter settings.  The next line reports the number of transactions completed and intended (the latter being just the product of number of clients and number of transactions per client); these will be equal unless the run failed before completion. ) The last two lines report the number of transactions per second, figured with and without counting the time to start database sessions.  The most important options are  - c  (number of clients),  -t  (number of transactions),  -T  (time limit), and  -f  (specify a custom script file). pgbench -L   limit --latency-limit= limit Transactions that last more than  limit  milliseconds are counted and reported separately, as  late .  They are counted and reported separately as  skipped .  -P   sec --progress= sec Show progress report every  sec  seconds.  The report includes the time since the beginning of the run, the tps since the last report, and the transaction latency average and standard deviation since the last report.  -r --report-latencies Report the average per-statement latency (execution time from the perspective of the client) of each command after the benchmark finishes.  When throttling is active, the transaction latency reported at the end of the run is calculated from the scheduled start times, so it includes the time each transaction had to wait for the previous 1757 .  The wait time is called the schedule lag time, and its average and maximum are also reported separately. , the time spent executing the transaction in the database, can be computed by subtracting the schedule lag time from the reported latency.  -s   scale_factor --scale= scale_factor Report the specified scale factor in pgbench's output.  However, when testing only custom benchmarks ( -f  option), the scale factor will be reported as 1 unless this option is used.  Common Options pgbench accepts the following command-line common arguments: -h   hostname --host= hostname The database server's host name -p   port --port= port The database server's port number -U   login --username= login The user name to connect.  Custom Scripts pgbench has support for running custom benchmark scenarios by replacing the default transaction script (described above) with a transaction script read from a file ( -f  option).  These meta commands are supported: \set  varname   expression Sets variable  varname  to a value calculated from  expression .  (This example also shows why it's important for each client session to have its own variables Š otherwise they'd not be independently touching different rows.  When both  --rate  and  --latency-limit  are used, the  time  for a skipped transaction will be reported as  skipped .  It then reports an average of those values, referred to as the latency for each statement, after the benchmark has finished. 212  END; If multiple script files are specified, the averages are reported separately for each script file.  Comparing average TPS values with and without latency reporting enabled is a good way to measure if the timing overhead is significant. ) --localedir Print the location of locale support files.  (This will be an empty string if locale support was not configured when PostgreSQL was built.  --sharedir Print the location of architecture-independent support files.  The archive file formats are designed to be portable across architectures.  They allow for selection and reordering of all archived items, support parallel restoration, and are compressed by default.  The  ﬁdirectoryﬂ  format is the only format that supports parallel dumps.  This format is compressed by default and also supports parallel dumps.  However, the tar format does not support compression.  For a consistent backup, the database server needs to support synchronized snapshots, a feature that was introduced in PostgreSQL 9.  The tar archive format currently does not support compression at all.  Its use for other purposes is not recommended or supported.  Note that if you use this option currently, you probably also want the dump be in  INSERT  format, as the  COPY FROM  during restore does not support row security.  It could be useful for a dump used to load a copy of the database for reporting or other read-only load sharing while the original database continues to be updated.  -p  port --port= port Specifies the TCP port or local Unix domain socket file extension on which the server is listening for connections.  This utility, like most other PostgreSQL utilities, also uses the environment variables supported by libpq (see  Section 33. 0 are supported.  Its use for other purposes is not recommended or supported.  -p  port --port= port Specifies the TCP port or local Unix domain socket file extension on which the server is listening for connections.  Environment PGHOST PGOPTIONS PGPORT PGUSER Default connection parameters This utility, like most other PostgreSQL utilities, also uses the environment variables supported by libpq (see  Section 33. pg_dumpall (It is not important to which database you connect here since the script file created by pg_dumpall will contain the appropriate commands to create and connect to the saved databases.  -p  port --port= port Specifies the TCP port or the local Unix-domain socket file extension on which the server is listening for connections.  Defaults to the value of the  PGPORT  environment variable or, if not set, to the port specified at compile time, usually 5432.  Environment pg_isready , like most other PostgreSQL utilities, also uses the environment variables supported by libpq (see  Section 33.  When this option is used, pg_receivewal will report a flush position to the server, indicating when each segment has been synchronized to disk so that the server can remove that segment if it is not otherwise needed.  When the replication client of pg_receivewal is configured on the server as a synchronous standby, then using a replication slot will report the flush position to the server, but only when a WAL file is closed. pg_receivewal -p  port --port= port Specifies the TCP port or local Unix domain socket file extension on which the server is listening for connections.  Environment This utility, like most other PostgreSQL utilities, uses the environment variables supported by libpq (see  Section 33.  The server will occasionally request the client to perform a flush and report the flush position to the server.  Specifying an interval of  0  disables issuing  fsync()  calls altogether, while still reporting progress to the server.  -p  port --port= port Specifies the TCP port or local Unix domain socket file extension on which the server is listening for connections. pg_recvlogical Environment This utility, like most other PostgreSQL utilities, uses the environment variables supported by libpq (see  Section 33.  The archive files are designed to be portable across architectures.  Only the custom and directory archive formats are supported with this option.  Note that this option currently also requires the dump be in  INSERT  format, as  COPY FROM does not support row security.  -p  port --port= port Specifies the TCP port or local Unix domain socket file extension on which the server is listening for connections.  Environment PGHOST PGOPTIONS PGPORT PGUSER Default connection parameters This utility, like most other PostgreSQL utilities, also uses the environment variables supported by libpq (see  Section 33.  -p  port --port= port Specifies the TCP port or the local Unix-domain socket file extension on which the server is listening for connections.  Defaults to the value of the  PGPORT  environment variable or, if not set, to the port specified at compile time, usually 5432.  In order to connect to a database you need to know the name of your target database, the host name and port number of the server, and what user name you want to connect as.  The default port number is determined at compile time.  Since the database server uses the same default, you will not have to specify the port in most cases.  \c  or  \connect [ -reuse-previous= on|off  ] [  dbname  [  username  ] [  host ] [  port  ] |  conninfo  ] Establishes a new connection to a PostgreSQL server.  The connection parameters to use can be specified either using a positional syntax (one or more of database name, user, host, and port), or 1812 .  Specifying any of  dbname ,  username ,  host  or  port  as  -  is equivalent to omitting that parameter.  The new connection can re-use connection parameters from the previous connection; not only database name, user, host, and port, but other settings such as  sslmode . dom 6432 => \c service=foo => \c "host=localhost port=5432 dbname=mydb connect_timeout=10  sslmode=disable" => \c -reuse-previous=on sslmode=require    -- changes only  sslmode => \c postgresql://tom@localhost/mydb?application_name=myapp \C [  title .  If there are multiple such rows, an error is reported. ) \lo_export  loid   filename Reads the large object with OID  loid  from the database and writes it to  filename .  Note that this is subtly different from the server function  lo_export , which acts with the permissions of the user that the database server runs as and on the server's file system.  \lo_import  filename  [  comment  ] Stores the file into a PostgreSQL large object.  Example: foo=>  \lo_import '/home/peter/pictures/photo. xcf' 'a picture of  me' lo_import 152801 The response indicates that the large object received object ID 152801, which can be used to access the newly-created large object in the future.  Note that this command is subtly different from the server-side  lo_import  because it acts as the local user on the local file system, rather than the server's user and file system.  In  latex-longtable  format, this controls the proportional width of each column containing a left-aligned data type.  This command is not available if psql was built without Readline support.  LASTOID The value of the last affected OID, as returned from an  INSERT  or  \lo_import  command.  In interactive mode, psql will return to the command prompt; otherwise, psql will exit, returning error code 3 to distinguish this case from fatal error conditions, which are reported using error code 1.  PORT The database server port to which you are currently connected.  VERBOSITY This variable can be set to the values  default ,  verbose , or  terse  to control the verbosity of error reports.  psql does not support embedded NUL bytes in variable values.  %> The port number at which the database server is listening.  Command-Line Editing psql supports the Readline library for convenient line editing and retrieval.  Tab-completion is also supported, although the completion logic makes no claim to be an SQL parser.  This utility, like most other PostgreSQL utilities, also uses the environment variables supported by libpq (see  Section 33.  -p  port --port= port Specifies the TCP port or local Unix domain socket file extension on which the server is listening for connections.  Environment PGDATABASE PGHOST PGPORT PGUSER Default connection parameters This utility, like most other PostgreSQL utilities, also uses the environment variables supported by libpq (see  Section 33.  -p  port --port= port Specifies the TCP port or local Unix domain socket file extension on which the server is listening for connections.  Environment PGDATABASE PGHOST PGPORT PGUSER Default connection parameters This utility, like most other PostgreSQL utilities, also uses the environment variables supported by libpq (see  Section 33. PostgreSQL Server Applications This part contains reference information for PostgreSQL server applications and support utilities.  For these reasons it is important to choose the right locale when running  initdb .  The character sets supported by the PostgreSQL server are described in  Section 23.  Locale support is described in  Section 23.  It is really not important what the superuser's name is, but one might choose to keep the customary name postgres, even if the operating system user's name is different.  If you don't plan on using password authentication, this is not important.  This utility, like most other PostgreSQL utilities, also uses the environment variables supported by libpq (see  Section 33.  This utility supports the options  -V  and  --version , which print the pg_controldata version and exit.  It also supports options  -?  and  --help , which output the supported arguments.  Use  --help  to see a list of supported signal names.  This is supported for the modes  start ,  stop ,  restart , promote , and  register , and is the default for those modes.  pg_ctl , like most other PostgreSQL utilities, also uses the environment variables supported by libpq (see  Section 33.  Examples Starting the Server To start the server, waiting until the server is accepting connections: $   pg_ctl start To start the server using port 5433, and running without  fsync , use: $   pg_ctl -o "-F -p 5433" start Stopping the Server To stop.  To restart using port 5433, disabling  fsync upon restart: $   pg_ctl -o "-F -p 5433" restart Showing the Server Status Here is sample status output from pg_ctl: $   pg_ctl status pg_ctl: server is running (PID: 13718) /usr/local/pgsql/bin/postgres .  -P --progress Enables progress reporting.  Turning this on will deliver an approximate progress report while copying data from the source cluster.  Environment When  --source-server  option is used, pg_rewind also uses the environment variables supported by libpq (see  Section 33.  pg_test_fsync reports average file sync operation time in microseconds for each  wal_sync_method , which can also be used to inform efforts to optimize the value of commit_delay .  It's the preferred way to track the system time when it's supported by the operating system and the TSC clock is reliable.  Trying to use TSC on some older multicore CPUs can give a reported time that's inconsistent among multiple cores.  If your system supports TSC time but doesn't default to that, it may be disabled for a good reason.  It is important that any external modules are also binary compatible, though this cannot be checked by pg_upgrade.  pg_upgrade supports upgrades from 8.  Install the new PostgreSQL binaries Install the new server's binaries and support files.  You can also specify user and port values, and whether you want the data files linked instead of the default copy behavior.  pg_upgrade defaults to running servers on port 50432 to avoid unintended client connections.  You can use the same port number for both clusters when doing an upgrade because the old and new clusters will not be running at the same time.  However, when checking an old running server, the old and new port numbers must be different.  Install the new PostgreSQL binaries on standby servers Make sure the new binaries and support files are installed on all standby servers.  Notes pg_upgrade does not support upgrading of databases containing these  reg*  OID-referencing system data types:  regproc ,  regprocedure ,  regoper ,  regoperator ,  regconfig , and regdictionary . ) All failure, rebuild, and reindex cases will be reported by pg_upgrade if they affect your installation; post-upgrade scripts to rebuild tables and indexes will be generated automatically.  If your file system supports file system snapshots or copy-on-write file copies, you can use that to make a backup of the old cluster and tablespaces, though the snapshot and copies must be created simultaneously or while the database server is down.  More than one  postgres  instance can run on a system at one time, so long as they use different data areas and different communication ports (see below).  The configuration parameters supported by PostgreSQL are described in  Chapter 19 .  PostgreSQL must have been compiled with support for SSL for this option to be available.  -p  port Specifies the TCP/IP port or local Unix domain socket file extension on which  postgres  is to listen for connections from client applications.  If you specify a port other than the default port, then all client applications must specify the same port using either command-line options or  PGPORT .  This is intended to give an opportunity to attach to the server process with a debugger. ) PGPORT Default port number (preferably set in the configuration file) Diagnostics A failure message mentioning  semget  or  shmget  probably indicates you need to configure your kernel to provide adequate shared memory and semaphores.  A failure message indicating inability to bind to a port might indicate that that port is already in use by some non-PostgreSQL process.  You might also get this error if you terminate  postgres  and immediately restart it using the same port; in this case, you must simply wait a few seconds until the operating system closes the port before trying again.  Finally, you might get this error if you specify a port number that your operating system considers to be reserved.  For example, many versions of Unix consider port numbers under 1024 to be  ﬁtrustedﬂ  and only permit the Unix superuser to access them.  Examples To start  postgres  in the background using default values, type: $   nohup postgres >logfile 2>&1 </dev/null & To start  postgres  with a specific port, e. , 1234: $   postgres -p 1234 To connect to this server using psql, specify this port with the -p option: $   psql -p 1234 or set the environment variable  PGPORT : $   export PGPORT=1234 $   psql Named run-time parameters can be set in either of thes.  Reporting Errors Within the Server .  Native Language Support .  Sampling Method Support Functions .  This master process is called  postgres  and listens at a specified TCP/IP port for incoming connections.  The PostgreSQL Rule System PostgreSQL supports a powerful  rule system  for the specification of  views  and ambiguous  view updates .  Each time a plan node is called, it must deliver one more row, or report that it is done delivering rows.  System Catalogs Catalog Name Purpose pg_aggregate aggregate functions pg_am index access methods pg_amop access method operators pg_amproc access method support procedures pg_attrdef column default values pg_attribute table columns ( ﬁattributesﬂ ) .  There is one row for each access method supported by the system.   pg_amproc The catalog  pg_amproc  stores information about support procedures associated with access method operator families.  There is one row for each support procedure belonging to an operator family. oid Right-hand input data type of associated operator amprocnum int2 Support procedure number amproc regproc pg_proc . oid OID of the procedure The usual interpretation of the  amproclefttype  and  amprocrighttype  fields is that they identify the left and right input types of the operator(s) that a particular support procedure supports.  For some access methods these match the input data type(s) of the support procedure itself, for others not.  There is a notion of  ﬁdefaultﬂ  support procedures for an index, which are those with amproclefttype  and  amprocrighttype  both equal to the index operator class's  opcintype .  Another important exception is that  ﬁautomatic I/O conversion castsﬂ , those performed using a data type's own I/O functions to convert to or from  text  or other string types, are not explicitly represented in  pg_cast . oid The index supporting this constraint, if it's a unique, primary key, foreign key, or exclusion constraint; else 0 confrelid oid pg_class . oid Schema containing the extension's exported objects extrelocatable bool True if extension can be relocated to another schema extversion text Version name for the extension extconfig oid[] pg_class . relnatts ) indisunique bool If true, this is a unique index indisprimary bool If true, this index represents the primary key of the table ( indisunique should always be true when this is true) indisexclusion bool If true, this index supports an exclu.  Zero if inline blocks are not supported. oid Owner of the operator oprkind char b  = infix ( ﬁbothﬂ ),  l = prefix ( ﬁleftﬂ ),  r  = postfix ( ﬁrightﬂ ) oprcanmerge bool This operator supports merge joins oprcanhash bool This operator supports hash joins oprleft oid pg_type .  Each operator family is a collection of operators and associated support routines that implement the semantics specified for a particular index access method.  Note in particular that the current definition only supports roles as referenced objects.  Zero is stored if this operation is not supported.  Zero is stored if this operation is not supported. oid The index supporting a unique, primary key, referential integrity, or exclusion constraint tgconstraint oid pg_constraint .  A dictionary depends on a text search template, which specifies all the implementation functions needed; the dictionary itself provides values for the user-settable parameters supported by the template. oid Type modifier input function, or 0 if type does not support modifiers typmodout regproc pg_proc .  If the type does not support collations, this will be zero.  A base type that supports collations will have DEFAULT_COLLATION_OID here.  Note that this view reports on the  current  contents of the files, not on what was last applied by the server.  Note that this view reports on the  current  contents of the file, not on what was last loaded by the server.  The protocol is supported over TCP/IP and also over Unix-domain sockets.  Port number 5432 has been registered with IANA as the customary TCP port number for servers supporting this protocol, but in practice any non-privileged port number can be used.  A single server can support multiple protocol versions.  If the major version requested by the client is not supported by the server, the connection will be rejected (for example, this would occur if the client requested protocol version 4.  If the minor version requested by the client is not supported by the server (e. 1, but the server supports only 3. 0), the server may either reject the connection or may respond with a NegotiateProtocolVersion message containing the highest minor protocol version which it supports.  There are a few cases (such as  NOTIFY ) wherein the backend will send unsolicited messages, but for the most part this portion of a session is driven by frontend requests.  The state retained between steps is represented by two types of objects:  prepared statements  and  portals .  A portal represents a ready-to-execute or already-partially-executed statement, with any missing parameter values filled in.  (For  SELECT  statements, a portal is equivalent to an open cursor, but we choose to use a different term since cursors don't handle non- SELECT  statements. ) The overall execution cycle consists of a  parse  step, which creates a prepared statement from a textual query string; a  bind  step, which creates a portal given a prepared statement and values for any needed parameters; and an  execute  step tha.  The backend can keep track of multiple prepared statements and portals (but note that these exist only within a session, and are never shared across sessions).  Existing prepared statements and portals are referenced by names assigned when they were created.  In addition, an  ﬁunnamedﬂ  prepared statement and portal exist. 4 the only supported formats are  ﬁtextﬂ  and  ﬁbinaryﬂ , but the protocol makes provision for future extensions.  Keep in mind that binary representations for complex data types might change across server versions; the text format is usually the more portable choice.  This is no longer supported. ) AuthenticationSCMCredential This response is only possible for local Unix-domain connections on platforms that support SCM credential messages.  NegotiateProtocolVersion The server does not support the minor protocol version requested by the client, but does support an earlier version of the protocol; this message indicates the highest supported minor version.  This message will also be sent if the client requested unsupported protocol options (i.  If the frontend does not support the authentication method requested by the server, then it should immediately close the connection.  It is still possible for the startup attempt to fail (ErrorResponse) or the server to decline support for the requested minor protocol version (NegotiateProtocolVersion), but in the normal case the backend will send some ParameterStatus messages, Ba.  Note The query string contained in a Parse message cannot include more than one SQL statement; else a syntax error is reported.  This restriction does not exist in the simple-query protocol, but it does exist in the extended protocol, because allowing prepared statements or portals to contain multiple commands would complicate the protocol unduly.  The Bind message gives the name of the source prepared statement (empty string denotes the unnamed prepared statement), the name of the destination portal (empty string denotes the unnamed portal), and the values to use for any parameter placeholder.  If successfully created, a named portal object lasts till the end of the current transaction, unless explicitly destroyed.  An unnamed portal is destroyed at the end of the transaction, or as soon as the next Bind statement specifying the unnamed portal as destination is issued.  (Note that a simple Query message also destroys the unnamed portal. ) Named portals must be explicitly closed before they can be redefined by another Bind message, but this is not required for the unnamed portal.  Named portals can also be created and accessed at the SQL command level, using  DECLARE CURSOR  and  FETCH .  Once a portal exists, it can be executed using an Execute message.  The Execute message specifies the portal name (empty string denotes the unnamed portal) and a maximum result-row count (zero meaning  ﬁfetch all rowsﬂ ).  The result-row count is only meaningful for portals containing commands that return row sets; in other cases the command is always executed to completion, and the row count is ignored.  If Execute terminates before completing the execution of a portal (due to reaching a nonzero result- row count), it will send a PortalSuspended message; the appearance of this message tells the frontend that another Execute should be issued against .  The CommandComplete message indicating completion of the source SQL command is not sent until the portal's execution is completed.  Therefore, an Execute phase is always terminated by the appearance of exactly one of these messages: CommandComplete, EmptyQueryResponse (if the portal was created from an empty query string), ErrorResponse, or PortalSuspended.  The Describe message (portal variant) specifies the name of an existing portal (or an empty string for the unnamed portal).  The response is a RowDescription message describing the rows that will be returned by executing the portal; or a NoData message if the portal does not contain a query that will return rows; or ErrorResponse if there is no such portal.  The Close message closes an existing prepared statement or portal and releases resources.  It is not an error to issue Close against a nonexistent statement or portal name.  Note that closing a prepared statement implicitly closes any open portals that were constructed from that statement.  Note The simple Query message is approximately equivalent to the series Parse, Bind, portal Describe, Execute, Close, Sync, using the unnamed prepared statement and portal objects and no parameters.  Also, if a  SET  command is rolled back, an appropriate ParameterStatus message will be generated to report the current effective value.  ( server_encoding , TimeZone , and  integer_datetimes  were not reported by releases before 8. 0; standard_conforming_strings  was not reported by releases before 8. 1;  IntervalStyle was not reported by releases before 8. 4;  application_name  was not reported by releases before 9.  SSL Session Encryption If PostgreSQL was built with SSL support, frontend/backend communications can be encrypted using SSL.  This would only occur if the server predates the addition of SSL support to PostgreSQL.  The client selects one of the supported mechanisms from the list, and sends a SASLInitialResponse message to the server.  PostgreSQL supports multiple character encodings, while SCRAM dictates UTF-8 to be used for the user name, so it might be impossible to represent the PostgreSQL user name in UTF-8.  EXPORT_SNAPSHOT , which is the default, will export the snapshot for use in other sessions.  snapshot_name  ( text ) The identifier of the snapshot exported by the command.  PROGRESS Request information required to generate a progress report.  size  ( int8 ) The approximate size of the tablespace, if progress report has been requested; otherwise it's null. ) Owner, group, and file mode are set if the underlying file system on the server supports it.  Currently only version  1  is supported.  String The name of the destination portal (an empty string selects the unnamed portal).  Byte1 ' S ' to close a prepared statement; or ' P ' to close a portal.  String The name of the prepared statement or portal to close (an empty string selects the unnamed prepared statement or portal). Frontend/Backend Protocol String An error message to report as the cause of failure.  Byte1 ' S ' to describe a prepared statement; or ' P ' to describe a portal.  String The name of the prepared statement or portal to describe (an empty string selects the unnamed prepared statement or portal).  String The name of the portal to execute (an empty string selects the unnamed portal).  Int32 Maximum number of rows to return, if portal contains a query that returns rows (ignored otherwise).  Int32 Newest minor protocol version supported by the server for the major protocol version requested by the client.  ParameterStatus (B) Byte1('S') Identifies the message as a run-time parameter status report. Frontend/Backend Protocol String The name of the run-time parameter being reported. Frontend/Backend Protocol PortalSuspended (B) Byte1('s') Identifies the message as a portal-suspended indicator. ) F File: the file name of the source-code location where the error was reported.  L Line: the line number of the source-code location where the error was reported.  R Routine: the name of the source-code routine reporting the error.  (A future release might support additional formats. ) Binary  COPY  is supported.  FunctionCall can now support passing NULL arguments to functions.  Reporting Errors Within the Server Error, warning, and log messages generated within the server code should be created using  ereport , or its older cousin  elog .   ereport  itself is just a shell function, that exists mainly for the syntactic convenience of making message generation look like a function call in the C source code.  The only parameter accepted directly by  ereport  is the severity level.  The primary message text and any optional message elements are generated by calling auxiliary functions, such as  errmsg , within the  ereport  call.  A typical call to  ereport  might look like this: ereport(ERROR,         (errcode(ERRCODE_DIVISION_BY_ZERO),          errmsg("division by zero"))); This specifies error severity level  ERROR  (a run-of-the-mill error).  Here is a more complex example: ereport(ERROR,         (errcode(ERRCODE_AMBIGUOUS_FUNCTION),          errmsg("function %s is not unique",                 func_signature_string(funcname, nargs,                                       NIL, actual_arg_ty.  If the severity level is  ERROR  or higher,  ereport  aborts the execution of the user-defined function and does not return to the caller.  If the severity level is lower than  ERROR ,  ereport  returns normally.  The available auxiliary routines for  ereport  are: Ł errcode(sqlerrcode)  specifies the SQLSTATE error identifier code for the condition. )  is like  errmsg , but with support for various plural forms of the message.   fmt_singular  is the English singular format,  fmt_plural  is the English 1  That is, the value that was current when the  ereport  call was reached; changes of  errno  within the auxiliary reporting routines will not affect it. )  is like  errdetail , but with support for various plural forms of the message.  This is useful for error details that are too security-sensitive or too bulky to include in the report sent to the client. )  is like  errdetail_log , but with support for various plural forms of the message. )  is not normally called directly from an  ereport message site; rather it is used in  error_context_stack  callback functions to provide information about the context in which an error occurred, such as the current location in a PL function.  Unlike the other auxiliary functions, this can be called more than once per  ereport  call; the successive strings thus supplied are concatenated with separating newlines.  Ł errtable(Relation rel)  specifies a relation whose name and schema name should be included as auxiliary fields in the error report.  Ł errtablecol(Relation rel, int attnum)  specifies a column whose name, table name, and schema name should be included as auxiliary fields in the error report.  Ł errtableconstraint(Relation rel, const char *conname)  specifies a table constraint whose name, table name, and schema name should be included as auxiliary fields in the error report.  Ł errdatatype(Oid datatypeOid)  specifies a data type whose name and schema name should be included as auxiliary fields in the error report.  Ł errdomainconstraint(Oid datatypeOid, const char *conname)  specifies a domain constraint whose name, domain name, and schema name should be included as auxiliary fields in the error report.  Ł errhidestmt(bool hide_stmt)  can be called to specify suppression of the  STATEMENT: portion of a message in the postmaster log.  Ł errhidecontext(bool hide_ctx)  can be called to specify suppression of the  CONTEXT: portion of a message in the postmaster log.  Note At most one of the functions  errtable ,  errtablecol ,  errtableconstraint , errdatatype , or  errdomainconstraint  should be used in an  ereport  call.  These functions should be used in error reports for which it's likely that applications would wish to have automatic error handling. ); is exactly equivalent to: ereport(level, (errmsg_internal("format string", .  Any message that is likely to be of interest to ordinary users should go through  ereport .  Putting the embedded text in parentheses has also been suggested, but it's unnatural if the embedded text is likely to be the most important part of the message, as is often the case.  Function Names Don't include the name of the reporting routine in the error text.  A few functions in postgres  are also deemed signal safe, importantly  SetLatch() .  Native Language Support 54.  To try out your work, follow the applicable portions of the installation instructions. Native Language Support The # character introduces a comment. Native Language Support 54. ) Ł If the original string contains a linguistic mistake, report that (or fix it yourself in the program source) and translate normally.  If the original string contains a factual mistake, report that (or fix it yourself) and do not translate it.  Mechanics This section describes how to implement native language support in a program or library that is part of the PostgreSQL distribution.  Adding NLS Support to a Program 1. Native Language Support . : fprintf(stderr, "panic level %d\n", lvl); would be changed to: fprintf(stderr, gettext("panic level %d\n"), lvl); ( gettext  is defined as a no-op if NLS support is not configured.  One common shortcut is to use: #define _(x) gettext(x) Another solution is feasible if the program does much of its communication through one or a few functions, such as  ereport()  in the backend.  If you have a function that supports pluralized messages, the item should look like  func:1,2 (identifying the singular and plural message arguments). Native Language Support Ł Do not construct sentences at run-time, like: printf("Files were %s.  It's often best to design the message to avoid the issue altogether, for instance like this: printf("number of copied files: %d", n); If you really want to construct a properly pluralized message, there is support for this, but it's a bit awkward.  When generating a primary or detail error message in  ereport() , you can write something like this: errmsg_plural("copied %d file",               "copied %d files",               n,               n) The first argument is the format string appropria.  The translator sees the two English forms as a group and has the opportunity to supply multiple substitute strings, with the appropriate one being selected based on the run-time value of  n .  If you need to pluralize a message that isn't going directly to an  errmsg  or  errdetail  report, you have to use the underlying function  ngettext .  An inline handler can be provided to allow the language to support anonymous code blocks executed via the  DO  command.  Typical checks then include verifying that the function's argument and result types are supported by the language, and that the function's body is syntactically correct in the language.  If it finds an error, it should report that via the normal  ereport()  error reporting mechanism.  If updating foreign tables is to be supported, the wrapper must handle that, too.  It is normally not important to release palloc'd memory, but for example open files and connections to remote servers should be cleaned up.  FDW Routines For Scanning Foreign Joins If an FDW supports performing foreign joins remotely (rather than by fetching both tables' data and doing the join locally), it should provide this callback function: void GetForeignJoinPaths (PlannerInfo *roo.  FDW Routines For Planning Post-Scan/Join Processing If an FDW supports performing remote post-scan/join processing, such as remote aggregation, it should provide this callback function: void GetForeignUpperPaths (PlannerInfo *root,                  .  FDW Routines For Updating Foreign Tables If an FDW supports writable foreign tables, it should provide some or all of the following callback functions depending on the needs and capabilities of the FDW: void 2059 .  To support that, this function can add extra hidden, or  ﬁjunkﬂ , target columns to the list of columns that are to be retrieved from the foreign table during an  UPDATE  or  DELETE .  Regardless, some slot must be returned to indicate success, or the query's reported row count will be wrong.  Regardless, some slot must be returned to indicate success, or the query's reported row count will be wrong.  Regardless, some slot must be returned to indicate success, or the query's reported row count will be wrong.  It is normally not important to release palloc'd memory, but for example open files and connections to remote servers should be cleaned up.  int IsForeignRelUpdatable (Relation rel); Report which update operations the specified foreign table supports.  The return value should be a bit mask of rule event numbers indicating which operations are supported by the foreign table, using the CmdType  enumeration; that is,  (1 << CMD_UPDATE) = 4  for  UPDATE ,  (1 << CMD_INSERT) = 8  for  INSERT , and  (1 .  This function is only needed if the FDW supports some tables that are updatable and some that are not.  Whether the query has the clause or not, the query's reported row count must be incremented by the FDW itself.  It is normally not important to release palloc'd memory, but for example open files and connections to the remote server should be cleaned up.  FDW Routines For Row Locking If an FDW wishes to support  late row locking  (as described in  Section 56. 5 ), it must provide the following callback functions: RowMarkType GetForeignRowMarkType (RangeTblEntry *rte,                        LockClauseStrength strength); Report which row-marking option to use for a foreign table.  If the FDW does not support collecting statistics for any tables, the  AnalyzeForeignTable pointer can be set to  NULL .  FDW Routines For  IMPORT FOREIGN SCHEMA List * ImportForeignSchema (ImportForeignSchemaStmt *stmt, Oid serverOid); Obtain a list of foreign table creation commands.  Within the  ImportForeignSchemaStmt  struct,  remote_schema  is the name of the remote schema from which tables are to be imported.   list_type  identifies how to filter table names: FDW_IMPORT_SCHEMA_ALL  means that all tables in the remote schema should be imported (in this case  table_list  is empty),  FDW_IMPORT_SCHEMA_LIMIT_TO  means to include only tables listed in  table_l.   options  is a list of options used for the import process.  For example, an FDW could use an option to define whether the  NOT NULL  attributes of columns should be imported.  These options need not have anything to do with those supported by the FDW as database object options. Writing A Foreign Data Wrapper The FDW may ignore the  local_schema  field of the  ImportForeignSchemaStmt , because the core server will automatically insert that name into the parsed  CREATE FOREIGN TABLE commands.  The function  IsImportableForeignTable() may be useful to test whether a given foreign-table name will pass the filter.  If the FDW does not support importing table definitions, the  ImportForeignSchema  pointer can be set to  NULL .  FDW Routines for Parallel Execution A  ForeignScan  node can, optionally, support parallel execution.  The following functions are all optional, but most are required if parallel execution is to be supported.  Therefore, it can be useful to define this method even when parallel execution is not supported.  Foreign Data Wrapper Helper Functions Several helper functions are exported from the core server so that authors of foreign data wrappers can get easy access to attributes of FDW-related objects, such as FDW options.  However, best practice is to use a representation that's dumpable by  nodeToString , for use with debugging support available in the backend.  In  GetForeignPlan , the  local_variable  portion of the join clause would be added to fdw_exprs , and then at run time the case works the same as for an ordinary restriction clause.  If an FDW supports remote joins,  GetForeignJoinPaths  should produce  ForeignPath s for potential remote joins in much the same way as  GetForeignPaths  works for base tables.  An FDW might additionally support direct execution of some plan actions that are above the level of scans and joins, such as grouping or aggregation.  INSERT  with an  ON CONFLICT  clause does not support specifying the conflict target, as unique constraints or exclusion constraints on remote tables are not locally known.  This in turn implies that ON CONFLICT DO UPDATE  is not supported, since the specification is mandatory there.  By default, PostgreSQL ignores locking considerations when interfacing to FDWs, but an FDW can perform early locking without any explicit support from the core code.  An alternative possibility is to perform late locking within the  ExecForeignUpdate  or  ExecForeignDelete  callback, but no special support is provided for this.  Writing A Table Sampling Method PostgreSQL's implementation of the  TABLESAMPLE  clause supports custom table sampling methods, in addition to the  BERNOULLI  and  SYSTEM  methods that are required by the SQL standard.  The result of the function must be a palloc'd struct of type  TsmRoutine , which contains pointers to support functions for the sampling method.  These support functions are plain C functions and are not visible or callable at the SQL level.  The support functions are described in  Section 57.  Sampling Method Support Functions The TSM handler function returns a palloc'd  TsmRoutine  struct containing pointers to the support functions described below.  It was returned by some previous NextSampleBlock  call, but the core code is allowed to call  NextSampleBlock  in advance of actually scanning pages, so as to support prefetching.  It is normally not important to release palloc'd memory, but any externally-visible resources should be cleaned up.  Writing A Custom Scan Provider PostgreSQL supports a set of experimental facilities which are intended to allow extension modules to add new scan types to the system.  Typically, the motivation for writing a custom scan provider will be to allow the use of some optimization not supported by the core system, such as caching or some form of hardware acceleration.   flags  is a bit mask, which should include CUSTOMPATH_SUPPORT_BACKWARD_SCAN  if the custom path can support a backward scan and CUSTOMPATH_SUPPORT_MARK_RESTORE  if it can support mark and restore.  Typically, a  CustomScanState , which need not support  copyObject , will actually be a larger structure embedding the above as its first member.  This callback is optional, and need only be supplied if this custom scan provider supports parallel execution.  This callback is optional, and need only be supplied if this custom scan provider supports parallel execution.  This callback is optional, and need only be supplied if this custom scan provider supports parallel execution.  This callback is optional, and need only be supplied if this custom scan provider supports parallel execution.  Further optimization effort is caused by the support of a variety of  join methods  (e.  The Institute of Automatic Control at the University of Mining and Technology, in Freiberg, Germany, encountered some problems when it wanted to use PostgreSQL as the backend for a decision support knowledge based system for the maintenance of an el.  The GEQO module allows the PostgreSQL query optimizer to support large join queries effectively through non-exhaustive search.  Then new candidates are generated by combining genes of more-fit candidates Š that is, by using randomly-chosen portions of known low-cost join sequences to create new sequences for consideration.  The  IndexAmRoutine struct, also called the access method's  API struct , includes fields specifying assorted fixed properties of the access method, such as whether it can support multicolumn indexes.  More importantly, it contains pointers to support functions for the access method, which do all of the real work to access indexes.  These support functions are plain C functions and are not visible or callable at the SQL level.  The support functions are described in  Section 60. Index Access Method Interface Definition     /* total number of support functions that this AM uses */     uint16      amsupport;     /* does AM support ORDER BY indexed column's value? */     bool        amcanorder;     /* does AM support ORDER BY r.  The  amcanmulticol  flag asserts that the access method supports multicolumn indexes, while  amoptionalkey  asserts that it allows scans where no indexable restriction clause is given for the first index column.  When  amcanmulticol  is false, amoptionalkey  essentially says whether the access method supports full-index scans without any restriction clause.  Access methods that support multiple index columns  must  support scans that omit restrictions on any or all of the columns after the first; however they are permitted to require some restriction to appear for the first index column, and this is sig.  A related restriction is that an index access method that supports multiple index columns  must  support indexing null values in columns after the first, because the planner will assume the index can be used for queries that do not restrict these co.  An index access method that does index nulls may also set  amsearchnulls , indicating that it supports  IS NULL  and  IS NOT NULL  clauses as search conditions.  If the access method supports unique indexes (its amcanunique  flag is true) then  checkUnique  indicates the type of uniqueness check to perform.  The statistics it contains will be used to update  pg_class , and will be reported by  VACUUM  if  VERBOSE  is given.  bool amcanreturn (Relation indexRelation, int attno); Check whether the index can support  index-only scans  on the given column, by returning the indexed column values for an index entry in the form of an  IndexTuple .  Returns TRUE if supported, else FALSE.  If the access method does not support index-only scans at all, the  amcanreturn  field in its  IndexAmRoutine  struct can be set to NULL.  When  validate  is true, the function should report a suitable error message if any of the options are unrecognized or have invalid values; when  validate  is false, invalid entries should be silently ignored.  Access methods that support ordering operators should implement AMPROP_DISTANCE_ORDERABLE  property testing, as the core code does not know how to do that and will return NULL.  For example, this might include testing that all required support functions are provided.  Problems should be reported with ereport  messages.  The purpose of an index, of course, is to support scans for tuples matching an indexable  WHERE condition, often called a  qualifier  or  scan key .  An index access method can support  ﬁplainﬂ  index scans,  ﬁbitmapﬂ  index scans, or both.  This provision supports  ﬁlossyﬂ  index operators. Index Access Method Interface Definition If the index supports  index-only scans  (i.  The  amgettuple  function need only be provided if the access method supports  ﬁplainﬂ  index scans.  Note: in the current implementation, support for this feature is conflated with support for lossy storage of the bitmap itself, and therefore callers recheck both the scan conditions and the partial index predicate (if any) for recheckable tuples.  The  amgetbitmap  function need only be provided if the access method supports  ﬁbitmapﬂ  index scans.  The access method need only support one remembered scan position per scan.  The  ammarkpos  function need only be provided if the access method supports ordered scans.  The  amrestrpos  function need only be provided if the access method supports ordered scans.  In addition to supporting ordinary index scans, some types of index may wish to support  parallel index scans , which allow multiple backends to cooperate in performing an index scan.  The following functions may be implemented to support parallel index scans: Size amestimateparallelscan (void); Estimate and return the number of bytes of dynamic shared memory which the access method will be needed to perform a parallel scan. ) It is not necessary to implement this function for access methods which do not support parallel scans or for which the number of additional bytes of storage required is zero.  It is not necessary to implement this function for access methods which do not support parallel scans or in cases where the shared memory space required needs no initialization.  The access method can report that the index is  lossy , or requires rechecks, for a particular query.  There are actually two different ways that an access method can support sorted output: 2091 .  Ł Access methods that support ordering operators should set  amcanorderbyop  to true. ) Access methods that support ordered scans must support  ﬁmarkingﬂ  a position in a scan and later returning to the marked position.  An access method that does not support ordered scans need not provide  ammarkpos  and amrestrpos  functions in  IndexAmRoutine ; set those pointers to NULL instead.  What is important is that insertions or deletions not cause the scan to miss or multiply return entries that were not themselves being inserted or deleted.  If the index stores the original indexed data values (and not some lossy representation of them), it is useful to support  index-only scans , in which the index returns the actual data not just the TID of the heap tuple.  First of all, amgetbitmap  returns all tuples at once and marking or restoring scan positions isn't supported.  Building an index type that supports concurrent updates usually requires extensive and subtle analysis of the required behavior.  An access method that supports this feature sets amcanunique  true.  (At present, only b-tree supports it.  Furthermore, immediately before reporting a uniqueness violation according to the above rules, the access method must recheck the liveness of the row being inserted.  If it is committed dead then no violation should be reported.  If, at the time of the recheck, both the inserted tuple and some other tuple with the same key are live, then the error must be reported.  The access method must allow duplicate entries into the index, and report any potential duplicates by returning FALSE from  aminsert .  The access method must identify any rows which might violate the unique constraint, but it is not an error for it to report false positives.  This allows the check to be done without waiting for other transactions to finish; conflicts reported here are not treated as errors and will be rechecked later, by which time they may no longer be conflicts.  Ł UNIQUE_CHECK_EXISTING  indicates that this is a deferred recheck of a row that was reported as a potential uniqueness violation.  If so, and if the target row is also still live, report error. Index Access Method Interface Definition It is recommended that in a  UNIQUE_CHECK_EXISTING  call, the access method further verify that the target row actually does have an existing entry in the index, and report error if not.  For example, PostgreSQL supports extensible B-trees and hash indexes.  But B-trees only support range predicates ( < ,  = ,  > ), and hash indexes only support equality queries.  Of course these methods have to be pretty fancy to support fancy queries, but for all the standard queries (B-trees, R-trees, etc.  The optional eighth method is  distance , which is needed if the operator class wishes to support ordered scans (nearest-neighbor searches).  The optional ninth method  fetch  is needed if the operator class wishes to support index-only scans.  This convention allows GiST to support both lossless and lossy index structures.       *      * Use GIST_LEAF(entry) to know where you're called in the  index tree,      * which comes handy when supporting the = operator for  example (you could      * check for non empty union() in non-leaf nodes and  equality in leaf      * node.  It's easy enough to support data types where this is not the case, by implementing the proper union algorithm in this GiST support method.  */     gistentryinit(*retval, PointerGetDatum(converted_datum),                   entry->rel, entry->page, entry->offset,  FALSE);     PG_RETURN_POINTER(retval); } If the compress method is lossy for leaf entries, the operator class cannot support i.  All the GiST support methods are normally called in short-lived memory contexts; that is, CurrentMemoryContext  will get reset after each tuple is processed.  It is therefore not very important to worry about pfree'ing everything you palloc.  However, in some cases it's useful for a support method to cache data across repeated calls. 2, PostgreSQL supports a more efficient method to build GiST indexes based on buffering, which can dramatically reduce the number of random I/ Os needed for non-ordered data sets.  The core system currently provides text search support (indexing for  tsvector  and tsquery ) as well as R-Tree equivalent functionality for some of the built-in geometric data types (see src/backend/access/gist/gistproc.  SP-GiST supports partitioned search trees, which facilitate development of a wide range of different non-balanced data structures, such as quad- trees, k-d trees, and radix trees (tries).   kd_point_ops supports the same operators but uses a different index data structure that may offer better performance in some applications.  In that case the operator class support functions must be able to reconstruct the original value using information accumulated from the inner tuples that are passed through to reach the leaf level.  There is also support for incrementally reconstructing the represented value when that is needed, and for passing down additional data (called  traverse values ) during a tree descent.  All five follow the convention of accepting two  internal  arguments, the first of which is a pointer to a C struct containing input values for the support method, while the second argument is a pointer to a C struct where output values must be plac.  The combination of these two prefixes and the downlink node's label (if any) must have the same meaning as the original prefix, because there is no opportunity to alter the node labels that are moved to the new lower-level tuple, nor to change any c.  All the SP-GiST support methods are normally called in a short-lived memory context; that is, CurrentMemoryContext  will be reset after processing of each tuple.  It is therefore not very important to worry about pfree'ing everything you palloc. ) If the indexed column is of a collatable data type, the index collation will be passed to all the support methods, using the standard  PG_GET_COLLATION()  mechanism.  Therefore, when indexing values of variable-length data types, long values can only be supported by methods such as radix trees, in which each level of the tree includes a prefix that is short enough to fit on a page, and the final leaf level includ.   jsonb_path_ops  supports fewer operators but offers better performance for those operators.  pmatch  is an output argument for use when partial match is supported.  The variable is initialized to  NULL  before call, so this argument can simply be ignored by operator classes that do not support partial match.  To support  ﬁpartial matchﬂ  queries, an operator class must provide the  comparePartial  method, and its  extractQuery  method must set the  pmatch  parameter when a partial-match query is encountered.  However, it is recommended that the SQL declarations of these three support functions use the opclass's indexed data type for the query  argument, even though the actual type might be something else depending on the operator.  If consistent response time is more important than update speed, use of pending entries can be disabled by turning off the  fastupdate  storage parameter for a GIN index.  Partial Match Algorithm GIN can support  ﬁpartial matchﬂ  queries, in which the query does not determine an exact match for one or more keys, but the possible matches fall within a reasonably narrow range of key values (within the key sorting order .  gin_fuzzy_search_limit The primary goal of developing GIN indexes was to create support for highly scalable full-text search in PostgreSQL, and there are often situations when a full-text search returns a very large set of results.  Note however that null key values contained within a non-null composite item or query value are supported.  The following  contrib  modules also contain GIN operator classes: btree_gin B-tree equivalent functionality for several data types hstore Module for storing (key, value) pairs intarray Enhanced support for  int[] pg_trgm Text similarity using trigr.  oi_opaque  can be used by the operator class routines to pass information between support procedures during an index scan.  The core distribution includes support for two types of operator classes: minmax and inclusion.  Note that assumptions about the semantics of operator strategies are embedded in the support procedures' source code.  Operator classes that implement completely different semantics are also possible, provided implementations of the four main support procedures described above are written. BRIN Indexes compatibility across major releases is not guaranteed: for example, additional support procedures might be required in later releases.  To write an operator class for a data type that implements a totally ordered set, it is possible to use the minmax support procedures alongside the corresponding operators, as shown in  Table 65.  Procedure and Support Numbers for Minmax Operator Classes Operator class member Object Support Procedure 1 internal function  brin_minmax_opcinfo() Support Procedure 2 internal function brin_minmax_add_value() Support Procedure 3 internal function b.  Procedure and Support Numbers for Inclusion Operator Classes Operator class member Object Dependency Support Procedure 1 internal function brin_inclusion_opcinfo() Support Procedure 2 internal function brin_inclusion_add_value() Support Procedure 3 .  Support function number 11 is the main function required to build the index.  Support procedure numbers 12 and 14 are provided to support irregularities of built-in data types.  Procedure number 12 is used to support network addresses from different families which are not mergeable.  Procedure number 14 is used to support empty ranges.  Both minmax and inclusion operator classes support cross-data-type operators, though with these the dependencies become more complicated.  It allows additional data types to be supported by defining extra sets of operators.  They require the dependency operator to be defined with the  STORAGE  data type as the left-hand- side argument and the other supported data type to be the right-hand-side argument of the supported operator. pid A lock file recording the current postmaster process ID (PID), cluster data directory path, postmaster start timestamp, port number, Unix- domain socket directory path (empty on Windows), first valid listen_address (IP address or  * , or empty if.  Only certain data types support TOAST Š there is no need to impose the overhead on data types that cannot produce large field values.  To support TOAST, a data type must have a variable-length ( varlena ) representation, in which, ordinarily, the first four-byte word of any stored value contains the total length of the value in bytes (including itself).  Therefore, the C-level functions supporting a TOAST-able data type must be careful about how they handle potentially TOASTed input values: an input might not actually consist of a four-byte length word and contents until after it's been  detoasted .  This alternative supports space-efficient storage of values shorter than 127 bytes, while still allowing the data type to grow to 1 GB at need.  The TOAST pointer mechanism supports this need by allowing a pass-by-reference Datum to point to either a standard varlena value (the on-disk representation) or a TOAST pointer that points to an expanded representation somewhere in memory.  Functions that do not know about the expanded representation, but simply apply  PG_DETOAST_DATUM  to their inputs, will automatically receive the traditional varlena representation; so support for an expanded representation can be introduced increme. ) The page size is basically only present as a cross-check; there is no support for having more than one page size in an installation.  The following column types are supported directly by  bootstrap.  For a non-unique column, there will normally be both a histogram and an MCV list, and  the histogram does not include the portion of the column population represented by the MCVs .  This gives an exact estimate of the selectivity within the portion of the table that is MCVs.  The histogram is then used in the same way as above to estimate the selectivity in the portion of the table that is not MCVs, and then the two numbers are combined to estimate the overall selectivity. 298387 for the portion of the histogram population that is less than  IAAAAA . 0001466             = 1  (rounding off) Notice that the number of rows estimated to be returned from the bitmap index scan reflects only the condition used with the index; this is important since it affects the cost estimate for the subsequent heap f.  Date/Time Support .  Supported Features .  Unsupported Features .  Index Support .  Index Support . ) For some types of errors, the server reports the name of a database object (a table, table column, data type, or constraint) associated with the error; for example, the name of the unique constraint that caused a  unique_violation  error.  Such names are supplied in separate fields of the error report message so that applications need not try to extract them from the possibly-localized human-readable text of the message.  Date/Time Support PostgreSQL uses an internal heuristic parser for all date/time input support. Date/Time Support 5. Date/Time Support In all cases, the UTC offset associated with a timestamp can be specified explicitly, using either a numeric UTC offset or a time zone abbreviation that corresponds to a fixed UTC offset. Date/Time Support B. Date/Time Support into a custom configuration file as needed. Date/Time Support The daylight-savings transition  rule  has the format dstdate  [  /   dsttime  ]  ,   stddate  [  /   stdtime  ] (As before, spaces should not be included in practice. Date/Time Support As an example,  CET-1CEST,M3. Date/Time Support at various times, PostgreSQL does not try, but rather follows the Gregorian calendar rules for all dates, even though this method is not historically accurate.  Although PostgreSQL supports Julian Date notation for input and output of dates (and also uses Julian dates for some internal datetime calculations), it does not observe the nicety of having dates run from noon to noon.  It is important to understand before studying  Table C.  Many of the features required by the SQL standard are supported, though sometimes with slightly differing syntax or function. SQL Conformance PostgreSQL supports most of the major features of SQL:2011.  In addition, there is a long list of supported optional features.  In the following two sections, we provide a list of those features that PostgreSQL supports, followed by a list of the features defined in SQL:2011 which are not yet supported in PostgreSQL.  Both of these lists are approximate: There might be minor details that are nonconforming for a feature that is listed as supported, and large parts of an unsupported feature might in fact be implemented.  Therefore, if a particular subfeature is not supported, the main feature is listed as unsupported even if some other subfeatures are supported.  Supported Features Identifier Package Description Comment B012 Embedded C B021 Direct SQL E011 Core Numeric data types E011-01 Core INTEGER and SMALLINT data types E011-02 Core REAL, DOUBLE PRECISION, and FLOAT data types E011-03 Core DECIMAL and NU. SQL Conformance Identifier Package Description Comment E101-04 Core Searched DELETE statement E111 Core Single row SELECT statement E121 Core Basic cursor support E121-01 Core DECLARE CURSOR E121-02 Core ORDER BY columns need not be in select list E1. SQL Conformance Identifier Package Description Comment S098 ARRAY_AGG S111 Enhanced object support ONLY in query expressions S201 SQL-invoked routines on arrays S201-01 Array parameters S201-02 Array as result type of functions S211 Enhanced object s.  Unsupported Features The following features defined in SQL:2011 are not implemented in this release of PostgreSQL. SQL Conformance Identifier Package Description Comment B221 Routine language Ada: VARCHAR and NUMERIC support E182 Core Module language F054 TIMESTAMP in DATE type precedence list F121 Basic diagnostics management F121-01 GET DIAGNOSTICS statement F1. SQL Conformance Identifier Package Description Comment F521 Enhanced integrity management Assertions F671 Enhanced integrity management Subqueries in CHECK intentionally omitted F693 SQL-session and client module collations F695 Translation support F. SQL Conformance Identifier Package Description Comment S026 Self-referencing structured types S027 Create method by specific method name S028 Permutable UDT options list S041 Basic object support Basic reference types S043 Enhanced object support Enh. SQL Conformance Identifier Package Description Comment T175 Generated columns T176 Sequence generator support T180 System-versioned tables T181 Application-time period tables T211 Active database, Enhanced integrity management Basic trigger capabilit. SQL Conformance Identifier Package Description Comment T511 Transaction counts T521 Named arguments in CALL statement T522 Default values for IN parameters of SQL- invoked procedures supported except DEFAULT key word in invocation T561 Holdable locat. 0 only X100 Host language support for XML: CONTENT option X101 Host language support for XML: DOCUMENT option X110 Host language support for XML: VARCHAR mapping X111 Host language support for XML: CLOB mapping X112 Host language support for XML: BLO. SQL Conformance Identifier Package Description Comment X113 Host language support for XML: STRIP WHITESPACE option X114 Host language support for XML: PRESERVE WHITESPACE option X131 Query-level XMLBINARY clause X132 XMLBINARY clause in DML X133 XMLB. 1 support X221 XML passing mechanism BY VALUE X231 XML(CONTENT(UNTYPED)) type X232 XML(CONTENT(ANY)) type X241 RETURNING CONTENT in XML publishing X242 RETURNING SEQUENCE in XML publishing X251 Persistent XML values of XML(DOCUMENT(UNTYPED)) type 222.  Only  BY VALUE  passing mechanism is supported The SQL standard defines two  passing mechanisms  that apply when passing an XML argument from SQL to an XML function or receiving a result:  BY REF , in which a particular XML value retains its node id.  Cannot pass named parameters to queries The XPath-based functions support passing one parameter to serve as the XPath expression's context item, but do not support passing additional values to be available to the expression as named parameters.  The PostgreSQL Project thanks Etienne Stalmans for reporting this problem.  In cases where these programs need to initiate additional connections, such as parallel processing or processing of multiple databases, the connection string was forgotten and just the basic connection parameters (database name, host, port, and user.  The PostgreSQL Project thanks Nick Cleaton for reporting this problem.  Ł Fix rare  ﬁlost saved point in indexﬂ  errors in scans of multicolumn GIN indexes (Tom Lane) Ł Fix unportable use of  getnameinfo()  in  pg_hba_file_rules  view (Tom Lane) On FreeBSD 11, and possibly other platforms, the view's  address  and  netm.  Ł Improve error handling in the server's  buffile  module (Thomas Munro) Fix some cases where I/O errors were indistinguishable from reaching EOF, or were not reported at all.  Ł Report out-of-disk-space errors properly in pg_dump and pg_basebackup (Justin Pryzby, Tom Lane, Álvaro Herrera) Some code paths could produce silly reports like  ﬁcould not write file: Successﬂ .  We have no reports of this error manifesting with stock zlib, but it can be seen when using IBM's zlibNX implementation.  Ł Support building our NLS code with Microsoft Visual Studio 2015 or later (Juan José Santamaría Flecha, Davinder Singh, Amit Kapila) Ł Avoid possible failure of our MSVC install script when there is a file named  configure  several levels above the. Release Notes Some error cases would be reported as  ﬁunexpected node typeﬂ  or the like, instead of the intended message.  Ł Add missing SQLSTATE values to a few error reports (Sawada Masahiko) Ł Fix PL/pgSQL to reliably refuse to execute an event trigger function as a plain function (Tom Lane) Ł Fix memory leak in libpq when using  sslmode=verify-full  (Roman Peshkurov.  Ł Fix ecpg to treat an argument of just  ﬁ - ﬂ  as meaning  ﬁread from stdinﬂ  on all platforms (Tom Lane) Ł Allow tab-completion of the filename argument to psql's  \gx  command (Vik Fearing) Ł Add pg_dump support for  ALTER . Release Notes Ł Improve error reporting in  to_date()  and  to_timestamp()  (Tom Lane, Álvaro Herrera) Reports about incorrect month or day names in input strings could truncate the input in the middle of a multi-byte character, leading to an imprope.  Ł Fix BRIN index logic to support hypothetical BRIN indexes (Julien Rouhaud, Heikki Linnakangas) Previously, if an  ﬁindex adviserﬂ  extension tried to get the planner to produce a plan involving a hypothetical BRIN index, that would fail, because t.  Ł Improve error reporting for attempts to use automatic updating of views with conditional  INSTEAD rules (Dean Rasheed) This has never been supported, but previously the error was thrown only at execution time, so that it could be masked by planner.  Ł Prevent a composite type from being included in itself indirectly via a range type (Tom Lane, Julien Rouhaud) Ł Disallow partition key expressions that return pseudo-types, such as  record  (Tom Lane) Ł Fix error reporting for index expressions of.  Ł Allow libpq to parse all GSS-related connection parameters even when the GSSAPI code hasn't been compiled in (Tom Lane) This makes the behavior similar to our SSL support, where it was long ago deemed to be a good idea to always accept all the rel.  Ł Fix multiple statistics entries reported by the LWLock statistics mechanism (Fujii Masao) 2243 . Release Notes The LWLock statistics code (which is not built by default; it requires compiling with  - DLWLOCK_STATS ) could report multiple entries for the same LWLock and backend process, as a result of faulty hashtable key creation.  Ł Reject include directives with empty file names in configuration files, and report include-file recursion more clearly (Ian Barwick, Tom Lane) Ł Avoid logging complaints about abandoned connections when using PAM authentication (Tom Lane) libpq-ba.  We have seen only one long- ago report that matches this bug, but it could cause problems for people trying to build modified PostgreSQL code or use atypical compiler options. Release Notes Ł Fix TAP tests to work with msys Perl, in cases where the build directory is on a non-root msys mount point (Noah Misch) Ł Support building Postgres with Microsoft Visual Studio 2019 (Haribabu Kommi) Ł In Visual Studio builds, honor  W.  Ł Support OpenSSL 1.  Ł Sync our copy of the timezone library with IANA tzcode release 2019b (Tom Lane) This adds support for zic's new  -b slim  option to reduce the size of the installed zone files.  The PostgreSQL Project thanks Alexander Lakhin for reporting this problem.  Ł Fix misleading error reports from reindexdb (Julien Rouhaud) Ł Ensure that vacuumdb returns correct status if an error occurs while using parallel jobs (Julien Rouhaud) Ł Fix  contrib/auto_explain  to not cause problems in parallel queries (Tom La.  Ł Fix behavior for an  UPDATE  or  DELETE  on an inheritance tree or partitioned table in which every table can be excluded (Amit Langote, Tom Lane) In such cases, the query did not report the correct set of output columns when a  RETURNING  clause .  Ł Add missing support for  CREATE TABLE IF NOT EXISTS .  Ł Report correct relation name in autovacuum's  pg_stat_activity  display during BRIN summarize operations (Álvaro Herrera) Ł Fix  ﬁfailed to build any  N -way joinsﬂ  planner failures with lateral references leading out of  FULL outer joins (Tom La.  Ł Fix error detection in directory scanning on Windows (Konstantin Knizhnik) Errors, such as lack of permissions to read the directory, were not detected or reported correctly; instead the code silently acted as though the directory were empty.  Changes Ł By default, panic instead of retrying after  fsync()  failure, to avoid possible data corruption (Craig Ringer, Thomas Munro) Some popular operating systems discard kernel data buffers when unable to write them out, reporting this as  fsyn.  Ł Avoid null-pointer-dereference crash on some platforms when pg_dump or pg_restore tries to report an error (Tom Lane) Ł Properly disregard  SIGPIPE  errors if  COPY FROM PROGRAM  stops reading the program's output early (Tom Lane) 2260 .  Ł Avoid crashes and excessive runtime with large inputs to  contrib/intarray 's gist__int_ops  index support (Andrew Gierth) Ł In configure, look for  python3  and then  python2  if  python  isn't found (Peter Eisentraut) This allows PL/Python to be.  Ł Support new Makefile variables  PG_CFLAGS ,  PG_CXXFLAGS , and  PG_LDFLAGS  in pgxs builds (Christoph Berg) This simplifies customization of extension build processes.  Ł Rename red-black tree support functions to use  rbt  prefix not  rb  prefix (Tom Lane) This avoids name collisions with Ruby functions, which broke PL/Ruby. , by VACUUM FULL  (Andres Freund) Ł Prevent starting the server with  wal_level  set to too low a value to support an existing replication slot (Andres Freund) Ł Avoid crash if a utility command causes infinite recursion (Tom Lane) Ł When initializin.  Ł Properly handle turning  full_page_writes  on dynamically (Kyotaro Horiguchi) Ł Fix possible crash due to double  free()  during SP-GiST rescan (Andrew Gierth) Ł Prevent mis-linking of src/port and src/common functions on ELF-based BSD platforms, .  Ł Fix psql, as well as documentation examples, to call  PQconsumeInput()  before each PQnotifies()  call (Tom Lane) This fixes cases in which psql would not report receipt of a  NOTIFY  message until after the next command.  Ł Make  src/port/snprintf.  Our thanks to Andrew Krasichkov for reporting this issue.  Ł Improve performance of lock releasing in standby server WAL replay (Thomas Munro) Ł Make logical WAL senders report streaming state correctly (Simon Riggs, Sawada Masahiko) The code previously mis-detected whether or not it had caught up with the .  Ł Fix  CREATE AGGREGATE  type checking so that parallelism support functions can be attached to variadic aggregates (Alexey Bashtanov) Ł Widen  COPY FROM 's current-line-number counter from 32 to 64 bits (David Rowley) This avoids two problems with .  Also, an incorrect error message was reported for an unparseable  hostaddr  value.  Also, when the  host ,  hostaddr , or  port  parameters contain comma-separated lists, libpq is now more careful to treat empty elements of a list as selecting the default behavior.  Ł Fix ecpg's support for  long long  variables on Windows, as well as other platforms that declare strtoll / strtoull  nonstandardly or not at all (Dang Minh Huong, Tom Lane) Ł Fix misidentification of SQL statement type in PL/pgSQL, when a rule cha. Release Notes Ł Fix crash in  contrib/ltree 's  lca()  function when the input array is empty (Pierre Ducroquet) Ł Fix various error-handling code paths in which an incorrect error code might be reported (Michael Paquier, Tom Lane, Magnus Hagander) Ł.  In a large table where  ANALYZE  samples only a small fraction of the pages, this meant that the overall tuple density estimate could not change very much, so that  reltuples  would change nearly proportionally to changes in the table's physical siz.  Ł Count the number of index tuples correctly during vacuuming of a GiST index (Andrey Borodin) Previously it reported the estimated number of heap tuples, which might be inaccurate, and is certainly wrong if the index is partial. 7 (Peter Eisentraut) Ł Support testing PL/Python and related modules when building with Python 3 and MSVC (Andrew Dunstan) Ł Fix errors in initial build of  contrib/bloom  indexes (Tomas Vondra, Tom Lane) Fix possible omission of the table's last tup.  Ł Fix incorrect reporting of PL/Python function names in error  CONTEXT  stacks (Tom Lane) An error occurring within a nested PL/Python function call (that is, one reached via a SPI query from another PL/Python function) would result in a stack trac.  Ł Allow  contrib/auto_explain 's  log_min_duration  setting to range up to  INT_MAX , or about 24 days instead of 35 minutes (Tom Lane) Ł Mark assorted GUC variables as  PGDLLIMPORT , to ease porting extension modules to Windows (Metin Doslu) E.  Ł Fix  UNION / INTERSECT / EXCEPT  over zero columns (Tom Lane) Ł Disallow identity columns on typed tables and partitions (Michael Paquier) These cases will be treated as unsupported features for now.  Ł Fix  has_sequence_privilege()  to support  WITH GRANT OPTION  tests, as other privilege-testing functions do (Joe Conway) Ł In databases using UTF8 encoding, ignore any XML declaration that asserts a different encoding (Pavel Stehule, Noah Misch) .  In encodings other than UTF8, we don't promise to support non-ASCII XML data anyway, so retain the previous behavior for bug compatibility.  Ł Allow a client that supports SCRAM channel binding (such as v11 or later libpq) to connect to a v10 server (Michael Paquier) v10 does not have this feature, and the connection-time negotiation about whether to use it was done incorrectly.  Ł Fix pg_dump to make ACL (permissions), comment, and security label entries reliably identifiable in archive output formats (Tom Lane) The  ﬁtagﬂ  portion of an ACL archive entry was usually just the name of the associated object.  Ł In ecpg, detect indicator arrays that do not have the correct length and report an error (David Rader) Ł Change the behavior of  contrib/cube 's  cube   ~>   int  operator to make it compatible with KNN search (Alexander Korotkov) The meaning of t.  Ł Fix incorrect selection of configuration-specific libraries for OpenSSL on Windows (Andrew Dunstan) Ł Support linking to MinGW-built versions of libperl (Noah Misch) This allows building PL/Perl with some common Perl distributions for Windows.  Ł Fix MSVC build to test whether 32-bit libperl needs  -D_USE_32BIT_TIME_T  (Noah Misch) Available Perl distributions are inconsistent about what they expect, and lack any reliable means of reporting it, so resort to a build-time test on what the li.  Ł On Windows, avoid encoding-conversion-related crashes when emitting messages very early in postmaster startup (Takayuki Tsunakawa) Ł Use our existing Motorola 68K spinlock code on OpenBSD as well as NetBSD (David Carlier) Ł Add support for spinloc.  Ł Correctly detect hashability of range data types (Tom Lane) The planner mistakenly assumed that any range type could be hashed for use in hash joins or hash aggregation, but actually it must check whether the range's subtype has hash support.  If it's necessary to support such old clients, you can use custom 1024-bit DH parameters instead of the compiled-in defaults. Release Notes The  password_encryption  server parameter no longer supports  off  or  plain .  The UNENCRYPTED  option is no longer supported in  CREATE/ALTER USER .  Ł Remove pg_dump/pg_dumpall support for dumping from pre-8.  Ł Remove support for floating-point timestamps and intervals (Tom Lane) This removes configure's  --disable-integer-datetimes  option.  Ł Remove server support for client/server protocol version 1. 0 (Tom Lane) This protocol hasn't had client support since PostgreSQL 6. Release Notes Ł Remove support for version-0 function calling conventions (Andres Freund) Extensions providing C-coded functions must now conform to version 1 calling conventions.  Parallel Queries Ł Support parallel B-tree index scans (Rahila Syed, Amit Kapila, Robert Haas, Rafia Sabih) This change allows B-tree index pages to be searched by separate parallel workers.  Ł Support parallel bitmap heap scans (Dilip Kumar) This allows a single index scan to dispatch parallel workers to process different areas of the heap.  Indexes Ł Add write-ahead logging support to hash indexes (Amit Kapila) This makes hash indexes crash-safe and replicatable.  Ł Improve hash index performance (Amit Kapila, Mithun Cy, Ashutosh Sharma) Ł Add SP-GiST index support for  INET  and  CIDR  data types (Emre Hasegeli) Ł Add option to allow BRIN index summarization to happen more aggressively (Álvaro Herrera) A new.  Ł Add function  pg_current_logfile()  to read logging collector's current stderr and csvlog output file names (Gilles Darold) Ł Report the address and port number of each listening socket in the server log during postmaster startup (Tom Lane) Also, .   pg_stat_activity Ł Add  pg_stat_activity  reporting of low-level wait states (Michael Paquier, Robert Haas, Rushabh Lathia) This change enables reporting of numerous low-level wait conditions, including latch waits, file reads/writes/fsyncs, client.  Authentication Ł Add  SCRAM-SHA-256  support for password negotiation and storage (Michael Paquier, Heikki Linnakangas) 2291 .  Ł Change the  password_encryption  server parameter from  boolean  to  enum  (Michael Paquier) This was necessary to support additional password hashing options.  Ł Support multiple RADIUS servers (Magnus Hagander) All the RADIUS related parameters are now plural and support a comma-separated list of servers.  Ł Add columns to  pg_stat_replication  to report replication delay times (Thomas Munro) The new columns are  write_lag ,  flush_lag , and  replay_lag .  Utility Commands Ł Add table  partitioning syntax  that automatically creates partition constraints and handles routing of tuple insertions and updates (Amit Langote) The syntax supports range and list partitioning.  Ł Allow multiple functions, operators, and aggregates to be dropped with a single  DROP  command (Peter Eisentraut) Ł Support  IF NOT EXISTS  in  CREATE SERVER ,  CREATE USER MAPPING , and  CREATE COLLATION  (Anastasia Lubennikova, Peter Eisentraut).  Data Types Ł Add full text search support for  JSON  and  JSONB  (Dmitry Dolgov) The functions  ts_headline()  and  to_tsvector()  can now be used on these data types.  Ł Add support for EUI-64 MAC addresses, as a new data type  macaddr8  (Haribabu Kommi) This complements the existing support for EUI-48 MAC addresses (type  macaddr ).   psql Ł Add conditional branch support to psql (Corey Huinker) This feature adds psql meta-commands  \if ,  \elif ,  \else , and  \endif .  Ł Rename  initdb  options  --noclean  and  --nosync  to be spelled  --no-clean  and  --no- sync  (Vik Fearing, Peter Eisentraut) The old spellings are still supported.  Ł Support using synchronized snapshots when dumping from a standby server (Petr Jelinek) 2297 . pid , not by attempting connections (Tom Lane) The postmaster has been changed to report its ready-for-connections status in  postmaster.  Ł Allow the  ICU  library to optionally be used for collation support (Peter Eisentraut) The ICU library has versioning that allows detection of collation changes between versions.  Ł Improve support for 64-bit atomics (Andres Freund) Ł Enable 64-bit atomic operations on ARM64 (Roman Shaposhnik) Ł Switch to using  clock_gettime() , if available, for duration measurements (Tom Lane) gettimeofday()  is still used if  clock_gettim. Release Notes Ł Remove SCO and Unixware ports (Tom Lane) Ł Overhaul documentation  build process  (Alexander Lakhin) Ł Use XSLT to build the PostgreSQL documentation (Peter Eisentraut) Previously Jade, DSSSL, and JadeTex were used.  Ł In postgres_fdw, push joins to the remote server in more cases (David Rowley, Ashutosh Bapat, Etsuro Fujita) Ł Properly support  OID  columns in postgres_fdw tables (Etsuro Fujita) Previously  OID  columns always returned zeros.  Ł Add indexing support to btree_gist for the  UUID  data type (Paul Jungwirth) Ł Add  amcheck  which can check the validity of B-tree indexes (Peter Geoghegan) Ł Show ignored constants as  $N  rather than  ?  in  pg_stat_statements  (Lukas Fittl) Ł .  Acknowledgments The following individuals (in alphabetical order) have contributed to this release as patch authors, committers, reviewers, testers, or reporters of issues.  These include porting tools, analysis utilities, and plug-in features that are not part of the core PostgreSQL system, mainly because they address a limited audience or are too experimental to be part of the main source tree.  adminpack adminpack  provides a number of support functions which pgAdmin and other administration and management tools can use to provide additional functionality, such as remote management of server log files.  In event of failure of the second rename step, it will try to rename  archivename  back to newname  before reporting the error.  The correctness of the access method functions behind index scans and other important operations relies on these invariants always holding.  For example, B-Tree index verification relies on comparisons made with one or more B-Tree support function 1 routines. 3  for details of operator class support functions.  Naturally, this query could easily be changed to call  bt_index_check  for every index in the database where verification is supported.  auth_delay auth_delay  causes the server to pause briefly before reporting authentication failure, to make brute-force attacks on database passwords more difficult.  Note that it does nothing to prevent denial- of-service attacks, and may even exacerbate them, since processes that are waiting before reporting authentication failure will still consume connection slots. milliseconds  ( int )  The number of milliseconds to wait before reporting an authentication failure.  A signature is a lossy representation of the indexed attribute(s), and as such is prone to reporting false positives; that is, it may be reported that an element is in the set, when it is not.  A traditional btree index is faster than a bloom index, but it can require many btree indexes to support all possible queries where one needs only a single bloom index.  Note however that bloom indexes only support equality queries, whereas btree indexes can also perform inequality and range searches.  Ł Only the  =  operator is supported for search.  But it is possible to add support for arrays with union and intersection operations in the future.  Ł bloom  access method doesn't support  UNIQUE  indexes.  Ł bloom  access method doesn't support searching for  NULL  values.  In addition to the typical B-tree search operators,  btree_gist  also provides index support for  <> ( ﬁnot equalsﬂ ).  Also, for data types for which there is a natural distance metric,  btree_gist  defines a distance operator  <-> , and provides GiST index support for nearest-neighbor searches using this operator.  There are provisions in the code to report an error if the password is determined to be easily crackable.  This operator is designed for KNN- GiST support.  They exist mainly to support the b-tree index operator class for  cube , which can be useful for example if you would like a UNIQUE constraint on a  cube  column.  Department of Energy for the years of faithful support of my database research.  dblink dblink  is a module that supports connections to other PostgreSQL databases from within a database session. 1 port=5432 dbname=mydb user=postgres password=mypasswd options=- csearch_path= .  If false, the remote error is locally reported as a NOTICE, and the function returns no rows.  If false, the remote error is locally reported as a NOTICE, and the function's return value is set to  ERROR .  If false, the remote error is locally reported as a NOTICE, and the function's return value is set to  ERROR .  If false, the remote error is locally reported as a NOTICE, and the function returns no rows.  If false, the remote error is locally reported as a NOTICE, and the function's return value is set to  ERROR .  If false, the remote error is locally reported as a NOTICE, and the function returns no rows.  Functions are provided to support input in latitude and longitude (in degrees), to support output of latitude and longitude, to calculate the great circle distance between two points and to easily specify a bounding box usable for index searches.  COPY 's  OIDS  and  FORCE_QUOTE  options are currently not supported by  file_fdw .  In principle non-superusers could be allowed to change the other options, but that's not supported at present.  The  difference  function converts two strings to their Soundex codes and then reports the number of matching code positions.  Indexes hstore  has GiST and GIN index support for the  @> ,  ? ,  ?&  and  ?|  operators.  For example: CREATE INDEX hidx ON testhstore USING GIST (h); CREATE INDEX hidx ON testhstore USING GIN (h); hstore  also supports  btree  or  hash  indexes for the  =  operator.  There is also support for indexed searches using some of the operators.  Index Support intarray  provides index support for the  && ,  @> ,  <@ , and  @@  operators, as well as regular array equality.  There is also a non-default GIN operator class  gin__int_ops  supporting the same operators.  Alternatively, prefix validation and hyphenation support may be dropped from a future version of this module.  Functions and Operators The  isn  module provides the standard comparison operators, plus B-tree and hash indexing support for all these data types. org/productssolutions/idkeys/support/prefix_list.  lo The  lo  module provides support for managing Large Objects (also called LOs or BLOBs). sport*@.  a label beginning with the case-insensitive prefix  sport d.  Here's an example  ltxtquery : Europe & Russia*@ & !Transportation This will match paths that contain the label  Europe  and any label beginning with  Russia  (case- insensitive), but not paths containing the label  Transportation .  The location of these words within the path is not important. ) ltree longest common ancestor of paths (up to 8 arguments supported) lca('1.  Indexes ltree  supports several types of indexes that can speed up the indicated operators: Ł B-tree index over  ltree :  < ,  <= ,  = ,  >= ,  > Ł GiST index over  ltree :  < ,  <= ,  = ,  >= ,  > ,  @> ,  <@ ,  @ ,  ~ ,  ? Example of creating such.  (The reverse is currently not supported, however.  Comments and bug reports are welcome. 17  lists the algorithms supported by the  crypt()  function.  Supported Algorithms for  crypt() Algorithm Max Password Length Adaptive? Salt Bits Output Length Description bf 72 yes 128 60 Blowfish- based, variant 2a md5 unlimited no 48 34 MD5-based crypt xdes 8 yes 24 20 Extended DES des 8 no 12 13 Original U.  Supported are both symmetric-key and public-key encryption. com software supports it fine.  To list keys: gpg --list-secret-keys To export a public key in ASCII-armor format: gpg -a --export KEYID > public. key To export a secret key in ASCII-armor format: gpg -a --export-secret-keys KEYID > secret.  Limitations of PGP Code Ł No support for signing.  Ł No support for encryption key as master key.  Ł No support for several subkeys.  Any digest algorithm OpenSSL supports is automatically picked up.  This is not possible with ciphers, which need to be supported explicitly.   prefetch  issues asynchronous prefetch requests to the operating system, if this is supported, or throws an error otherwise.   read  reads the requested range of blocks; unlike  prefetch , this is synchronous and supported on all platforms and builds, but may be slower. Additional Supplied Modules it is important to understand that there are only limited guarantees around the stability of the  queryid hash value.  This feature is intended to support external tools that might wish to avoid the overhead of repeatedly retrieving query texts of indeterminate length.  The module requires additional shared memory proportional to  pg_stat_statements.  pg_trgm The  pg_trgm  module provides functions and operators for determining the similarity of alphanumeric text based on trigram matching, as well as index operator classes that support fast searching for similar strings.  Index Support The  pg_trgm  module provides GiST and GIN index operator classes that allow you to create an index over a text column for the purpose of very fast similarity searches.  These index types support the above-described similarity operators, and additionally support trigram-based index searches for  LIKE , ILIKE ,  ~  and  ~*  queries.  (These indexes do not support equality nor simple comparison operators, so you may need a regular B-tree index too. 1, these index types also support index searches for  LIKE  and  ILIKE , for example SELECT * FROM test_trgm WHERE t LIKE '%foo%bar'; The index search works by extracting trigrams from the search string and then looking these up in the index. 3, these index types also support index searches for regular-expression matches ( ~  and  ~*  operators), for example SELECT * FROM test_trgm WHERE t ~ '(foo|bar)'; The index search works by extracting trigrams from the regular expression and then lo.  The reported values can also disagree because of a change that occurs after  pg_visibility  examines the visibility map and before it examines the data page. ) Note that  postgres_fdw  currently lacks support for  INSERT  statements with an  ON CONFLICT DO UPDATE  clause.  However, the  ON CONFLICT DO NOTHING  clause is supported, provided a unique index inference specification is omitted.  Note however that the  information_schema  views will report a  postgres_fdw  foreign table to be updatable (or not) according to the setting of this option, without any check of the remote server.  Importing Options postgres_fdw  is able to import foreign table definitions using  IMPORT FOREIGN SCHEMA .  If the remote tables to be imported have columns of user-defined data types, the local server must have compatible types of the same names.  Importing behavior can be customized with the following options (given in the  IMPORT FOREIGN SCHEMA  command): 2426 . Additional Supplied Modules import_collate This option controls whether column  COLLATE  options are included in the definitions of foreign tables imported from a foreign server.  import_default This option controls whether column  DEFAULT  expressions are included in the definitions of foreign tables imported from a foreign server.  The  IMPORT  will fail altogether if an imported default expression uses a function or operator that does not exist locally.  import_not_null This option controls whether column  NOT NULL  constraints are included in the definitions of foreign tables imported from a foreign server.  Note that constraints other than  NOT NULL  will never be imported from the remote tables.  Although PostgreSQL does support  CHECK  constraints on foreign tables, there is no provision for importing them automatically, because of the risk that a constraint expression could evaluate differently on the local and remote servers.  So if you wish to import  CHECK  constraints, you must do so manually, and you should verify the semantics of each one carefully.  Partitioned tables are imported, unless they are a partition of some other table.  Note that it is currently not supported by  postgres_fdw  to prepare the remote transaction for two- phase commit. 89  listening on port  5432 . 89', port '5432', dbname  'foreign_db'); A user mapping, defined with  CREATE USER MAPPING , is needed as well to identify the role that will be used on the remote server: CREATE USER MAPPING FOR local_user         SERVER foreign_server         OPTIO.  The operators supported by the GiST operator class are shown in  Table F.  Department of Energy for the years of faithful support of my database research.  sepgsql sepgsql  is a loadable module that supports label-based mandatory access control (MAC) based on SELinux security policy. 13 or higher (although some distributions may backport the necessary rules into older policy versions).  Adjust the paths shown as appropriate for your installation: $ export PGDATA=/path/to/data/directory $ initdb $ vi $PGDATA/postgresql.  When  sepgsql  is in use, security labels are automatically assigned to supported database objects at creation time.  Row-level access control PostgreSQL supports row-level access, but  sepgsql  does not.  timetravel()  is the general trigger function that supports this behavior.  In both cases the old status is reported.  However, ordering of the categories within a group is not important.  If the ordering of siblings of the same parent is important, include the  orderby_fld  parameter to specify which field to order siblings by.  It is important that the  branch_delim  string not appear in any key values, else  connectby  may incorrectly report an infinite-recursion error.  SYSTEM_ROWS  does not support the  REPEATABLE  clause. Additional Supplied Modules SYSTEM_TIME  does not support the  REPEATABLE  clause. org/pkg/lib/uuid/ , it is not well maintained, and is becoming increasingly difficult to port to newer platforms.  oid2name also accepts the following command-line arguments for connection parameters: -d   database database to connect to -H   host database server's host -p   port database server's port -U   username user name to connect as -P   password password.  -p   port Database server's port. Additional Supplied Programs pg_standby pg_standby  Š  supports the creation of a PostgreSQL warm standby server Synopsis pg_standby  [ option . ]  archivelocation   nextwalfile   walfilepath [ restartwalfile ] Description pg_standby supports creation of a  ﬁwarm standbyﬂ  database server.  This is the only supported behavior so this option is useless.  So by default, we will wait 5 secs, 10 secs, then 15 secs before reporting the failure back to the standby server.  Please report package status to the documentation mailing list, and we will include that information here.  Installation on FreeBSD The FreeBSD Documentation Project is itself a heavy user of DocBook, so it comes as no surprise that there is a full set of  ﬁportsﬂ  of the documentation tools available on FreeBSD.  The following ports need to be installed to build the documentation on FreeBSD.  macOS If you use MacPorts, the following will get you set up: sudo port install docbook-sgml-4. Documentation Other XSL-FO processors can also be used manually, but the automated build process only supports FOP.  There is one important thing to note with PSGML: its author assumed that your main SGML DTD directory would be  /usr/local/lib/sgml . Acronyms NLS National Language Support 30 ODBC Open Database Connectivity 31 OID Object Identifier OLAP Online Analytical Processing 32 OLTP Online Transaction Processing 33 ORDBMS Object-Relational Database Management System 34 PAM Pluggable Authent.  Some white papers and technical reports from the original POSTGRES development team are available at the University of California, Berkeley, Computer Science Department  web site 1 .  Proceedings and Articles [ports12]  ﬁ Serializable Snapshot Isolation in PostgreSQL 3 ﬂ. conf,  575 pg_identify_object,  317 pg_identify_object_as_address,  317 pg_import_system_collations,  333 pg_index,  1931 pg_indexam_has_property,  312 pg_indexes,  1974 pg_indexes_size,  331 pg_index_column_has_property,  312 pg_index_has_property, . start_proc configuration parameter,  1179 point,  147 ,  248 point-in-time recovery,  623 policy,  69 polygon,  148 ,  248 polymorphic function,  983 polymorphic type,  983 popen,  246 populate_record,  2369 port,  745 port configuration parameter,  
max_connections	 Both advisory locks and regular locks are stored in a shared memory pool whose size is defined by the configuration variables  max_locks_per_transaction  and  max_connections .  As above, you might be able to work around the problem by starting the server with a reduced number of allowed connections ( max_connections ), but you'll eventually want to increase the kernel limit. , sets) at least ceil((max_connections + autovacuum_max_workers + max_worker_processes + 5) / 16)  plus room for other applications SEMMNS Maximum number of semaphores system-wide ceil((max_connections + autovacuum_max_workers + max_worker_processes .  When using System V semaphores, PostgreSQL uses one semaphore per allowed connection ( max_connections ), allowed autovacuum worker process ( autovacuum_max_workers ) and allowed background process ( max_worker_processes ), in sets of 16.  The maximum number of semaphores in the system is set by  SEMMNS , which consequently must be at least as high as  max_connections  plus autovacuum_max_workers  plus  max_worker_processes , plus one extra for each 16 allowed connections plus workers.  Hence this parameter must be at least  ceil((max_connections + autovacuum_max_workers + max_worker_processes + 5) / 16) .  When using POSIX semaphores, the number of semaphores needed is the same as for System V, that is one semaphore per allowed connection ( max_connections ), allowed autovacuum worker process ( autovacuum_max_workers ) and allowed background process (.  In many cases, it may be better to reduce  max_connections  and instead make use of external connection-pooling software.  max_connections  ( integer )  Determines the maximum number of concurrent connections to the database server.  At most  max_connections  connections can ever be active simultaneously.  Whenever the number of active concurrent connections is at least  max_connections minus  superuser_reserved_connections , new connections will be accepted only for superusers, and no new replication connections will be accepted.  The value must be less than the value of max_connections .  If you are using prepared transactions, you will probably want  max_prepared_transactions  to be at least as large as  max_connections , so that every session can have a prepared transaction pending.  WAL sender processes count towards the total number of connections, so the parameter cannot be set higher than  max_connections .  max_locks_per_transaction  ( integer )  The shared lock table tracks locks on  max_locks_per_transaction  * ( max_connections  + max_prepared_transactions ) objects (e.  max_pred_locks_per_transaction  ( integer )  The shared predicate lock table tracks locks on  max_pred_locks_per_transaction  * ( max_connections  +  max_prepared_transactions ) objects (e.  Note that the number of running workers does not count towards max_connections  or  superuser_reserved_connections  limits. High Availability, Load Balancing, and Replication Ł max_connections Ł max_prepared_transactions Ł max_locks_per_transaction Ł max_worker_processes It is important that the administrator select appropriate settings for  max_standby_archive_delay  and.  pg_dump will open  njobs  + 1 connections to the database, so make sure your  max_connections setting is high enough to accommodate all connections. vacuumdb vacuumdb will open  njobs  connections to the database, so make sure your  max_connections setting is high enough to accommodate all connections.  Specifying this option is equivalent to setting the  max_connections  configuration parameter.  You might be able to postpone reconfiguring your kernel by decreasing  shared_buffers  to reduce the shared memory consumption of PostgreSQL, and/or by reducing  max_connections  to reduce the semaphore consumption. Index max_connections configuration parameter,  510 max_files_per_process configuration parameter,  517 max_function_args configuration parameter,  562 max_identifier_length configuration parameter,  562 max_index_keys configuration parameter,  562 m
unix_socket_directories	 One way to prevent spoofing of  local  connections is to use a Unix domain socket directory ( unix_socket_directories ) that has write permission only for a trusted local user.  unix_socket_directories  ( string )  Specifies the directory of the Unix-domain socket(s) on which the server is to listen for connections from client applications. lock  will be created in each of the unix_socket_directories  directories.  There, one can achieve a similar effect by pointing unix_socket_directories  to a directory having search permission limited to the desired audience.  Or you could set the  unix_socket_directories  configuration parameter to place the socket file in a suitably restricted directory.  Specifying this option is equivalent to setting the  unix_socket_directories configuration parameter
authentication_timeout	 Security and Authentication authentication_timeout  ( integer )      Maximum time to complete client authentication, in seconds
ssl	 sslinfo .   pg_stat_ssl  View .  Input and output formats follow Classless Internet Domain Routing conventions. , it avoids them when retrieving common values, so the earlier example really only saves index size, it is not required to avoid index usage), and grossly incorrect plan choices are cause for a bug report. ) --with-openssl   Build with support for SSL (encrypted) connections.  openssl Required for SSL support. openssl.  To use a server-side third party library such as python or openssl, this library  must  also be 64-bit.  To do that, the server must be configured to accept only hostssl  connections ( Section 20.  The TCP client must connect using  sslmode=verify-ca  or  verify-full  and have the appropriate root certificate file installed ( Section 33. conf  file allows administrators to specify which hosts can use non- encrypted connections ( host ) and which require SSL-encrypted connections ( hostssl ).  With SSL support compiled in, the PostgreSQL server can be started with SSL enabled by setting the parameter  ssl  to  on  in  postgresql.  By default, this file is named openssl. cnf  and is located in the directory reported by  openssl version -d .  While a list of ciphers can be specified in the OpenSSL configuration file, you can specify ciphers specifically for use by the database server by modifying  ssl_ciphers  in  postgresql. key , respectively, in the server's data directory, but other names and locations can be specified using the configuration parameters ssl_cert_file  and  ssl_key_file .  Using Client Certificates To require the client to supply a trusted certificate, place certificates of the root certificate authorities (CAs) you trust in a file in the data directory, set the parameter  ssl_ca_file  in  postgresql. conf  to the new file name, and add the authentication option  clientcert=1  to the appropriate  hostssl 500 .  Certificate Revocation List (CRL) entries are also checked if the parameter  ssl_crl_file  is set. conf  lines specified as  hostssl .  SSL Server File Usage File Contents Effect ssl_cert_file  ( $PGDATA/ server. crt ) server certificate sent to client to indicate server's identity ssl_key_file  ( $PGDATA/ server. key ) server private key proves server certificate was sent by the owner; does not indicate certificate owner is trustworthy ssl_ca_file  ( $PGDATA/ root. crt ) trusted certificate authorities checks that client certificate is signed by a trusted certificate authority ssl_crl_file  ( $PGDATA/ root. com  with the server's host name: openssl req -new -x509 -days 365 -nodes -text -out server.  To create a server certificate whose identity can be validated by clients, first create a certificate signing request (CSR) and a public/private key file: openssl req -new -nodes -text -out root. key Then, sign the request with the key to create a root certificate authority (using the default OpenSSL configuration file location on Linux): openssl x509 -req -in root. csr -text -days 3650 \   -extfile /etc/ssl/openssl. crt Finally, create a server certificate signed by the new root certificate authority: openssl req -new -nodes -text -out server. key openssl x509 -req -in server.  It is also possible to create a chain of trust that includes intermediate certificates: # root openssl req -new -nodes -text -out root. key openssl x509 -req -in root. csr -text -days 3650 \   -extfile /etc/ssl/openssl. crt # intermediate openssl req -new -nodes -text -out intermediate. key openssl x509 -req -in intermediate. csr -text -days 1825 \   -extfile /etc/ssl/openssl. crt # leaf openssl req -new -nodes -text -out server. key openssl x509 -req -in server.  ssl  ( boolean )  Enables SSL connections.  ssl_ca_file  ( string )  Specifies the name of the file containing the SSL server certificate authority (CA).  ssl_cert_file  ( string )  Specifies the name of the file containing the SSL server certificate.  ssl_crl_file  ( string )  Specifies the name of the file containing the SSL server certificate revocation list (CRL).  ssl_key_file  ( string )  Specifies the name of the file containing the SSL server private key.  ssl_ciphers  ( string )  Specifies a list of SSL cipher suites that are allowed to be used by SSL connections.  Use the command  openssl ciphers -v 'HIGH:MEDIUM:+3DES:!aNULL'  to see actual details for the currently installed OpenSSL version.  ssl_prefer_server_ciphers  ( boolean )  Specifies whether to use the server's SSL cipher preferences, rather than the client's.  ssl_ecdh_curve  ( string )  Specifies the name of the curve to use in ECDH key exchange.  The full list of available curves can be shown with the command  openssl ecparam -list_curves . Server Configuration ssl_dh_params_file  ( string )  Specifies the name of the file containing Diffie-Hellman parameters used for so-called ephemeral DH family of SSL ciphers.  You can create your own DH parameters file with the command  openssl dhparam -out dhparams.  To avoid uselessly long delays in such cases, the actual delay is calculated as vacuum_cost_delay  *  accumulated_balance  /  vacuum_cost_limit  with a maximum of  vacuum_cost_delay  * 4.  A record can have one of the seven formats local       database    user    auth-method   [ auth-options ] host        database    user    address    auth-method   [ auth-options ] hostssl     database    user    address    auth-method   [ auth-optio. Client Authentication hostnossl   database    user    address    auth-method   [ auth-options ] host        database    user    IP-address    IP-mask    auth-method   [ auth- options ] hostssl     database    user    IP-address    IP-mask    auth-met.  hostssl This record matches connection attempts made using TCP/IP, but only when the connection is made with SSL encryption.  Furthermore, SSL must be enabled by setting the  ssl  configuration parameter (see  Section 18.  Otherwise, the  hostssl  record is ignored except for logging a warning that it cannot match any connections.  hostnossl This record type has the opposite behavior of  hostssl ; it only matches connection attempts made over TCP/IP that do not use SSL.  This field only applies to  host ,  hostssl , and  hostnossl  records.  These fields only apply to  host ,  hostssl , and  hostnossl  records.  In addition to the method-specific options listed below, there is one method-independent authentication option  clientcert , which can be specified in any  hostssl  record.  This is a dangerous privilege and should not be used carelessly; it is best to do most of your work as a role that is not a superuser.  This course of action might be advisable if one has carelessly added a bunch of junk in  template1 . xml 4   https://ssl.  Ideally, database servers could work together seamlessly.  pg_stat_ssl One row per connection (regular and replication), showing information about SSL used on this connection.  See  pg_stat_ssl  for details.   pg_stat_ssl  View Column Type Description pid integer Process ID of a backend or WAL sender process ssl boolean True if SSL is used on this connection version text Version of SSL in use, or NULL if SSL is not in use on this connection cipher text N.  This field is truncated if the DN field is longer than  NAMEDATALEN  (64 characters in a standard build) The  pg_stat_ssl  view will contain one row per backend or WAL sender process, showing statistics about SSL usage on this connection.  For improved compatibility with JDBC connection URIs, instances of parameter  ssl=true  are translated into  sslmode=require .  sslmode This option determines whether or with what priority a secure SSL TCP/IP connection will be negotiated with the server.  sslmode  is ignored for Unix domain socket communication.  requiressl This option is deprecated in favor of the  sslmode  setting.  If set to 1, an SSL connection to the server is required (this is equivalent to  sslmode   require ).  If set to 0 (default), libpq will negotiate the connection type with the server (equivalent to  sslmode prefer ).  sslcompression If set to 1 (default), data sent over SSL connections will be compressed.  sslcert This parameter specifies the file name of the client SSL certificate, replacing the default ~/.  sslkey This parameter specifies the location for the secret key used for the client certificate.  sslrootcert This parameter specifies the name of a file containing SSL certificate authority (CA) certificate(s).  sslcrl This parameter specifies the file name of the SSL certificate revocation list (CRL). ) gsslib GSS library to use for GSSAPI authentication.  PQsslInUse Returns true (1) if the connection uses SSL, false (0) if not.  int PQsslInUse(const PGconn *conn); 752 . libpq - C Library PQsslAttribute Returns SSL-related information about the connection.  const char *PQsslAttribute(const PGconn *conn, const char  *attribute_name); The list of available attributes varies depending on the SSL library being used, and the type of connection.  PQsslAttributeNames Return an array of SSL attribute names available.  const char * const * PQsslAttributeNames(const PGconn *conn); PQsslStruct Return a pointer to an SSL-implementation-specific object describing the connection.  void *PQsslStruct(const PGconn *conn, const char *struct_name); The struct(s) available depend on the SSL implementation in use. h> #include <openssl/ssl.      SSL *ssl;     dbconn = PQconnectdb(. libpq - C Library     ssl = PQsslStruct(dbconn, "OpenSSL");     if (ssl)     {         /* use OpenSSL functions to access ssl */     } This structure can be used to verify encryption levels, check server certificates, and more.  PQgetssl  Returns the SSL structure used in the connection, or null if SSL is not in use.  void *PQgetssl(const PGconn *conn); This function is equivalent to  PQsslStruct(conn, "OpenSSL") .  To check if a connection uses SSL, call  PQsslInUse instead, and for more details about the connection, use  PQsslAttribute .  Ł   PGSSLMODE  behaves the same as the  sslmode  connection parameter.  Ł   PGREQUIRESSL  behaves the same as the  requiressl  connection parameter.  Ł   PGSSLCOMPRESSION  behaves the same as the  sslcompression  connection parameter.  Ł   PGSSLCERT  behaves the same as the  sslcert  connection parameter.  Ł   PGSSLKEY  behaves the same as the  sslkey  connection parameter.  Ł   PGSSLROOTCERT  behaves the same as the  sslrootcert  connection parameter.  Ł   PGSSLCRL  behaves the same as the  sslcrl  connection parameter.  Ł   PGGSSLIB  behaves the same as the  gsslib  connection parameter. com description:port=5439 description:dbname=mydb description:user=mydb_user description:sslmode=require might be queried with the following LDAP URL: ldap://ldap.  By default, this file is named openssl. cnf  and is located in the directory reported by  openssl version -d .  If the parameter  sslmode  is set to  verify-ca , libpq will verify that the server is trustworthy by checking the certificate chain up to the root certificate stored on the client.  If  sslmode  is set to  verify-full , libpq will  also  verify that the server host name matches the name stored in the server certificate.  The location of the root certificate file and the CRL can be changed by setting the connection parameters  sslrootcert  and  sslcrl  or the environment variables  PGSSLROOTCERT  and PGSSLCRL .  Note For backwards compatibility with earlier versions of PostgreSQL, if a root CA file exists, the behavior of  sslmode = require  will be the same as that of  verify-ca , meaning the server certificate is validated against the CA.  The location of the certificate and key files can be overridden by the connection parameters  sslcert  and  sslkey  or the environment variables  PGSSLCERT  and  PGSSLKEY .  Protection Provided in Different Modes The different values for the  sslmode  parameter provide different levels of protection.  In libpq, secure connections can be ensured by setting the  sslmode  parameter to  verify-full  or verify-ca , and providing the system with a root certificate to verify against. 1  illustrates the risks the different sslmode  values protect against, and what statement they make about security and overhead.  SSL Mode Descriptions sslmode Eavesdropping protection MITM protection Statement disable No No I don't care about security, and I don't want to pay the overhead of encryption.  The default value for  sslmode  is  prefer .  SSL Library Initialization If your application initializes  libssl  and/or  libcrypto  libraries and libpq is built with SSL support, you should call  PQinitOpenSSL  to tell libpq that the  libssl  and/or  libcrypto libraries have been initialized b.  void PQinitOpenSSL(int do_ssl, int do_crypto); When  do_ssl  is non-zero, libpq will initialize the OpenSSL library before first opening a database connection.  void PQinitSSL(int do_ssl); This function is equivalent to  PQinitOpenSSL(do_ssl, do_ssl) .  This way, composite types can be mapped into structures almost seamlessly, even though ECPG does not understand the composite type itself.  }  blocks when used under C++, so they should work seamlessly in C++.  It is not terribly difficult for a malicious user to create trojan-horse objects that will compromise later execution of a carelessly-written extension script, allowing that user to acquire superuser privileges.  It is not terribly difficult for a malicious user to create trojan-horse objects that will compromise later execution of a carelessly-written extension script, allowing that user to acquire superuser privileges.  The object owner can, of course,  REVOKE  both default and expressly granted privileges.  Ł All WAL records required for the backup must contain sufficient full-page writes, which requires you to enable  full_page_writes  on the master and not to use a tool like pg_compresslog as archive_command  to remove full-page writes from WAL files.  For example: $  psql "service=myservice sslmode=require" $  psql postgresql://dbmaster:5433/mydb?sslmode=require This way you can also use LDAP for connection parameter lookup as described in  Section 33.  The new connection can re-use connection parameters from the previous connection; not only database name, user, host, and port, but other settings such as  sslmode . dom 6432 => \c service=foo => \c "host=localhost port=5432 dbname=mydb connect_timeout=10  sslmode=disable" => \c -reuse-previous=on sslmode=require    -- changes only  sslmode => \c postgresql://tom@localhost/mydb?application_name=myapp \C [  title .  Note This feature was shamelessly plagiarized from Bash.  Note This feature was shamelessly plagiarized from Bash.  Note This feature was shamelessly plagiarized from Bash.  Note This feature was shamelessly plagiarized from Bash.  Note This feature was shamelessly plagiarized from tcsh.  Unfortunately, rsync needlessly copies files associated with temporary and unlogged tables because these files don't normally exist on standby servers.  This convention allows GiST to support both lossless and lossy index structures.  sslinfo .  This is not a complete solution; extensions that depend on other extensions can still be at risk if installed carelessly.  Ł Add missing SQLSTATE values to a few error reports (Sawada Masahiko) Ł Fix PL/pgSQL to reliably refuse to execute an event trigger function as a plain function (Tom Lane) Ł Fix memory leak in libpq when using  sslmode=verify-full  (Roman Peshkurov.  Ł Change the default value of the  log_directory  server parameter from  pg_log  to  log  (Andreas Karlsson) Ł Add configuration option  ssl_dh_params_file  to specify file name for custom OpenSSL DH parameters (Heikki Linnakangas) This replaces the.  See  ssl_dh_params_file .  The options that affect it are  --with-zlib  and  --with-openssl .  sslinfo The  sslinfo  module provides information about the SSL certificate that the current client provided when connecting to PostgreSQL.  This extension won't build at all unless the installation was configured with  --with-openssl .  Functions Provided ssl_is_used() returns boolean   Returns TRUE if current connection to server uses SSL, and FALSE otherwise.  ssl_version() returns text   Returns the name of the protocol used for the SSL connection (e.  ssl_cipher() returns text   Returns the name of the cipher used for the SSL connection (e.  ssl_client_cert_present() returns boolean   Returns TRUE if current client has presented a valid SSL client certificate to the server, and FALSE otherwise. Additional Supplied Modules ssl_client_serial() returns numeric   Returns serial number of current client certificate.  ssl_client_dn() returns text   Returns the full subject of the current client certificate, converting character data into the current database encoding.  ssl_issuer_dn() returns text   Returns the full issuer name of the current client certificate, converting character data into the current database encoding.  Encoding conversions are handled the same as for  ssl_client_dn .  ssl_client_dn_field(fieldname text) returns text   This function returns the value of the specified field in the certificate subject, or NULL if the field is not present.  ssl_issuer_field(fieldname text) returns text   Same as  ssl_client_dn_field , but for the certificate issuer rather than the certificate subject.  ssl_extension_info() returns setof record   Provide information about extensions of client certificate: extension name, extension value, and if it is a critical extension. ru> E-Mail of Cryptocom OpenSSL development group:  <openssl@cryptocom. Documentation Ł textproc/docbook-sgml Ł textproc/docbook-xml Ł textproc/docbook-xsl Ł textproc/dsssl-docbook-modular Ł textproc/libxslt Ł textproc/fop Ł textproc/opensp To install the required packages with  pkg , use: pkg install docbook-sgml docboo.  ANSI American National Standards Institute 1 API Application Programming Interface 2 ASCII American Standard Code for Information Interchange 3 BKI Backend Interface CA Certificate Authority 4 CIDR Classless Inter-Domain Routing 5 CPAN Comprehensive. org/wiki/Classless_Inter-Domain_Routing 6   https://www. Index ssl_cipher,  2442 ssl_ciphers configuration parameter,  512 ssl_client_cert_present,  2442 ssl_client_dn,  2443 ssl_client_dn_field,  2443 ssl_client_serial,  2443 ssl_crl_file configuration parameter,  512 ssl_dh_params_file configuration para
ssl_cert_file	key , respectively, in the server's data directory, but other names and locations can be specified using the configuration parameters ssl_cert_file  and  ssl_key_file .  SSL Server File Usage File Contents Effect ssl_cert_file  ( $PGDATA/ server.  ssl_cert_file  ( string )  Specifies the name of the file containing the SSL server certificate
ssl_key_file	key , respectively, in the server's data directory, but other names and locations can be specified using the configuration parameters ssl_cert_file  and  ssl_key_file . crt ) server certificate sent to client to indicate server's identity ssl_key_file  ( $PGDATA/ server.  ssl_key_file  ( string )  Specifies the name of the file containing the SSL server private key
tcp_keepalives_idle	 tcp_keepalives_idle  ( integer )  Specifies the number of seconds of inactivity after which TCP should send a keepalive message to the client.  On systems that support the keepalive socket option, setting  tcp_keepalives_idle , tcp_keepalives_interval  and  tcp_keepalives_count  helps the primary promptly notice a broken connection
tcp_keepalives_interval	 tcp_keepalives_interval  ( integer )  Specifies the number of seconds after which a TCP keepalive message that is not acknowledged by the client should be retransmitted.  On systems that support the keepalive socket option, setting  tcp_keepalives_idle , tcp_keepalives_interval  and  tcp_keepalives_count  helps the primary promptly notice a broken connection
tcp_keepalives_count	 tcp_keepalives_count  ( integer )  Specifies the number of TCP keepalives that can be lost before the server's connection to the client is considered dead.  On systems that support the keepalive socket option, setting  tcp_keepalives_idle , tcp_keepalives_interval  and  tcp_keepalives_count  helps the primary promptly notice a broken connection
shared_buffers	 As a temporary workaround, you can try starting the server with a smaller-than-normal number of buffers ( shared_buffers ).  In some cases, it may help to lower memory-related configuration parameters, particularly  shared_buffers  and  work_mem .  Linux Huge Pages Using huge pages reduces overhead when using large contiguous chunks of memory, as PostgreSQL does, particularly when using large values of  shared_buffers .  An example of what this file might look like is: # This is a comment log_connections = yes log_destination = 'syslog' search_path = '"$user", public' shared_buffers = 128MB One parameter is specified per line.  Memory shared_buffers  ( integer )  Sets the amount of memory the database server uses for shared memory buffers. Server Configuration If you have a dedicated database server with 1GB or more of RAM, a reasonable starting value for  shared_buffers  is 25% of the memory in your system.  There are some workloads where even larger settings for  shared_buffers  are effective, but because PostgreSQL also relies on the operating system cache, it is unlikely that an allocation of more than 40% of RAM to  shared_buffers  will work better .  Larger settings for shared_buffers  usually require a corresponding increase in  max_wal_size , in order to spread out the process of writing large quantities of new or changed data over a longer period of time.  Often that will result in greatly reduced transaction latency, but there also are some cases, especially with workloads that are bigger than  shared_buffers , but smaller than the OS's page cache, where performance might degrade.  Often that will result in greatly reduced transaction latency, but there also are some cases, especially with workloads that are bigger than  shared_buffers , but smaller than the OS's page cache, where performance might degrade.  The default setting of -1 selects a size equal to 1/32nd (about 3%) of  shared_buffers , but not less than  64kB nor more than the size of one WAL segment, typically  16MB .  Often that will result in greatly reduced transaction latency, but there also are some cases, especially with workloads that are bigger than  shared_buffers , but smaller than the OS's page cache, where performance might degrade.  The meaning of some configuration variables (such as shared_buffers ) is influenced by  block_size .  Short Option Key Short Option Equivalent -B  x shared_buffers =  x -d  x log_min_messages = DEBUG x -e datestyle = euro -fb ,  -fh ,  -fi ,  -fm ,  -fn ,  -fo ,  -fs ,  -ft enable_bitmapscan = off , enable_hashjoin = off , enable_indexscan = off , e.  (If this happens often, it implies that  shared_buffers is too small or the background writer control parameters need adjustment.  This setting will often help to reduce transaction latency, but it also can have an adverse effect on performance; particularly for workloads that are bigger than  shared_buffers , but smaller than the OS's page cache.  Specifying this option is equivalent to setting the shared_buffers  configuration parameter.  You might be able to postpone reconfiguring your kernel by decreasing  shared_buffers  to reduce the shared memory consumption of PostgreSQL, and/or by reducing  max_connections  to reduce the semaphore consumption.  shared_buffers relfilenode oid pg_class
huge_pages	 To enforce the use of huge pages, you can set  huge_pages  to  on  in 494 .  huge_pages  ( enum )  Enables/disables the use of huge memory pages.  With  huge_pages  set to  try , the server will try to use huge pages, but fall back to using normal allocation if that fails
temp_buffers	 temp_buffers  ( integer )  Sets the maximum number of temporary buffers used by each database session.  A session will allocate temporary buffers as needed up to the limit given by  temp_buffers .  The cost of setting a large value in sessions that do not actually need many temporary buffers is only a buffer descriptor, or about 64 bytes, per increment in  temp_buffers 
max_prepared_transactions	 max_prepared_transactions  ( integer )  Sets the maximum number of transactions that can be in the  ﬁpreparedﬂ  state simultaneously (see PREPARE TRANSACTION ).  If you are using prepared transactions, you will probably want  max_prepared_transactions  to be at least as large as  max_connections , so that every session can have a prepared transaction pending.  max_locks_per_transaction  ( integer )  The shared lock table tracks locks on  max_locks_per_transaction  * ( max_connections  + max_prepared_transactions ) objects (e.  max_pred_locks_per_transaction  ( integer )  The shared predicate lock table tracks locks on  max_pred_locks_per_transaction  * ( max_connections  +  max_prepared_transactions ) objects (e. High Availability, Load Balancing, and Replication Ł max_connections Ł max_prepared_transactions Ł max_locks_per_transaction Ł max_worker_processes It is important that the administrator select appropriate settings for  max_standby_archive_delay  and.  You need not consider this at all if your setting of max_prepared_transactions  is 0.  If you have not set up an external transaction manager to track prepared transactions and ensure they get closed out promptly, it is best to keep the prepared-transaction feature disabled by setting  max_prepared_transactions  to zero
work_mem	 Increase  maintenance_work_mem . Full Text Search Note that GIN index build time can often be improved by increasing  maintenance_work_mem , while GiST index build time is not sensitive to that parameter.  Increase  maintenance_work_mem Temporarily increasing the  maintenance_work_mem  configuration variable when loading large amounts of data can lead to improved performance. , larger than normal) values for  maintenance_work_mem  and max_wal_size .  It's still useful to increase  max_wal_size while loading the data, but don't bother increasing  maintenance_work_mem ; rather, you'd do that while manually recreating indexes and foreign keys afterwards.  In some cases, it may help to lower memory-related configuration parameters, particularly  shared_buffers  and  work_mem .  work_mem  ( integer )  Specifies the amount of memory to be used by internal sort operations and hash tables before writing to temporary disk files.  Therefore, the total memory used could be many times the value of  work_mem ; it is necessary to keep this fact in mind when choosing the value.  maintenance_work_mem  ( integer )  Specifies the maximum amount of memory to be used by maintenance operations, such as VACUUM ,  CREATE INDEX , and  ALTER TABLE ADD FOREIGN KEY .  Since only one of these operations can be executed at a time by a database session, and an installation normally doesn't have many of them running concurrently, it's safe to set this value significantly larger than  work_mem .  It may be useful to control for this by separately setting  autovacuum_work_mem .  Setting  maintenance_work_mem  to its default value usually prevents utility command external sorts (e.  autovacuum_work_mem  ( integer )  Specifies the maximum amount of memory to be used by each autovacuum worker process.  It defaults to -1, indicating that the value of  maintenance_work_mem  should be used instead.  This should be taken into account when choosing a value for this setting, as well as when configuring other settings that control resource utilization, such as  work_mem .  Resource limits such as  work_mem  are applied individually to each worker, which means the total utilization may be much higher across all processes than it would normally be for any single process. Server Configuration Short Option Equivalent -S  x work_mem =  x -tpa ,  -tpl ,  -te log_parser_stats = on , log_planner_stats = on , log_executor_stats = on -W  x post_auth_delay =  x 567 .  max_dead_tuples bigint Number of dead tuples that we can store before needing to perform an index vacuum cycle, based on maintenance_work_mem .  It may happen multiple times per vacuum if  maintenance_work_mem  is insufficient to store the number of dead tuples found.  When testing against an already-installed server, ordering differences can also be caused by non-C locale settings or non-default parameter settings, such as custom values of  work_mem  or the planner cost parameters.  Currently, the point at which data begins being written to disk is controlled by the  work_mem  configuration variable.  It is advisable to set  maintenance_work_mem  to a reasonably large value (but not more than the amount of RAM you can dedicate to the  CLUSTER  operation) before clustering.  The planner will consider using hash aggregation for such a query only if the hash table is estimated to fit in  work_mem ; therefore, large values of this parameter discourage use of hash aggregation.  For most index methods, the speed of creating an index is dependent on the setting of maintenance_work_mem .  See the description of the  work_mem  configuration parameter in Section 19.  Because of limited  maintenance_work_mem ,  ambulkdelete  might need to be called more than once when many tuples are to be deleted.  maintenance_work_mem Build time for a GIN index is very sensitive to the  maintenance_work_mem  setting; it doesn't pay to skimp on work memory during index creation
maintenance_work_mem	 Increase  maintenance_work_mem . Full Text Search Note that GIN index build time can often be improved by increasing  maintenance_work_mem , while GiST index build time is not sensitive to that parameter.  Increase  maintenance_work_mem Temporarily increasing the  maintenance_work_mem  configuration variable when loading large amounts of data can lead to improved performance. , larger than normal) values for  maintenance_work_mem  and max_wal_size .  It's still useful to increase  max_wal_size while loading the data, but don't bother increasing  maintenance_work_mem ; rather, you'd do that while manually recreating indexes and foreign keys afterwards.  maintenance_work_mem  ( integer )  Specifies the maximum amount of memory to be used by maintenance operations, such as VACUUM ,  CREATE INDEX , and  ALTER TABLE ADD FOREIGN KEY .  Setting  maintenance_work_mem  to its default value usually prevents utility command external sorts (e.  It defaults to -1, indicating that the value of  maintenance_work_mem  should be used instead.  max_dead_tuples bigint Number of dead tuples that we can store before needing to perform an index vacuum cycle, based on maintenance_work_mem .  It may happen multiple times per vacuum if  maintenance_work_mem  is insufficient to store the number of dead tuples found.  It is advisable to set  maintenance_work_mem  to a reasonably large value (but not more than the amount of RAM you can dedicate to the  CLUSTER  operation) before clustering.  For most index methods, the speed of creating an index is dependent on the setting of maintenance_work_mem .  Because of limited  maintenance_work_mem ,  ambulkdelete  might need to be called more than once when many tuples are to be deleted.  maintenance_work_mem Build time for a GIN index is very sensitive to the  maintenance_work_mem  setting; it doesn't pay to skimp on work memory during index creation
replacement_sort_tuples	 replacement_sort_tuples  ( integer )  When the number of tuples to be sorted is smaller than this number, a sort will produce its first output run using replacement selection rather than quicksort
autovacuum_work_mem	 It may be useful to control for this by separately setting  autovacuum_work_mem .  autovacuum_work_mem  ( integer )  Specifies the maximum amount of memory to be used by each autovacuum worker process
max_stack_depth	 max_stack_depth  ( integer )  Specifies the maximum safe depth of the server's execution stack.  Setting  max_stack_depth  higher than the actual kernel limit will mean that a runaway recursive function can crash an individual backend process.  Insufficient Stack Depth If the  errors  test results in a server crash at the  select infinite_recurse()  command, it means that the platform's limit on process stack size is smaller than the  max_stack_depth  parameter indicates. Regression Tests with the default value of  max_stack_depth ).  If you are unable to do that, an alternative is to reduce the value of  max_stack_depth .  On platforms supporting  getrlimit() , the server should automatically choose a safe value of max_stack_depth ; so unless you've manually overridden this setting, a failure of this kind is a reportable bug
dynamic_shared_memory_type	 Ł dynamic_shared_memory_type  must be set to a value other than  none .  dynamic_shared_memory_type  ( enum )  Specifies the dynamic shared memory implementation that the server should use
temp_file_limit	 Disk temp_file_limit  ( integer )  Specifies the maximum amount of disk space that a process can use for temporary files, such as sort and hash temporary files, or the storage file for a held cursor
max_files_per_process	 If you find this happening, and you do not want to alter the system-wide limit, you can set PostgreSQL's max_files_per_process  configuration parameter to limit the consumption of open files.  Kernel Resource Usage max_files_per_process  ( integer )  Sets the maximum number of simultaneously open files allowed to each server subprocess. Index max_connections configuration parameter,  510 max_files_per_process configuration parameter,  517 max_function_args configuration parameter,  562 max_identifier_length configuration parameter,  562 max_index_keys configuration parameter,  562 m
vacuum_cost_delay	 When the accumulated cost reaches a limit (specified by  vacuum_cost_limit ), the process performing the operation will sleep for a short period of time, as specified by  vacuum_cost_delay .  To enable it, set the vacuum_cost_delay  variable to a nonzero value.  vacuum_cost_delay  ( integer )  The length of time, in milliseconds, that the process will sleep when the cost limit has been exceeded.  Note that on many systems, the effective resolution of sleep delays is 10 milliseconds; setting  vacuum_cost_delay  to a value that is not a multiple of 10 might have the same results as setting it to the next higher multiple of 10. Server Configuration When using cost-based vacuuming, appropriate values for  vacuum_cost_delay  are usually quite small, perhaps 10 or 20 milliseconds.  To avoid uselessly long delays in such cases, the actual delay is calculated as vacuum_cost_delay  *  accumulated_balance  /  vacuum_cost_limit  with a maximum of  vacuum_cost_delay  * 4.  autovacuum_vacuum_cost_delay  ( integer )  Specifies the cost delay value that will be used in automatic  VACUUM  operations.  If -1 is specified, the regular  vacuum_cost_delay  value will be used.  However, any workers processing tables whose per-table  autovacuum_vacuum_cost_delay  or  autovacuum_vacuum_cost_limit storage parameters have been set are not considered in the balancing algorithm. CREATE TABLE autovacuum_vacuum_cost_delay ,  toast. autovacuum_vacuum_cost_delay ( integer ) Per-table value for  autovacuum_vacuum_cost_delay  parameter
vacuum_cost_page_hit	 vacuum_cost_page_hit  ( integer )  The estimated cost for vacuuming a buffer found in the shared buffer cache
vacuum_cost_page_miss	 vacuum_cost_page_miss  ( integer )  The estimated cost for vacuuming a buffer that has to be read from disk
vacuum_cost_page_dirty	 vacuum_cost_page_dirty  ( integer )  The estimated cost charged when vacuum modifies a block that was previously clean
vacuum_cost_limit	 When the accumulated cost reaches a limit (specified by  vacuum_cost_limit ), the process performing the operation will sleep for a short period of time, as specified by  vacuum_cost_delay .  vacuum_cost_limit  ( integer )  The accumulated cost that will cause the vacuuming process to sleep.  To avoid uselessly long delays in such cases, the actual delay is calculated as vacuum_cost_delay  *  accumulated_balance  /  vacuum_cost_limit  with a maximum of  vacuum_cost_delay  * 4.  autovacuum_vacuum_cost_limit  ( integer )  Specifies the cost limit value that will be used in automatic  VACUUM  operations.  If -1 is specified (which is the default), the regular  vacuum_cost_limit  value will be used.  However, any workers processing tables whose per-table  autovacuum_vacuum_cost_delay  or  autovacuum_vacuum_cost_limit storage parameters have been set are not considered in the balancing algorithm.  autovacuum_vacuum_cost_limit ,  toast. autovacuum_vacuum_cost_limit ( integer ) Per-table value for  autovacuum_vacuum_cost_limit  parameter
bgwriter_delay	 bgwriter_delay  ( integer )  Specifies the delay between activity rounds for the background writer.  It then sleeps for  bgwriter_delay  milliseconds, and repeats.  When there are no dirty buffers in the buffer pool, though, it goes into a longer sleep regardless of  bgwriter_delay .  Note that on many systems, the effective resolution of sleep delays is 10 milliseconds; setting  bgwriter_delay  to a value that is not a multiple of 10 might have the same results as setting it to the next higher multiple of 10
bgwriter_lru_maxpages	 bgwriter_lru_maxpages  ( integer )  In each round, no more than this many buffers will be written by the background writer.  (However, no more than  bgwriter_lru_maxpages  buffers will be written per round.  Smaller values of  bgwriter_lru_maxpages  and  bgwriter_lru_multiplier  reduce the extra I/O load caused by the background writer, but make it more likely that server processes will have to issue writes for themselves, delaying interactive queries.  Ł Make the maximum value of  bgwriter_lru_maxpages  effectively unlimited (Jim Nasby) E
bgwriter_lru_multiplier	 bgwriter_lru_multiplier  ( floating point )  The number of dirty buffers written in each round is based on the number of new buffers that have been needed by server processes during recent rounds.  The average recent need is multiplied by bgwriter_lru_multiplier  to arrive at an estimate of the number of buffers that will be needed during the next round.  Smaller values of  bgwriter_lru_maxpages  and  bgwriter_lru_multiplier  reduce the extra I/O load caused by the background writer, but make it more likely that server processes will have to issue writes for themselves, delaying interactive queries
bgwriter_flush_after	 bgwriter_flush_after  ( integer )  Whenever more than  bgwriter_flush_after  bytes have been written by the background writer, attempt to force the OS to issue these writes to the underlying storage
effective_io_concurrency	 Asynchronous Behavior effective_io_concurrency  ( integer )  Sets the number of concurrent disk I/O operations that PostgreSQL expects can be executed simultaneously.  Currently, the only available parameters are seq_page_cost ,  random_page_cost  and  effective_io_concurrency .  Currently, the only available parameters are seq_page_cost ,  random_page_cost  and  effective_io_concurrency .  Locking Ł Reduce locking required to change table parameters (Simon Riggs, Fabrízio Mello) For example, changing a table's  effective_io_concurrency  setting can now be done with a more lightweight lock
max_worker_processes	 The total number of background workers that can exist at any one time is limited by both  max_worker_processes  and  max_parallel_workers .  If this occurrence is frequent, consider increasing  max_worker_processes and  max_parallel_workers  so that more workers can be run simultaneously or alternatively reducing  max_parallel_workers_per_gather  so that the planner requests fewer worker.  This will happen if any of the following conditions are met: Ł No background workers can be obtained because of the limitation that the total number of background workers cannot exceed  max_worker_processes . , sets) at least ceil((max_connections + autovacuum_max_workers + max_worker_processes + 5) / 16)  plus room for other applications SEMMNS Maximum number of semaphores system-wide ceil((max_connections + autovacuum_max_workers + max_worker_processes .  When using System V semaphores, PostgreSQL uses one semaphore per allowed connection ( max_connections ), allowed autovacuum worker process ( autovacuum_max_workers ) and allowed background process ( max_worker_processes ), in sets of 16.  The maximum number of semaphores in the system is set by  SEMMNS , which consequently must be at least as high as  max_connections  plus autovacuum_max_workers  plus  max_worker_processes , plus one extra for each 16 allowed connections plus workers.  Hence this parameter must be at least  ceil((max_connections + autovacuum_max_workers + max_worker_processes + 5) / 16) . Server Configuration max_worker_processes  ( integer )  Sets the maximum number of background processes that the system can support.  Parallel workers are taken from the pool of processes established by max_worker_processes , limited by  max_parallel_workers .  Also, note that a setting for this value which is higher than max_worker_processes  will have no effect, since parallel workers are taken from the pool of worker processes established by that setting.  Logical replication workers are taken from the pool defined by  max_worker_processes . High Availability, Load Balancing, and Replication Ł max_connections Ł max_prepared_transactions Ł max_locks_per_transaction Ł max_worker_processes It is important that the administrator select appropriate settings for  max_standby_archive_delay  and. Logical Replication max_worker_processes  may need to be adjusted to accommodate for replication workers, at least ( max_logical_replication_workers  +  1 ).  Note that some extensions and parallel queries also take worker slots from  max_worker_processes .  The maximum number of registered background workers is limited by  max_worker_processes .  The actual number of workers chosen by the planner may be less, for example due to the setting of  max_worker_processes 
max_parallel_workers_per_gather	 The number of background workers that the planner will consider using is limited to at most max_parallel_workers_per_gather .  If this occurrence is frequent, consider increasing  max_worker_processes and  max_parallel_workers  so that more workers can be run simultaneously or alternatively reducing  max_parallel_workers_per_gather  so that the planner requests fewer worker.  Ł max_parallel_workers_per_gather  must be set to a value which is greater than zero.  This is a special case of the more general principle that no more workers should be used than the number configured via  max_parallel_workers_per_gather . Parallel Query idea to set  max_parallel_workers_per_gather  to zero in sessions where it is likely, so as to avoid generating query plans that may be suboptimal when run serially.  When changing this value, consider also adjusting  max_parallel_workers  and max_parallel_workers_per_gather .  max_parallel_workers_per_gather  ( integer )  Sets the maximum number of workers that can be started by a single  Gather  or Gather Merge  node.  When increasing or decreasing this value, consider also adjusting max_parallel_workers_per_gather .  Ł Enable parallelism by default by changing the default setting of  max_parallel_workers_per_gather to  2 
max_parallel_workers	 The number of background workers that the planner will consider using is limited to at most max_parallel_workers_per_gather .  The total number of background workers that can exist at any one time is limited by both  max_worker_processes  and  max_parallel_workers .  If this occurrence is frequent, consider increasing  max_worker_processes and  max_parallel_workers  so that more workers can be run simultaneously or alternatively reducing  max_parallel_workers_per_gather  so that the planner requests fewer worker.  Ł max_parallel_workers_per_gather  must be set to a value which is greater than zero.  This is a special case of the more general principle that no more workers should be used than the number configured via  max_parallel_workers_per_gather .  Ł No background workers can be obtained because of the limitation that the total number of background workers launched for purposes of parallel query cannot exceed  max_parallel_workers . Parallel Query idea to set  max_parallel_workers_per_gather  to zero in sessions where it is likely, so as to avoid generating query plans that may be suboptimal when run serially.  When changing this value, consider also adjusting  max_parallel_workers  and max_parallel_workers_per_gather .  max_parallel_workers_per_gather  ( integer )  Sets the maximum number of workers that can be started by a single  Gather  or Gather Merge  node.  Parallel workers are taken from the pool of processes established by max_worker_processes , limited by  max_parallel_workers .  max_parallel_workers  ( integer )  Sets the maximum number of workers that the system can support for parallel queries.  When increasing or decreasing this value, consider also adjusting max_parallel_workers_per_gather .  Ł Enable parallelism by default by changing the default setting of  max_parallel_workers_per_gather to  2 
old_snapshot_threshold	) old_snapshot_threshold  ( integer )  Sets the minimum time that a snapshot can be used without risk of a  snapshot too old  error occurring when using the snapshot.  This does not prevent cleanup of dead rows which have reached the age specified by old_snapshot_threshold .  This setting does not override the behavior of  old_snapshot_threshold  on the primary; a snapshot on the standby which exceeds the primary's age threshold can become invalid, resulting in cancellation of transactions on the standby.  This is because  old_snapshot_threshold  is intended to provide an absolute limit on the time which dead rows can contribute to bloat, which would otherwise be violated because of the configuration of a standby
backend_flush_after	 backend_flush_after  ( integer )  Whenever more than  backend_flush_after  bytes have been written by a single backend, attempt to force the OS to issue these writes to the underlying storage. sample_rate configuration parameter, 2313 avals,  2368 average,  288 avg,  288 B B-tree  (see  index ) backend_flush_after configuration parameter,  520 Background workers,  1267 backslash escapes,  33 backslash_quote configuration parameter,  559 ba
fsync	 1867 pg_test_fsync .  (They can guarantee crash safety more cheaply by doing an  fsync  at the end than by writing WAL.  Ł Turn off  fsync ; there is no need to flush data to disk.  Doing so will limit the amount of dirty data in the kernel's page cache, reducing the likelihood of stalls when an fsync  is issued at the end of a checkpoint, or when the OS writes data back in larger batches in the background.  Doing so will limit the amount of dirty data in the kernel's page cache, reducing the likelihood of stalls when an  fsync is issued at the end of a checkpoint, or when the OS writes data back in larger batches in the background.  fsync  ( boolean )  If this parameter is on, the PostgreSQL server will try to make sure that updates are physically written to disk, by issuing  fsync()  system calls or various equivalent methods (see wal_sync_method ). Server Configuration While turning off  fsync  is often a performance benefit, this can result in unrecoverable data corruption in the event of a power failure or system crash.  Thus it is only advisable to turn off fsync  if you can easily recreate your entire database from external data.  Examples of safe circumstances for turning off  fsync  include the initial loading of a new database cluster from a backup file, using a database cluster for processing a batch of data after which the database will be thrown away and recreated, or f.  High quality hardware alone is not a sufficient justification for turning off  fsync .  For reliable recovery when changing  fsync  off to on, it is necessary to force all modified buffers in the kernel to durable storage.  This can be done while the cluster is shutdown or while  fsync is on by running  initdb --sync-only , running  sync , unmounting the file system, or rebooting the server.  In many situations, turning off  synchronous_commit  for noncritical transactions can provide much of the potential performance benefit of turning off  fsync , without the attendant risks of data corruption.  fsync  can only be set in the  postgresql. ) Unlike  fsync , setting this parameter to  off  does not create any risk of database inconsistency: an operating system or database crash might result in some recent allegedly-committed transactions being lost, but the database state will be just t.  If  fsync  is off then this setting is irrelevant, since WAL file updates will not be forced out at all.  Possible values are: Ł open_datasync  (write WAL files with  open()  option  O_DSYNC ) Ł fdatasync  (call  fdatasync()  at each commit) Ł fsync  (call  fsync()  at each commit) Ł fsync_writethrough  (call  fsync()  at each commit, forcing write-thro.  The risks are similar to turning off fsync , though smaller, and it should be turned off only based on the same circumstances recommended for that parameter.  Also, no delays are performed if fsync  is disabled.  Doing so will limit the amount of dirty data in the kernel's page cache, reducing the likelihood of stalls when an fsync  is issued at the end of the checkpoint, or when the OS writes data back in larger batches in the background.  CheckpointerCommLock Waiting to manage fsync requests.  buffer-checkpoint- sync-start () Probe that fires after dirty buffers have been written to the kernel, and before starting to issue fsync requests.  Alternatively, set wal_sync_method  to  fsync  or  fsync_writethrough , which prevent write caching. Reliability and the Write-Ahead Log Ł On macOS, write caching can be prevented by setting  wal_sync_method  to fsync_writethrough .  You can run the  pg_test_fsync  program to see if you are affected.  Furthermore, when the server is processing many small concurrent transactions, one  fsync  of the log file may suffice to commit many transactions.  Asynchronous commit provides behavior different from setting  fsync  = off.   fsync  is a server-wide setting that will alter the behavior of all transactions.  In many scenarios, asynchronous commit provides most of the performance improvement that could be obtained by turning off  fsync , but without the risk of data corruption.  Otherwise, these pages may be kept in the OS's page cache, inducing a stall when  fsync  is issued at the end of a checkpoint.  No sleep will occur if  fsync  is not enabled, or if fewer than  commit_siblings  other sessions are currently in active transactions; this avoids sleeping when it's unlikely that any other session will commit soon.  The  pg_test_fsync  program can be used to measure the average time in microseconds that a single WAL flush operation takes.  All the options should be the same in terms of reliability, with the exception of fsync_writethrough , which can sometimes force a flush of the disk cache even when other options do not do so.  You can test the speeds of different options using the  pg_test_fsync  program.  Note that this parameter is irrelevant if  fsync  has been turned off.  1867 pg_test_fsync .  -F  interval_seconds --fsync-interval= interval_seconds Specifies how often pg_recvlogical should issue  fsync()  calls to ensure the output file is safely flushed to disk.  Specifying an interval of  0  disables issuing  fsync()  calls altogether, while still reporting progress to the server.  1867 pg_test_fsync .  Examples Starting the Server To start the server, waiting until the server is accepting connections: $   pg_ctl start To start the server using port 5433, and running without  fsync , use: $   pg_ctl -o "-F -p 5433" start Stopping the Server To stop.  To restart using port 5433, disabling  fsync upon restart: $   pg_ctl -o "-F -p 5433" restart Showing the Server Status Here is sample status output from pg_ctl: $   pg_ctl status pg_ctl: server is running (PID: 13718) /usr/local/pgsql/bin/postgres . pg_test_fsync pg_test_fsync  Š  determine fastest  wal_sync_method  for PostgreSQL Synopsis pg_test_fsync  [ option . ] Description pg_test_fsync is intended to give you a reasonable idea of what the fastest  wal_sync_method  is on your specific system, as well as supplying diagnostic information in the event of an identified I/O problem.  However, differences shown by pg_test_fsync might not make any significant difference in real database throughput, especially since many database servers are not speed-limited by their write-ahead logs.  pg_test_fsync reports average file sync operation time in microseconds for each  wal_sync_method , which can also be used to inform efforts to optimize the value of commit_delay .  Options pg_test_fsync accepts the following command-line options: -f --filename Specifies the file name to write test data in. ) The default is pg_test_fsync.  -V --version Print the pg_test_fsync version and exit.  -? --help Show help about pg_test_fsync command line arguments, and exit.  -F Disables  fsync  calls for improved performance, at the risk of data corruption in the event of a system crash.  Specifying this option is equivalent to disabling the  fsync  configuration parameter.  Ł Ensure that SLRU directories are properly fsync'd during checkpoints (Thomas Munro) This prevents possible data loss in a subsequent operating system crash.  Ł Make pg_test_fsync use binary I/O mode on Windows (Michael Paquier) Previously it wrote the test file in text mode, which is not an accurate reflection of PostgreSQL's actual usage.  Ł Ensure that  fsync()  is applied only to files that are opened read/write (Andres Freund, Michael Paquier) Some code paths tried to do this after opening a file read-only, but on some platforms that causes ﬁbad file descriptorﬂ  or similar errors.  Ł Detect file read errors during pg_basebackup (Jeevan Chalke) Ł In pg_basebackup, don't fsync output files until the end of backup (Michael Paquier) 2247 . Release Notes The previous coding could result in timeout failures if fsync was slow.  Changes Ł By default, panic instead of retrying after  fsync()  failure, to avoid possible data corruption (Craig Ringer, Thomas Munro) Some popular operating systems discard kernel data buffers when unable to write them out, reporting this as  fsyn.  If we reissue the  fsync()  request it will succeed, but in fact the data has been lost, so continuing risks database corruption.   pg_stat_activity Ł Add  pg_stat_activity  reporting of low-level wait states (Michael Paquier, Robert Haas, Rushabh Lathia) This change enables reporting of numerous low-level wait conditions, including latch waits, file reads/writes/fsyncs, client.  Reliability Ł After creating or unlinking files, perform an fsync on their parent directory (Michael Paquier) This reduces the risk of data loss after a power failure. Release Notes Ł Issue  fsync()  on the output files generated by pg_dump and pg_dumpall (Michael Paquier) This provides more security that the output is safely stored on disk before the program exits.  Ł Be more careful about fsync'ing in all required places in pg_basebackup and pg_receivewal (Michael Paquier) Ł Add pg_basebackup option  --no-sync  to disable fsync (Michael Paquier) Ł Improve pg_basebackup's handling of which directories to skip (
synchronous_commit	 synchronous_commit Modes .  Ł Turn off  synchronous_commit ; there might be no need to force WAL writes to disk on every commit.  In many situations, turning off  synchronous_commit  for noncritical transactions can provide much of the potential performance benefit of turning off  fsync , without the attendant risks of data corruption.  synchronous_commit  ( enum )  Specifies how much WAL processing must complete before the database server returns a  ﬁsuccessﬂ  indication to the client.  So, turning  synchronous_commit  off can be a useful alternative when performance is more important than exact certainty about the durability of a transaction.  If  synchronous_standby_names  is non-empty,  synchronous_commit  also controls whether transaction commits will wait for their WAL records to be processed on the standby server(s).  For example, to make a single multistatement transaction commit asynchronously when the default is the opposite, issue  SET LOCAL synchronous_commit TO OFF  within the transaction. 1  summarizes the capabilities of the  synchronous_commit  settings.  synchronous_commit Modes synchronous_commit setting local durable commit standby durable commit after PG crash standby durable commit after OS crash standby query consistency remote_apply Ł Ł Ł Ł on Ł Ł Ł   remote_write Ł Ł     local Ł       off    .  Even when synchronous replication is enabled, individual transactions can be configured not to wait for replication by setting the  synchronous_commit  parameter to  local  or  off .  This level of protection is referred to as 2-safe replication in computer science theory, and group-1-safe (group-safe and 1-safe) when  synchronous_commit  is set to remote_write .  synchronous_commit  must also be set to  on , but since this is the default value, typically no change is required.  synchronous_commit  can be set by individual users, so it can be configured in the configuration file, for particular users or databases, or dynamically by applications, in order to control the durability guarantee on a per-transaction basis.  In the case that synchronous_commit  is set to  remote_apply , the standby sends reply messages when the commit record is replayed, making the transaction visible.  Setting  synchronous_commit  to  remote_write  will cause each commit to wait for confirmation that the standby has received the commit record and written it out to its own operating system, but not for the data to be flushed to disk on the standby.  Setting  synchronous_commit  to  remote_apply  will cause each commit to wait until the current synchronous standbys report that they have replayed the transaction, making it visible to user queries.  Planning for High Availability synchronous_standby_names  specifies the number and names of synchronous standbys that transaction commits made when  synchronous_commit  is set to  on ,  remote_apply  or remote_write  will wait for responses from.  If you need to re-create a standby server while transactions are waiting, make sure that the commands pg_start_backup() and pg_stop_backup() are run in a session with  synchronous_commit  =  off , otherwise those requests will wait forever for the s.  Warning Synchronous replication is affected by this setting when  synchronous_commit  is set to  remote_apply ; every  COMMIT  will need to wait to be applied.  This can be used to gauge the delay that  synchronous_commit level  remote_write  incurred while committing if this server was configured as a synchronous standby.  This can be used to gauge the delay that synchronous_commit  level on  incurred while committing if this server was configured as a synchronous standby.  This can be used to gauge the delay that  synchronous_commit level  remote_apply  incurred while committing if this server was configured as a synchronous standby.  The commit mode is controlled by the user-settable parameter  synchronous_commit , which can be changed in any of the ways that a configuration parameter can be set.  The mode used for any one transaction depends on the value of synchronous_commit  when transaction commit begins.  Certain utility commands, for instance  DROP TABLE , are forced to commit synchronously regardless of the setting of  synchronous_commit .  That can lead to a  COMMIT  not immediately being decoded in a directly following pg_logical_slot_get_changes()  when  synchronous_commit  is set to  off .  The allowed options are  slot_name  and  synchronous_commit new_owner The user name of the new owner of the subscription.  synchronous_commit  ( enum ) The value of this parameter overrides the  synchronous_commit  setting.  This means that setting  synchronous_commit  for the subscriber to  off  when the subscription is used for synchronous replication might increase the latency for  COMMIT  on the publisher.  In this scenario, it can be advantageous to set  synchronous_commit  to  local  or higher.  Since pg_receivewal does not apply WAL, you should not allow it to become a synchronous standby when  synchronous_commit  equals  remote_apply .  To avoid this, you should either configure an appropriate value for  synchronous_standby_names , or specify  application_name for pg_receivewal that does not match it, or change the value of  synchronous_commit  to something other than  remote_apply.  subsynccommit text Contains the value of the synchronous_commit setting for the subscription workers
full_page_writes	  pg_control_checkpoint  Columns Column Name Data Type checkpoint_lsn pg_lsn prior_lsn pg_lsn redo_lsn pg_lsn redo_wal_file text timeline_id integer prev_timeline_id integer full_page_writes boolean next_xid text next_oid oid next_multixact_id xid ne.  Ł Turn off  full_page_writes ; there is no need to guard against partial page writes.  If you turn this parameter off, also consider turning off  full_page_writes .  full_page_writes  ( boolean )  When this parameter is on, the PostgreSQL server writes the entire content of each disk page to WAL during the first modification of that page after a checkpoint.  wal_compression  ( boolean )  When this parameter is  on , the PostgreSQL server compresses a full page image written to WAL when  full_page_writes  is on or during a base backup.  However, if you normally run the server with  full_page_writes  disabled, you might notice a drop in performance while the backup runs since  full_page_writes  is effectively forced on during backup mode.  Depending on your system hardware and software, the risk of partial writes might be small enough to ignore, in which case you can significantly reduce the total volume of archived logs by turning off page snapshots using the  full_page_writes  param.  An area for future development is to compress archived WAL data by removing unnecessary page copies even when  full_page_writes  is on. , ZFS), you can turn off this page imaging by turning off the  full_page_writes  parameter.  If  full_page_writes is set (as is the default), there is another factor to consider.  When  full_page_writes  is set and the system is very busy, setting  wal_buffers  higher will help smooth response times during the period immediately following each checkpoint.  Because the entire content of data pages is saved in the log on the first page modification after a checkpoint (assuming  full_page_writes  is not disabled), all pages changed since the checkpoint will be restored to a consistent state.  You will also need to enable  full_page_writes  on the master.  Ł All WAL records required for the backup must contain sufficient full-page writes, which requires you to enable  full_page_writes  on the master and not to use a tool like pg_compresslog as archive_command  to remove full-page writes from WAL files.   full_page_writes  must also be set to  on , but is enabled by default.  Ł Properly handle turning  full_page_writes  on dynamically (Kyotaro Horiguchi) Ł Fix possible crash due to double  free()  during SP-GiST rescan (Andrew Gierth) Ł Prevent mis-linking of src/port and src/common functions on ELF-based BSD platforms, . Index functions and operators,  152 full_page_writes configuration parameter,  523 function,  187 default values for arguments,  991 in the FROM clause,  106 internal,  1001 invocation,  42 mixed notation,  53 named argument,  985 named notation,  53
wal_compression	 wal_compression  ( boolean )  When this parameter is  on , the PostgreSQL server compresses a full page image written to WAL when  full_page_writes  is on or during a base backup
wal_log_hints	 wal_log_hints  ( boolean )  When this parameter is  on , the PostgreSQL server writes the entire content of each disk page to WAL during the first modification of that page after a checkpoint, even for non-critical modifications of so-called hint bi.  pg_rewind requires that the target server either has the  wal_log_hints  option enabled in postgresql.  Ł Avoid repeated marking of dead btree index entries as dead (Masahiko Sawada) While functionally harmless, this led to useless WAL traffic when checksums are enabled or wal_log_hints  is on. Index wal_debug configuration parameter,  565 wal_keep_segments configuration parameter,  527 wal_level configuration parameter,  521 wal_log_hints configuration parameter,  524 wal_receiver_status_interval configuration parameter, 530 wal_receiver_t
wal_buffers	 wal_buffers  ( integer )  The amount of shared memory used for WAL data that has not yet been written to disk.  (If this happens often, it implies that  wal_buffers  is too small.  On such systems one should increase the number of WAL buffers by modifying the  wal_buffers  parameter.  When  full_page_writes  is set and the system is very busy, setting  wal_buffers  higher will help smooth response times during the period immediately following each checkpoint
wal_writer_delay	 (The maximum delay is three times  wal_writer_delay .  wal_writer_delay  ( integer )  Specifies how often the WAL writer flushes WAL.  After flushing WAL it sleeps for wal_writer_delay  milliseconds, unless woken up by an asynchronously committing transaction.  If the last flush happened less than  wal_writer_delay  milliseconds ago and less than  wal_writer_flush_after  bytes of WAL have been produced since, then WAL is only written to the operating system, not flushed to disk.  Note that on many systems, the effective resolution of sleep delays is 10 milliseconds; setting  wal_writer_delay  to a value that is not a multiple of 10 might have the same results as setting it to the next higher multiple of 10.  If the last flush happened less than wal_writer_delay  milliseconds ago and less than  wal_writer_flush_after  bytes of WAL have been produced since, then WAL is only written to the operating system, not flushed to disk.  The duration of the risk window is limited because a background process (the  ﬁWAL writerﬂ ) flushes unwritten WAL records to disk every  wal_writer_delay  milliseconds.  The actual maximum duration of the risk window is three times  wal_writer_delay  because the WAL writer is designed to favor writing whole pages at a time during busy periods
wal_writer_flush_after	 If the last flush happened less than  wal_writer_delay  milliseconds ago and less than  wal_writer_flush_after  bytes of WAL have been produced since, then WAL is only written to the operating system, not flushed to disk.  wal_writer_flush_after  ( integer )  Specifies how often the WAL writer flushes WAL.  If the last flush happened less than wal_writer_delay  milliseconds ago and less than  wal_writer_flush_after  bytes of WAL have been produced since, then WAL is only written to the operating system, not flushed to disk.  If  wal_writer_flush_after  is set to  0  then WAL data is flushed immediately
commit_delay	 commit_delay  ( integer )  commit_delay  adds a time delay, measured in microseconds, before a WAL flush is initiated.  However, it also increases latency by up to  commit_delay microseconds for each WAL flush.  The default  commit_delay  is zero (no delay). 3,  commit_delay  behaved differently and was much less effective: it affected only commits, rather than all WAL flushes, and waited for the entire configured delay even if the WAL flush was completed sooner.  commit_siblings  ( integer )  Minimum number of concurrent open transactions to require before performing the commit_delay  delay.  commit_delay  also sounds very similar to asynchronous commit, but it is actually a synchronous commit method (in fact,  commit_delay  is ignored during an asynchronous commit).  commit_delay  causes a delay just before a transaction flushes WAL to disk, in the hope that a single flush executed by one such transaction can also serve other transactions committing at about the same time.  The  commit_delay  parameter defines for how many microseconds a group commit leader process will sleep after acquiring a lock within  XLogFlush , while group commit followers queue up behind the leader.  Note that on some platforms, the resolution of a sleep request is ten milliseconds, so that any nonzero  commit_delay  setting between 1 and 10000 microseconds would have the same effect.  Since the purpose of  commit_delay  is to allow the cost of each flush operation to be amortized across concurrently committing transactions (potentially at the expense of transaction latency), it is 715 .  The higher that cost is, the more effective  commit_delay  is expected to be in increasing transaction throughput, up to a point.  A value of half of the average time the program reports it takes to flush after a single 8kB write operation is often the most effective setting for  commit_delay , so this value is recommended as the starting point to use when optimizing for a part.  While tuning  commit_delay  is particularly useful when the WAL log is stored on high-latency rotating disks, benefits can be significant even on storage media with very fast sync times, such as solid-state drives or RAID arrays with a battery-backe.  Note that it is quite possible that a setting of  commit_delay  that is too high can increase transaction latency by so much that total transaction throughput suffers.  When  commit_delay  is set to zero (the default), it is still possible for a form of group commit to occur, but each group will consist only of sessions that reach the point where they need to flush their commit records during the window in which th.  At higher client counts a  ﬁgangway effectﬂ  tends to occur, so that the effects of group commit become significant even when  commit_delay  is zero, and thus explicitly setting  commit_delay  tends to help less.  Setting  commit_delay  can only help when (1) there are some concurrently committing transactions, and (2) throughput is limited to some degree by commit rate; but with high rotational latency this setting can be effective in increasing transaction .  pg_test_fsync reports average file sync operation time in microseconds for each  wal_sync_method , which can also be used to inform efforts to optimize the value of commit_delay . Index column reference,  40 col_description,  318 comment about database objects,  318 in SQL,  38 COMMENT,  1394 COMMIT,  1398 COMMIT PREPARED,  1399 commit_delay configuration parameter,  525 commit_siblings configuration parameter,  525 common tab
commit_siblings	 Because the delay is just wasted if no other transactions become ready to commit, a delay is only performed if at least  commit_siblings  other transactions are active when a flush is about to be initiated.  commit_siblings  ( integer )  Minimum number of concurrent open transactions to require before performing the commit_delay  delay.  No sleep will occur if  fsync  is not enabled, or if fewer than  commit_siblings  other sessions are currently in active transactions; this avoids sleeping when it's unlikely that any other session will commit soon.  Higher values of  commit_siblings  should be used in such cases, whereas smaller  commit_siblings  values are often helpful on higher latency media. Index column reference,  40 col_description,  318 comment about database objects,  318 in SQL,  38 COMMENT,  1394 COMMIT,  1398 COMMIT PREPARED,  1399 commit_delay configuration parameter,  525 commit_siblings configuration parameter,  525 common tab
checkpoint_timeout	 This is because loading a large amount of data into PostgreSQL will cause checkpoints to occur more often than the normal checkpoint frequency (specified by the  checkpoint_timeout  configuration variable).  Ł Increase  max_wal_size  and  checkpoint_timeout ; this reduces the frequency of checkpoints, but increases the storage requirements of  /pg_wal .  Checkpoints checkpoint_timeout  ( integer )  Maximum time between automatic WAL checkpoints, in seconds.  No warnings will be generated if  checkpoint_timeout  is less than  checkpoint_warning .  A checkpoint is begun every  checkpoint_timeout  seconds, or if  max_wal_size  is about to be exceeded, whichever comes first.  If no WAL has been written since the previous checkpoint, new checkpoints will be skipped even if  checkpoint_timeout has passed.  Reducing  checkpoint_timeout  and/or  max_wal_size  causes checkpoints to occur more often.  The I/O rate is adjusted so that the checkpoint finishes when the given fraction of  checkpoint_timeout  seconds have elapsed, or before  max_wal_size  is exceeded, whichever is sooner.  A restartpoint is triggered when a checkpoint record is reached if at least  checkpoint_timeout seconds have passed since the last restartpoint, or if WAL size is about to exceed  max_wal_size 
max_wal_size	 Increase  max_wal_size .  Increase  max_wal_size Temporarily increasing the  max_wal_size  configuration variable can also make large data loads faster.  By increasing max_wal_size  temporarily during bulk data loads, the number of checkpoints that are required can be reduced. , larger than normal) values for  maintenance_work_mem  and max_wal_size .  It's still useful to increase  max_wal_size while loading the data, but don't bother increasing  maintenance_work_mem ; rather, you'd do that while manually recreating indexes and foreign keys afterwards.  Ł Increase  max_wal_size  and  checkpoint_timeout ; this reduces the frequency of checkpoints, but increases the storage requirements of  /pg_wal .  Larger settings for shared_buffers  usually require a corresponding increase in  max_wal_size , in order to spread out the process of writing large quantities of new or changed data over a longer period of time.  checkpoint_warning  ( integer )  Write a message to the server log if checkpoints caused by the filling of checkpoint segment files happen closer together than this many seconds (which suggests that  max_wal_size  ought 525 .  max_wal_size  ( integer )  Maximum size to let the WAL grow during automatic checkpoints.  This is a soft limit; WAL size can exceed  max_wal_size  under special circumstances, like under heavy load, a failing  archive_command , or a high  wal_keep_segments  setting.  A checkpoint is begun every  checkpoint_timeout  seconds, or if  max_wal_size  is about to be exceeded, whichever comes first.  Reducing  checkpoint_timeout  and/or  max_wal_size  causes checkpoints to occur more often.  If checkpoints happen closer together than  checkpoint_warning  seconds, a message will be output to the server log recommending increasing  max_wal_size .  Bulk operations such as large  COPY  transfers might cause a number of such warnings to appear if you have not set  max_wal_size  high enough.  The I/O rate is adjusted so that the checkpoint finishes when the given fraction of  checkpoint_timeout  seconds have elapsed, or before  max_wal_size  is exceeded, whichever is sooner. Reliability and the Write-Ahead Log The number of WAL segment files in  pg_wal  directory depends on  min_wal_size , max_wal_size  and the amount of WAL generated in previous checkpoint cycles.  If, due to a short-term peak of log output rate,  max_wal_size is exceeded, the unneeded segment files will be removed until the system gets back under this limit.  Independently of  max_wal_size ,  wal_keep_segments  + 1 most recent WAL files are kept at all times.  A restartpoint is triggered when a checkpoint record is reached if at least  checkpoint_timeout seconds have passed since the last restartpoint, or if WAL size is about to exceed  max_wal_size .  However, because of limitations on when a restartpoint can be performed,  max_wal_size  is often exceeded during recovery, by up to one checkpoint cycle's worth of WAL.  ( max_wal_size  is never a hard limit anyway, so you should always leave plenty of headroom to avoid running out of disk space
min_wal_size	 min_wal_size  ( integer )  As long as WAL disk usage stays below this setting, old WAL files are always recycled for future use at a checkpoint, rather than removed. Reliability and the Write-Ahead Log The number of WAL segment files in  pg_wal  directory depends on  min_wal_size , max_wal_size  and the amount of WAL generated in previous checkpoint cycles.  min_wal_size  puts a minimum on the amount of WAL files recycled for future usage; that much WAL is always recycled for future use, even if the system is idle and the WAL usage estimate suggests that little WAL is needed
checkpoint_completion_target	 checkpoint_completion_target  ( floating point )  Specifies the target of checkpoint completion, as a fraction of total time between checkpoints.  This is because it performs a checkpoint, and the I/O required for the checkpoint will be spread out over a significant period of time, by default half your inter-checkpoint interval (see the configuration parameter checkpoint_completion_target ).  This is because it performs a checkpoint, and the I/O required for the checkpoint will be spread out over a significant period of time, by default half your inter-checkpoint interval (see the configuration parameter checkpoint_completion_target ).  That period is controlled by  checkpoint_completion_target , which is given as a fraction of the checkpoint interval.  On a system that's very close to maximum I/O throughput during normal operation, you might want to increase  checkpoint_completion_target  to reduce the I/O load from checkpoints.  Although checkpoint_completion_target  can be set as high as 1
checkpoint_flush_after	 checkpoint_flush_after  ( integer )  Whenever more than  checkpoint_flush_after  bytes have been written while performing a checkpoint, attempt to force the OS to issue these writes to the underlying storage.  On Linux and POSIX platforms  checkpoint_flush_after  allows to force the OS that pages written by the checkpoint should be flushed to disk after a configurable number of bytes
checkpoint_warning	 checkpoint_warning  ( integer )  Write a message to the server log if checkpoints caused by the filling of checkpoint segment files happen closer together than this many seconds (which suggests that  max_wal_size  ought 525 .  No warnings will be generated if  checkpoint_timeout  is less than  checkpoint_warning .  As a simple sanity check on your checkpointing parameters, you can set the  checkpoint_warning  parameter.  If checkpoints happen closer together than  checkpoint_warning  seconds, a message will be output to the server log recommending increasing  max_wal_size 
archive_mode	 When this parameter is set to true,  pg_stop_backup  will wait for WAL to be archived when archiving is enabled; on the standby, this means that it will wait only when  archive_mode = always .  To prevent incremental WAL logging while loading, disable archiving and streaming replication, by setting  wal_level  to  minimal ,  archive_mode  to  off , and max_wal_senders  to zero.  To do that, set  archive_mode  to  off ,  wal_level  to  minimal , and  max_wal_senders  to zero before loading the dump.  Archiving archive_mode  ( enum )  When  archive_mode  is enabled, completed WAL segments are sent to archive storage by setting  archive_command .  archive_mode  and  archive_command  are separate variables so that  archive_command can be changed without leaving archiving mode.  archive_mode  cannot be enabled when  wal_level  is set to  minimal .  It is ignored unless  archive_mode  was enabled at server start.  If  archive_command  is an empty string (the default) while  archive_mode  is enabled, WAL archiving is temporarily disabled, but the server continues to accumulate WAL segment files in the expectation that a command will soon be provided.  To enable WAL archiving, set the  wal_level  configuration parameter to  replica  or higher, archive_mode  to  on , and specify the shell command to use in the  archive_command  configuration parameter.  On a primary, if  archive_mode  is enabled and the wait_for_archive  parameter is  true ,  pg_stop_backup  does not return until the last segment has been archived.  On a standby,  archive_mode  must be  always  in order for pg_stop_backup  to wait.  If  archive_mode  is enabled,  pg_stop_backup  does not return until the last segment has been archived.  To prepare for low level standalone hot backups, make sure  wal_level  is set to  replica  or higher,  archive_mode  to  on , and set up an archive_command  that performs archiving only when a  switch file  exists.  When the standby has its own WAL archive, set  archive_mode  to  always , and the standby will call the archive command for every WAL segment it receives, whether it's by restoring from the archive or by streaming replication.  If  archive_mode  is set to  on , the archiver is not enabled during recovery or standby mode.  Ensure that  archive_mode ,  archive_command  and  archive_timeout  are set appropriately on the primary (see  Section 25.  Ł Ensure that standby servers will archive WAL timeline history files when  archive_mode  is set to  always  (Grigory Smolkin, Fujii Masao) This oversight could lead to failure of subsequent PITR recovery attempts
archive_timeout	 archive_timeout  ( integer )  The  archive_command  is only invoked for completed WAL segments.  To limit how old unarchived data can be, you can set  archive_timeout  to force the server to switch to a new WAL segment file periodically.  Therefore, it is unwise to use a very short  archive_timeout  Š it will bloat your archive storage.   archive_timeout settings of a minute or so are usually reasonable.  To put a limit on how old unarchived data can be, you can set  archive_timeout  to force the server to switch to a new WAL segment file at least that often.  It is therefore unwise to set a very short archive_timeout  Š it will bloat your archive storage.   archive_timeout  settings of a minute or so are usually reasonable.  The size of the data loss window in file-based log shipping can be limited by use of the  archive_timeout  parameter, which can be set as low as a few seconds.  With streaming replication, archive_timeout  is not required to reduce the data loss window.   archive_timeout can be used to make that delay shorter.  It is possible to use a simple timeout facility, especially if used in conjunction with a known  archive_timeout  setting on the primary.  Ensure that  archive_mode ,  archive_command  and  archive_timeout  are set appropriately on the primary (see  Section 25.  (If WAL archiving is being used and you want to put a lower limit on how often files are archived in order to bound potential data loss, you should adjust the  archive_timeout  parameter rather than the checkpoint parameters.  For this reason, it is not necessary to set  archive_timeout  when using pg_receivewal
max_wal_senders	 To prevent incremental WAL logging while loading, disable archiving and streaming replication, by setting  wal_level  to  minimal ,  archive_mode  to  off , and max_wal_senders  to zero.  To do that, set  archive_mode  to  off ,  wal_level  to  minimal , and  max_wal_senders  to zero before loading the dump.  max_wal_senders  ( integer )  Specifies the maximum number of concurrent connections from standby servers or streaming base backup clients (i.  Also ensure  max_wal_senders  is set to a sufficiently large value in the configuration file of the primary server.  Set the maximum number of concurrent connections from the standby servers (see  max_wal_senders for details).  To use cascading replication, set up the cascading standby so that it can accept replication connections (that is, set  max_wal_senders  and  hot_standby , and configure  host-based authentication ).  And  max_wal_senders  should be set to at least the same as max_replication_slots  plus the number of physical replicas that are connected at the same time. 1 ) and that max_wal_senders  is set sufficiently high to allow an additional connection.  The server must also be configured with  max_wal_senders  set high enough to leave at least one session available for the backup and one for WAL streaming (if used).  To take a backup from the standby, set up the standby so that it can accept replication connections (that is, set max_wal_senders  and  hot_standby , and configure  host-based authentication ).  Therefore, it will use up two connections configured by the  max_wal_senders  parameter.  The server must also be configured with  max_wal_senders  set high enough to leave at least one session available for the stream. Release Notes Ł Reduce configuration changes necessary to perform streaming backup and replication (Magnus Hagander, Dang Minh Huong) Specifically, the defaults were changed for  wal_level ,  max_wal_senders ,  max_replication_slots , and  hot_standb
wal_keep_segments	 This is a soft limit; WAL size can exceed  max_wal_size  under special circumstances, like under heavy load, a failing  archive_command , or a high  wal_keep_segments  setting.  wal_keep_segments  ( integer )  Specifies the minimum number of past log file segments kept in the  pg_wal  directory, in case a standby server needs to fetch them for streaming replication.  If a standby server connected to the sending server falls behind by more than wal_keep_segments  segments, the sending server might remove a WAL segment still needed by the standby, in which case the replication connection will be terminated.  If wal_keep_segments  is zero (the default), the system doesn't keep any extra segments for standby purposes, so the number of old WAL segments available to standby servers is a function of the location of the previous checkpoint and status of WAL a.  You can avoid this by setting  wal_keep_segments  to a value large enough to ensure that WAL segments are not recycled too early, or by configuring a replication slot for the standby.  In lieu of using replication slots, it is possible to prevent the removal of old WAL segments using wal_keep_segments , or by storing the segments in an archive using  archive_command .  Independently of  max_wal_size ,  wal_keep_segments  + 1 most recent WAL files are kept at all times.  Therefore, it is necessary for the  wal_keep_segments  parameter to be set high enough that the log is not removed before the end of the backup. Index wal_debug configuration parameter,  565 wal_keep_segments configuration parameter,  527 wal_level configuration parameter,  521 wal_log_hints configuration parameter,  524 wal_receiver_status_interval configuration parameter, 530 wal_receiver_t
wal_sender_timeout	Server Configuration wal_sender_timeout  ( integer )  Terminate replication connections that are inactive longer than the specified number of milliseconds
max_replication_slots	 max_replication_slots  ( integer )  Specifies the maximum number of replication slots (see  Section 26.  If replication slots will be used, ensure that  max_replication_slots  is set sufficiently high as well.  On the publisher side,  wal_level  must be set to  logical , and  max_replication_slots must be set to at least the number of subscriptions expected to connect, plus some reserve for table synchronization.  And  max_wal_senders  should be set to at least the same as max_replication_slots  plus the number of physical replicas that are connected at the same time.  The subscriber also requires the  max_replication_slots  to be set.  Before you can use logical decoding, you must set  wal_level  to  logical  and  max_replication_slots to at least 1. Release Notes Ł Reduce configuration changes necessary to perform streaming backup and replication (Magnus Hagander, Dang Minh Huong) Specifically, the defaults were changed for  wal_level ,  max_wal_senders ,  max_replication_slots , and  hot_standb
track_commit_timestamp	 They only provide useful data when  track_commit_timestamp  configuration option is enabled and only for transactions that were committed after it was enabled.  track_commit_timestamp  ( boolean )  Record commit time of transactions
vacuum_defer_cleanup_age	 vacuum_defer_cleanup_age  ( integer )  Specifies the number of transactions by which  VACUUM  and HOT updates will defer cleanup of dead row versions.  Similarly,  hot_standby_feedback  and  vacuum_defer_cleanup_age  provide protection against relevant rows being removed by vacuum, but the former provides no protection during any time period when the standby is not connected, and the latter often n.  Another option is to increase  vacuum_defer_cleanup_age  on the primary server, so that dead rows will not be cleaned up as quickly as they normally would be.  However it is difficult to guarantee any specific execution- time window with this approach, since  vacuum_defer_cleanup_age  is measured in transactions executed on the primary server.  On the primary, parameters  wal_level  and  vacuum_defer_cleanup_age  can be used.   vacuum_defer_cleanup_age  has no effect as long as the server remains in standby mode, though it will become relevant if the standby becomes primary.  Ł Ensure that pg_upgrade runs with  vacuum_defer_cleanup_age  set to zero in the target cluster (Bruce Momjian) If the target cluster's configuration has been modified to set  vacuum_defer_cleanup_age to a nonzero value, that prevented freezing of t
hot_standby	6, this parameter also allowed the values  archive  and  hot_standby .  You should also consider setting  hot_standby_feedback  on standby server(s) as an alternative to using this parameter.  hot_standby  ( boolean )  Specifies whether or not you can connect and run queries during recovery, as described in Section 26.  hot_standby_feedback  ( boolean )  Specifies whether or not a hot standby will send feedback to the primary or upstream standby about queries currently executing on the standby.  Similarly,  hot_standby_feedback  and  vacuum_defer_cleanup_age  provide protection against relevant rows being removed by vacuum, but the former provides no protection during any time period when the standby is not connected, and the latter often n.  To use cascading replication, set up the cascading standby so that it can accept replication connections (that is, set  max_wal_senders  and  hot_standby , and configure  host-based authentication ).  User's Overview When the  hot_standby  parameter is set to true on a standby server, it will begin accepting connections once the recovery has brought the system to a consistent state.  The first option is to set the parameter  hot_standby_feedback , which prevents  VACUUM  from removing recently-dead rows and so cleanup conflicts do not occur.  If standby servers connect and disconnect frequently, you might want to make adjustments to handle the period when  hot_standby_feedback  feedback is not being provided.  Administrator's Overview If  hot_standby  is  on  in  postgresql.  On the standby, parameters  hot_standby ,  max_standby_archive_delay  and max_standby_streaming_delay  can be used.  If  hot_standby  is not enabled, a setting of pause  will act the same as  shutdown .   hot_standby_feedback  will be delayed by use of this feature which could lead to bloat on the master; use both together with care. , when the client connected to this WAL sender backend_xmin xid This standby's  xmin  horizon reported by hot_standby_feedback .  To take a backup from the standby, set up the standby so that it can accept replication connections (that is, set max_wal_senders  and  hot_standby , and configure  host-based authentication ).  If a slot's name is provided via  slot_name , it will be updated as replication progresses so that the server knows which WAL segments, and if  hot_standby_feedback  is on which transactions, are still needed by the standby
max_standby_archive_delay	Server Configuration max_standby_archive_delay  ( integer )  When Hot Standby is active, this parameter determines how long the standby server should wait before canceling standby queries that conflict with about-to-be-applied WAL entries, as describ.   max_standby_archive_delay  applies when WAL data is being read from WAL archive (and is therefore not current).  Note that  max_standby_archive_delay  is not the same as the maximum length of time a query can run before cancellation; rather it is the maximum total time allowed to apply any one WAL segment's data.  So the cancel mechanism has parameters,  max_standby_archive_delay  and  max_standby_streaming_delay , that define the maximum allowed delay in WAL application.  Once the delay specified by  max_standby_archive_delay  or max_standby_streaming_delay  has been exceeded, conflicting queries will be canceled. High Availability, Load Balancing, and Replication finite value for  max_standby_archive_delay  or  max_standby_streaming_delay  can be considered similar to setting  statement_timeout .  For example, consider increasing  max_standby_archive_delay  so that queries are not rapidly canceled by conflicts in WAL archive files during disconnected periods. High Availability, Load Balancing, and Replication Ł max_connections Ł max_prepared_transactions Ł max_locks_per_transaction Ł max_worker_processes It is important that the administrator select appropriate settings for  max_standby_archive_delay  and.  max_standby_archive_delay  and  max_standby_streaming_delay  have no effect if set on the primary.  On the standby, parameters  hot_standby ,  max_standby_archive_delay  and max_standby_streaming_delay  can be used
max_standby_streaming_delay	 max_standby_streaming_delay  ( integer )  When Hot Standby is active, this parameter determines how long the standby server should wait before canceling standby queries that conflict with about-to-be-applied WAL entries, as described in  Section 26.   max_standby_streaming_delay  applies when WAL data is being received via streaming replication.  Note that  max_standby_streaming_delay  is not the same as the maximum length of time a query can run before cancellation; rather it is the maximum total time allowed to apply WAL data once it has been received from the primary server.  So the cancel mechanism has parameters,  max_standby_archive_delay  and  max_standby_streaming_delay , that define the maximum allowed delay in WAL application.  Once the delay specified by  max_standby_archive_delay  or max_standby_streaming_delay  has been exceeded, conflicting queries will be canceled. High Availability, Load Balancing, and Replication finite value for  max_standby_archive_delay  or  max_standby_streaming_delay  can be considered similar to setting  statement_timeout .  You should also consider increasing max_standby_streaming_delay  to avoid rapid cancellations by newly-arrived streaming WAL entries after reconnection.  This will allow more time for queries to execute before they are canceled on the standby, without having to set a high max_standby_streaming_delay .  This action occurs immediately, whatever the setting of max_standby_streaming_delay .  max_standby_archive_delay  and  max_standby_streaming_delay  have no effect if set on the primary.  On the standby, parameters  hot_standby ,  max_standby_archive_delay  and max_standby_streaming_delay  can be used
wal_receiver_status_interval	 wal_receiver_status_interval  ( integer )  Specifies the minimum frequency for the WAL receiver process on the standby to send information about replication progress to the primary or upstream standby, where it can be seen using the    pg_stat_repli.  Feedback messages will not be sent more frequently than once per wal_receiver_status_interval .  Note that  wal_receiver_timeout ,  wal_receiver_status_interval  and wal_retrieve_retry_interval  configuration parameters affect the logical replication workers as well.  The standby sends reply messages each time a new batch of WAL data is written to disk, unless  wal_receiver_status_interval  is set to zero on the standby. Index wal_debug configuration parameter,  565 wal_keep_segments configuration parameter,  527 wal_level configuration parameter,  521 wal_log_hints configuration parameter,  524 wal_receiver_status_interval configuration parameter, 530 wal_receiver_t
hot_standby_feedback	 You should also consider setting  hot_standby_feedback  on standby server(s) as an alternative to using this parameter.  hot_standby_feedback  ( boolean )  Specifies whether or not a hot standby will send feedback to the primary or upstream standby about queries currently executing on the standby.  Similarly,  hot_standby_feedback  and  vacuum_defer_cleanup_age  provide protection against relevant rows being removed by vacuum, but the former provides no protection during any time period when the standby is not connected, and the latter often n.  The first option is to set the parameter  hot_standby_feedback , which prevents  VACUUM  from removing recently-dead rows and so cleanup conflicts do not occur.  If standby servers connect and disconnect frequently, you might want to make adjustments to handle the period when  hot_standby_feedback  feedback is not being provided.   hot_standby_feedback  will be delayed by use of this feature which could lead to bloat on the master; use both together with care. , when the client connected to this WAL sender backend_xmin xid This standby's  xmin  horizon reported by hot_standby_feedback .  If a slot's name is provided via  slot_name , it will be updated as replication progresses so that the server knows which WAL segments, and if  hot_standby_feedback  is on which transactions, are still needed by the standby
wal_receiver_timeout	Server Configuration wal_receiver_timeout  ( integer )  Terminate replication connections that are inactive longer than the specified number of milliseconds.  Note that  wal_receiver_timeout ,  wal_receiver_status_interval  and wal_retrieve_retry_interval  configuration parameters affect the logical replication workers as well. Release Notes Erroneous logic prevented  wal_receiver_timeout  from working in logical replication deployments
wal_retrieve_retry_interval	 wal_retrieve_retry_interval  ( integer )  Specify how long the standby server should wait when WAL data is not available from any sources (streaming replication, local  pg_wal  or WAL archive) before retrying to retrieve WAL data.  Note that  wal_receiver_timeout ,  wal_receiver_status_interval  and wal_retrieve_retry_interval  configuration parameters affect the logical replication workers as well
max_logical_replication_workers	 max_logical_replication_workers  ( int )  Specifies maximum number of logical replication workers.  The synchronization workers are taken from the pool defined by max_logical_replication_workers .   max_logical_replication_workers  must be set to at least the number of subscriptions, again plus some reserve for the table synchronization. Logical Replication max_worker_processes  may need to be adjusted to accommodate for replication workers, at least ( max_logical_replication_workers  +  1 )
max_sync_workers_per_subscription	 max_sync_workers_per_subscription  ( integer )  Maximum number of synchronization workers per subscription
enable_bitmapscan	 enable_bitmapscan  ( boolean )    Enables or disables the query planner's use of bitmap-scan plan types.  Short Option Key Short Option Equivalent -B  x shared_buffers =  x -d  x log_min_messages = DEBUG x -e datestyle = euro -fb ,  -fh ,  -fi ,  -fm ,  -fn ,  -fo ,  -fs ,  -ft enable_bitmapscan = off , enable_hashjoin = off , enable_indexscan = off , e
enable_hashagg	 enable_hashagg  ( boolean )  Enables or disables the query planner's use of hashed aggregation plan types. Index enable_gathermerge configuration parameter,  532 enable_hashagg configuration parameter,  532 enable_hashjoin configuration parameter,  532 enable_indexonlyscan configuration parameter,  532 enable_indexscan configuration parameter,  532 enable
enable_hashjoin	 enable_hashjoin  ( boolean )  Enables or disables the query planner's use of hash-join plan types.  Short Option Key Short Option Equivalent -B  x shared_buffers =  x -d  x log_min_messages = DEBUG x -e datestyle = euro -fb ,  -fh ,  -fi ,  -fm ,  -fn ,  -fo ,  -fs ,  -ft enable_bitmapscan = off , enable_hashjoin = off , enable_indexscan = off , e. Index enable_gathermerge configuration parameter,  532 enable_hashagg configuration parameter,  532 enable_hashjoin configuration parameter,  532 enable_indexonlyscan configuration parameter,  532 enable_indexscan configuration parameter,  532 enable
enable_indexscan	 enable_indexscan  ( boolean )    Enables or disables the query planner's use of index-scan plan types.  Short Option Key Short Option Equivalent -B  x shared_buffers =  x -d  x log_min_messages = DEBUG x -e datestyle = euro -fb ,  -fh ,  -fi ,  -fm ,  -fn ,  -fo ,  -fs ,  -ft enable_bitmapscan = off , enable_hashjoin = off , enable_indexscan = off , e.  For example, if for some reason you want to disable index scans (hint: not a good idea) anytime you connect, you can use: ALTER ROLE myname SET enable_indexscan TO off; This will save the setting (but not set it immediately).  In subsequent connections by this role it will appear as though  SET enable_indexscan TO off  had been executed just before the session started.  For example, changing parameters such as  enable_seqscan  or enable_indexscan  could cause plan changes that would affect the results of tests that use EXPLAIN .  Examples To disable index scans by default in the database  test : ALTER DATABASE test SET enable_indexscan TO off; Compatibility The  ALTER DATABASE  statement is a PostgreSQL extension. Index enable_gathermerge configuration parameter,  532 enable_hashagg configuration parameter,  532 enable_hashjoin configuration parameter,  532 enable_indexonlyscan configuration parameter,  532 enable_indexscan configuration parameter,  532 enable
enable_indexonlyscan	 enable_indexonlyscan  ( boolean )  Enables or disables the query planner's use of index-only-scan plan types (see  Section 11. Index enable_gathermerge configuration parameter,  532 enable_hashagg configuration parameter,  532 enable_hashjoin configuration parameter,  532 enable_indexonlyscan configuration parameter,  532 enable_indexscan configuration parameter,  532 enable
enable_material	 enable_material  ( boolean )  Enables or disables the query planner's use of materialization
enable_mergejoin	 enable_mergejoin  ( boolean )  Enables or disables the query planner's use of merge-join plan types
enable_nestloop	 For instance, turning off sequential scans ( enable_seqscan ) and nested-loop joins ( enable_nestloop ), which are the most basic plans, will force the system to use a different plan.  enable_nestloop  ( boolean )  Enables or disables the query planner's use of nested-loop join plans
enable_seqscan	 For instance, turning off sequential scans ( enable_seqscan ) and nested-loop joins ( enable_nestloop ), which are the most basic plans, will force the system to use a different plan.  But if we force an index scan to be used, we see: SET enable_seqscan TO off; 428 .  enable_seqscan  ( boolean )    Enables or disables the query planner's use of sequential scan plan types.  For example, changing parameters such as  enable_seqscan  or enable_indexscan  could cause plan changes that would affect the results of tests that use EXPLAIN 
enable_sort	) For example, if we're unconvinced that sequential-scan-and-sort is the best way to deal with table  onek  in the previous example, we could try SET enable_sort = off; EXPLAIN SELECT * FROM tenk1 t1, onek t2 WHERE t1.  enable_sort  ( boolean )  Enables or disables the query planner's use of explicit sort steps.  This method is often faster than the index scan method, but if the disk space requirement is intolerable, you can disable this choice by temporarily setting  enable_sort  to  off 
enable_tidscan	 enable_tidscan  ( boolean )  Enables or disables the query planner's use of TID scan plan types
seq_page_cost	 Traditional practice is to measure the costs in units of disk page fetches; that is, seq_page_cost  is conventionally set to  1.  The estimated cost is computed as (disk pages read *  seq_page_cost ) + (rows scanned *  cpu_tuple_cost ).  By default,  seq_page_cost  is 1.  By default, these cost variables are based on the cost of sequential page fetches; that is,  seq_page_cost  is conventionally set to  1.  seq_page_cost  ( floating point )  Sets the planner's estimate of the cost of a disk page fetch that is part of a series of sequential fetches.  Reducing this value relative to  seq_page_cost  will cause the system to prefer index scans; raising it will make index scans look relatively more expensive.  Tip Although the system will let you set  random_page_cost  to less than seq_page_cost , it is not physically sensible to do so.  Currently, the only available parameters are seq_page_cost ,  random_page_cost  and  effective_io_concurrency .  Setting either value for a particular tablespace will override the planner's usual estimate of the cost of reading pages from tables in that tablespace, as established by the configuration parameters of the same name (see  seq_page_cost ,  random_pa.  Currently, the only available parameters are seq_page_cost ,  random_page_cost  and  effective_io_concurrency .  Setting either value for a particular tablespace will override the planner's usual estimate of the cost of reading pages from tables in that tablespace, as established by the configuration parameters of the same name (see  seq_page_cost ,  random_pa. c : a sequential disk block fetch has cost  seq_page_cost , a nonsequential fetch has cost  random_page_cost , and the cost of processing one index row should usually be taken as  cpu_index_tuple_cost .  A generic estimator might do this: /*  * Our generic assumption is that the index pages will be read  * sequentially, so they cost seq_page_cost each, not  random_page_cost. startup; *indexTotalCost = seq_page_cost * numIndexPages +     (cpu_index_tuple_cost + index_qual_cost. permissive configuration parameter,  2435 sequence,  278 and serial type,  129 sequential scan,  532 seq_page_cost configuration parameter,  533 serial,  129 serial2,  129 serial4,  129 serial8,  129 serializable,  410 Serializable Snapshot Isolation
random_page_cost	 It may be helpful to encourage the use of index scans by reducing  random_page_cost  and/or increasing  cpu_tuple_cost .  random_page_cost  ( floating point )  Sets the planner's estimate of the cost of a non-sequentially-fetched disk page.  If you believe a 90% cache rate is an incorrect assumption for your workload, you can increase random_page_cost to better reflect the true cost of random storage reads.  Correspondingly, if your data is likely to be completely in cache, such as when the database is smaller than the total server memory, decreasing random_page_cost can be appropriate. , solid-state drives, might also be better modeled with a lower value for random_page_cost, e.  Tip Although the system will let you set  random_page_cost  to less than seq_page_cost , it is not physically sensible to do so.  Currently, the only available parameters are seq_page_cost ,  random_page_cost  and  effective_io_concurrency .  Currently, the only available parameters are seq_page_cost ,  random_page_cost  and  effective_io_concurrency . c : a sequential disk block fetch has cost  seq_page_cost , a nonsequential fetch has cost  random_page_cost , and the cost of processing one index row should usually be taken as  cpu_index_tuple_cost .  A generic estimator might do this: /*  * Our generic assumption is that the index pages will be read  * sequentially, so they cost seq_page_cost each, not  random_page_cost. Index use in PL/pgSQL,  1121 quote_literal,  199 in PL/Perl,  1188 use in PL/pgSQL,  1121 quote_nullable,  200 in PL/Perl,  1188 use in PL/pgSQL,  1121 R radians,  191 radius,  246 RADIUS,  583 RAISE in PL/pgSQL,  1143 random,  193 random_page_cost c
cpu_tuple_cost	 It may be helpful to encourage the use of index scans by reducing  random_page_cost  and/or increasing  cpu_tuple_cost .  The estimated cost is computed as (disk pages read *  seq_page_cost ) + (rows scanned *  cpu_tuple_cost ). 0 and  cpu_tuple_cost  is 0. Server Configuration cpu_tuple_cost  ( floating point )  Sets the planner's estimate of the cost of processing each row during a query
cpu_index_tuple_cost	 cpu_index_tuple_cost  ( floating point )  Sets the planner's estimate of the cost of processing each index entry during an index scan. c : a sequential disk block fetch has cost  seq_page_cost , a nonsequential fetch has cost  random_page_cost , and the cost of processing one index row should usually be taken as  cpu_index_tuple_cost . startup; *indexTotalCost = seq_page_cost * numIndexPages +     (cpu_index_tuple_cost + index_qual_cost
cpu_operator_cost	 However, the scan will still have to visit all 10000 rows, so the cost hasn't decreased; in fact it has gone up a bit (by 10000 *  cpu_operator_cost , to be exact) to reflect the extra CPU time spent checking the  WHERE  condition.  cpu_operator_cost  ( floating point )  Sets the planner's estimate of the cost of processing each operator or function executed during a query.  execution_cost A positive number giving the estimated execution cost for the function, in units of cpu_operator_cost . oid Implementation language or call interface of this function procost float4 Estimated execution cost (in units of cpu_operator_cost ); if proretset , this is cost per row returned prorows float4 Estimated number of result rows (zero if not proretse.  In addition, an appropriate multiple of cpu_operator_cost  should be charged for any comparison operators invoked during index processing (especially evaluation of the indexquals themselves)
parallel_tuple_cost	 Parallel Plan Tips If a query that is expected to do so does not produce a parallel plan, you can try reducing parallel_setup_cost  or  parallel_tuple_cost .  parallel_tuple_cost  ( floating point )  Sets the planner's estimate of the cost of transferring one tuple from a parallel worker process to another process
parallel_setup_cost	 Parallel Plan Tips If a query that is expected to do so does not produce a parallel plan, you can try reducing parallel_setup_cost  or  parallel_tuple_cost .  parallel_setup_cost  ( floating point )  Sets the planner's estimate of the cost of launching parallel worker processes
min_parallel_table_scan_size	 min_parallel_table_scan_size  ( integer )  Sets the minimum amount of table data that must be scanned in order for a parallel scan to be considered.  Ł Add  min_parallel_table_scan_size  and  min_parallel_index_scan_size  server parameters to control parallel queries (Amit Kapila, Robert Haas) These replace  min_parallel_relation_size , which was found to be too generic
min_parallel_index_scan_size	 min_parallel_index_scan_size  ( integer )  Sets the minimum amount of index data that must be scanned in order for a parallel scan to be considered.  Ł Add  min_parallel_table_scan_size  and  min_parallel_index_scan_size  server parameters to control parallel queries (Amit Kapila, Robert Haas) These replace  min_parallel_relation_size , which was found to be too generic
effective_cache_size	 effective_cache_size  ( integer )  Sets the planner's assumption about the effective size of the disk cache that is available to a single query.  With  OFF  it is disabled, with  ON  it is enabled, and with  AUTO  it is initially disabled, but turned on on-the-fly once the index size reaches  effective_cache_size .  By default, a GiST index build switches to the buffering method when the index size reaches effective_cache_size 
geqo	 (The switch-over threshold is set by the  geqo_threshold  run-time parameter.  For example, env PGOPTIONS="-c geqo=off -c statement_timeout=5min" psql Other clients and libraries might provide their own mechanisms, via the shell or otherwise, that allow the user to alter session settings without direct use of SQL commands.  geqo  ( boolean )      Enables or disables genetic query optimization.  It is usually best not to turn it off in production; the  geqo_threshold  variable provides more granular control of GEQO. Server Configuration geqo_threshold  ( integer )  Use genetic query optimization to plan queries with at least this many  FROM  items involved.  geqo_effort  ( integer )  Controls the trade-off between planning time and query plan quality in GEQO.  geqo_effort  doesn't actually do anything directly; it is only used to compute the default values for the other variables that influence GEQO behavior (described below).  geqo_pool_size  ( integer )  Controls the pool size used by GEQO, that is the number of individuals in the genetic population.  If it is set to zero (the default setting) then a suitable value is chosen based on  geqo_effort  and the number of tables in the query.  geqo_generations  ( integer )  Controls the number of generations used by GEQO, that is the number of iterations of the algorithm.  If it is set to zero (the default setting) then a suitable value is chosen based on  geqo_pool_size .  geqo_selection_bias  ( floating point )  Controls the selection bias used by GEQO.  geqo_seed  ( floating point )  Controls the initial value of the random number generator used by GEQO to select random paths through the join order search space.  Setting this value to  geqo_threshold  or more may trigger use of the GEQO planner, resulting in non-optimal plans.  Setting this value to  geqo_threshold  or more may trigger use of the GEQO planner, resulting in non-optimal plans. Managing Databases to issue  SET geqo TO off .  To make this setting the default within a particular database, you can execute the command: ALTER DATABASE mydb SET geqo TO off; This will save the setting (but not set it immediately).  In subsequent connections to this database it will appear as though  SET geqo TO off;  had been executed just before the session started.  For example, setting this to  -c geqo=off  sets the session's value of the  geqo  parameter to  off .  (Equivalent to  SET geqo TO . SHOW SHOW DateStyle;  DateStyle -----------  ISO, MDY (1 row) Show the current setting of the parameter  geqo : SHOW geqo;  geqo ------  on (1 row) Show all settings: SHOW ALL;             name         | setting |                description          .  In order to determine a reasonable (not necessarily optimal) query plan in a reasonable amount of time, PostgreSQL uses a Genetic Query Optimizer  (see  Chapter 59 ) when the number of joins exceeds a threshold (see geqo_threshold ).  If the query uses fewer than  geqo_threshold  relations, a near-exhaustive search is conducted to find the best join sequence.  When  geqo_threshold  is exceeded, the join sequences considered are determined by heuristics, as described in  Chapter 59 .  To avoid surprising changes of the selected plan, each run of the GEQO algorithm restarts its random number generator with the current  geqo_seed  parameter setting.  As long as  geqo_seed  and the other GEQO parameters are kept fixed, the same plan will be generated for a given query (and other planner inputs such as statistics).  To experiment with different search paths, try changing  geqo_seed .  In file  src/backend/optimizer/geqo/geqo_main
geqo_threshold	 (The switch-over threshold is set by the  geqo_threshold  run-time parameter.  It is usually best not to turn it off in production; the  geqo_threshold  variable provides more granular control of GEQO. Server Configuration geqo_threshold  ( integer )  Use genetic query optimization to plan queries with at least this many  FROM  items involved.  Setting this value to  geqo_threshold  or more may trigger use of the GEQO planner, resulting in non-optimal plans.  Setting this value to  geqo_threshold  or more may trigger use of the GEQO planner, resulting in non-optimal plans.  In order to determine a reasonable (not necessarily optimal) query plan in a reasonable amount of time, PostgreSQL uses a Genetic Query Optimizer  (see  Chapter 59 ) when the number of joins exceeds a threshold (see geqo_threshold ).  If the query uses fewer than  geqo_threshold  relations, a near-exhaustive search is conducted to find the best join sequence.  When  geqo_threshold  is exceeded, the join sequences considered are determined by heuristics, as described in  Chapter 59 
geqo_effort	 geqo_effort  ( integer )  Controls the trade-off between planning time and query plan quality in GEQO.  geqo_effort  doesn't actually do anything directly; it is only used to compute the default values for the other variables that influence GEQO behavior (described below).  If it is set to zero (the default setting) then a suitable value is chosen based on  geqo_effort  and the number of tables in the query
geqo_pool_size	 geqo_pool_size  ( integer )  Controls the pool size used by GEQO, that is the number of individuals in the genetic population.  If it is set to zero (the default setting) then a suitable value is chosen based on  geqo_pool_size 
geqo_generations	 geqo_generations  ( integer )  Controls the number of generations used by GEQO, that is the number of iterations of the algorithm
geqo_selection_bias	 geqo_selection_bias  ( floating point )  Controls the selection bias used by GEQO
geqo_seed	 geqo_seed  ( floating point )  Controls the initial value of the random number generator used by GEQO to select random paths through the join order search space.  To avoid surprising changes of the selected plan, each run of the GEQO algorithm restarts its random number generator with the current  geqo_seed  parameter setting.  As long as  geqo_seed  and the other GEQO parameters are kept fixed, the same plan will be generated for a given query (and other planner inputs such as statistics).  To experiment with different search paths, try changing  geqo_seed 
default_statistics_target	2 ), running  ANALYZE  manually, increasing the value of the  default_statistics_target  configuration parameter, and increasing the amount of statistics collected for specific columns using  ALTER TABLE SET STATISTICS .  Other Planner Options default_statistics_target  ( integer )  Sets the default statistics target for table columns without a column-specific target set via  ALTER TABLE SET STATISTICS . Routine Database Maintenance Tasks See  ALTER TABLE SET STATISTICS , or change the database-wide default using the default_statistics_target  configuration parameter.  The target can be set in the range 0 to 10000; alternatively, set it to -1 to revert to using the system default statistics target ( default_statistics_target ).  The extent of analysis can be controlled by adjusting the  default_statistics_target  configuration variable, or on a column-by-column basis by setting the per-column statistics target with  ALTER TABLE . ) The maximum number of entries in the array fields can be controlled on a column-by-column basis using the  ALTER TABLE SET STATISTICS  command, or globally by setting the default_statistics_target  run-time parameter. Index changing,  67 default_statistics_target configuration parameter,  535 default_tablespace configuration parameter,  551 default_text_search_config configuration parameter, 556 default_transaction_deferrable configuration parameter,  552 default_
constraint_exclusion	 Ensure that the  constraint_exclusion  configuration parameter is not disabled in postgresql.  Ensure that the  constraint_exclusion  configuration parameter is not disabled in postgresql.  As an example: SET constraint_exclusion = on; SELECT count(*) FROM measurement WHERE logdate >= DATE  '2008-01-01'; Without constraint exclusion, the above query would scan each of the partitions of the  measurement table.  You can use the  EXPLAIN  command to show the difference between a plan with constraint_exclusion  on and a plan with it off.  A typical unoptimized plan for this type of table setup is: SET constraint_exclusion = off; EXPLAIN SELECT count(*) FROM measurement WHERE logdate >= DATE  '2008-01-01';                                           QUERY PLAN --------------------------.  When we enable constraint exclusion, we get a significantly cheaper plan that will deliver the same answer: SET constraint_exclusion = on; EXPLAIN SELECT count(*) FROM measurement WHERE logdate >= DATE  '2008-01-01';                                 .  The default (and recommended) setting of  constraint_exclusion  is actually neither  on  nor  off , but an intermediate setting called  partition , which causes the technique to be applied only to queries that are likely to be working on partitioned.  constraint_exclusion  ( enum )    Controls the query planner's use of table constraints to optimize queries.  The allowed values of constraint_exclusion  are  on  (examine constraints for all tables),  off  (never examine 535 
cursor_tuple_fraction	 cursor_tuple_fraction  ( floating point )  Sets the planner's estimate of the fraction of a cursor's rows that will be retrieved
from_collapse_limit	 The planner tries to avoid getting stuck in huge join search problems by not collapsing a subquery if more than  from_collapse_limit   FROM  items would result in the parent query.  from_collapse_limit  and  join_collapse_limit  are similarly named because they do almost the same thing: one controls when the planner will  ﬁflatten outﬂ  subqueries, and the other controls when it will flatten out explicit joins.  Typically you would either set  join_collapse_limit equal to  from_collapse_limit  (so that explicit joins and subqueries act similarly) or set join_collapse_limit  to 1 (if you want to control join order with explicit joins).  from_collapse_limit  ( integer )  The planner will merge sub-queries into upper queries if the resulting  FROM  list would have no more than this many items.  By default, this variable is set the same as  from_collapse_limit , which is appropriate for most uses
join_collapse_limit	 To force the planner to follow the join order laid out by explicit  JOIN s, set the  join_collapse_limit run-time parameter to 1. ; With  join_collapse_limit  = 1, this forces the planner to join A to B before joining them to other tables, but doesn't constrain its choices otherwise.  from_collapse_limit  and  join_collapse_limit  are similarly named because they do almost the same thing: one controls when the planner will  ﬁflatten outﬂ  subqueries, and the other controls when it will flatten out explicit joins.  Typically you would either set  join_collapse_limit equal to  from_collapse_limit  (so that explicit joins and subqueries act similarly) or set join_collapse_limit  to 1 (if you want to control join order with explicit joins).  join_collapse_limit  ( integer )  The planner will rewrite explicit  JOIN  constructs (except  FULL JOIN s) into lists of  FROM  items whenever a list of no more than this many items would result. Index right,  102 self,  12 join_collapse_limit configuration parameter,  536 JSON,  157 functions and operators,  269 JSONB,  157 jsonb containment,  159 existence,  159 indexes on,  161 jsonb_agg,  289 jsonb_array_elements,  273 jsonb_array_element
force_parallel_mode	Server Configuration force_parallel_mode  ( enum )  Allows the use of parallel queries for testing purposes even in cases where no performance benefit is expected.  The allowed values of  force_parallel_mode  are  off  (use parallel mode only when it is expected to improve performance),  on  (force parallel query for all queries for which it is thought to be safe), and  regress  (like  on , but with additional .  Practical use of these functions seems to pose little hazard unless  force_parallel_mode  is turned on
logging_collector	 logging_collector  must be enabled to generate CSV-format log output.  logging_collector  ( boolean )  This parameter enables the  logging collector , which is a background process that captures log messages sent to stderr and redirects them into log files.  log_directory  ( string )  When  logging_collector  is enabled, this parameter determines the directory in which log files will be created.  log_filename  ( string )  When  logging_collector  is enabled, this parameter sets the file names of the created log files.  log_file_mode  ( integer )  On Unix systems this parameter sets the permissions for log files when  logging_collector is enabled.  log_rotation_age  ( integer )  When  logging_collector  is enabled, this parameter determines the maximum lifetime of an individual log file.  log_rotation_size  ( integer )  When  logging_collector  is enabled, this parameter determines the maximum size of an individual log file.  log_truncate_on_rotation  ( boolean )  When  logging_collector  is enabled, this parameter will cause PostgreSQL to truncate (overwrite), rather than append to, any existing log file of the same name.  There is a built-in log rotation facility, which you can use by setting the configuration parameter logging_collector  to  true  in  postgresql
log_file_mode	 log_file_mode  ( integer )  On Unix systems this parameter sets the permissions for log files when  logging_collector is enabled
log_truncate_on_rotation	 log_truncate_on_rotation  ( boolean )  When  logging_collector  is enabled, this parameter will cause PostgreSQL to truncate (overwrite), rather than append to, any existing log file of the same name. %a ,  log_truncate_on_rotation  to  on , and log_rotation_age  to  1440 .  %H%M ,  log_truncate_on_rotation  to  on ,  log_rotation_age  to  60 , and log_rotation_size  to  1000000 .  Set  log_truncate_on_rotation  to  on  so that old log data isn't mixed with the new in the same file
log_rotation_age	 log_rotation_age  ( integer )  When  logging_collector  is enabled, this parameter determines the maximum lifetime of an individual log file. %a ,  log_truncate_on_rotation  to  on , and log_rotation_age  to  1440 .  %H%M ,  log_truncate_on_rotation  to  on ,  log_rotation_age  to  60 , and log_rotation_size  to  1000000 .  Set  log_filename  and  log_rotation_age  to provide a consistent, predictable naming scheme for your log files
log_rotation_size	 log_rotation_size  ( integer )  When  logging_collector  is enabled, this parameter determines the maximum size of an individual log file.  %H%M ,  log_truncate_on_rotation  to  on ,  log_rotation_age  to  60 , and log_rotation_size  to  1000000 .  Set  log_rotation_size  to 0 to disable size-based log rotation, as it makes the log file name difficult to predict
syslog_sequence_numbers	 syslog_sequence_numbers  ( boolean )  When logging to syslog and this is on (the default), then each message will be prefixed by an increasing sequence number (such as  [2] )
syslog_split_messages	 syslog_split_messages  ( boolean )  When logging to syslog is enabled, this parameter determines how messages are delivered to syslog
log_min_duration_statement	Server Configuration log_min_duration_statement  ( integer )  Causes the duration of each completed statement to be logged if the statement ran for at least the specified number of milliseconds.  Note The difference between setting this option and setting  log_min_duration_statement  to zero is that exceeding  log_min_duration_statement  forces the text of the query to be logged, but this option doesn't.  Thus, if  log_duration  is  on  and log_min_duration_statement  has a positive value, all durations are logged but the query text is included only for statements exceeding the threshold
debug_print_parse	 debug_print_parse  ( boolean )  debug_print_rewritten  ( boolean )  debug_print_plan  ( boolean )  These parameters enable various debugging output to be emitted.  debug_pretty_print  ( boolean )  When set,  debug_pretty_print  indents the messages produced by debug_print_parse ,  debug_print_rewritten , or  debug_print_plan .  These query trees can be shown in the server log if you set the configuration parameters  debug_print_parse ,  debug_print_rewritten , or  debug_print_plan 
debug_print_rewritten	 debug_print_parse  ( boolean )  debug_print_rewritten  ( boolean )  debug_print_plan  ( boolean )  These parameters enable various debugging output to be emitted.  debug_pretty_print  ( boolean )  When set,  debug_pretty_print  indents the messages produced by debug_print_parse ,  debug_print_rewritten , or  debug_print_plan .  These query trees can be shown in the server log if you set the configuration parameters  debug_print_parse ,  debug_print_rewritten , or  debug_print_plan 
debug_print_plan	 debug_print_parse  ( boolean )  debug_print_rewritten  ( boolean )  debug_print_plan  ( boolean )  These parameters enable various debugging output to be emitted.  debug_pretty_print  ( boolean )  When set,  debug_pretty_print  indents the messages produced by debug_print_parse ,  debug_print_rewritten , or  debug_print_plan .  These query trees can be shown in the server log if you set the configuration parameters  debug_print_parse ,  debug_print_rewritten , or  debug_print_plan 
debug_pretty_print	 debug_pretty_print  ( boolean )  When set,  debug_pretty_print  indents the messages produced by debug_print_parse ,  debug_print_rewritten , or  debug_print_plan 
log_checkpoints	 log_checkpoints  ( boolean )  Causes checkpoints and restartpoints to be logged in the server log
log_connections	 An example of what this file might look like is: # This is a comment log_connections = yes log_destination = 'syslog' search_path = '"$user", public' shared_buffers = 128MB One parameter is specified per line.  For example, postgres -c log_connections=yes -c log_destination='syslog' Settings provided in this way override those set via  postgresql.  log_connections  ( boolean )  Causes each attempted connection to the server to be logged, as well as successful completion of client authentication.  The log output provides information similar to log_connections , plus the duration of the session
log_disconnections	 log_disconnections  ( boolean )  Causes session terminations to be logged
log_duration	Server Configuration log_duration  ( boolean )  Causes the duration of every completed statement to be logged.  Thus, if  log_duration  is  on  and log_min_duration_statement  has a positive value, all durations are logged but the query text is included only for statements exceeding the threshold
log_hostname	 log_hostname  ( boolean )  By default, connection log messages only show the IP address of the connecting host.  Also, you may wish to enable the configuration parameter  log_hostname  to see the client's host name instead of the IP address in the log.  This field will only be non- null for IP connections, and only when  log_hostname  is enabled.  This field will only be non- null for IP connections, and only when  log_hostname  is enabled
log_line_prefix	 If you are not using syslog, it is recommended that you log the PID or session ID using log_line_prefix  so that you can link the statement message to the later duration message using the process ID or session ID.  It can also be included in regular log entries via the  log_line_prefix  parameter.  log_line_prefix  ( string )  This is a  printf -style string that is output at the beginning of each log line. ' ||        to_hex(pid) FROM pg_stat_activity; Tip If you set a nonempty value for  log_line_prefix , you should usually make its last character be a space, to provide visual separation from the rest of the log line. Server Configuration log_line_prefix = '%m [%p] %q%u@%d/%a ' log_lock_waits  ( boolean )  Controls whether a log message is produced when a session waits longer than  deadlock_timeout to acquire a lock.  Logging Ł Change the default value of  log_line_prefix  to include current timestamp (with milliseconds) and the process ID in each line of postmaster log output (Christoph Berg) The previous default was an empty prefix
log_lock_waits	Server Configuration log_line_prefix = '%m [%p] %q%u@%d/%a ' log_lock_waits  ( boolean )  Controls whether a log message is produced when a session waits longer than  deadlock_timeout to acquire a lock.  When  log_lock_waits  is set, this parameter also determines the length of time to wait before a log message is issued about the lock wait
log_replication_commands	 log_replication_commands  ( boolean )  Causes each replication command to be logged in the server log.  Replication commands are logged in the server log when log_replication_commands  is enabled
log_temp_files	 log_temp_files  ( integer )  Controls logging of temporary file names and sizes. , sorting or hashing), and regardless of the  log_temp_files setting.  All temporary files are counted, regardless of why the temporary file was created, and regardless of the  log_temp_files setting
log_timezone	 You cannot set the configuration parameters  TimeZone  or  log_timezone  to a time zone abbreviation, but you can use abbreviations in date/time input values and with the  AT TIME ZONE  operator.  (Note that if there are any time-zone-dependent  % -escapes, the computation is done in the zone specified by  log_timezone .  log_timezone  ( string )  Sets the time zone used for timestamps written in the server log
cluster_name	 cluster_name  ( string )  Sets the cluster name that appears in the process title for all server processes in this cluster.  Only printable ASCII characters may be used in the  cluster_name  value. ) If  cluster_name  has been configured the cluster name will also be shown in  ps  output: $ psql -c 'SHOW cluster_name'  cluster_name --------------  server1 (1 row) $ ps aux|grep server1 postgres   27093  0
update_process_title	 update_process_title  ( boolean )  Enables updating of the process title every time a new SQL command is received by the server.  If you have turned off  update_process_title  then the activity indicator is not updated; the process title is set only once when a new process is launched
track_activities	 track_activities  ( boolean )  Enables the collection of information on the currently executing command of each session, along with the time when that command began execution. ) The parameter  track_activities  enables monitoring of the current command being executed by any server process.  However, current-query information collected by  track_activities  is always up-to-date.  Ł disabled : This state is reported if  track_activities  is disabled in this backend.  Ł Mark some timeout and statistics-tracking GUC variables as  PGDLLIMPORT , to allow extensions to access them on Windows (Pascal Legrand) This applies to  idle_in_transaction_session_timeout ,  lock_timeout , statement_timeout ,  track_activities ,
track_counts	 track_counts  ( boolean )  Enables collection of statistics on database activity.  This is on by default; however,  track_counts  must also be enabled for autovacuum to work.  These checks use the statistics collection facility; therefore, autovacuum cannot be used unless  track_counts  is set to  true .  The parameter  track_counts  controls whether statistics are collected about table and index accesses.  If this is undesirable, you can set parameter  track_counts  to false via  PGOPTIONS  or the  ALTER USER  command
track_io_timing	 track_io_timing  ( boolean )  Enables timing of database I/O calls.  The parameter  track_io_timing  enables monitoring of block read and write times
track_functions	Server Configuration track_functions  ( enum )  Enables tracking of function call counts and time used.  The parameter  track_functions  enables tracking of usage of user-defined functions.  The  track_functions  parameter controls exactly which functions are tracked
track_activity_query_size	 track_activity_query_size  ( integer )  Specifies the number of bytes reserved to track the currently executing command for each active session, for the  pg_stat_activity . Monitoring Database Activity Column Type Description text is truncated at 1024 bytes; this value can be changed via the parameter track_activity_query_size 
stats_temp_directory	 stats_temp_directory  ( string )  Sets the directory to store temporary statistics data in.  If  stats_temp_directory  is set and is under the data directory then the contents of that directory can also be omitted.  These files are stored in the directory named by the  stats_temp_directory  parameter, pg_stat_tmp  by default.  For better performance,  stats_temp_directory  can be pointed at a RAM-based file system, decreasing physical I/O requirements
log_parser_stats	 Statistics Monitoring log_statement_stats  ( boolean )  log_parser_stats  ( boolean )  log_planner_stats  ( boolean )  log_executor_stats  ( boolean )  For each query, output performance statistics of the respective module to the server log. Server Configuration Short Option Equivalent -S  x work_mem =  x -tpa ,  -tpl ,  -te log_parser_stats = on , log_planner_stats = on , log_executor_stats = on -W  x post_auth_delay =  x 567 
log_planner_stats	 Statistics Monitoring log_statement_stats  ( boolean )  log_parser_stats  ( boolean )  log_planner_stats  ( boolean )  log_executor_stats  ( boolean )  For each query, output performance statistics of the respective module to the server log. Server Configuration Short Option Equivalent -S  x work_mem =  x -tpa ,  -tpl ,  -te log_parser_stats = on , log_planner_stats = on , log_executor_stats = on -W  x post_auth_delay =  x 567 
log_executor_stats	 Statistics Monitoring log_statement_stats  ( boolean )  log_parser_stats  ( boolean )  log_planner_stats  ( boolean )  log_executor_stats  ( boolean )  For each query, output performance statistics of the respective module to the server log. Server Configuration Short Option Equivalent -S  x work_mem =  x -tpa ,  -tpl ,  -te log_parser_stats = on , log_planner_stats = on , log_executor_stats = on -W  x post_auth_delay =  x 567 
log_statement_stats	 An example: SELECT set_config('log_statement_stats', 'off', false);  set_config ------------  off (1 row) 9.  Statistics Monitoring log_statement_stats  ( boolean )  log_parser_stats  ( boolean )  log_planner_stats  ( boolean )  log_executor_stats  ( boolean )  For each query, output performance statistics of the respective module to the server log.  log_statement_stats  reports total statement statistics, while the others report per-module statistics.   log_statement_stats  cannot be enabled together with any of the per-module options
autovacuum	 Note that if the autovacuum daemon is enabled, it might run  ANALYZE  automatically; see  Section 24. , sets) at least ceil((max_connections + autovacuum_max_workers + max_worker_processes + 5) / 16)  plus room for other applications SEMMNS Maximum number of semaphores system-wide ceil((max_connections + autovacuum_max_workers + max_worker_processes .  When using System V semaphores, PostgreSQL uses one semaphore per allowed connection ( max_connections ), allowed autovacuum worker process ( autovacuum_max_workers ) and allowed background process ( max_worker_processes ), in sets of 16.  The maximum number of semaphores in the system is set by  SEMMNS , which consequently must be at least as high as  max_connections  plus autovacuum_max_workers  plus  max_worker_processes , plus one extra for each 16 allowed connections plus workers.  Hence this parameter must be at least  ceil((max_connections + autovacuum_max_workers + max_worker_processes + 5) / 16) .  When using POSIX semaphores, the number of semaphores needed is the same as for System V, that is one semaphore per allowed connection ( max_connections ), allowed autovacuum worker process ( autovacuum_max_workers ) and allowed background process (.  Note that when autovacuum runs, up to  autovacuum_max_workers  times this memory may be allocated, so be careful not to set the default value too high.  It may be useful to control for this by separately setting  autovacuum_work_mem .  autovacuum_work_mem  ( integer )  Specifies the maximum amount of memory to be used by each autovacuum worker process.  This parameter is on by default, because the autovacuum daemon needs the collected information.  Automatic Vacuuming These settings control the behavior of the  autovacuum  feature.  autovacuum  ( boolean )  Controls whether the server should run the autovacuum launcher daemon.  This is on by default; however,  track_counts  must also be enabled for autovacuum to work. conf  file or on the server command line; however, autovacuuming can be disabled for individual tables by changing table storage parameters.  Note that even when this parameter is disabled, the system will launch autovacuum processes if necessary to prevent transaction ID wraparound.  log_autovacuum_min_duration  ( integer )  Causes each action executed by autovacuum to be logged if it ran for at least the specified number of milliseconds.  Setting this to zero logs all autovacuum actions.  Minus-one (the default) disables logging autovacuum actions.  In addition, when this parameter is set to any value other than  -1 , a message will be logged if an autovacuum action is skipped due to the existence of a conflicting lock.  Enabling this parameter can be helpful in tracking autovacuum 548 .  autovacuum_max_workers  ( integer )  Specifies the maximum number of autovacuum processes (other than the autovacuum launcher) that may be running at any one time.  autovacuum_naptime  ( integer )  Specifies the minimum delay between autovacuum runs on any given database.  autovacuum_vacuum_threshold  ( integer )  Specifies the minimum number of updated or deleted tuples needed to trigger a  VACUUM  in any one table.  autovacuum_analyze_threshold  ( integer )  Specifies the minimum number of inserted, updated or deleted tuples needed to trigger an ANALYZE  in any one table.  autovacuum_vacuum_scale_factor  ( floating point )  Specifies a fraction of the table size to add to  autovacuum_vacuum_threshold  when deciding whether to trigger a  VACUUM .  autovacuum_analyze_scale_factor  ( floating point )  Specifies a fraction of the table size to add to  autovacuum_analyze_threshold  when deciding whether to trigger an  ANALYZE .  autovacuum_freeze_max_age  ( integer )  Specifies the maximum age (in transactions) that a table's  pg_class .  Note that the system will launch autovacuum processes to prevent wraparound even when autovacuum is otherwise disabled.  autovacuum_multixact_freeze_max_age  ( integer )  Specifies the maximum age (in multixacts) that a table's  pg_class .  Note that the system will launch autovacuum processes to prevent wraparound even when autovacuum is otherwise disabled.  autovacuum_vacuum_cost_delay  ( integer )  Specifies the cost delay value that will be used in automatic  VACUUM  operations.  autovacuum_vacuum_cost_limit  ( integer )  Specifies the cost limit value that will be used in automatic  VACUUM  operations.  Note that the value is distributed proportionally among the running autovacuum workers, if there is more than one, so that the sum of the limits for each worker does not exceed the value of this variable.  Although users can set this value anywhere from zero to two billions,  VACUUM  will silently limit the effective value to 95% of autovacuum_freeze_max_age , so that a periodical manual  VACUUM  has a chance to run before an anti-wraparound autovacuu.  Although users can set this value anywhere from zero to one billion,  VACUUM  will silently limit the effective value to half the value of  autovacuum_freeze_max_age , so that there is not an unreasonably short time between forced autovacuums.  Although users can set this value anywhere from zero to two billions,  VACUUM  will silently limit the effective value to 95% of autovacuum_multixact_freeze_max_age , so that a periodical manual  VACUUM  has a chance to run before an anti-wraparound.  Although users can set this value anywhere from zero to one billion,  VACUUM will silently limit the effective value to half the value of  autovacuum_multixact_freeze_max_age , so that there is not an unreasonably short time between forced autovacuu.  For many installations, it is sufficient to let vacuuming be performed by the  autovacuum daemon , which is described in Section 24.  You might need to adjust the autovacuuming parameters described there to obtain best results for your situation.  Administrators who rely on autovacuuming may still wish to skim this material to help them understand and adjust autovacuuming.  The autovacuum daemon attempts to work this way, and in fact will never issue VACUUM FULL .  Using the autovacuum daemon alleviates this problem, since the daemon schedules vacuuming dynamically in response to update activity.  For those not using autovacuum, a typical approach is to schedule a database-wide  VACUUM  once a day during a low-usage period, supplemented by more frequent vacuuming of heavily-updated tables as necessary.  The autovacuum daemon, if enabled, will automatically issue  ANALYZE  commands whenever the content of a table has changed sufficiently.  Tip The autovacuum daemon does not issue  ANALYZE  commands for foreign tables, since it has no means of determining how often that might be useful.  To ensure that this does not happen, autovacuum is invoked on any table that might contain unfrozen rows with XIDs older than the age specified by the configuration parameter  autovacuum_freeze_max_age .  (This will happen even if autovacuum is disabled. ) This implies that if a table is not otherwise vacuumed, autovacuum will be invoked on it approximately once every  autovacuum_freeze_max_age  minus  vacuum_freeze_min_age  transactions.  However, for static tables (including tables that receive inserts, but no updates or deletes), there is no need to vacuum for space reclamation, so it can be useful to try to maximize the interval between forced autovacuums on very large static tabl.  Obviously one can do this either by increasing autovacuum_freeze_max_age  or decreasing  vacuum_freeze_min_age . 95 * autovacuum_freeze_max_age ; a setting higher than that will be capped to the maximum.  A value higher than  autovacuum_freeze_max_age  wouldn't make sense because an anti- wraparound autovacuum would be triggered at that point anyway, and the 0.  As a rule of thumb,  vacuum_freeze_table_age  should be set to a value somewhat below autovacuum_freeze_max_age , leaving enough gap so that a regularly scheduled  VACUUM  or an autovacuum triggered by normal delete and update activity is run in tha.  Setting it too close could lead to anti-wraparound autovacuums, even though the table was recently vacuumed to reclaim space, whereas lower values lead to more frequent aggressive vacuuming.  The sole disadvantage of increasing  autovacuum_freeze_max_age  (and vacuum_freeze_table_age  along with it) is that the  pg_xact  and  pg_commit_ts subdirectories of the database cluster will take more space, because it must store the commit status. Routine Database Maintenance Tasks autovacuum_freeze_max_age  horizon.  The commit status uses two bits per transaction, so if autovacuum_freeze_max_age  is set to its maximum allowed value of two billion,  pg_xact can be expected to grow to about half a gigabyte and  pg_commit_ts  to about 20GB.  If this is trivial compared to your total database size, setting  autovacuum_freeze_max_age  to its maximum allowed value is recommended.  If no  relfrozenxid -advancing  VACUUM  is issued on the table until autovacuum_freeze_max_age  is reached, an autovacuum will soon be forced for the table.  If for some reason autovacuum fails to clear old XIDs from a table, the system will begin to emit warning messages like this when the database's oldest XIDs reach eleven million transactions from the wraparound point: WARNING:  database "mydb" must .  As a safety device, an aggressive vacuum scan will occur for any table whose multixact-age is greater than  autovacuum_multixact_freeze_max_age .  Both of these kinds of aggressive scans will occur even if autovacuum is nominally disabled.  The Autovacuum Daemon PostgreSQL has an optional but highly recommended feature called  autovacuum , whose purpose is to automate the execution of  VACUUM  and  ANALYZE   commands.  When enabled, autovacuum checks for tables that have had a large number of inserted, updated or deleted tuples.  These checks use the statistics collection facility; therefore, autovacuum cannot be used unless  track_counts  is set to  true .  In the default configuration, autovacuuming is enabled and the related configuration parameters are appropriately set.  The  ﬁautovacuum daemonﬂ  actually consists of multiple processes.  There is a persistent daemon process, called the  autovacuum launcher , which is in charge of starting  autovacuum worker  processes for all databases.  The launcher will distribute the work across time, attempting to start one worker within each database every  autovacuum_naptime  seconds.  (Therefore, if the installation has  N databases, a new worker will be launched every  autovacuum_naptime / N  seconds. ) A maximum of  autovacuum_max_workers  worker processes are allowed to run at the same time.  If there are more than  autovacuum_max_workers  databases to be processed, the next database will be processed as soon as the first worker finishes.   log_autovacuum_min_duration  can be set to monitor autovacuum workers' activity. Routine Database Maintenance Tasks If several large tables all become eligible for vacuuming in a short amount of time, all autovacuum workers might become occupied with vacuuming those tables for a long period.  Tables whose  relfrozenxid  value is more than  autovacuum_freeze_max_age  transactions old are always vacuumed (this also applies to those tables whose freeze max age has been modified via storage parameters; see below).  The vacuum threshold is defined as: vacuum threshold = vacuum base threshold + vacuum scale factor *  number of tuples where the vacuum base threshold is  autovacuum_vacuum_threshold , the vacuum scale factor is autovacuum_vacuum_scale_factor , and .  Temporary tables cannot be accessed by autovacuum. conf , but it is possible to override them (and many other autovacuum control parameters) on a per-table basis; see  Storage Parameters  for more information.  When multiple workers are running, the autovacuum cost delay parameters (see  Section 19.  However, any workers processing tables whose per-table  autovacuum_vacuum_cost_delay  or  autovacuum_vacuum_cost_limit storage parameters have been set are not considered in the balancing algorithm.  If a process attempts to acquire a lock that conflicts with the  SHARE UPDATE EXCLUSIVE  lock held by autovacuum, lock acquisition will interrupt the autovacuum.  However, if the autovacuum is running to prevent transaction ID wraparound (i. , the autovacuum query name in the  pg_stat_activity  view ends with  (to prevent wraparound) ), the autovacuum is not automatically interrupted. , ANALYZE) can effectively prevent autovacuums from ever completing. 0  58504  2244 ?        Ss   18:02   0:00  postgres: autovacuum launcher process postgres  15558  0.  (The  ﬁstats collectorﬂ  process will not be present if you have set the system not to start the statistics collector; likewise the  ﬁautovacuum launcherﬂ  process can be disabled.  pg_stat_progress_vacuum One row for each backend (including autovacuum worker processes) running  VACUUM , showing current progress.  If this field is null, it indicates either that the client is connected via a Unix socket on the server machine or that this is an internal process such as autovacuum.  Possible types are autovacuum launcher , autovacuum worker , background worker , background writer , client backend , checkpointer ,  startup , walreceiver ,  walsender and  walwriter .  AutovacuumLock Autovacuum worker or launcher waiting to update or read the current state of autovacuum workers.  AutoVacuumMain Waiting in main loop of autovacuum launcher process. Monitoring Database Activity Column Type Description analyze_count bigint Number of times this table has been manually analyzed autoanalyze_count bigint Number of times this table has been analyzed by the autovacuum daemon The  pg_stat_all_tables  vi.  VACUUM Progress Reporting Whenever  VACUUM  is running, the  pg_stat_progress_vacuum  view will contain one row for each backend (including autovacuum worker processes) that is currently vacuuming.  SHARE UPDATE EXCLUSIVE  lock will be taken for fillfactor and autovacuum storage parameters, as well as the planner parameter  parallel_workers .  In the default PostgreSQL configuration, the autovacuum daemon (see  Section 24.  When autovacuum is disabled, it is a good idea to run  ANALYZE  periodically, or 1385 .  The autovacuum daemon, however, will only consider inserts or updates on the parent table itself when deciding whether to trigger an automatic analyze for that table.  The  autovacuum daemon  cannot access and therefore cannot vacuum or analyze temporary tables.  autovacuum_enabled ,  toast. autovacuum_enabled  ( boolean ) Enables or disables the autovacuum daemon for a particular table.  If true, the autovacuum daemon will perform automatic  VACUUM  and/or  ANALYZE  operations on this table following the rules discussed in  Section 24.  If false, this table will not be autovacuumed, except to prevent transaction ID wraparound.  Note that the autovacuum daemon does not run at all (except to prevent transaction ID wraparound) if the autovacuum  parameter is false; setting individual tables' storage parameters does not override that.  autovacuum_vacuum_threshold ,  toast. autovacuum_vacuum_threshold ( integer ) Per-table value for  autovacuum_vacuum_threshold  parameter.  autovacuum_vacuum_scale_factor ,  toast. autovacuum_vacuum_scale_factor ( floating point ) Per-table value for  autovacuum_vacuum_scale_factor  parameter.  autovacuum_analyze_threshold  ( integer ) Per-table value for  autovacuum_analyze_threshold  parameter.  autovacuum_analyze_scale_factor  ( floating point ) Per-table value for  autovacuum_analyze_scale_factor  parameter. CREATE TABLE autovacuum_vacuum_cost_delay ,  toast. autovacuum_vacuum_cost_delay ( integer ) Per-table value for  autovacuum_vacuum_cost_delay  parameter.  autovacuum_vacuum_cost_limit ,  toast. autovacuum_vacuum_cost_limit ( integer ) Per-table value for  autovacuum_vacuum_cost_limit  parameter.  autovacuum_freeze_min_age ,  toast. autovacuum_freeze_min_age  ( integer ) Per-table value for  vacuum_freeze_min_age  parameter.  Note that autovacuum will ignore per- table  autovacuum_freeze_min_age  parameters that are larger than half the system-wide autovacuum_freeze_max_age  setting.  autovacuum_freeze_max_age ,  toast. autovacuum_freeze_max_age  ( integer ) Per-table value for  autovacuum_freeze_max_age  parameter.  Note that autovacuum will ignore per-table  autovacuum_freeze_max_age  parameters that are larger than the system-wide setting (it can only be set smaller).  autovacuum_freeze_table_age ,  toast. autovacuum_freeze_table_age ( integer ) Per-table value for  vacuum_freeze_table_age  parameter.  autovacuum_multixact_freeze_min_age , toast. autovacuum_multixact_freeze_min_age  ( integer ) Per-table value for  vacuum_multixact_freeze_min_age  parameter.  Note that autovacuum will ignore per-table  autovacuum_multixact_freeze_min_age  parameters that are larger than half the system-wide  autovacuum_multixact_freeze_max_age  setting.  autovacuum_multixact_freeze_max_age , toast. autovacuum_multixact_freeze_max_age  ( integer ) Per-table value for  autovacuum_multixact_freeze_max_age  parameter.  Note that autovacuum will ignore per-table  autovacuum_multixact_freeze_max_age  parameters that are larger than the system-wide setting (it can only be set smaller).  autovacuum_multixact_freeze_table_age , toast. autovacuum_multixact_freeze_table_age  ( integer ) Per-table value for  vacuum_multixact_freeze_table_age  parameter.  log_autovacuum_min_duration ,  toast. log_autovacuum_min_duration ( integer ) Per-table value for  log_autovacuum_min_duration  parameter.  Normally the  autovacuum daemon  will take care of that automatically.  But if a table has recently had substantial changes in its contents, you might need to do a manual  ANALYZE  rather than wait for autovacuum to catch up with the changes.  PostgreSQL includes an  ﬁautovacuumﬂ  facility which can automate routine vacuum maintenance.  If autovacuum is enabled it can result in unpredictable changes in measured performance.  It is recommended that the access method do nothing except post-insert cleanup in such a call, and that only in an autovacuum worker process.  Proper use of autovacuum can minimize both of these problems. , via autovacuum).  Foreground cleanup operations can be avoided by increasing  gin_pending_list_limit  or making autovacuum more aggressive.  This process can be invoked manually using the  brin_summarize_range(regclass, bigint) or  brin_summarize_new_values(regclass)  functions; automatically when  VACUUM processes the table; or by automatic summarization executed by autovacuum, as inser.  When autosummarization is enabled, each time a page range is filled a request is sent to autovacuum for it to execute a targeted summarization for that range, to be fulfilled at the end of the next worker run on the same database.  It also caused autovacuum to cease functioning, which could have dire long-term effects if the surviving client sessions make a lot of data changes.  Ł Report correct relation name in autovacuum's  pg_stat_activity  display during BRIN summarize operations (Álvaro Herrera) Ł Fix  ﬁfailed to build any  N -way joinsﬂ  planner failures with lateral references leading out of  FULL outer joins (Tom La.  Ł Ensure relation caches are updated properly after renaming constraints (Amit Langote) Ł Make autovacuum more aggressive about removing leftover temporary tables, and also remove leftover temporary tables during  DISCARD TEMP  (Álvaro Herrera) 2259.  This has been observed to result in  reltuples  becoming so much larger than reality as to effectively shut off autovacuuming.  Ł Prevent dangling-pointer dereference when a C-coded before-update row trigger returns the  ﬁoldﬂ tuple (Rushabh Lathia) Ł Reduce locking during autovacuum worker scheduling (Jeff Janes) The previous behavior caused drastic loss of potential worker. Release Notes Ł Fix crash in autovacuum when extended statistics are defined for a table but can't be computed (Álvaro Herrera) Ł Fix null-pointer crashes for some types of LDAP URLs appearing in  pg_hba. milliseconds configuration parameter,  2311 auto-increment  (see  serial ) autocommit bulk-loading data,  437 psql,  1832 autovacuum configuration parameters,  548 general information,  619 autovacuum configuration parameter,  548 autovacuum_analyze_
log_autovacuum_min_duration	 log_autovacuum_min_duration  ( integer )  Causes each action executed by autovacuum to be logged if it ran for at least the specified number of milliseconds.   log_autovacuum_min_duration  can be set to monitor autovacuum workers' activity.  log_autovacuum_min_duration ,  toast. log_autovacuum_min_duration ( integer ) Per-table value for  log_autovacuum_min_duration  parameter
autovacuum_max_workers	, sets) at least ceil((max_connections + autovacuum_max_workers + max_worker_processes + 5) / 16)  plus room for other applications SEMMNS Maximum number of semaphores system-wide ceil((max_connections + autovacuum_max_workers + max_worker_processes .  When using System V semaphores, PostgreSQL uses one semaphore per allowed connection ( max_connections ), allowed autovacuum worker process ( autovacuum_max_workers ) and allowed background process ( max_worker_processes ), in sets of 16.  The maximum number of semaphores in the system is set by  SEMMNS , which consequently must be at least as high as  max_connections  plus autovacuum_max_workers  plus  max_worker_processes , plus one extra for each 16 allowed connections plus workers.  Hence this parameter must be at least  ceil((max_connections + autovacuum_max_workers + max_worker_processes + 5) / 16) .  When using POSIX semaphores, the number of semaphores needed is the same as for System V, that is one semaphore per allowed connection ( max_connections ), allowed autovacuum worker process ( autovacuum_max_workers ) and allowed background process (.  Note that when autovacuum runs, up to  autovacuum_max_workers  times this memory may be allocated, so be careful not to set the default value too high.  autovacuum_max_workers  ( integer )  Specifies the maximum number of autovacuum processes (other than the autovacuum launcher) that may be running at any one time. ) A maximum of  autovacuum_max_workers  worker processes are allowed to run at the same time.  If there are more than  autovacuum_max_workers  databases to be processed, the next database will be processed as soon as the first worker finishes
autovacuum_naptime	 autovacuum_naptime  ( integer )  Specifies the minimum delay between autovacuum runs on any given database.  The launcher will distribute the work across time, attempting to start one worker within each database every  autovacuum_naptime  seconds.  (Therefore, if the installation has  N databases, a new worker will be launched every  autovacuum_naptime / N  seconds
autovacuum_vacuum_threshold	 autovacuum_vacuum_threshold  ( integer )  Specifies the minimum number of updated or deleted tuples needed to trigger a  VACUUM  in any one table.  autovacuum_vacuum_scale_factor  ( floating point )  Specifies a fraction of the table size to add to  autovacuum_vacuum_threshold  when deciding whether to trigger a  VACUUM .  The vacuum threshold is defined as: vacuum threshold = vacuum base threshold + vacuum scale factor *  number of tuples where the vacuum base threshold is  autovacuum_vacuum_threshold , the vacuum scale factor is autovacuum_vacuum_scale_factor , and .  autovacuum_vacuum_threshold ,  toast. autovacuum_vacuum_threshold ( integer ) Per-table value for  autovacuum_vacuum_threshold  parameter
autovacuum_analyze_threshold	 autovacuum_analyze_threshold  ( integer )  Specifies the minimum number of inserted, updated or deleted tuples needed to trigger an ANALYZE  in any one table.  autovacuum_analyze_scale_factor  ( floating point )  Specifies a fraction of the table size to add to  autovacuum_analyze_threshold  when deciding whether to trigger an  ANALYZE .  autovacuum_analyze_threshold  ( integer ) Per-table value for  autovacuum_analyze_threshold  parameter
autovacuum_vacuum_scale_factor	 autovacuum_vacuum_scale_factor  ( floating point )  Specifies a fraction of the table size to add to  autovacuum_vacuum_threshold  when deciding whether to trigger a  VACUUM .  The vacuum threshold is defined as: vacuum threshold = vacuum base threshold + vacuum scale factor *  number of tuples where the vacuum base threshold is  autovacuum_vacuum_threshold , the vacuum scale factor is autovacuum_vacuum_scale_factor , and .  autovacuum_vacuum_scale_factor ,  toast. autovacuum_vacuum_scale_factor ( floating point ) Per-table value for  autovacuum_vacuum_scale_factor  parameter
autovacuum_analyze_scale_factor	 autovacuum_analyze_scale_factor  ( floating point )  Specifies a fraction of the table size to add to  autovacuum_analyze_threshold  when deciding whether to trigger an  ANALYZE .  autovacuum_analyze_scale_factor  ( floating point ) Per-table value for  autovacuum_analyze_scale_factor  parameter
autovacuum_freeze_max_age	 autovacuum_freeze_max_age  ( integer )  Specifies the maximum age (in transactions) that a table's  pg_class .  Although users can set this value anywhere from zero to two billions,  VACUUM  will silently limit the effective value to 95% of autovacuum_freeze_max_age , so that a periodical manual  VACUUM  has a chance to run before an anti-wraparound autovacuu.  Although users can set this value anywhere from zero to one billion,  VACUUM  will silently limit the effective value to half the value of  autovacuum_freeze_max_age , so that there is not an unreasonably short time between forced autovacuums.  To ensure that this does not happen, autovacuum is invoked on any table that might contain unfrozen rows with XIDs older than the age specified by the configuration parameter  autovacuum_freeze_max_age . ) This implies that if a table is not otherwise vacuumed, autovacuum will be invoked on it approximately once every  autovacuum_freeze_max_age  minus  vacuum_freeze_min_age  transactions.  Obviously one can do this either by increasing autovacuum_freeze_max_age  or decreasing  vacuum_freeze_min_age . 95 * autovacuum_freeze_max_age ; a setting higher than that will be capped to the maximum.  A value higher than  autovacuum_freeze_max_age  wouldn't make sense because an anti- wraparound autovacuum would be triggered at that point anyway, and the 0.  As a rule of thumb,  vacuum_freeze_table_age  should be set to a value somewhat below autovacuum_freeze_max_age , leaving enough gap so that a regularly scheduled  VACUUM  or an autovacuum triggered by normal delete and update activity is run in tha.  The sole disadvantage of increasing  autovacuum_freeze_max_age  (and vacuum_freeze_table_age  along with it) is that the  pg_xact  and  pg_commit_ts subdirectories of the database cluster will take more space, because it must store the commit status. Routine Database Maintenance Tasks autovacuum_freeze_max_age  horizon.  The commit status uses two bits per transaction, so if autovacuum_freeze_max_age  is set to its maximum allowed value of two billion,  pg_xact can be expected to grow to about half a gigabyte and  pg_commit_ts  to about 20GB.  If this is trivial compared to your total database size, setting  autovacuum_freeze_max_age  to its maximum allowed value is recommended.  If no  relfrozenxid -advancing  VACUUM  is issued on the table until autovacuum_freeze_max_age  is reached, an autovacuum will soon be forced for the table.  Tables whose  relfrozenxid  value is more than  autovacuum_freeze_max_age  transactions old are always vacuumed (this also applies to those tables whose freeze max age has been modified via storage parameters; see below).  Note that autovacuum will ignore per- table  autovacuum_freeze_min_age  parameters that are larger than half the system-wide autovacuum_freeze_max_age  setting.  autovacuum_freeze_max_age ,  toast. autovacuum_freeze_max_age  ( integer ) Per-table value for  autovacuum_freeze_max_age  parameter.  Note that autovacuum will ignore per-table  autovacuum_freeze_max_age  parameters that are larger than the system-wide setting (it can only be set smaller)
autovacuum_multixact_freeze_max_age	 autovacuum_multixact_freeze_max_age  ( integer )  Specifies the maximum age (in multixacts) that a table's  pg_class .  Although users can set this value anywhere from zero to two billions,  VACUUM  will silently limit the effective value to 95% of autovacuum_multixact_freeze_max_age , so that a periodical manual  VACUUM  has a chance to run before an anti-wraparound.  Although users can set this value anywhere from zero to one billion,  VACUUM will silently limit the effective value to half the value of  autovacuum_multixact_freeze_max_age , so that there is not an unreasonably short time between forced autovacuu.  As a safety device, an aggressive vacuum scan will occur for any table whose multixact-age is greater than  autovacuum_multixact_freeze_max_age .  Note that autovacuum will ignore per-table  autovacuum_multixact_freeze_min_age  parameters that are larger than half the system-wide  autovacuum_multixact_freeze_max_age  setting.  autovacuum_multixact_freeze_max_age , toast. autovacuum_multixact_freeze_max_age  ( integer ) Per-table value for  autovacuum_multixact_freeze_max_age  parameter.  Note that autovacuum will ignore per-table  autovacuum_multixact_freeze_max_age  parameters that are larger than the system-wide setting (it can only be set smaller)
autovacuum_vacuum_cost_delay	 autovacuum_vacuum_cost_delay  ( integer )  Specifies the cost delay value that will be used in automatic  VACUUM  operations.  However, any workers processing tables whose per-table  autovacuum_vacuum_cost_delay  or  autovacuum_vacuum_cost_limit storage parameters have been set are not considered in the balancing algorithm. CREATE TABLE autovacuum_vacuum_cost_delay ,  toast. autovacuum_vacuum_cost_delay ( integer ) Per-table value for  autovacuum_vacuum_cost_delay  parameter
autovacuum_vacuum_cost_limit	 autovacuum_vacuum_cost_limit  ( integer )  Specifies the cost limit value that will be used in automatic  VACUUM  operations.  However, any workers processing tables whose per-table  autovacuum_vacuum_cost_delay  or  autovacuum_vacuum_cost_limit storage parameters have been set are not considered in the balancing algorithm.  autovacuum_vacuum_cost_limit ,  toast. autovacuum_vacuum_cost_limit ( integer ) Per-table value for  autovacuum_vacuum_cost_limit  parameter
check_function_bodies	 check_function_bodies  ( boolean )  This parameter is normally on.  Validator functions should typically honor the  check_function_bodies  parameter: if it is turned off then any expensive or context-sensitive checking should be skipped. ) While the choice of exactly what to check is mostly left to the discretion of the validator function, note that the core  CREATE FUNCTION  code only executes  SET  clauses attached to a function when check_function_bodies  is on.  Therefore, checks whose results might be affected by GUC parameters definitely should be skipped when  check_function_bodies  is off, to avoid false failures when reloading a dump.  To mitigate this risk, be more careful about the search_path  used to run an installation script; disable  check_function_bodies  within the script; and fix catalog-adjustment queries used in some contrib modules to ensure they are secure
default_transaction_read_only	 default_transaction_read_only  ( boolean )    A read-only SQL transaction cannot alter non-temporary tables.  The session default transaction modes can also be set by setting the configuration parameters default_transaction_isolation ,  default_transaction_read_only , and  default_transaction_deferrable 
default_transaction_deferrable	 default_transaction_deferrable  ( boolean )    When running at the  serializable  isolation level, a deferrable read-only SQL transaction may be delayed before it is allowed to proceed.  The session default transaction modes can also be set by setting the configuration parameters default_transaction_isolation ,  default_transaction_read_only , and  default_transaction_deferrable . Index changing,  67 default_statistics_target configuration parameter,  535 default_tablespace configuration parameter,  551 default_text_search_config configuration parameter, 556 default_transaction_deferrable configuration parameter,  552 default_
statement_timeout	 For example, env PGOPTIONS="-c geqo=off -c statement_timeout=5min" psql Other clients and libraries might provide their own mechanisms, via the shell or otherwise, that allow the user to alter session settings without direct use of SQL commands.  statement_timeout  ( integer )  Abort any statement that takes more than the specified number of milliseconds, starting from the time the command arrives at the server from the client. Server Configuration Setting  statement_timeout  in  postgresql.  Unlike  statement_timeout , this timeout can only occur while waiting for locks.  Note that if  statement_timeout  is nonzero, it is rather pointless to set  lock_timeout to the same or larger value, since the statement timeout would always trigger first.  If you wish to place a time limit on the execution of  pg_stop_backup , set an appropriate  statement_timeout  value, but make note that if  pg_stop_backup  terminates because of this your backup may not be valid.  If you wish to place a time limit on the execution of  pg_stop_backup , set an appropriate  statement_timeout  value, but make note that if  pg_stop_backup  terminates because of this your backup may not be valid. High Availability, Load Balancing, and Replication finite value for  max_standby_archive_delay  or  max_standby_streaming_delay  can be considered similar to setting  statement_timeout .  The timeout may be specified in any of the formats accepted by  SET statement_timeout .  The timeout may be specified in any of the formats accepted by  SET statement_timeout .  Ł Mark some timeout and statistics-tracking GUC variables as  PGDLLIMPORT , to allow extensions to access them on Windows (Pascal Legrand) This applies to  idle_in_transaction_session_timeout ,  lock_timeout , statement_timeout ,  track_activities ,.  Ł Prevent  idle_in_transaction_session_timeout  from being ignored when a statement_timeout  occurred earlier (Lukas Fittl) Ł Fix low-probability loss of  NOTIFY  messages due to XID wraparound (Marko Tiikkaja, Tom Lane) If a session executed no que
lock_timeout	Server Configuration log_line_prefix = '%m [%p] %q%u@%d/%a ' log_lock_waits  ( boolean )  Controls whether a log message is produced when a session waits longer than  deadlock_timeout to acquire a lock.  lock_timeout  ( integer )  Abort any statement that waits longer than the specified number of milliseconds while attempting to acquire a lock on a table, index, row, or other database object.  Note that if  statement_timeout  is nonzero, it is rather pointless to set  lock_timeout to the same or larger value, since the statement timeout would always trigger first.  Setting  lock_timeout  in  postgresql.  Lock Management deadlock_timeout  ( integer )      This is the amount of time, in milliseconds, to wait on a lock before checking to see if there is a deadlock condition.  If you are trying to investigate locking delays you might want to set a shorter than normal  deadlock_timeout .  Ł Mark some timeout and statistics-tracking GUC variables as  PGDLLIMPORT , to allow extensions to access them on Windows (Pascal Legrand) This applies to  idle_in_transaction_session_timeout ,  lock_timeout , statement_timeout ,  track_activities ,
idle_in_transaction_session_timeout	 The configuration parameter  idle_in_transaction_session_timeout  may be used to automatically disconnect lingering sessions.  idle_in_transaction_session_timeout  ( integer )  Terminate any session with an open transaction that has been idle for longer than the specified duration in milliseconds.  Ł Mark some timeout and statistics-tracking GUC variables as  PGDLLIMPORT , to allow extensions to access them on Windows (Pascal Legrand) This applies to  idle_in_transaction_session_timeout ,  lock_timeout , statement_timeout ,  track_activities ,.  Ł Prevent  idle_in_transaction_session_timeout  from being ignored when a statement_timeout  occurred earlier (Lukas Fittl) Ł Fix low-probability loss of  NOTIFY  messages due to XID wraparound (Marko Tiikkaja, Tom Lane) If a session executed no que. Index I icount,  2373 ICU,  458 ,  603 ,  1422 ident,  580 identifier length,  31 syntax of,  31 IDENTIFY_SYSTEM,  2008 ident_file configuration parameter,  509 idle_in_transaction_session_timeout configuration parameter,  553 idx,  2373 IFNULL,  282
vacuum_freeze_min_age	 vacuum_freeze_min_age  ( integer )  Specifies the cutoff age (in transactions) that  VACUUM  should use to decide whether to freeze row versions while scanning a table.  vacuum_freeze_min_age  controls how old an XID value has to be before rows bearing that XID will be frozen.  The maximum time that a table can go unvacuumed is two billion transactions minus the vacuum_freeze_min_age  value at the time of the last aggressive vacuum. ) This implies that if a table is not otherwise vacuumed, autovacuum will be invoked on it approximately once every  autovacuum_freeze_max_age  minus  vacuum_freeze_min_age  transactions.  Obviously one can do this either by increasing autovacuum_freeze_max_age  or decreasing  vacuum_freeze_min_age . ) One disadvantage of decreasing  vacuum_freeze_min_age  is that it might cause  VACUUM  to do useless work: freezing a row version is a waste of time if the row is modified soon thereafter (causing it to acquire a new XID).  When  VACUUM  scans every page in the table that is not already all-frozen, it should set  age(relfrozenxid)  to a value just a little more than the vacuum_freeze_min_age  setting that was used (more by the number of transactions started since the  .  autovacuum_freeze_min_age ,  toast. autovacuum_freeze_min_age  ( integer ) Per-table value for  vacuum_freeze_min_age  parameter.  Note that autovacuum will ignore per- table  autovacuum_freeze_min_age  parameters that are larger than half the system-wide autovacuum_freeze_max_age  setting.  Specifying  FREEZE  is equivalent to performing VACUUM  with the  vacuum_freeze_min_age  and  vacuum_freeze_table_age  parameters set to zero
vacuum_freeze_table_age	 vacuum_freeze_table_age  ( integer )  VACUUM  performs an aggressive scan if the table's  pg_class .   vacuum_freeze_table_age  controls when  VACUUM  does that: all-visible but not all-frozen pages are scanned if the number of transactions that have passed since the last such scan is greater than  vacuum_freeze_table_age  minus vacuum_freeze_min_ag.  Setting  vacuum_freeze_table_age  to 0 forces  VACUUM  to use this more aggressive strategy for all scans.  The effective maximum for  vacuum_freeze_table_age  is 0.  As a rule of thumb,  vacuum_freeze_table_age  should be set to a value somewhat below autovacuum_freeze_max_age , leaving enough gap so that a regularly scheduled  VACUUM  or an autovacuum triggered by normal delete and update activity is run in tha.  The sole disadvantage of increasing  autovacuum_freeze_max_age  (and vacuum_freeze_table_age  along with it) is that the  pg_xact  and  pg_commit_ts subdirectories of the database cluster will take more space, because it must store the commit status.  This happens when  relfrozenxid  is more than vacuum_freeze_table_age  transactions old, when  VACUUM 's  FREEZE  option is used, or when all pages that are not already all-frozen happen to require vacuuming to remove dead row versions. ) If the  relfrozenxid  value of the table is more than vacuum_freeze_table_age  transactions old, an aggressive vacuum is performed to freeze old tuples and advance  relfrozenxid ; otherwise, only pages that have been modified since the last vacuum .  autovacuum_freeze_table_age ,  toast. autovacuum_freeze_table_age ( integer ) Per-table value for  vacuum_freeze_table_age  parameter.  Specifying  FREEZE  is equivalent to performing VACUUM  with the  vacuum_freeze_min_age  and  vacuum_freeze_table_age  parameters set to zero
vacuum_multixact_freeze_min_age	 vacuum_multixact_freeze_min_age  ( integer )  Specifies the cutoff age (in multixacts) that  VACUUM  should use to decide whether to replace multixact IDs with a newer transaction ID or multixact ID while scanning a table.  Whenever  VACUUM  scans any part of a table, it will replace any multixact ID it encounters which is older than  vacuum_multixact_freeze_min_age  by a different value, which can be the zero value, a single transaction ID, or a newer multixact ID.  autovacuum_multixact_freeze_min_age , toast. autovacuum_multixact_freeze_min_age  ( integer ) Per-table value for  vacuum_multixact_freeze_min_age  parameter.  Note that autovacuum will ignore per-table  autovacuum_multixact_freeze_min_age  parameters that are larger than half the system-wide  autovacuum_multixact_freeze_max_age  setting
vacuum_multixact_freeze_table_age	 vacuum_multixact_freeze_table_age  ( integer )  VACUUM  performs an aggressive scan if the table's  pg_class .  If this value is older than vacuum_multixact_freeze_table_age , an aggressive vacuum is forced.  autovacuum_multixact_freeze_table_age , toast. autovacuum_multixact_freeze_table_age  ( integer ) Per-table value for  vacuum_multixact_freeze_table_age  parameter
gin_fuzzy_search_limit	 gin_fuzzy_search_limit  ( integer )  Soft upper limit of the size of the set returned by GIN index scans.  gin_fuzzy_search_limit The primary goal of developing GIN indexes was to create support for highly scalable full-text search in PostgreSQL, and there are often situations when a full-text search returns a very large set of results. GIN Indexes To facilitate controlled execution of such queries, GIN has a configurable soft upper limit on the number of rows returned: the  gin_fuzzy_search_limit  configuration parameter
gin_pending_list_limit	 gin_pending_list_limit  ( integer )  Sets the maximum size of the GIN pending list which is used when  fastupdate  is enabled. CREATE INDEX gin_pending_list_limit Custom  gin_pending_list_limit  parameter.  When the table is vacuumed or autoanalyzed, or when  gin_clean_pending_list  function is called, or if the pending list becomes larger than gin_pending_list_limit , the entries are moved to the main GIN data structure using the same bulk insert tech.  gin_pending_list_limit During a series of insertions into an existing GIN index that has  fastupdate  enabled, the system will clean up the pending-entry list whenever the list grows larger than gin_pending_list_limit .  Foreground cleanup operations can be avoided by increasing  gin_pending_list_limit  or making autovacuum more aggressive.  gin_pending_list_limit  can be overridden for individual GIN indexes by changing storage parameters, which allows each GIN index to have its own cleanup threshold
datestyle	 Date Input Example Description 1999-01-08 ISO 8601; January 8 in any mode (recommended format) January 8, 1999 unambiguous in any  datestyle  input mode 1/8/1999 January 8 in  MDY  mode; August 1 in  DMY  mode 1/18/1999 January 18 in  MDY  mode; rej.  Date Order Conventions datestyle  Setting Input Ordering Example Output SQL, DMY day / month / year 17/12/1997 15:37:16. 00 PST Postgres, DMY day / month / year Wed 17 Dec 07:37:16 1997 PST The date/time style can be selected by the user using the  SET datestyle  command, the  DateStyle parameter in the  postgresql.  An example: SELECT current_setting('datestyle');  current_setting -----------------  ISO, MDY (1 row) If there is no setting named  setting_name ,  current_setting  throws an error unless missing_ok  is supplied and is  true .  Short Option Key Short Option Equivalent -B  x shared_buffers =  x -d  x log_min_messages = DEBUG x -e datestyle = euro -fb ,  -fh ,  -fi ,  -fm ,  -fn ,  -fo ,  -fs ,  -ft enable_bitmapscan = off , enable_hashjoin = off , enable_indexscan = off , e.  (Equivalent to  SET datestyle TO .  Examples Set the schema search path: SET search_path TO my_schema, public; Set the style of date to traditional POSTGRES with  ﬁday before monthﬂ  input convention: SET datestyle TO postgres, dmy; Set the time zone for Berkeley, California: SET TIME
timezone	  pg_timezone_abbrevs .   pg_timezone_names .   pg_timezone_abbrevs  Columns .   pg_timezone_names  Columns .  If no time zone is stated in the input string, then it is assumed to be in the time zone indicated by the system's  TimeZone  parameter, and is converted to UTC using the offset for the  timezone  zone.  When a  timestamp with time zone  value is output, it is always converted from UTC to the current  timezone  zone, and displayed as local time in that zone.  To see the time in another time zone, either change  timezone  or use the  AT TIME ZONE  construct (see  Section 9.  Conversions between  timestamp without time zone  and  timestamp with time zone normally assume that the  timestamp without time zone  value should be taken or given as timezone  local time.  All timezone-aware dates and times are stored internally in UTC.  The recognized time zone names are listed in the  pg_timezone_names  view (see  Section 51.  The recognized abbreviations are listed in the  pg_timezone_abbrevs  view (see Section 51.  You cannot set the configuration parameters  TimeZone  or  log_timezone  to a time zone abbreviation, but you can use abbreviations in date/time input values and with the  AT TIME ZONE  operator. Data Types Ł In addition to the timezone names and abbreviations, PostgreSQL will accept POSIX-style time zone specifications, as described in  Section B.  To complicate matters, some jurisdictions have used the same timezone abbreviation to mean different UTC offsets at different times; for example, in Moscow  MSK  has meant UTC+3 in some years and UTC+4 in others.  In all cases, timezone names and abbreviations are recognized case-insensitively. ) Neither timezone names nor abbreviations are hard-wired into the server; they are obtained from configuration files stored under  . /share/timezone/  and  . /share/timezonesets/ of the installation directory (see  Section B. 5 make_timestamptz( year int ,  month int ,  day int ,  hour int ,  min   int , sec   double precision , [ timezone   text ]) timestamp with time zone Create timestamp with time zone from year, month, day, hour, minute and seconds fields; if  timezon.  This will adjust for the number of days in each month, timezone changes, and daylight saving time adjustments.  The sample results were produced with  timezone = 'US/Eastern' ; there is a daylight saving time change between the two dates used: SELECT EXTRACT(EPOCH FROM timestamptz '2013-07-01 12:00:00') -        EXTRACT(EPOCH FROM timestamptz '2013-03-01 12:0. 5 timezone The time zone offset from UTC, measured in seconds. ) timezone_hour The hour component of the time zone offset timezone_minute The minute component of the time zone offset week The number of the ISO 8601 week-numbering week of the year.  The function  timezone ( zone ,  timestamp )  is equivalent to the SQL-conforming construct timestamp  AT TIME ZONE  zone .  (Note that if there are any time-zone-dependent  % -escapes, the computation is done in the zone specified by  log_timezone .  log_timezone  ( string )  Sets the time zone used for timestamps written in the server log.  timezone_abbreviations  ( string )    Sets the collection of time zone abbreviations that will be accepted by the server for datetime input.  (Equivalent to  SET timezone TO .  Examples Set the  timezone  configuration variable to its default value: RESET timezone; Compatibility RESET  is a PostgreSQL extension. SET SET  Š  change a run-time parameter Synopsis SET [ SESSION | LOCAL ]  configuration_parameter  { TO | = } {  value  | ' value ' | DEFAULT } SET [ SESSION | LOCAL ] TIME ZONE {  timezone  | LOCAL | DEFAULT } Description The  SET  command changes r.  The seed can also be set by invoking the function  setseed : SELECT setseed( value ); TIME ZONE SET TIME ZONE  value  is an alias for  SET timezone TO  value .  LOCAL DEFAULT Set the time zone to your local time zone (that is, the server's default value of  timezone ).  Timezone settings given as numbers or intervals are internally translated to POSIX timezone syntax.   pg_timezone_abbrevs .   pg_timezone_names .   pg_timezone_abbrevs The view  pg_timezone_abbrevs  provides a list of time zone abbreviations that are currently recognized by the datetime input routines.  The contents of this view change when the timezone_abbreviations  run-time parameter is modified.   pg_timezone_abbrevs  Columns Name Type Description abbrev text Time zone abbreviation utc_offset interval Offset from UTC (positive means east of Greenwich) is_dst boolean True if this is a daylight-savings abbreviation While most timezone abbrevia.   pg_timezone_names The view  pg_timezone_names  provides a list of time zone names that are recognized by SET TIMEZONE , along with their associated abbreviations, UTC offsets, and daylight-savings status. ) Unlike the abbreviations shown in  pg_timezone_abbrevs , many of these names imply a set of daylight-savings transition date rules.   pg_timezone_names  Columns Name Type Description name text Time zone name abbrev text Time zone abbreviation utc_offset interval Offset from UTC (positive means east of Greenwich) is_dst boolean True if currently observing daylight savings 51.  Date/Time Configuration Files Since timezone abbreviations are not well standardized, PostgreSQL provides a means to customize the set of abbreviations accepted by the server.  The  timezone_abbreviations  run-time parameter determines the active set of abbreviations. /share/timezonesets/  of the installation directory.  By adding or altering files in that directory, the administrator can set local policy for timezone abbreviations.  timezone_abbreviations  can be set to any file name found in  . /share/ timezonesets/ , if the file's name is entirely alphabetic.  (The prohibition against non-alphabetic characters in  timezone_abbreviations  prevents reading files outside the intended directory, as well as reading editor backup files and other extraneous files. ) A timezone abbreviation file can contain blank lines and comments beginning with  # .  Alternatively, a  time_zone_name  can be given, referencing a zone name defined in the IANA timezone database. /share/timezonesets/ directory.  Without this, conflicting definitions of the same timezone abbreviation are considered an error. txt , etc, containing information about every time zone abbreviation known to be in use according to the IANA timezone database.  Note that these files cannot be directly referenced as timezone_abbreviations  settings, because of the dot embedded in their names.  Caution Time zone abbreviations defined in the configuration file override non-timezone meanings built into PostgreSQL. /share/timezonesets/ , it is up to you to make backups Š a normal database dump will not include this directory.  The four timezone names  EST5EDT ,  CST6CDT ,  MST7MDT , and  PST8PDT  look like they are POSIX zone specifications.  Ł Sync our copy of the timezone library with IANA tzcode release 2020d (Tom Lane) This absorbs upstream's change of zic's default output option from  ﬁfatﬂ  to  ﬁslimﬂ .  Ł Avoid possibly leaking an open-file descriptor for a directory in  pg_ls_dir() , pg_timezone_names() ,  pg_tablespace_databases() , and allied functions (Justin Pryzby) Ł Fix polymorphic-function type resolution to correctly infer the actual type .  Prefer any other match to the C library's timezone behavior over these two.  Ł Adjust  pg_timezone_names  view to show the  Factory  time zone if and only if it has a short abbreviation (Tom Lane) Historically, IANA set up this artificial zone with an  ﬁabbreviationﬂ  like  Local time zone must be set--see zic manual page .  Ł Sync our copy of the timezone library with IANA tzcode release 2019b (Tom Lane) This adds support for zic's new  -b slim  option to reduce the size of the installed zone files.  This does not affect either the actual UTC offset or the timezone abbreviations in use; the only known effect is that the  is_dst  column in the pg_timezone_names  view will now be true in winter and false in summer in these cases. 4 built-in functions (Rainer Orth) Ł Sync our copy of the timezone library with IANA tzcode release 2018e (Tom Lane) This fixes the zic timezone data compiler to cope with negative daylight-savings offsets.  While the PostgreSQL project will not immediately ship such timezone data, zic might be used with timezone data obtained directly from IANA, so it seems prudent to update zic now
extra_float_digits	 Note The  extra_float_digits  setting controls the number of extra significant digits included when a floating point value is converted to text for output.  extra_float_digits  ( integer )      This parameter adjusts the number of digits displayed for floating-point values, including float4 ,  float8 , and geometric data types.  postgres_fdw  likewise establishes remote session settings for various parameters: Ł TimeZone  is set to  UTC Ł DateStyle  is set to  ISO Ł IntervalStyle  is set to  postgres Ł extra_float_digits  is set to  3  for remote servers 9
lc_messages	 lc_messages  ( string )  Sets the language in which messages are displayed
lc_monetary	 The fractional precision is determined by the database's  lc_monetary  setting. 07 Since the output of this data type is locale-sensitive, it might not work to load  money  data into a database that has a different setting of  lc_monetary .  To avoid problems, before restoring a dump into a new database make sure  lc_monetary  has the same or equivalent value as in the database that was dumped. ) Ł The pattern characters  S ,  L ,  D , and  G  represent the sign, currency symbol, decimal point, and thousands separator characters defined by the current locale (see  lc_monetary  and  lc_numeric ).  lc_monetary  ( string )  Sets the locale to use for formatting monetary amounts, for example with the  to_char  family of functions
lc_numeric	) Ł The pattern characters  S ,  L ,  D , and  G  represent the sign, currency symbol, decimal point, and thousands separator characters defined by the current locale (see  lc_monetary  and  lc_numeric ).  lc_numeric  ( string )  Sets the locale to use for formatting numbers, for example with the  to_char  family of functions
lc_time	,  12th FX  prefix fixed format global option (see usage notes) FX Month DD Day TM  prefix translation mode (print localized day and month names based on lc_time ) TMMonth SP  suffix spell mode (not implemented) DDSP Usage notes for date/time formatt.  The built-in default is  ISO, MDY , but initdb will initialize the configuration file with a setting that corresponds to the behavior of the chosen  lc_time  locale.  lc_time  ( string )  Sets the locale to use for formatting dates and times, for example with the  to_char  family of functions.  Ł Fix handling of  lc_time  settings that imply an encoding different from the database's encoding (Juan José Santamaría Flecha, Tom Lane) Localized month or day names that include non-ASCII characters previously caused unexpected errors or wrong ou
default_text_search_config	 Note All the text search functions that accept an optional  regconfig  argument will use the configuration specified by  default_text_search_config  when that argument is omitted. ) During installation an appropriate configuration is selected and  default_text_search_config  is set accordingly in  postgresql.  Otherwise, you can set  default_text_search_config  in each session.   default_text_search_config  is used only when this argument is omitted.  Alternatively we could omit the configuration parameters: SELECT title FROM pgweb WHERE to_tsvector(body) @@ to_tsquery('friend'); This query will use the configuration set by  default_text_search_config .  This is because the index contents must be unaffected by  default_text_search_config .  As shown in the example above, the query can depend on  default_text_search_config .  The configuration to be used to parse the document can be specified by  config ; if  config  is omitted, the  default_text_search_config configuration is used.  The configuration parameter  default_text_search_config  specifies the name of the default configuration, which is the one used by text search functions if an explicit configuration parameter is omitted. pg'; SET SHOW default_text_search_config;  default_text_search_config 396 .  It uses the configuration specified by  config , or default_text_search_config  if that argument is omitted.  default_text_search_config  ( string )  Selects the text search configuration that is used by those variants of the text search functions that do not have an explicit argument specifying the configuration.  See  default_text_search_config  for further information. Index changing,  67 default_statistics_target configuration parameter,  535 default_tablespace configuration parameter,  551 default_text_search_config configuration parameter, 556 default_transaction_deferrable configuration parameter,  552 default_
deadlock_timeout	Server Configuration log_line_prefix = '%m [%p] %q%u@%d/%a ' log_lock_waits  ( boolean )  Controls whether a log message is produced when a session waits longer than  deadlock_timeout to acquire a lock.  Lock Management deadlock_timeout  ( integer )      This is the amount of time, in milliseconds, to wait on a lock before checking to see if there is a deadlock condition.  If you are trying to investigate locking delays you might want to set a shorter than normal  deadlock_timeout 
max_locks_per_transaction	 Both advisory locks and regular locks are stored in a shared memory pool whose size is defined by the configuration variables  max_locks_per_transaction  and  max_connections .  max_locks_per_transaction  ( integer )  The shared lock table tracks locks on  max_locks_per_transaction  * ( max_connections  + max_prepared_transactions ) objects (e. High Availability, Load Balancing, and Replication Ł max_connections Ł max_prepared_transactions Ł max_locks_per_transaction Ł max_worker_processes It is important that the administrator select appropriate settings for  max_standby_archive_delay  and.  Since the server acquires a lock per LO removed, removing too many LOs in one transaction risks exceeding max_locks_per_transaction 
max_pred_locks_per_transaction	 You can avoid this by increasing  max_pred_locks_per_transaction , max_pred_locks_per_relation , and/or  max_pred_locks_per_page .  max_pred_locks_per_transaction  ( integer )  The shared predicate lock table tracks locks on  max_pred_locks_per_transaction  * ( max_connections  +  max_prepared_transactions ) objects (e.  Values greater than or equal to zero mean an absolute limit, while negative values mean  max_pred_locks_per_transaction  divided by the absolute value of this setting
max_pred_locks_per_relation	 You can avoid this by increasing  max_pred_locks_per_transaction , max_pred_locks_per_relation , and/or  max_pred_locks_per_page .  max_pred_locks_per_relation  ( integer )  This controls how many pages or tuples of a single relation can be predicate-locked before the lock is promoted to covering the whole relation.  Ł Allow tuning of predicate lock promotion thresholds (Dagfinn Ilmari Mannsåker) Lock promotion can now be controlled through two new server parameters, max_pred_locks_per_relation  and  max_pred_locks_per_page 
max_pred_locks_per_page	 You can avoid this by increasing  max_pred_locks_per_transaction , max_pred_locks_per_relation , and/or  max_pred_locks_per_page .  max_pred_locks_per_page  ( integer )  This controls how many rows on a single page can be predicate-locked before the lock is promoted to covering the whole page.  Ł Allow tuning of predicate lock promotion thresholds (Dagfinn Ilmari Mannsåker) Lock promotion can now be controlled through two new server parameters, max_pred_locks_per_relation  and  max_pred_locks_per_page 
array_nulls	2 versions of PostgreSQL, the  array_nulls configuration parameter can be turned  off  to suppress recognition of  NULL  as a NULL.  Previous PostgreSQL Versions array_nulls  ( boolean )  This controls whether the array input parser recognizes unquoted  NULL  as specifying a null array element. Index searching,  170 ARRAY,  48 determination of result type,  351 array_agg,  288 ,  2372 array_append,  284 array_cat,  284 array_dims,  284 array_fill,  284 array_length,  284 array_lower,  284 array_ndims,  284 array_nulls configuration paramete
default_with_oids	 This column is only present if the table was created using  WITH OIDS , or if the  default_with_oids  configuration variable was set at the time.  OIDs are not added to user-created tables, unless  WITH OIDS  is specified when the table is created, or the  default_with_oids  configuration variable is enabled.  default_with_oids  ( boolean )  This controls whether  CREATE TABLE  and  CREATE TABLE AS  include an OID column in newly-created tables, if neither  WITH OIDS  nor  WITHOUT OIDS  is specified.  If  OIDS  is not specified, the default setting depends upon the  default_with_oids  configuration parameter.  If the presence of OIDs is not explicitly specified, the  default_with_oids  configuration variable is used.  To add OIDs to the table created by  SELECT INTO , enable the  default_with_oids  configuration variable
escape_string_warning	 In addition to  standard_conforming_strings , the configuration parameters escape_string_warning  and  backslash_quote  govern treatment of backslashes in string constants.  escape_string_warning  ( boolean )    When on, a warning is issued if a backslash ( \ ) appears in an ordinary string literal ( '
lo_compat_privileges	 lo_compat_privileges  ( boolean )  In PostgreSQL releases prior to 9.  To adjust this behavior for compatibility with prior releases, see the  lo_compat_privileges  run-time parameter
operator_precedence_warning	 If you are concerned about whether these changes have silently broken something, you can test your application with the configuration parameter operator_precedence_warning  turned on to see if any warnings are logged.  operator_precedence_warning  ( boolean )  When on, the parser will emit a warning for any construct that might have changed meanings since PostgreSQL 9.  Ł Fix incorrect  operator_precedence_warning  checks involving unary minus operators (Rikard Falkeborn) Ł Disallow  NaN  as a value for floating-point server parameters (Tom Lane) Ł Rearrange  REINDEX  processing to avoid assertion failures when rei
quote_all_identifiers	 quote_all_identifiers  ( boolean )  When the database generates SQL, force all identifiers to be quoted, even if they are not (currently) keywords
standard_conforming_strings	) Caution If the configuration parameter  standard_conforming_strings  is  off , then PostgreSQL recognizes backslash escapes in both regular and escape string constants.  In addition to  standard_conforming_strings , the configuration parameters escape_string_warning  and  backslash_quote  govern treatment of backslashes in string constants. ) Also, the Unicode escape syntax for string constants only works when the configuration parameter standard_conforming_strings  is turned on.  Note If you have  standard_conforming_strings  turned off, any backslashes you write in literal string constants will need to be doubled.  Note If you have  standard_conforming_strings  turned off, any backslashes you write in literal string constants will need to be doubled. ' syntax) and  standard_conforming_strings  is off.  standard_conforming_strings  ( boolean )    This controls whether ordinary string literals ( '. 0;  standard_conforming_strings  was not reported by releases before 8.  If no value for  standard_conforming_strings  is reported, applications can assume it is  off , that is, backslashes are treated as escapes in string literals. 0; standard_conforming_strings  was not reported by releases before 8.  The quoting rules for the label are the same as a standard SQL string with standard_conforming_strings  turned on.  Ł In ecpglib, correctly handle backslashes in string literals depending on whether standard_conforming_strings  is set (Tsunakawa Takayuki) Ł Make ecpglib's Informix-compatibility mode ignore fractional digits in integer input strings, as expected (.  But if you're passing it as a quoted literal constant, then any single-quote characters and (depending on the setting of the standard_conforming_strings  configuration parameter) backslash characters need to be escaped correctly
synchronize_seqscans	 synchronize_seqscans  ( boolean )  This allows sequential scans of large tables to synchronize with each other, so that concurrent scans read the same block at about the same time and hence share the I/O workload
transform_null_equals	 However, if that cannot be done the  transform_null_equals configuration variable is available.  Platform and Client Compatibility transform_null_equals  ( boolean )    When on, expressions of the form  expr  = NULL  (or  NULL =  expr ) are treated as  expr IS NULL , that is, they return true if  expr  evaluates to the null value, and false oth
exit_on_error	 Error Handling exit_on_error  ( boolean )  If true, any error will terminate the current session
restart_after_crash	 restart_after_crash  ( boolean )  When set to true, which is the default, PostgreSQL will automatically reinitialize after a backend crash
data_sync_retry	 data_sync_retry  ( boolean )  When set to false, which is the default, PostgreSQL will raise a PANIC-level error on failure to flush modified data files to the filesystem.  Ł Fix possible crash with a SubPlan (sub- SELECT ) within a multi-row  VALUES  list (Tom Lane) Ł Fix crash after FileClose() failure (Noah Misch) This issue could only be observed with  data_sync_retry  enabled, since otherwise FileClose() failure w.  A new server parameter  data_sync_retry  has been added to control this; if you are certain that your kernel does not discard dirty data buffers in such scenarios, you can set  data_sync_retry  to on  to restore the old behavior
include_dir	conf  file can also contain  include_dir  directives, which specify an entire directory of configuration files to include.  These look like include_dir 'directory' 507 . conf : include_dir 'conf
