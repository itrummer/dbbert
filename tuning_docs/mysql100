filenr,sentence
0,5 MySQL Performance Tuning – 5 Steps for Database Developers - Database Management - Blogs - Quest Community
0,Products
0,View all products
0,Free trials
0,Buy online
0,Product lines
0,ApexSQL
0,Change Auditor
0,Enterprise Reporter
0,Foglight Database Monitoring
0,Foglight Evolve
0,KACE
0,Metalogix
0,Migration Manager
0,NetVault Backup
0,One Identity
0,QoreStor
0,Quest On Demand
0,Rapid Recovery
0,Recovery Manager
0,RemoteScan
0,SharePlex
0,Spotlight
0,Stat
0,Toad
0,Featured products
0,Cloud Management
0,Cloud Access Manager
0,"Foglight for Virtualization, Enterprise Edition"
0,Identity Manager
0,On Demand Migration for Email
0,Quest On Demand
0,Rapid Recovery
0,Data Protection
0,Foglight for Virtualization
0,NetVault
0,NetVault for Office 365
0,QorePortal
0,QoreStor
0,Rapid Recovery
0,vRanger
0,Database Management
0,Foglight for Databases
0,Litespeed for SQL Server
0,SharePlex
0,Spotlight SQL Server Enterprise
0,Toad Data Point
0,Toad DevOps Toolkit
0,Toad Edge
0,Toad for Oracle
0,Toad for SQL Server
0,Identity & Access Management
0,Active Roles
0,Defender
0,Identity Manager
0,Identity Manager Data Governance
0,One Identity Safeguard
0,Password Manager
0,Privileged Access Suite for Unix
0,Starling Connect
0,Starling Two-Factor Authentication
0,syslog-ng
0,Microsoft Platform Management
0,Active Administrator
0,Change Auditor
0,Enterprise Reporter
0,GPOADmin
0,InTrust
0,Metalogix
0,Migration Manager
0,On Demand Migration for Email
0,Quest On Demand
0,Recovery Manager
0,Performance Monitoring
0,Foglight Capacity Director
0,Foglight Hybrid Cloud Manager
0,Foglight for Databases
0,Foglight for Operating Systems
0,Foglight for Oracle
0,Foglight for PostgreSQL
0,Foglight for SQL Server
0,Foglight for Storage Management
0,Foglight for Virtualization
0,Spotlight on SQL Server
0,Unified Endpoint Management
0,Desktop Authority Management Suite
0,KACE Cloud Mobile Device Manager
0,KACE Desktop Authority
0,KACE Privilege Manager
0,KACE Systems Deployment Appliance
0,KACE Systems Management Appliance
0,RemoteScan
0,Solutions
0,View all Solutions
0,Industries
0,Education
0,Energy
0,Federal Government
0,Financial Services
0,Healthcare
0,State & Local Government
0,Platforms
0,Active Directory
0,Cisco
0,DB2
0,Exchange
0,Google
0,Hyper-v
0,Lotus Notes
0,OneDrive for Business
0,Office 365
0,Oracle
0,SAP/Sybase
0,SharePoint
0,SQL Server
0,Teams
0,Unix/Linux
0,VMware
0,Windows Server
0,Cloud Management
0,Data Protection
0,Overview
0,Backup & Recovery
0,Cloud Management
0,Deduplication & Compression
0,Disaster Recovery
0,Office 365 Data Protection
0,Virtualization Management
0,Database Management
0,Overview
0,Administration
0,Cloud Migration
0,Data Preparation and Provisioning
0,Development
0,DevOps
0,Performance Monitoring
0,Replication
0,Supported Platforms
0,IBM DB2
0,MySQL
0,Oracle
0,PostgreSQL
0,SAP Solutions
0,SQL Server
0,GDPR Compliance
0,Identity & Access Management
0,Overview
0,Identity Administration
0,Identity Governance
0,Privileged Access Management
0,AD Account Lifecycle Management
0,Access Control
0,Cloud
0,Log Management
0,Microsoft Platform Management
0,Overview
0,Mergers & Acquisitions
0,Migration & Consolidation
0,Office 365 Migration & Management
0,Security & Compliance
0,Windows Backup & Recovery
0,Supported Platforms
0,Active Directory
0,Cisco
0,Exchange
0,Google
0,Groupwise
0,Lotus Notes
0,Office 365
0,OneDrive for Business
0,SharePoint
0,SQL Server
0,Teams
0,Unix/Linux
0,Windows Server
0,Performance Monitoring
0,Overview
0,Database Performance Monitoring
0,Operating System Monitoring
0,Storage Performance & Utilization Management
0,Supported Platforms
0,Active Directory
0,DB2
0,Exchange
0,Java
0,Hyper-V
0,.NET
0,Oracle
0,SAP/Sybase
0,Storage
0,SQL Server
0,VMware
0,Unified Endpoint Management
0,Overview
0,Endpoint Compliance
0,Endpoint Security
0,Endpoint Visibility
0,Industries
0,Education
0,Healthcare
0,Supported Platforms
0,Internet of things
0,Microsoft® Windows
0,MAC
0,UNIX/LinuX
0,Resources
0,Blogs
0,Blogs A to Z
0,Data Protection
0,Database Management
0,Microsoft Platform Management
0,Performance Monitoring
0,Unified Endpoint Management
0,Customer Stories
0,Documents
0,Events
0,Webcasts
0,Technical Documentation
0,Videos
0,Whitepapers
0,Trials
0,Services
0,Consulting Services
0,Overview
0,Microsoft Platform Services
0,Data Protection Services
0,Unified Endpoint Management
0,Performance Monitoring Services
0,Database Management Services
0,Educational Services
0,Support Services
0,Support
0,Support Home
0,By Product
0,All Products
0,AppAssure
0,Archive Manager
0,Change Auditor
0,Desktop Authority
0,DR Series
0,Foglight
0,KACE
0,Migration Manager
0,NetVault
0,Rapid Recovery
0,SharePlex
0,Toad
0,vRanger
0,Contact Support
0,Overview
0,Customer Service
0,Licensing Assistance
0,Renewal Assistance
0,Technical Support
0,Download Software
0,Knowledge Base
0,My Account
0,My Products
0,My Service Requests
0,My Licenses
0,My Groups
0,My Profile
0,Policies & Procedures
0,Consulting Services
0,Microsoft Platform Management
0,Data Protection
0,Unified Endpoint Management
0,Performance Monitoring
0,Database Management
0,Technical Documentation
0,Educational Services
0,User Forums
0,Video Tutorials
0,Partners
0,Overview
0,Partner Circle Log In
0,Become a Partner
0,Find a Partner
0,Partner Community
0,Communities
0,Home
0,Blogs
0,Data Protection
0,Database Management
0,ITNinja
0,Microsoft Platform Management
0,Performance Monitoring
0,Toad World Blog
0,Unified Endpoint Management
0,Forums
0,All Product Forums
0,Active Administrator
0,Desktop Authority
0,Foglight
0,ITNinja
0,Migration Manager for Active Directory
0,NetVault
0,Rapid Recovery
0,Toad World Forum
0,Social Networks
0,Facebook
0,LinkedIn
0,Twitter@Quest
0,Twitter@QuestSupport
0,YouTube
0,製品情報
0,すべての製品情報を見る
0,Change Auditor
0,Foglight
0,KACE
0,Metalogix
0,Migration Manager
0,Migrator for Notes to SharePoint
0,NetVault Backup
0,On Demand Migration for Email
0,QoreStor
0,Rapid Recovery
0,Recovery Manager
0,SharePlex
0,Spotlight
0,Toad
0,ソリューション
0,すべてのプラットフォームを見る
0,クラウド管理
0,GDPRコンプライアンス
0,データ保護
0,クラウド管理
0,ディザスタリカバリ
0,バックアップとリカバリ
0,Office 365 データ保護
0,仮想化管理
0,重複除外と複製
0,データベース管理
0,DevOps
0,データの準備と分析
0,データベースのクラウド移行
0,データベースパフォーマンス監視
0,データベース管理
0,データベース複製ソフトウェアツール
0,統合エンドポイント管理
0,エンドポイントコンプライアンス
0,エンドポイントセキュリティ
0,エンドポイントの可視化
0,Microsoftプラットフォーム管理
0,ハイブリッドActive Directoryのセキュリティとガバナンス
0,Microsoftプラットフォームの移行計画と統合
0,セキュリティとコンプライアンス
0,情報アーカイブおよびストレージ管理ソリューション
0,Windowsのバックアップとリカバリ
0,Microsoft Serverのパフォーマンスと可用性
0,レポート作成機能
0,グループポリシーと権限
0,パフォーマンス監視
0,サービス
0,コンサルティングサービス
0,Microsoftプラットフォーム管理
0,データ保護
0,統合エンドポイント管理
0,パフォーマンス監視
0,データベース管理
0,トレーニングと認定資格
0,サポートサービス
0,サポート
0,サポートホーム
0,製品で検索
0,All Products
0,AppAssure
0,Archive Manager
0,Change Auditor
0,Desktop Authority
0,DR Series
0,Foglight
0,KACE
0,Migration Manager
0,NetVault
0,Rapid Recovery
0,SharePlex
0,Toad
0,vRanger
0,お問い合わせ
0,すべて
0,カスタマサービス
0,ライセンス アシスタンス
0,更新のアシスタンス
0,技術サポート
0,コミュニティフォーラム
0,ソフトウェアのダウンロード
0,ナレッジベース
0,マイアカウント
0,マイ プロダクト
0,Myサービスリクエスト
0,マイ ライセンス
0,マイ グループ
0,マイ プロファイル
0,ポリシーおよび手順
0,コンサルティングサービス
0,Microsoftプラットフォーム管理
0,データ保護
0,統合エンドポイント管理
0,パフォーマンス監視
0,データベース管理
0,リリースノートおよびガイド
0,教育サービス
0,ビデオチュートリアル
0,トライアル
0,パートナー
0,Partner Circleへのログイン
0,パートナーになる
0,Find a Partner
0,パートナーコミュニティ
0,コミュニティ
0,Quest Community
0,Site
0,Search
0,User
0,Site
0,Search
0,User
0,Blogs
0,Database Management
0,5 MySQL Performance Tuning Tips
0,Data Protection
0,Database Management
0,Microsoft Platform Management
0,Performance Monitoring
0,Unified Endpoint Management
0,Quest
0,More
0,Cancel
0,New
0,5 MySQL Performance Tuning Tips
0,Actions
0,Subscribe by email
0,Posts RSS
0,More
0,Cancel
0,Tags
0,Database Performance
0,MySQL
0,Janis Griffin
0,"Jul 30, 2020"
0,"If you pride yourself on your SQL optimization skills, then this is a good time to advance your MySQL performance tuning."
0,"For one thing, MySQL has spent the last seven years as a very close #2 in the race with #1- Oracle and #3- Microsoft SQL Server, according to DB-Engines."
0,"For another, MySQL is an open source database, something you can’t say about #1 or #3. If you and your fellow database professionals are gravitating toward open source, MySQL is currently the one to beat. It’s available on FreeBSD, Linux, OS X, Solaris and Windows. It supports almost 20 different programming languages, from Ada to Tcl, which means it has something for just about every developer."
0,"And, MySQL has made recent news by changing obsolete terminology written into its source code and documentation. Their creators and engineers are paying attention."
0,The need for MySQL performance tuning
0,"But even with the #2 database, you can still write inefficient SQL statements and launch queries that run slowly. For that, there’s MySQL performance tuning, which I’ll describe in the five tips below."
0,Tip 1: Monitor wait time
0,"How long are your users waiting for a response from the database? MySQL provides wait events and thread states to help you answer that question. By monitoring each step that a SQL statement takes in the database until it sends back a response, you draw a clear picture of where the query spends time. And that’s where you start your MySQL performance tuning."
0,"In early versions of MySQL, you couldn’t look at the Performance_Schema database or the Information_Schema database without causing locking or affecting performance. Nowadays, MySQL offers better information on both current and historical events at the levels of statements, stages and waits. For example, the number of tables in Performance_Schema that you can query for tuning information grew from 17 to 87 between versions 5.5 and 5.7. Version 8 includes a transactional data dictionary for the database objects (you’ll query those in Tip 3) that used to be stored only in underlying files."
0,Query for wait time data
0,Start your MySQL performance tuning with this query on the main tables associated with wait time:
0,"INSERT INTO wta_data SELECTt.processlist_id AS conn_id, t.processlist_user AS user, t.processlist_host AS host, t.processlist_db AS db, LOWER(t.processlist_state) AS state, t.processlist_info AS current_sql, MD5(t.processlist_info) AS current_sql_md5, t.processlist_command AS command, w.event_id AS wait_event_id, w.end_event_id AS wait_event_end_id, w.event_name AS wait_event_name, w.operation AS wait_operation, w.object_schema AS wait_schema, w.object_name AS wait_object_name, w.object_type AS wait_type, w.index_name AS wait_index, s.sql_text AS statement_sql, MD5(s.sql_text) AS statement_sql_md5, CASE WHEN RIGHT(s.digest_text, 3)= '...' THEN 1 ELSE 0 END AS statement_digest_truncated, s.digest AS statement_digest, s.event_id AS statement_event_id, s.end_event_id AS statement_event_end_id, conn.attr_value AS program_nameFROM performance_schema.threads AS t LEFT JOINperformance_schema.events_waits_current AS w ON w.thread_id = t.thread_idAND w.end_event_id IS NULL AND w.event_name <> 'idle'LEFT JOIN performance_schema.events_statements_current AS s ON s.thread_id= t.thread_id AND s.digest IS NOT NULLLEFT JOIN (SELECT processlist_id, GROUP_CONCAT(attr_value ORDER BYattr_name DESC SEPARATOR '~^~') attr_valueFROM performance_schema.session_connect_attrs WHERE attr_name IN('program_name','_client_name') GROUP BY processlist_id ) connON t.processlist_id = conn.processlist_idWHERE t.instrumented = 'YES' AND t.processlist_id <> connection_id()AND (t.PROCESSLIST_COMMAND <> 'Sleep');Notice that the query excludes threads containing a Sleep command and those with a wait event of idle."
0,"To study wait time, run that query repeatedly at some interval — say, one minute — and save the results to a table (here, wta_data)."
0,Find the queries that are spending the most time in the database
0,"Now run a query that groups the data by sql_text, wait_operation and time_in_seconds for each wait event, then sums the total time the query spends."
0,"select w.sql_text, w.wait_operation, w.time_in_seconds, tot.tot_time from(select substr(current_sql, 1, 60) sql_text,"
0,"wait_operation, count(*) time_in_seconds"
0,"from wta_data w group by substr(current_sql, 1, 60), wait_operation) w c(select substr(current_sql, 1,60) sql_text, count(*) tot_time"
0,from wta_data
0,"group by substr(current_sql, 1,60)) totwhere w.wait_operation is not nulland w.sql_text = tot.sql_textorder by tot.tot_time, time_in_seconds;"
0,"Then, examine the query output (this is from a sample database for a rental company):"
0,"The middle two rows reveal that one SQL statement spent 21 seconds on a read and 56 times that long (1183 seconds) on a fetch. The bottom two rows show that another SQL statement spent 39 seconds on a read and 35 times that long (1370 seconds) on a fetch. That’s an awful lot of fetching for not much reading. Clearly, there’s room for improvement."
0,"Analyzing wait time reveals important baseline metrics you’ll need in MySQL performance tuning. In this case, the focus is on the big difference between rows examined and rows sent. Average wait time itself is another useful metric. Baseline metrics allow you to set a tuning target, when you can say, “That’s good enough.” Then you move on to the next performance problem."
0,Tip 2: Review the execution plan
0,"The execution plan reveals the cost of each operation. That helps you find the most expensive steps, such as I/O (rows examined, rows sent), full table scans, index scans, filtering predicates and column data types."
0,MySQL gives you several options for generating an execution plan:
0,Run explain from a command prompt.
0,"Running EXPLAIN FORMAT=JSON generates a JSON-formatted explain plan with extended and partition information, plus detail on the operations including which part of the key was used."
0,"SET OPTIMIZER_TRACE=""ENABLED=ON""; puts the trace in memory to write information to the optimizer_trace table in information_schema. It captures the costs of all the operations that the optimizer considered for the current plan (version 5.6.3 and above)."
0,"MySQL Workbench offers a graphical view of the plan, shown below."
0,"That execution plan shows table name, type of data access, references and the number of rows that the optimizer estimates it will examine when it runs — all useful in MySQL performance tuning."
0,"In general, the earlier in your query that you apply filtering predicates, the less data you discard later. The execution plan makes it easy to see how early or late your query is applying filters. Does your query launch any temporary or filesort activities? If so, the execution plan shows you where they occur so you can try to reduce the amount of data you’re sending to them."
0,"Note the Cost Hint that MySQL Workbench displays (gray box above), advising you that the full table scan on student can be an expensive operation."
0,Tip 3: Gather object information
0,"Next, you’ll want to know as much as possible about the underlying tables in a query. Useful information includes table size, whether it’s really a table or just a view, the cardinality of columns in where clauses, and the use of wildcards and scalar functions."
0,A quick way to gather that information is the mysqlshow --status command.
0,"To review indexes and constraints, run show indexes."
0,"That shows you which columns are in the index, especially the left leading column in a multi-column index."
0,"Can the query even use the index? If your query applies functions on an indexed column, the optimizer won’t use the index at all, and you may need to rewrite the query."
0,"Review the existing keys, constraints and relationships. For queries that join many tables, create an Entity Relationship Diagram (ERD) that shows you how the tables are related to one another."
0,Tip 4: Find the driving table
0,"Your goal in MySQL performance tuning is to drive the query with the table that returns the least data. In other words, you compare the number of rows in the final result set to the number of rows the query examined."
0,"Using the driving table reduces the number of logical reads. To find the driving table, you study Joins (Right, Left, Outer) and filtering predicates to figure out how to filter as early in the query as possible."
0,"Below is a partial execution plan for a query in a university billing application. The red operation shows a high number of rows read (more than 8,700) from the student table. In fact, it’s every row in the table, which is inefficient."
0,Here is a sample query for all students taking a class called “MySQL Performance Tuning”:
0,"SELECT s.fname, s.lname, r.signup_dateFROM student sINNER JOIN registration rON s.student_id = r.student_idINNER JOIN class cON r.class_id = c.class_idWHERE c.name"
0,"= 'MYSQL PERFORMANCE TUNING'AND r.signup_date BETWEENDATE_SUB(@BillDate, INTERVAL 1 DAY) and @BillDateAND r.cancelled = 'N';"
0,"Using SQL diagramming, you determine that class is the driving table. You run the following SQL to add and view two foreign keys:"
0,"ALTER TABLE registration ADD FOREIGN KEY (student_id) REFERENCES student(student_id);ALTER TABLE registration ADD FOREIGN KEY (class_id) REFERENCES class(class_id);SELECT table_name,column_name,constraint_name,referenced_table_name,referenced_column_nameFROM"
0,information_schema.key_column_usageWHERE
0,table_schema = 'csu' AND table_name = 'registration'AND referenced_column_name IS NOT NULL;
0,"With the addition of those foreign key constraints, the optimizer chooses a better execution plan using class as the driving table. Although it’s still running that full table scan, several important numbers are improved:"
0,Before foreign keys
0,After foreign keys
0,Executions
0,28K
0,348K
0,Average Wait Time
0,114 ms
0,4 ms
0,Rows Examined/Sent
0,84K/2
0,1.4K/2
0,"To send the same number of rows, just 1/60th of the rows (1.4/84) now need to be examined. Plus, the ratio of executions to average wait time is vastly improved."
0,"You may find room for even more improvement, for example, by reviewing your choice of indexes to reduce the number of rows examined. In MySQL you can create a covering index, which includes all the columns in the query. That prevents extra I/O because the index can satisfy the query. Or you can create a partial index, say, on a table containing a large varchar column, where the index would be too large to traverse. MySQL supports indexes on a virtual column, which works similarly to a function index in Oracle, except that the data is stored only in the index."
0,"Note, though, that adding indexes doesn’t always improve performance. Fortunately, MySQL Workbench makes it easy to capture metrics before and after you experiment so you can see whether and how you’ve improved things."
0,Tip 5: Identify performance inhibitors
0,"Finally, most of the low-hanging fruit of MySQL performance tuning lies in small programming mistakes that have a big impact:"
0,Cursor or row-by-row processing
0,Parallel query processing — Sometimes it’s better to split a complex report into multiple queries and run them in parallel.
0,Hard-coded hints allow you to execute the query with the plan that you want to test. But hints can become stale or forgotten over time.
0,"Wild cards like * may seem handy, but they just generate I/O, add to CPU workload and thrash memory. Beware of SELECT * (often used by SQL generators such as EMF, LINQ and NHibernate) and the missing where clause."
0,Implicit data conversions are a mismatch of datatypes between the column and a parameter. They impact CPU consumption and can turn off index usage.
0,"Performance is often hampered by mistakes that database developers introduce when they’re rushing to beat a release deadline and feeling pressure to get new features working. Usually, the mistakes are easy to fix, but it does take time and effort to find them."
0,Start tuning the performance of your MySQL databases
0,"While the details vary from one database vendor to another, the five main steps of MySQL performance tuning apply:"
0,Monitor wait time
0,Review the execution plan
0,Gather object information
0,Find the driving table
0,Identify performance inhibitors
0,"As a MySQL database professional or developer, you’ll find that the methodical approach described here will help you get the most out of this open source database."
0,"The Fundamental Guide to SQL Server Query OptimizationWant more background on wait times, execution plans, object information, driving tables and performance inhibitors? Now that you’ve seen how to apply MySQL performance tuning, take a deeper dive into SQL query optimization in general."
0,Learn more about how optimizers use the execution plan. Walk through SQL diagramming. See how a covering index can affect I/O. Follow all 5 tips through a couple of real-world case studies.
0,Download the e-Book
0,sales2
0,7 months ago
0,Above Information is good. Interested one can also visit INDIA Access web for relevant services.
0,Cancel
0,Down
0,Reply
0,More
0,Cancel
0,Mark.Kurtz
0,8 months ago
0,very nice.  Thanks for writing this.
0,Cancel
0,Down
0,Reply
0,More
0,Cancel
0,Phil.Rodas
0,8 months ago
0,As always. Great job Janice!
0,Cancel
0,Down
0,Reply
0,More
0,Cancel
0,Related Content
0,Company
0,About Us
0,Buy
0,Contact Us
0,Careers
0,News
0,Resources
0,Blogs
0,Customer Stories
0,Documents
0,Events
0,Videos
0,Support
0,Professional Services
0,Renew Support
0,Technical Support
0,Training & Certification
0,Support Services
0,Social Networks
0,Facebook
0,Instagram
0,LinkedIn
0,Twitter
0,YouTube
0,© 2021 Quest Software Inc. ALL RIGHTS RESERVED.
0,Legal
0,Terms of Use
0,Privacy
0,Community Feedback & Support
0,会社名
0,会社情報
0,お問い合わせ
0,採用情報
0,ニュース
0,リソース
0,ブログ
0,お客様の事例
0,ドキュメント
0,イベント
0,ビデオ
0,サポート
0,プロフェッショナルサービス
0,サポートの更新
0,テクニカルサポート
0,トレーニングと認定資格
0,サポートサービス
0,ソーシャルネットワーク
0,Facebook
0,Instagram
0,LinkedIn
0,Twitter
0,YouTube
0,© 2021 Quest Software Inc. ALL RIGHTS RESERVED.
0,「法務」
0,ご利用規約
0,個人情報保護方針
0,コミュニティのフィードバックとサポート
1,"MySQL Performance Tuning: Tips, Scripts and Tools"
1,"hayden_james@linuxhayden-james@linux:~$ _home/about/services/feedback/sponsors/contact/MySQL Performance Tuning: Tips, Scripts and Tools August 10, 2020 by Hayden James, in Blog LinuxWith MySQL, common configuration mistakes can cause serious performance problems. In fact, if you misconfigure just one of the many config parameters, it can cripple performance. Of course, the performance of MySQL is often tied to the efficiency of your MySQL queries. It’s important to ensure that your performance issues are not due to poorly written MySQL queries. You can use MySQL’s slow query log, log_queries_not_using_indexes or APM tools which offer MySQL performance monitoring such as Datadog, Instrumental, Panopta, Site24x7, Solarwinds, and other monitoring tools.MySQL tuning is quite an expansive topic. As such, today I won’t try to place any recommended config lines, values, or settings here. Be very cautious with recommended-settings based articles. This post assumes that you’ve already optimized your queries and now seek guidance with selecting the best performance config options (ex: my.cnf) for MySQL. This can vary greatly case by case as there’s very little no one-size-fits-all advice. Therefore, also included below the tips, are additional links to popular free MySQL tuning scripts and tools. Stay up to date with the latest MySQL server versionsWith each new version of MySQL released, there’s substantial performance and feature enhancements over previous versions. So the most important advice would be to upgrade, upgrade, upgrade. Have a look at some version performance comparisons here.If you are seeking additional features or flexibility you may already be using MariaDB, or Percona, which are enhanced drop-in replacements for MySQL Server. If you’ve seen notable improvements in using MariaDB or Percona over stock MySQL please share your experience below. They are both great options. MySQL Performance Tuning AdviceBefore continuing please have a look at the following MySQL performance tuning articles: MySQL Database Performance: Avoid this common mistake and note that due to the limitations of MySQL query cache, it has been deprecated as of MySQL 5.7.20 and is removed in MySQL 8.0.Other than the tuning scripts listed below, try to avoid online advice unless its via mysql.com or those that reference directly MySQL, Pecona’s, or MariaDB articles or documentation. You will notice that both of the above blog posts reference or quote MySQL’s docs. There’s a ton of conflicting advice and opinions online. As such, my advice is to always crosscheck your config changes with official documentation. This includes everything I say here. In fact, when venturing to change MySQL’s defaults, unless you have a basis for changes, its best to leave it as is. When there’s doubt… stick with the defaults.  Always, base your changes on benchmarks, comparisons, and time-tested firsthand data. Selecting MySQL Storage EngineThis is simple, use InnoDB and avoid MyISAM when possible. For these reasons:Versions of MySQL 5.5 and greater have switched to the InnoDB engine to ensure referential integrity constraints and higher concurrency.InnoDB has better crash recovery.InnoDB has row-level locking, MyISAM can only do full table-level locking.Like MyISAM, InnoDB now has FULLTEXT search indexes as of MySQL 5.6InnoDB supports transactions, foreign keys and relationship constraints, MyISAM does not. MySQL Performance Tuning ScriptsYou cannot replace Professional MySQL tuning with scripts. Scripts serve as basic guides, sometimes spot-on, but most times loose guides which will only solve the most grievous misconfigured parameters. Use them as a starting point. Meaning, before you contact a professional to tune MySQL use these tuning scripts so that at the very least you don’t have any so-called embarrassing config in your my.cnf file. For example, join_buffer_size set to 4GB when the total DB size is less than 1GB.Now, let’s look at popular scripts and tools available for MySQL performance tuning: MySqlTuner, Tuning-Primer, MySQLreport, Percona Toolkit, and phpMyAdmin Advisor. MySQLTunerA script thas is written in Perl that will assist you with your MySQL configuration and make recommendations for increased performance and stability.MySQLTuner is maintained and indicator collect is increasing week after week supporting a lot of configurations such as Galera Cluster, TokuDB, Performance schema, Linux OS metrics, InnoDB, MyISAM, Aria, etc. – MySQLTuner on Github. Tuning-PrimerThis script takes information from “SHOW STATUS LIKE…” and “SHOW VARIABLES LIKE…” to produce sane recommendations for tuning server variables. It is compatible with all versions of MySQL 3.23 and higher (including 5.1).The original script is no longer updated. I’ve been using this Tuning-primer version on Github which fully supports MariaDB. Percona ToolkitPercona Toolkit is a collection of advanced open-source command-line tools, developed to perform a variety of MySQL tasks that are too difficult or complex to perform manually – freeing your DBAs for work that helps you achieve your business goals.Useful tools include: pt-align, pt-archiver, pt-config-diff, pt-deadlock-logger, pt-diskstats, pt-duplicate-key-checker, pt-fifo-split, pt-find, pt-fingerprint, pt-fk-error-logger, pt-heartbeat, pt-index-usage, pt-ioprofile, pt-kill, pt-mext, pt-mongodb-query-digest, pt-mongodb-summary, pt-mysql-summary, pt-online-schema-change, pt-pg-summary, pt-pmp, pt-query-digest, pt-secure-collect, pt-show-grants, pt-sift, pt-slave-delay, pt-slave-find, pt-slave-restart, pt-stalk, pt-summary, pt-table-checksum, pt-table-sync, pt-table-usage, pt-upgrade, pt-variable-advisor and pt-visual-explain. phpMyAdmin AdvisorThe Advisor system provides recommendations on server variables by analyzing MySQL status variables.phpMyAdmin is a free software tool written in PHP, intended to handle the administration of MySQL over the Web. Visit: phpmyadmin. MysqlreportMysqlreport transforms the values from SHOW STATUS into an easy-to-read report that provides an in-depth understanding of how well MySQL is running. mysqlreport is a better alternative (and practically the only alternative) to manually interpreting SHOW STATUS. Tags: apm, mariadb, mysql, performance, scripts, sysadminsDownload my free 101 Useful Linux Commands (PDF).Also, I'll notify you when new Linux articles are published. - Subscribe now and receive my free PDF. (Average of 1 or 2 emails per month, sent only on Mondays.)← 60 Linux Networking commands and scriptsLinux Commands frequently used by Linux Sysadmins – Part 1 →Popular Topicssysadmins, linux, performance, server, apm, security, debian, hosting, CentOS, phpWith over 4 million readers, this blog features Linux server administration and Linux server performance articles.Most Popular Articles100 Top Server Monitoring & APM Solutions60 Linux Networking commands and scripts90 frequently used Linux CommandsBest Linux Distros for DesktopHome Lab Beginners guide – HardwareUnderstanding PHP memory_limit“MySQL server has gone away” – Solution(s)iowait – How does it affect Linux performance?Almost Always Add Swap SpaceHow to Securely Copy Files Using SCP examplesRecent Articles 50 Top Server Monitoring, APM and Observability Tools Kali Linux non-root and no pen-test tools install w/ screenshots Arch Linux installer, Project POCKIT and more | biweekly #6 Using the find command in Linux with examples Improving OpenVPN performance and throughput BeagleV, Asahi Linux, JingPad A1, and more | biweekly #5 85% of all Smartphones are powered by Linux Raspberry Pi Performance: Add ZRAM and these Kernel Parameters Replacing Cloudflare with CSF Firewall PHP 8 Compatibility Check and Performance Tips.© 2021 Hayden James. Privacy Policy, Terms. Connect: Twitter, Linkedin, Newsletter. >_"
3,MySQL Query Performance Optimization Tips | Section
3,Platform
3,select_all Edge AppSpace
3,SolutionHub
3,Performance / CDN
3,Security
3,Virtual Waiting Room
3,A/B Testing
3,Search AppSpace
3,AppStack
3,Node.js Edge Hosting
3,RunStack
3,Containers
3,Serverless
3,gps_fixed Core Platform
3,Section Control Plane
3,Edge AppSpace
3,Adaptive Edge Engine (AEE)
3,Global Edge Network
3,Solutions
3,SaaS
3,PaaS & Hosting Providers
3,Edge App Hosting
3,Docs
3,Resources
3,Blog
3,Case Studies
3,Edge Content Library
3,Solution Briefs
3,Product Videos
3,Engineering Education
3,About Section
3,Partners
3,Changelog
3,Pricing
3,Contact
3,Log In
3,Get Started
3,Platform
3,dvrEdge Solutions
3,SaaS
3,PaaS & Hosting Providers
3,Edge App Hosting
3,select_allEdge Services
3,Performance / CDN
3,Security
3,Virtual Waiting Room
3,<<<<<<< HEAD
3,HTTP/2
3,businessUse Cases
3,Enterprise
3,Ecommerce
3,SaaS
3,Gaming
3,IoT / IIoT
3,Magento
3,WordPress
3,Drupal
3,BigCommerce
3,securityApplication Security
3,Web Application Firewall
3,IP Blocking
3,SSL Certificates
3,DDoS Mitigation
3,Bad Bot Management
3,Content Security Policy
3,=======
3,A/B Testing
3,Node.js
3,View All Modules
3,>>>>>>> 7da9c59409ee517c21a0018dd10dd79484e2a3c8
3,gps_fixedCore Platform
3,Adaptive Edge Engine (AEE)
3,Observability
3,Global Edge Network
3,DevOps Tooling
3,Docs
3,Resources
3,Blog
3,Case Studies
3,Content Library
3,Solution Briefs
3,Changelog
3,Engineering Education
3,Partners
3,About Section
3,Pricing
3,Contact
3,Log In
3,Get Started
3,MySQL Query Performance Optimization Tips
3,"October 1, 2020"
3,"This article goes through common tips for optimizing MySQL queries. As data volume in your database grows, retrieving data from the database and other database operations become complex. This also requires more computing resources."
3,Introduction
3,"Most applications are database driven. Poorly designed SQL queries can significantly downgrade the performance of database-driven applications. According to this MySQL developers guide, you can be proactive and plan for optimizations or troubleshoot queries and configurations after experiencing problems. This article will focus on the optimization of individual SQL statements and database structure."
3,Prerequisites
3,"To fully understand this article, prior knowledge of the MySQL database is essential. An understanding of different SQL queries and how they work is critical. Free MySQL tutorials for beginners are available on MySQL tutorial and tutorials point."
3,Benefits of MySQL Database Queries Optimization
3,We optimize for speed and resources. Optimized queries can run faster and require less computing power.
3,Tips for MySQL Queries Optimization
3,Fast queries are about response time. The goal is to have queries return the required result in the shortest time possible. How much time does a query take to execute? Most of the tools used to query a MySQL database give details on time taken to run a query.
3,"The most straightforward query cost metrics used in MySQL are query response time, the number of rows scanned, and the number of rows returned. The more the number of rows read, the higher the cost of the query. The screenshots below show the time taken to run a query in both CLI and MySQL Workbench."
3,Optimizing Queries with EXPLAIN
3,"The EXPLAIN statement provides information about how MySQL executes a statement. According to the MySQL documentation, EXPLAIN works alongside SELECT, DELETE, INSERT, REPLACE, and UPDATE statements. It displays information from a built-in MySQL optimizer regarding the statement execution plan and the number of rows scanned in each table. Thus we can determine the cost of the query. The query below shows how EXPLAIN works with the SELECT statement."
3,EXPLAIN SELECT * FROM world_x.city LIMIT 5000;
3,MySQL EXPLAIN query output
3,MySQL Query Log
3,"In MySQL, slow queries are logged in an built-in query log. Once you find the slow queries in the query log, use the EXPLAIN statement to determine why the queries are slow and optimize them."
3,Optimizing Database Schema
3,"The database structure is very crucial in performance optimization. There are several ways in which we can optimize database structure, including:"
3,"Limiting the number of columns: MySQL has a limit of 4096 columns per table. Use fewer columns for better performance. If possible, do not use more than a hundred columns unless your business logic requires that. Tables with more columns require more CPU time to process."
3,"Normalize Tables: Normalizing keeps all data non-redundant. The database that is in this state is called 3NF (third normal form). The 3NF ensures that lengthy values such as names, addresses, categories, and contact details are not repeated. Instead, they are represented as IDs across multiple smaller tables. For more details on database normalization, refer here."
3,Use the Most Appropriate Data Types: There are more than 20 different data types in MySQL designed for different uses.
3,"Some of the data types include Timestamp, DateTime, Integer, ENUM, Float, Double, Char, LongText, and Text. Tables should be designed to minimize space used on a disk. Tables that occupy less disk space results in smaller indexes that can be processed in a shorter duration. For example, if a table will host less than 100 records, you should use the TINYINT data type for the unique ID as it takes less space than INT."
3,"Avoid Null Values. Declare columns to be NOT NULL where possible. This enables better use of indexes. NULL values increase the processing power needed for testing whether each value is NULL, making SQL operations slower."
3,Use Indexes
3,"Think of records as content in a book. If you want to learn on a particular subtopic, you would go to the index pages, look for the subtopic you want, then get the page where the subtopic is. Indexes work the same way. They are used to find rows with specific column values much faster. Without using an index, MySQL must begin searching in the first row and go through the whole table to find the required records. Tables with a huge amount of data are more costly to query."
3,"With the use of an index, MySQL can faster determine the position to seek in the middle of the data file. This is done without going through all the rows and is much faster than reading every row sequentially. Refer to the MySQL developer guide for more information on indexes."
3,"You can create a single-column or multiple column indexes, as shown below, respectively."
3,CREATE INDEX tablename_columnname_idx ON tablename (columnname);
3,"CREATE INDEX tablename_column1name_column2name_idx ON tablename (column1name, column2name);"
3,Use Wildcards at the End of a Phrase
3,"In MySQL, wildcards are used in conjunction with the LIKE operator and NOT LIKE operator. They are used to search for data matching some search criteria. You can learn more about wildcards here."
3,"Wildcards result in the most expansive scan when searching for data, which is very inefficient. Leading wildcards are the most inefficient, especially when combined with ending wild cards. In such a case, MySQL has to search all the records for a match. Thus you should avoid leading wild cards. See the queries below, one is using a leading wildcard and another one is using an ending wildcard."
3,SELECT * FROM city WHERE name LIKE '%Al%';
3,SELECT * FROM city WHERE name LIKE 'Al%';
3,Specify Columns in SELECT Function
3,"SELECT * (select all) is used as a shortcut to query all columns available in a table. This requires more resources than using a SELECT statement with only the columns you need for that specific query. For example, a customer table with 20 different columns and a hundred thousand entries. If you want to select a city with ID and Name only; try to use"
3,"SELECT ID, Name, District FROM city;"
3,instead of
3,SELECT * FROM city;
3,The second example will take more time to run to completion.
3,Avoid SELECT DISTINCT
3,"DISTINCT is used to remove duplicate rows with SELECT statements. The DISTINCT command requires more sorting and reading of the database, which requires more processing power. DISTINCT can be replaced with GROUP BY to get the same results. See the two queries below."
3,"SELECT col1, col2 FROM table GROUP BY col1, col2;"
3,"SELECT DISTINCT col1, col2 FROM table;"
3,Use LIMIT
3,"Sometimes we need a specified number of rows from a result set. The LIMIT clause is used in the query to specify the number of rows instead of fetching the whole result set. Fetching the entire result set requires more resources compared to fetching a specified number of rows. See the queries below, one without LIMIT, another one with the LIMIT clause."
3,"SELECT ID, Name, District FROM city;"
3,"SELECT ID, Name, District FROM city LIMIT 10;"
3,MySQL Query Caching
3,"MySQL Query Caching provides database caching functionality. The SELECT statement text and the retrieved result are stored in the cache. When you make a similar query to the one already in the cache, MySQL will respond and give a query already in the cache. In this way, fewer resources are used, and your query runs faster."
3,"This works best with a database where more select queries are made. Once the table is updated, the cached query and result become invalid. Thus, caching may not work with an application that updates the table frequently."
3,The command below is used to check if query cache is enabled in MySQL.
3,SHOW VARIABLES LIKE 'have_query_cache';
3,"If the query cache is not set, set the query cache by following guidelines on MySQL Documentation."
3,Converting OUTER JOINs to INNER JOINs
3,"An INNER JOIN returns rows that contain columns from both tables. Unlike INNER JOIN, OUTER JOIN returns rows where no matches have been found on both tables. Therefore, OUTER JOIN does more work than INNER JOIN, increasing total execution time."
3,Use INNER JOIN whenever possible. It would be a waste of performance to use OUTER JOIN when you don’t need the data outside specified columns. We have a sample database with two tables as follows:
3,"student - student_id, first_name, last_name"
3,"orders - id, date, amount, customer_id"
3,An INNER JOIN query to the table would be as it is shown below.
3,SELECT
3,"C.id, C.name, O.amount, O.date FROM customers C"
3,INNER JOIN orders O ON O.customer_id = C.id;
3,An OUTER JOIN query to the table would be as it is shown below.
3,"NOTE: MySQL does not support FULL OUTER JOIN, but other SQL dialects such as PostgreSQL do."
3,SELECT
3,"C.id, C.name, O.amount, O.date FROM customers C"
3,FULL OUTER JOIN orders O ON O.customer_id = C.id;
3,Optimize LIKE Statements with UNION Clause
3,"The OR operator is used to combine two Boolean expressions and return true when either of the conditions is met. When using comparison operator ‘or’ in a query, MySQL optimizer may incorrectly choose a full table scan to retrieve the result set. This makes the query run slower. A UNION clause runs faster and gives the same result."
3,Consider the query below:
3,SELECT * FROM city WHERE Name LIKE 'C%' or District LIKE 'C%';
3,"Below are the optimized versions of the query above using the UNION ALL and UNION operators, respectively."
3,SELECT * FROM city WHERE Name LIKE 'C%' UNION ALL SELECT * FROM city WHERE District LIKE 'C%';
3,SELECT * FROM city WHERE Name LIKE 'C%' UNION SELECT * FROM city WHERE District LIKE 'C%';
3,"In the first query above, we used UNION ALL, while the second one, we have used UNION. By default, UNION returns distinct rows while UNION ALL allows duplicate rows. UNION generally runs faster than UNION ALL."
3,Conclusion
3,MySQL development is ongoing. More tips to optimize queries are developed every day. This article is a guide on how to make better queries and make more stable database applications. Query with no doubt.
3,Peer Review Contributions by: Gregory Manley
3,About the author
3,Benson Kariuki
3,"Benson Kariuki is a graduate computer science student. He is a passionate and solution-oriented computer scientist. His interests are Web Development with WordPress, Big Data, and Machine Learning."
3,This article was contributed by a student member of Section's Engineering Education Program. Please report any errors or innaccuracies to enged@section.io.
3,Want to learn more about the EngEd Program?
3,Discover Section's community-generated pool of resources from the next generation of engineers.
3,Learn more
3,QUICK LINKS // More Section offerings
3,Edge Modules
3,Varnish Cache
3,Nginx/Lua
3,SiteSpect
3,Optidash
3,Cloudinary
3,ModSecurity
3,SignalSciences
3,ThreatX
3,Wallarm
3,Snapt
3,PerimeterX
3,Radware Bot Manager
3,Content Security Policy
3,Virtual Waiting Room
3,Hugo
3,Node.js
3,Custom Workload
3,View All Modules
3,DevOps
3,Real Time Metrics
3,Log Management
3,Real User Monitoring
3,Instant Global Deployments
3,Developer PoP
3,Instant Cache Purge
3,Managed SSL Certificates
3,APIs
3,Endpoints
3,Global Edge Network
3,Custom Edge Network
3,Private Edge Network
3,Origin PoP
3,Performance & Scalability
3,Dynamic Content Caching
3,Static Asset Caching
3,HTML Streaming
3,Anonymous Page Caching
3,Image Optimization
3,Mobile Optimization
3,Virtual Waiting Room
3,HTTP/2
3,Edge Delivery
3,Load Balancing
3,Maintenance Pages
3,Anycast DNS Hosting
3,SSL Certificates
3,Static Site Deployment
3,Application Security
3,Web Application Firewall
3,IP Blocking
3,SSL Certificates
3,DDoS Mitigation
3,Bad Bot Management
3,Content Security Policy
3,Use Cases
3,SaaS
3,PaaS & Hosting Providers
3,Edge App Hosting
3,Enterprise
3,E-Commerce
3,Gaming
3,IoT/IIoT
3,BigCommerce
3,Magento
3,WordPress
3,Drupal
3,Join our Slack community
3,Add to Slack
3,Company
3,About
3,Careers
3,Legals
3,Resources
3,Blog
3,Case Studies
3,Content Library
3,Solution Briefs
3,Partners
3,Changelog
3,Support
3,Docs
3,Community Slack
3,Help & Support
3,Platform Status
3,Pricing
3,Contact Us
3,Section supports many open source projects including:
3,© 2020 Section
3,Privacy Policy
3,Terms of Service
5,MySQL 101: Parameters to Tune for MySQL Performance - Percona Database Performance Blog
5,Percona Live
5,About Us
5,Contact Us
5,Services
5,Support
5,MySQL Support
5,MongoDB Support
5,MariaDB Support
5,PostgreSQL Support
5,DBaaS Support
5,High Availability Support
5,Flexible Pricing
5,Support Tiers
5,Technical Account Managers
5,Managed Services
5,Percona Managed Database Services
5,Percona Advanced Managed Database Service
5,Consulting
5,Percona Cloud Cover
5,Percona Open Source Advance
5,Percona and Microsoft Azure Partnership
5,Policies
5,Training
5,Products
5,MySQL Database Software
5,Percona Distribution for MySQL
5,Percona Server for MySQL
5,Percona XtraDB Cluster
5,Percona XtraBackup
5,MongoDB Database Software
5,Percona Distribution for MongoDB
5,Percona Server for MongoDB
5,Percona Backup for MongoDB
5,PostgreSQL Database Software
5,Percona Monitoring and Management
5,Percona Kubernetes Operators
5,Open Source Database Tools
5,Percona Toolkit
5,Percona DBaaS Command Line Tool
5,Solutions
5,Eliminate Vendor Lock-In
5,Embrace the Cloud
5,Optimize Database Performance
5,Reduce Costs and Complexity
5,Resources
5,Calculators
5,2020 Survey Results
5,Solution Briefs
5,White Papers
5,Webinars
5,Case Studies
5,Datasheets
5,Ebooks
5,Videos
5,Technical Presentations
5,Documentation
5,About
5,About Percona
5,Contact Us
5,Customers
5,Careers
5,Percona Lifestyle
5,In The News
5,Percona Live
5,Events
5,Community
5,Forums
5,Community Blog
5,PMM Community Contributions
5,MySQL 101: Parameters to Tune for MySQL Performance
5,Back to the Blog
5,Jun
5,2020
5,Brian Sumpter2020-07-02T12:09:44-04:00
5,By Brian Sumpter
5,"Insight for DBAs, Monitoring, MySQL"
5,"insight for DBAs, Monitoring, MySQL, mysql-and-variants"
5,3 Comments
5,"While there is no magic bullet for MySQL tuning, there are a few areas that can be focused on upfront that can dramatically improve the performance of your MySQL installation. While much information has been published on this topic over the years, I wanted to break down some of the more critical settings that anyone can implement with no guesswork required."
5,"Depending on the version of MySQL you are running, some of the default values used in this post may differ from your install, but the premise is still largely the same."
5,Initial MySQL performance tuning can be broken down to the following categories:
5,Tuning for your hardware
5,Tuning for best performance / best practices
5,Tuning for your workload
5,Tuning MySQL for Your Hardware
5,"Depending on the hardware you have installed MySQL on, some variables need to be set based on the machine (or VM) specifications. The following variables are largely dependent on your hardware:"
5,innodb_buffer_pool_size
5,"Generally, set to 50% – 70% of your total RAM as a starting point."
5,It does not need to be set any larger than the total database size.
5,"Percona Monitoring and Management (PMM) can offer additional insight, showing your buffer pool usage and allowing you to tune accordingly."
5,innodb_log_file_size
5,This is generally set between 128M – 2G.
5,Should be large enough to hold at most an hour or so of logs.
5,This is more than enough so that MySQL can reorder writes to use sequential I/O during the flushing and checkpointing processes.
5,"PMM can offer additional insight, as if you are using more than 50% of your log space, you may benefit from a log file size increase."
5,innodb_flush_log_at_trx_commit
5,Setting to “1” (default in 5.7) gives the most durability.
5,"Setting to “0” or “2” will give more performance, but less durability."
5,innodb_flush_method
5,Setting this to O_DIRECT will avoid a performance penalty from double buffering.
5,MySQL Tuning for Best Performance/Best Practices
5,innodb_file_per_table
5,Setting this to “ON” will generate an independent InnoDB table space for every table in the database.
5,innodb_stats_on_metadata
5,Setting this to “OFF” avoids unnecessary updating of InnoDB statistics and can greatly improve read speeds.
5,innodb_buffer_pool_instances
5,"A best practice is to set this to “8” unless the buffer pool size is < 1G, in which case set to “1”."
5,query_cache_type & query_cache_size
5,Setting both of these to “0” will entirely disable the query cache.
5,Tuning for Your Workload
5,"To tune further, more information will be required. The best way to gather this information is to install a MySQL monitoring / graphing tool like Percona Monitoring and Management platform. Once you have a tool installed, we can dive into the individual metrics and start customizing based on the data."
5,"I would recommend starting with one of the most impactful variables – the innodb_buffer_pool_size.  Compare the RAM and number of free pages on your instance to the total buffer pool size. Based on these metrics, you can determine if you need to increase or decrease your overall buffer pool size setting."
5,"Next, take a look at your metrics for the InnoDB Log File usage. The rule of thumb is that your log files should hold approximately one hour of data. If you see that your data written to the log files hourly exceeds the total size of the log files, you would want to increase the innodb_log_file_size variable and restart MySQL. You could also verify with “SHOW ENGINE INNODB STATUS;” via the MySQL CLI to assist in calculating a good InnoDB log file size."
5,Other Settings
5,Other InnoDB settings that can be further tuned for better performance are:
5,innodb_autoinc_lock_mode
5,Setting this to “2” (interleaved mode) can remove the need for an auto-inc lock (at the table level) and can increase performance when using multi-row insert statements to insert values into a table with an auto increment primary key. Note that this requires either ROW or MIXED binlog format.
5,innodb_io_capacity / innodb_io_capacity_max
5,"These settings will impact your database if you are utilizing a write-heavy workflow. This does not apply to read (SELECT) traffic. To tune these values, it is best to know how many iops your system can perform. It is a good idea to run sysbench or another benchmark tool to determine your storage throughput."
5,"PMM can offer additional insight, showing your IO usage and allowing you to tune accordingly."
5,In Summary
5,"While this is by no means a comprehensive article on MySQL tuning, the suggestions above should clear some of the low hanging fruit and get your system closer to an ideal setup. As with all database tuning, your process should be an ongoing one based on current information."
5,"Examine the settings proposed above, and implement if they make sense for your environment/workload."
5,Install a good monitoring tool to give insight into the database (Percona Monitoring and Management is our suggestion).
5,Stay current on your monitoring graphs to determine other areas where you may need to tune.
5,Our solution brief “Get Up and Running with Percona Server for MySQL” outlines setting up a MySQL® database on-premises using Percona Server for MySQL. It includes failover and basic business continuity components.
5,Download PDF
5,Related
5,Author
5,Brian Sumpter
5,Share this post
5,FacebookTwitterLinkedInEmail
5,Comments (3)
5,Jim Tommaney
5,Reply
5,"Hi Brian,"
5,Great write-up!
5,Impact of innodb_stats_on_metadata was completely new to me.
5,"We also evaluated innodb_io_capacity and innodb_read_ahead_threshold at various settings, no measurable benefit for query workloads that I could find either."
5,We found good results with changing buffer_pool instances and read_io_threads together in our environment:
5,"Cloud storage, analytic queries, concurrent workload, parallel query enabled (Ali Cloud), 64 cores, o_direct."
5,innodb_buffer_pool_instances (change from 8 to 32)
5,innodb_read_io_threads (change from 4 to 16)
5,Under a 100% physical I/O workload this resulted in a 4x speedup.
5,"You mileage will vary, test before production usage."
5,"Most query workloads are not 100% PIO, so actual benefits likely much lower."
5,"Cheers,"
5,Jim
5,"June 30, 2020 at 2:08 pm"
5,Jie Zhou
5,Reply
5,Why innodb_buffer_pool_instances has a best practice to set it to “8”
5,"July 1, 2020 at 12:04 pm"
5,Brian Sumpter
5,Reply
5,"Hi Jie Zhou. The MySQL default is to set innodb_buffer_pool_instances to 8 (in MySQL version 5.7 and up) as this is a good starting point for most general use cases. As per the MySQL documentation, this option takes effect only when you set innodb_buffer_pool_size to a size of 1GB or more. The total size you specify is divided among all the buffer pools. For best efficiency, specify a combination of innodb_buffer_pool_instances and innodb_buffer_pool_size so that each buffer pool instance is at least 1GB."
5,"July 8, 2020 at 1:11 pm"
5,Leave a Reply					Cancel reply
5,How Can We Help?
5,"Percona's experts can maximize your application performance with our open source database support, managed services or consulting."
5,Contact us
5,Subscribe Want to get weekly updates listing the latest blog posts? Subscribe now and we'll send you an update every Friday at 1pm ET.
5,Subscribe to our blog
5,CategoriesMySQL(3402)Insight for DBAs(1601)Percona Software(1553)Percona Events(875)MongoDB(571)Insight for Developers(493)Benchmarks(345)Percona Live(336)Webinars(301)Cloud(297)PostgreSQL(189)Monitoring(185)MariaDB(159)Percona Services(154)Security(130)ProxySQL(130)Hardware and Storage(106)Storage Engine(56)Database Trends(55)Percona Announcements(12)   Percona Blog RSS Feed
5,Upcoming WebinarsOptimize and Troubleshoot MySQL using PMM
5,MongoDB Backups Overview
5,Introduction to pg_stat_monitor
5,Moving your Database to the Cloud: Top 3 Things to Consider
5,What’s Old Is New; What’s Coming Is Here: Percona Offerings for MySQL 5.6 and DBaaS in PMM
5,All Webinars
5,Services
5,Support
5,Managed Services
5,Consulting
5,Training
5,Products
5,MySQL Software
5,MongoDB Software
5,PostgreSQL Distribution
5,Kubernetes
5,Monitoring & Management
5,Resources
5,Solution Briefs
5,White Papers
5,Webinars
5,Case Studies
5,Datasheets
5,Documentation
5,More
5,Blog
5,Community Blog
5,Technical Forum Help
5,About
5,Customers
5,Newsroom
5,About
5,Careers
5,Contact Us
5,Sales & General Inquiries
5,(888) 316-9775 (USA)
5,(208) 473-2904 (USA)
5,+44 203 608 6727 (UK)
5,0-808-169-6490 (UK)
5,0-800-724-4569 (GER)
5,"MySQL, InnoDB, MariaDB and MongoDB are trademarks of their respective owners. Proudly running Percona Server for MySQL"
5,Terms of Use |
5,Privacy |
5,Copyright |
5,Legal
5,Copyright © 2006-2021 Percona LLC.
6,Performance tips  |  Cloud SQL for MySQL  |  Google Cloud
6,Why Google
6,close
6,Transform your business with innovative solutions
6,"Whether your business is early in its journey or well on its way to digital transformation, Google Cloud's solutions and technologies help solve your toughest challenges."
6,Learn more
6,Why Google Cloud
6,Choosing Google Cloud
6,Reasons why businesses choose us.
6,Multicloud
6,Run your apps wherever you need them.
6,Trust and security
6,Keep your data secure and compliant.
6,Global infrastructure
6,Build on the same infrastructure Google uses.
6,Data analytics
6,Make smarter decisions with the leading data platform.
6,Open cloud
6,"Scale with open, flexible technology."
6,Sustainability
6,Run on the cleanest cloud in the industry.
6,Analyst reports
6,See how Google Cloud ranks.
6,Customer stories
6,Learn how businesses use Google Cloud.
6,Google Cloud Blog
6,Read the latest story and product updates.
6,Solutions
6,close
6,Industry Solutions
6,"Reduce cost, increase operational agility, and capture new market opportunities."
6,Retail
6,Analytics and collaboration tools for the retail value chain.
6,Consumer Packaged Goods
6,Solutions for CPG digital transformation and brand growth.
6,Financial Services
6,"Computing, data management, and analytics tools for financial services."
6,Healthcare and Life Sciences
6,Health-specific solutions to enhance the patient experience.
6,Media and Entertainment
6,Solutions for content production and distribution operations.
6,Telecommunications
6,Hybrid and multi-cloud services to deploy and monetize 5G.
6,Gaming
6,AI-driven solutions to build and scale games faster.
6,Manufacturing
6,Migration and AI tools to optimize the manufacturing value chain.
6,Supply Chain and Logistics
6,Digital supply chain solutions built in the cloud.
6,Government
6,"Data storage, AI, and analytics solutions for government agencies."
6,Education
6,Teaching tools to provide more engaging learning experiences.
6,Small and Medium Business
6,"Explore SMB solutions for web hosting, app development, AI, analytics, and more."
6,Not seeing what you're looking for?
6,See all solutions
6,Application Modernization
6,"Develop and run applications anywhere, using cloud-native technologies like containers, serverless, and service mesh."
6,Hybrid and Multi-cloud Application Platform
6,Platform for modernizing legacy apps and building new apps.
6,Cloud-Native App Development
6,"End-to-end solution for building, deploying, and managing apps."
6,Serverless Solutions
6,"Fully managed environment for developing, deploying and scaling apps."
6,DevOps
6,Processes and resources for implementing DevOps in your org.
6,Continuous Delivery (CD)
6,End-to-end automation from source to production.
6,Continuous Integration (CI)
6,Fast feedback on code changes at scale.
6,Mainframe Modernization
6,Automated tools and prescriptive guidance for moving to the cloud.
6,Hosting
6,Services and infrastructure for building web apps and websites.
6,Artificial Intelligence
6,Add intelligence and efficiency to your business with AI and machine learning.
6,Build and Use AI
6,Products to build and use artificial intelligence.
6,Contact Center AI
6,AI model for speaking with customers and assisting human agents.
6,Document AI
6,Machine learning and AI to unlock insights from your documents.
6,Cloud Talent Solution
6,AI with job search and talent acquisition capabilities.
6,Business Application Platform
6,"Speed up the pace of innovation without coding, using APIs, apps, and automation."
6,New Business Channels Using APIs
6,Attract and empower an ecosystem of developers and partners.
6,Unlocking Legacy Applications Using APIs
6,Cloud services for extending and modernizing legacy apps.
6,Open Banking APIx
6,Simplify and accelerate secure delivery of open banking compliant APIs.
6,Databases
6,"Migrate and manage enterprise data with security, reliability, high availability, and fully managed data services."
6,Database Migration
6,Guides and tools to simplify your database migration life cycle.
6,Database Modernization
6,Upgrades to modernize your operational database infrastructure.
6,Google Cloud Databases
6,"Database services to migrate, manage, and modernize data."
6,Migrate Oracle workloads to Google Cloud
6,"Rehost, replatform, rewrite your Oracle workloads."
6,Open Source Databases
6,Fully managed open source databases with enterprise-grade support.
6,SQL Server on Google Cloud
6,Options for running SQL Server virtual machines on Google Cloud.
6,Digital Transformation
6,"Accelerate business recovery and ensure a better future with solutions that enable hybrid and multi-cloud, generate intelligent insights, and keep your workers connected."
6,Business Continuity
6,Proactively plan and prioritize workloads.
6,Digital Innovation
6,Reimagine your operations and unlock new opportunities.
6,Operational Efficiency
6,Prioritize investments and optimize costs.
6,COVID-19 Solutions
6,Get work done more safely and securely.
6,COVID-19 Solutions for the Healthcare Industry
6,How Google is helping healthcare meet extraordinary challenges.
6,Infrastructure Modernization
6,"Migrate quickly with solutions for SAP, VMware, Windows, Oracle, and other workloads."
6,Application Migration
6,Discovery and analysis tools for moving to the cloud.
6,SAP on Google Cloud
6,Certifications for running SAP applications and SAP HANA.
6,High Performance Computing
6,"Compute, storage, and networking options to support any workload."
6,Windows on Google Cloud
6,Tools and partners for running Windows workloads.
6,Data Center Migration
6,"Migration solutions for VMs, apps, databases, and more."
6,Active Assist
6,Automatic cloud resource optimization and increased security.
6,Virtual Desktops
6,Remote work solutions for desktops and applications (VDI & DaaS).
6,Rapid Assessment & Migration Program (RAMP)
6,End-to-end migration program to simplify your path to the cloud.
6,Productivity and Collaboration
6,Change the way teams work with solutions designed for humans and built for impact.
6,Google Workspace
6,Collaboration and productivity tools for enterprises.
6,Google Workspace Essentials
6,Secure video meetings and modern collaboration for teams.
6,Cloud Identity
6,Unified platform for IT admins to manage user devices and apps.
6,Chrome Enterprise
6,"Chrome OS, Chrome Browser, and Chrome devices built for business."
6,Cloud Search
6,Enterprise search for employees to quickly find company information.
6,Security
6,"Detect, investigate, and respond to online threats to help protect your business."
6,Security Analytics and Operations
6,Solution for analyzing petabytes of security telemetry.
6,Web App and API Protection
6,Threat and fraud protection for your web applications and APIs.
6,Smart Analytics
6,"Generate instant insights from data at any scale with a serverless, fully managed analytics platform that significantly simplifies analytics."
6,Data Warehouse Modernization
6,Data warehouse to jumpstart your migration and unlock insights.
6,Stream Analytics
6,"Insights from ingesting, processing, and analyzing event streams."
6,Marketing Analytics
6,"Solutions for collecting, analyzing, and activating customer data."
6,Data Lake Modernization
6,Services for building and modernizing your data lake.
6,Business Intelligence
6,"Data analytics tools for collecting, analyzing, and activating BI."
6,Products
6,close
6,Featured Products
6,Compute Engine
6,Virtual machines running in Google’s data center.
6,Cloud Storage
6,"Object storage that’s secure, durable, and scalable."
6,Cloud SDK
6,Command-line tools and libraries for Google Cloud.
6,Cloud SQL
6,"Relational database services for MySQL, PostgreSQL, and SQL server."
6,Google Kubernetes Engine
6,Managed environment for running containerized apps.
6,BigQuery
6,Data warehouse for business agility and insights.
6,Cloud CDN
6,Content delivery network for delivering web and video.
6,Dataflow
6,Streaming analytics for stream and batch
6,processing.
6,Operations
6,"Monitoring, logging, and application performance suite."
6,Cloud Run
6,Fully managed environment for running containerized apps.
6,Anthos
6,Platform for modernizing existing apps and building new ones.
6,Not seeing what you're looking for?
6,See all products (100+)
6,AI and Machine Learning
6,Speech-to-Text
6,Speech recognition and transcription supporting 125 languages.
6,Vision AI
6,"Custom and pre-trained models to detect emotion, text, more."
6,Text-to-Speech
6,Speech synthesis in 220+ voices and 40+ languages.
6,Cloud Translation
6,"Language detection, translation, and glossary support."
6,Cloud Natural Language
6,Sentiment analysis and classification of unstructured text.
6,AutoML
6,Custom machine learning model training and development.
6,AI Platform
6,"Platform for training, hosting, and managing ML models."
6,Video AI
6,Video classification and recognition using machine learning.
6,AI Infrastructure
6,Options for every business to train deep learning and machine learning models cost-effectively.
6,Dialogflow
6,Conversation applications and systems development suite for virtual agents.
6,AutoML Tables
6,Service for training ML models with structured data.
6,Not seeing what you're looking for?
6,See all AI and machine learning products
6,API Management
6,Apigee API Management
6,Manage the full life cycle of APIs anywhere with visibility and control.
6,Cloud Endpoints
6,Deployment and development management for APIs on Google Cloud.
6,Cloud Healthcare API
6,Solution to bridge existing care systems and apps on Google Cloud.
6,AppSheet
6,No-code development platform to build and extend applications.
6,API Gateway
6,"Develop, deploy, secure, and manage APIs with a fully managed gateway."
6,Compute
6,Compute Engine
6,Virtual machines running in Google’s data center.
6,App Engine
6,Serverless application platform for apps and back ends.
6,Cloud GPUs
6,"GPUs for ML, scientific computing, and 3D visualization."
6,Migrate for Compute Engine
6,Server and virtual machine migration to Compute Engine.
6,Preemptible VMs
6,Compute instances for batch jobs and fault-tolerant workloads.
6,Shielded VMs
6,Reinforced virtual machines on Google Cloud.
6,Sole-Tenant Nodes
6,"Dedicated hardware for compliance, licensing, and management."
6,Bare Metal
6,Infrastructure to run specialized workloads on Google Cloud.
6,Recommender
6,Usage recommendations for Google Cloud products and services.
6,VMware Engine
6,"Fully managed, native VMware Cloud Foundation software stack."
6,Cloud Run
6,Fully managed environment for running containerized apps.
6,Not seeing what you're looking for?
6,See all compute products
6,Containers
6,Google Kubernetes Engine
6,Managed environment for running containerized apps.
6,Container Registry
6,"Registry for storing, managing, and securing Docker images."
6,Container Security
6,Container environment security for each stage of the life cycle.
6,Cloud Build
6,Solution for running build steps in a Docker container.
6,Deep Learning Containers
6,"Containers with data science frameworks, libraries, and tools."
6,Kubernetes Applications
6,Containerized apps with prebuilt deployment and unified billing.
6,Artifact Registry
6,Package manager for build artifacts and dependencies.
6,Knative
6,Components to create Kubernetes-native cloud-based software.
6,Cloud Run
6,Fully managed environment for running containerized apps.
6,Cloud Code
6,"IDE support to write, run, and debug Kubernetes applications."
6,Data Analytics
6,BigQuery
6,Data warehouse for business agility and insights.
6,Looker
6,"Platform for BI, data applications, and embedded analytics."
6,Dataflow
6,Streaming analytics for stream and batch processing.
6,Pub/Sub
6,Messaging service for event ingestion and delivery.
6,Dataproc
6,Service for running Apache Spark and Apache Hadoop clusters.
6,Cloud Data Fusion
6,Data integration for building and managing data pipelines.
6,Cloud Composer
6,Workflow orchestration service built on Apache Airflow.
6,Data Catalog
6,"Metadata service for discovering, understanding and managing data."
6,Dataprep
6,Service to prepare data for analysis and machine learning.
6,Google Data Studio
6,"Interactive data suite for dashboarding, reporting, and analytics."
6,Google Marketing Platform
6,Marketing platform unifying advertising and analytics.
6,Cloud Life Sciences
6,"Tools for managing, processing, and transforming biomedical data."
6,Databases
6,Cloud Bigtable
6,"Cloud-native wide-column database for large scale, low-latency workloads."
6,Firestore
6,"Cloud-native document database for building rich mobile, web, and IoT apps."
6,Memorystore
6,In-memory database for managed Redis and Memcached.
6,Cloud Spanner
6,Cloud-native relational database with unlimited scale and 99.999% availability.
6,Cloud SQL
6,"Fully managed database for MySQL, PostgreSQL, and SQL Server."
6,Database Migration Service
6,"Serverless, minimal downtime migrations to Cloud SQL."
6,Bare Metal
6,Infrastructure to run specialized workloads on Google Cloud.
6,Firebase Realtime Database
6,NoSQL database for storing and syncing data in real time.
6,Developer Tools
6,Artifact Registry
6,Universal package manager for build artifacts and dependencies.
6,Cloud Build
6,Continuous integration and continuous delivery platform.
6,Cloud Code
6,"IDE support to write, run, and debug Kubernetes applications."
6,Cloud Deployment Manager
6,Service for creating and managing Google Cloud resources.
6,Cloud SDK
6,Command line tools and libraries for Google Cloud.
6,Cloud Scheduler
6,Cron job scheduler for task automation and management.
6,Cloud Source Repositories
6,"Private Git repository to store, manage, and track code."
6,Cloud Tasks
6,Task management service for asynchronous task execution.
6,Container Registry
6,Private Docker storage for container images on Google Cloud.
6,Tekton
6,Kubernetes-native resources for declaring CI/CD pipelines.
6,Not seeing what you're looking for?
6,See all developer tools
6,Healthcare and Life Sciences
6,Apigee Healthcare APIx
6,FHIR API-based digital service production.
6,Cloud Healthcare API
6,Solution for bridging existing care systems and apps on Google Cloud.
6,Cloud Life Sciences
6,"Tools for managing, processing, and transforming biomedical data."
6,Healthcare Natural Language AI
6,Real-time insights from unstructured medical text.
6,Hybrid and Multi-cloud
6,Anthos
6,Platform for modernizing existing apps and building new ones.
6,Looker
6,"Platform for BI, data applications, and embedded analytics."
6,Cloud Run for Anthos
6,Integration that provides a serverless development platform on GKE.
6,Google Cloud Marketplace for Anthos
6,Containerized apps with prebuilt deployment and unified billing.
6,Migrate for Anthos
6,Tool to move workloads and existing applications to GKE.
6,Operations
6,"Monitoring, logging, and application performance suite."
6,Cloud Build
6,Service for executing builds on Google Cloud infrastructure.
6,Traffic Director
6,Traffic control pane and management for open service mesh.
6,Apigee API Management
6,"API management, development, and security platform."
6,Internet of Things
6,Cloud IoT Core
6,"IoT device management, integration, and"
6,connection service.
6,Edge TPU
6,ASIC designed to run ML inference and AI at the edge.
6,Management Tools
6,Cloud Shell
6,Interactive shell environment with a built-in command line.
6,Cloud Console
6,Web-based interface for managing and monitoring cloud apps.
6,Cloud Deployment Manager
6,Service for creating and managing Google Cloud resources.
6,Cloud Mobile App
6,App to
6,manage Google Cloud services from your mobile device.
6,Cloud APIs
6,Programmatic interfaces for
6,Google Cloud services.
6,Private Catalog
6,Service catalog for admins managing internal enterprise solutions.
6,Cost Management
6,"Tools for monitoring, controlling, and optimizing your costs."
6,Intelligent Management
6,"Tools for easily managing performance, security, and cost."
6,Media and Gaming
6,Game Servers
6,Game server management service running on Google Kubernetes Engine.
6,OpenCue
6,Open source render manager for visual effects and animation.
6,Migration
6,Application Migration
6,App migration to the cloud for low-cost refresh cycles.
6,BigQuery Data Transfer Service
6,Data import service for scheduling and moving data into BigQuery.
6,Cloud Data Transfer
6,Tools and services for transferring your data to Google Cloud.
6,Cloud Foundation Toolkit
6,Reference templates for Deployment Manager and Terraform.
6,Database Migration Service
6,"Serverless, minimal downtime migrations to Cloud SQL."
6,Migrate for Anthos
6,Components for migrating VMs into system containers on GKE.
6,Migrate for Compute Engine
6,Components for migrating VMs and physical servers to Compute Engine.
6,Rapid Assessment & Migration Program (RAMP)
6,End-to-end migration program to simplify your path to the cloud.
6,Transfer Appliance
6,Storage server for moving large volumes of data to Google Cloud.
6,Transfer Service
6,Data transfers from online and on-premises sources to Cloud Storage.
6,VMware Engine
6,Migrate and run your VMware workloads natively on Google Cloud.
6,Networking
6,Cloud Armor
6,Security policies and defense against web and DDoS attacks.
6,Cloud CDN
6,Content delivery network for serving web and video content.
6,Cloud DNS
6,Domain name system for reliable and low-latency name lookups.
6,Cloud Load Balancing
6,Service for distributing traffic across applications and regions.
6,Cloud NAT
6,NAT service for giving private instances internet access.
6,Hybrid Connectivity
6,"Connectivity options for VPN, peering, and enterprise needs."
6,Network Intelligence Center
6,"Network monitoring, verification, and optimization platform."
6,Network Service Tiers
6,Cloud network options
6,"based on performance, availability, and cost."
6,Network Telemetry
6,"VPC flow logs for network monitoring, forensics, and security."
6,Traffic Director
6,Traffic control pane and management for open service mesh.
6,Virtual Private Cloud
6,Virtual network for Google Cloud resources and cloud-based services.
6,Service Directory
6,"Platform for discovering, publishing, and connecting services."
6,Operations
6,Cloud Logging
6,"Google Cloud audit, platform, and application logs management."
6,Cloud Monitoring
6,Infrastructure and application health with rich metrics.
6,Error Reporting
6,Application error identification and analysis.
6,Kubernetes Engine Monitoring
6,GKE app development and troubleshooting.
6,Cloud Trace
6,Tracing system collecting latency data from applications.
6,Cloud Profiler
6,CPU and heap profiler for analyzing application performance.
6,Cloud Debugger
6,Real-time application state inspection and in-production debugging.
6,Intelligent Operations
6,"Tools for easily optimizing performance, security, and cost."
6,Security and Identity
6,Cloud IAM
6,Permissions management system for Google Cloud resources.
6,Assured Workloads
6,Compliance and security controls for sensitive workloads.
6,Cloud Key Management
6,Manage encryption keys on Google Cloud.
6,Confidential Computing
6,Encrypt data in use with Confidential VMs.
6,Security Command Center
6,Platform for defending against threats to your Google Cloud assets.
6,Cloud Data Loss Prevention
6,"Sensitive data inspection, classification, and redaction platform."
6,Managed Service for Microsoft Active Directory
6,Hardened service running Microsoft® Active Directory (AD).
6,Access Transparency
6,Cloud provider visibility through near real-time logs.
6,Titan Security Key
6,Two-factor authentication device for user account protection.
6,Secret Manager
6,"Store API keys, passwords, certificates, and other sensitive data."
6,BeyondCorp Enterprise
6,Zero trust solution for secure application and resource access.
6,Not seeing what you're looking for?
6,See all security and identity products
6,Serverless Computing
6,Cloud Run
6,Fully managed environment for running containerized apps.
6,Cloud Functions
6,Platform for creating functions that respond to cloud events.
6,App Engine
6,Serverless application platform for apps and back ends.
6,Workflows
6,Workflow orchestration for serverless products and API services.
6,Storage
6,All Storage Products
6,Cloud-based storage services for your business.
6,Cloud Storage
6,"Object storage that’s secure, durable, and scalable."
6,Filestore
6,File storage that is highly scalable and secure.
6,Persistent Disk
6,Block storage for virtual machine instances running on Google Cloud.
6,Cloud Storage for Firebase
6,Object storage for storing and serving user-generated content.
6,Local SSD
6,Block storage that is locally attached for high-performance needs.
6,Archival Storage
6,Data archive that offers online access speed at ultra low cost.
6,Cloud Data Transfer
6,Tools and services for transferring your data to Google Cloud.
6,Google Workspace Essentials
6,Secure video meetings and modern collaboration for teams.
6,Pricing
6,close
6,Do more for less with Google Cloud
6,Our customer-friendly pricing means more overall value to your business.
6,Contact Us
6,Google Cloud Platform
6,Overview
6,Pay only for what you use with no lock-in
6,Price list
6,Pricing details on each Google Cloud product
6,Calculators
6,Calculate your cloud savings
6,Free on Google Cloud
6,Learn and build on Google Cloud for free
6,More Cloud Products
6,Google Workspace
6,Google Maps Platform
6,Cloud Identity
6,Apigee
6,Firebase
6,Zync Render
6,Getting started
6,close
6,Get started with Google Cloud
6,"Start building right away on our secure, intelligent platform. New customers can use a $300 free credit to get started with any GCP product."
6,Try GCP Free
6,Get Started
6,Resources to Start on Your Own
6,Quickstarts
6,View short tutorials to help you get started
6,GCP Marketplace
6,Deploy ready-to-go solutions in a few clicks
6,Training
6,Enroll in on-demand or classroom training
6,Certification
6,Become Google Cloud Certified
6,Get Help from an Expert
6,Consulting
6,Jump-start your project with help from Google
6,Technical Account Management
6,Get long-term guidance from Google
6,Find a Partner
6,Work with a Partner in our global network
6,Become a Partner
6,Join Google Cloud's Partner program
6,More ways to get started
6,Docs
6,Support
6,Docs
6,Support
6,Language
6,English
6,Deutsch
6,Español – América Latina
6,Français
6,Português – Brasil
6,中文 – 简体
6,日本語
6,한국어
6,Cloud SQL
6,Overview
6,Guides
6,Reference
6,Samples
6,Support
6,Resources
6,Contact Us
6,Get started for free
6,Why Google
6,More
6,Solutions
6,More
6,Products
6,More
6,Pricing
6,More
6,Getting started
6,More
6,Docs
6,Overview
6,Guides
6,Reference
6,Samples
6,Support
6,Resources
6,Support
6,Console
6,Contact Us
6,Get started for free
6,Cloud SQL for MySQL
6,All APIs and reference
6,Error codes
6,Cloud SQL MetricsCloud SQL metrics
6,REST Reference
6,v1beta4REST Resources
6,backupRunsOverviewdeletegetinsertlist
6,databasesOverviewdeletegetinsertlistpatchupdate
6,flagsOverviewlist
6,instancesOverviewaddServerCaclonedeletedemoteMasterexportfailovergetimportinsertlistlistServerCaspatchpromoteReplicaresetSslConfigrestartrestoreBackuprotateServerCastartReplicastopReplicatruncateLogupdate
6,operationsOverviewgetlist
6,projects.instancesOverviewrescheduleMaintenancestartExternalSyncverifyExternalSyncSettings
6,sslCertsOverviewcreateEphemeraldeletegetinsertlist
6,tiersOverviewlist
6,usersOverviewdeleteinsertlistupdateTypesDiskEncryptionConfigurationDiskEncryptionStatusExternalSyncModeOperationErrorSqlDatabaseVersion
6,API overviewUsing the Cloud SQL Admin APIConfiguring VPC Service Controls
6,API basicsAuthorizing requestsPerformance tips
6,Client libraries and sample code
6,gcloud sqlOverviewReference
6,Transform your business with innovative solutions
6,Learn more
6,Why Google Cloud
6,Choosing Google Cloud
6,Multicloud
6,Trust and security
6,Global infrastructure
6,Data analytics
6,Open cloud
6,Sustainability
6,Analyst reports
6,Customer stories
6,Google Cloud Blog
6,Industry Solutions
6,Retail
6,Consumer Packaged Goods
6,Financial Services
6,Healthcare and Life Sciences
6,Media and Entertainment
6,Telecommunications
6,Gaming
6,Manufacturing
6,Supply Chain and Logistics
6,Government
6,Education
6,Small and Medium Business
6,See all solutions
6,Application Modernization
6,Hybrid and Multi-cloud Application Platform
6,Cloud-Native App Development
6,Serverless Solutions
6,DevOps
6,Continuous Delivery (CD)
6,Continuous Integration (CI)
6,Mainframe Modernization
6,Hosting
6,Artificial Intelligence
6,Build and Use AI
6,Contact Center AI
6,Document AI
6,Cloud Talent Solution
6,Business Application Platform
6,New Business Channels Using APIs
6,Unlocking Legacy Applications Using APIs
6,Open Banking APIx
6,Databases
6,Database Migration
6,Database Modernization
6,Google Cloud Databases
6,Migrate Oracle workloads to Google Cloud
6,Open Source Databases
6,SQL Server on Google Cloud
6,Digital Transformation
6,Business Continuity
6,Digital Innovation
6,Operational Efficiency
6,COVID-19 Solutions
6,COVID-19 Solutions for the Healthcare Industry
6,Infrastructure Modernization
6,Application Migration
6,SAP on Google Cloud
6,High Performance Computing
6,Windows on Google Cloud
6,Data Center Migration
6,Active Assist
6,Virtual Desktops
6,Rapid Assessment & Migration Program (RAMP)
6,Productivity and Collaboration
6,Google Workspace
6,Google Workspace Essentials
6,Cloud Identity
6,Chrome Enterprise
6,Cloud Search
6,Security
6,Security Analytics and Operations
6,Web App and API Protection
6,Smart Analytics
6,Data Warehouse Modernization
6,Stream Analytics
6,Marketing Analytics
6,Data Lake Modernization
6,Business Intelligence
6,Featured Products
6,Compute Engine
6,Cloud Storage
6,Cloud SDK
6,Cloud SQL
6,Google Kubernetes Engine
6,BigQuery
6,Cloud CDN
6,Dataflow
6,Operations
6,Cloud Run
6,Anthos
6,See all products (100+)
6,AI and Machine Learning
6,Speech-to-Text
6,Vision AI
6,Text-to-Speech
6,Cloud Translation
6,Cloud Natural Language
6,AutoML
6,AI Platform
6,Video AI
6,AI Infrastructure
6,Dialogflow
6,AutoML Tables
6,See all AI and machine learning products
6,API Management
6,Apigee API Management
6,Cloud Endpoints
6,Cloud Healthcare API
6,AppSheet
6,API Gateway
6,Compute
6,Compute Engine
6,App Engine
6,Cloud GPUs
6,Migrate for Compute Engine
6,Preemptible VMs
6,Shielded VMs
6,Sole-Tenant Nodes
6,Bare Metal
6,Recommender
6,VMware Engine
6,Cloud Run
6,See all compute products
6,Containers
6,Google Kubernetes Engine
6,Container Registry
6,Container Security
6,Cloud Build
6,Deep Learning Containers
6,Kubernetes Applications
6,Artifact Registry
6,Knative
6,Cloud Run
6,Cloud Code
6,Data Analytics
6,BigQuery
6,Looker
6,Dataflow
6,Pub/Sub
6,Dataproc
6,Cloud Data Fusion
6,Cloud Composer
6,Data Catalog
6,Dataprep
6,Google Data Studio
6,Google Marketing Platform
6,Cloud Life Sciences
6,Databases
6,Cloud Bigtable
6,Firestore
6,Memorystore
6,Cloud Spanner
6,Cloud SQL
6,Database Migration Service
6,Bare Metal
6,Firebase Realtime Database
6,Developer Tools
6,Artifact Registry
6,Cloud Build
6,Cloud Code
6,Cloud Deployment Manager
6,Cloud SDK
6,Cloud Scheduler
6,Cloud Source Repositories
6,Cloud Tasks
6,Container Registry
6,Tekton
6,See all developer tools
6,Healthcare and Life Sciences
6,Apigee Healthcare APIx
6,Cloud Healthcare API
6,Cloud Life Sciences
6,Healthcare Natural Language AI
6,Hybrid and Multi-cloud
6,Anthos
6,Looker
6,Cloud Run for Anthos
6,Google Cloud Marketplace for Anthos
6,Migrate for Anthos
6,Operations
6,Cloud Build
6,Traffic Director
6,Apigee API Management
6,Internet of Things
6,Cloud IoT Core
6,Edge TPU
6,Management Tools
6,Cloud Shell
6,Cloud Console
6,Cloud Deployment Manager
6,Cloud Mobile App
6,Cloud APIs
6,Private Catalog
6,Cost Management
6,Intelligent Management
6,Media and Gaming
6,Game Servers
6,OpenCue
6,Migration
6,Application Migration
6,BigQuery Data Transfer Service
6,Cloud Data Transfer
6,Cloud Foundation Toolkit
6,Database Migration Service
6,Migrate for Anthos
6,Migrate for Compute Engine
6,Rapid Assessment & Migration Program (RAMP)
6,Transfer Appliance
6,Transfer Service
6,VMware Engine
6,Networking
6,Cloud Armor
6,Cloud CDN
6,Cloud DNS
6,Cloud Load Balancing
6,Cloud NAT
6,Hybrid Connectivity
6,Network Intelligence Center
6,Network Service Tiers
6,Network Telemetry
6,Traffic Director
6,Virtual Private Cloud
6,Service Directory
6,Operations
6,Cloud Logging
6,Cloud Monitoring
6,Error Reporting
6,Kubernetes Engine Monitoring
6,Cloud Trace
6,Cloud Profiler
6,Cloud Debugger
6,Intelligent Operations
6,Security and Identity
6,Cloud IAM
6,Assured Workloads
6,Cloud Key Management
6,Confidential Computing
6,Security Command Center
6,Cloud Data Loss Prevention
6,Managed Service for Microsoft Active Directory
6,Access Transparency
6,Titan Security Key
6,Secret Manager
6,BeyondCorp Enterprise
6,See all security and identity products
6,Serverless Computing
6,Cloud Run
6,Cloud Functions
6,App Engine
6,Workflows
6,Storage
6,All Storage Products
6,Cloud Storage
6,Filestore
6,Persistent Disk
6,Cloud Storage for Firebase
6,Local SSD
6,Archival Storage
6,Cloud Data Transfer
6,Google Workspace Essentials
6,Do more for less with Google Cloud
6,Contact Us
6,Google Cloud Platform
6,Overview
6,Price list
6,Calculators
6,Free on Google Cloud
6,More Cloud Products
6,Google Workspace
6,Google Maps Platform
6,Cloud Identity
6,Apigee
6,Firebase
6,Zync Render
6,Get started with Google Cloud
6,Try GCP Free
6,Get Started
6,Resources to Start on Your Own
6,Quickstarts
6,GCP Marketplace
6,Training
6,Certification
6,Get Help from an Expert
6,Consulting
6,Technical Account Management
6,Find a Partner
6,Become a Partner
6,More ways to get started
6,Home
6,Docs
6,Cloud SQL
6,Documentation
6,MySQL
6,Reference
6,Send feedback
6,Performance tips
6,"This document covers some techniques you can use to improve the performance of your application. In some cases, examples from other APIs or generic APIs are used to illustrate the ideas presented. However, the same concepts are applicable to the Cloud SQL Admin API."
6,Compression using gzip
6,"An easy and convenient way to reduce the bandwidth needed for each request is to enable gzip compression. Although this requires additional CPU time to uncompress the results, the trade-off with network costs usually makes it very worthwhile."
6,"In order to receive a gzip-encoded response you must do two things: Set an Accept-Encoding header, and modify your user agent to contain the string gzip. Here is an example of properly formed HTTP headers for enabling gzip compression:"
6,Accept-Encoding: gzip
6,User-Agent: my program (gzip)
6,Working with partial resources
6,Another way to improve the performance of your API calls is by sending and receiving only the portion of the
6,"data that you're interested in. This lets your application avoid transferring, parsing, and storing unneeded fields, so it can use resources including network, CPU, and memory more efficiently."
6,There are two types of partial requests:
6,Partial response: A request where you specify which fields to include in the response (use the fields request parameter).
6,Patch: An update request where you send only the fields you want to change (use the PATCH HTTP verb).
6,More details on making partial requests are provided in the following sections.
6,Partial response
6,"By default, the server sends back the full representation of a resource after processing requests. For better performance, you can ask the server to send only the fields you really need and get a partial response instead."
6,"To request a partial response, use the fields request parameter to specify the fields you want returned. You can use this parameter with any request that returns response data."
6,"Note that the fields parameter only affects the response data; it does not affect the data that you need to send, if any. To reduce the amount of data you send when modifying resources, use a patch request."
6,Example
6,"The following example shows the use of the fields parameter with a generic (fictional) ""Demo"" API."
6,Simple request: This HTTP GET request omits the fields parameter and returns the full resource.
6,https://www.googleapis.com/demo/v1
6,"Full resource response: The full resource data includes the following fields, along with many others that have been omitted for brevity."
6,"""kind"": ""demo"","
6,...
6,"""items"": ["
6,"""title"": ""First title"","
6,"""comment"": ""First comment."","
6,"""characteristics"": {"
6,"""length"": ""short"","
6,"""accuracy"": ""high"","
6,"""followers"": [""Jo"", ""Will""],"
6,"""status"": ""active"","
6,...
6,"""title"": ""Second title"","
6,"""comment"": ""Second comment."","
6,"""characteristics"": {"
6,"""length"": ""long"","
6,"""accuracy"": ""medium"""
6,"""followers"": [ ],"
6,"""status"": ""pending"","
6,...
6,...
6,Request for a partial response: The following request for this same resource uses the fields parameter to significantly reduce the amount of data returned.
6,"https://www.googleapis.com/demo/v1?fields=kind,items(title,characteristics/length)"
6,"Partial response: In response to the request above, the server sends back a response that contains only the kind information along with a pared-down items array that includes only HTML title and length characteristic information in each item."
6,200 OK
6,"""kind"": ""demo"","
6,"""items"": [{"
6,"""title"": ""First title"","
6,"""characteristics"": {"
6,"""length"": ""short"""
6,"}, {"
6,"""title"": ""Second title"","
6,"""characteristics"": {"
6,"""length"": ""long"""
6,...
6,Note that the response is a JSON object that includes only the selected fields and their enclosing parent objects.
6,"Details on how to format the fields parameter is covered next, followed by more details about what exactly gets returned in the response."
6,Fields parameter syntax summary
6,"The format of the fields request parameter value is loosely based on XPath syntax. The supported syntax is summarized below, and additional examples are provided in the following section."
6,Use a comma-separated list to select multiple fields.
6,Use a/b to select a field b that is nested within field a; use a/b/c to select a field c nested within b.
6,"Exception: For API responses that use ""data"" wrappers, where the response is nested within a data object that looks like data: { ... }, do not include ""data"" in the fields specification."
6,"Including the data object with a fields specification like data/a/b causes an error. Instead, just use a fields specification like a/b."
6,"Use a sub-selector to request a set of specific sub-fields of arrays or objects by placing expressions in parentheses ""( )""."
6,"For example: fields=items(id,author/email) returns only the item ID and author's email for each element in the items array. You can also specify a single sub-field, where fields=items(id) is equivalent to fields=items/id."
6,"Use wildcards in field selections, if needed."
6,For example: fields=items/pagemap/* selects all objects in a pagemap.
6,More examples of using the fields parameter
6,The examples below include descriptions of how the fields parameter value affects the response.
6,"Note: As with all query parameter values, the fields parameter value must be URL encoded. For better readability, the examples in this document omit the encoding."
6,"Identify the fields you want returned, or make field selections."
6,"The fields request parameter value is a comma-separated list of fields, and each field is specified relative to the root of the response. Thus, if you are performing a list operation, the response is a collection, and it generally includes an array of resources. If you are performing an operation that returns a single resource, fields are specified relative to that resource. If the field you select is (or is part of) an array, the server returns the selected portion of all elements in the array."
6,Here are some collection-level examples:
6,Examples
6,Effect
6,items
6,"Returns all elements in the items array, including all fields in each element, but no other fields."
6,"etag,items"
6,Returns both the etag field
6,and all elements in the items array.
6,items/title
6,Returns only the title field for all elements in the items array.
6,"Whenever a nested field is returned, the response includes the enclosing parent objects. The parent fields do not include any other child fields unless they are also selected explicitly."
6,context/facets/label
6,"Returns only the label field for all members of the facets array, which is itself nested under the context object."
6,items/pagemap/*/title
6,"For each element in the items array, returns only the title field (if present) of all objects that are children of pagemap."
6,Here are some resource-level examples:
6,Examples
6,Effect
6,title
6,Returns the title field of the requested resource.
6,author/uri
6,Returns the uri sub-field of the author object in the requested resource.
6,links/*/href
6,Returns the href field of all objects that are children of links.
6,Request only parts of specific fields using sub-selections.
6,"By default, if your request specifies particular fields, the server returns the objects or array elements in their entirety. You can specify a response that includes only certain sub-fields. You do this using ""( )"" sub-selection syntax, as in the example below."
6,Example
6,Effect
6,"items(title,author/uri)"
6,Returns only the values of the title and author's uri for each element in the items array.
6,Handling partial responses
6,"After a server processes a valid request that includes the fields query parameter, it sends back an HTTP 200 OK status code, along with the requested data. If the fields query parameter has an error or is otherwise invalid, the server returns an HTTP 400 Bad Request status code, along with an error message telling the user what was wrong with their fields selection (for example, ""Invalid field selection a/b"")."
6,Here is the partial response example shown in the introductory section above. The request uses the fields parameter to specify which fields to return.
6,"https://www.googleapis.com/demo/v1?fields=kind,items(title,characteristics/length)"
6,The partial response looks like this:
6,200 OK
6,"""kind"": ""demo"","
6,"""items"": [{"
6,"""title"": ""First title"","
6,"""characteristics"": {"
6,"""length"": ""short"""
6,"}, {"
6,"""title"": ""Second title"","
6,"""characteristics"": {"
6,"""length"": ""long"""
6,...
6,"Note: For APIs that support query parameters for data pagination (maxResults and nextPageToken, for example), use those parameters to reduce the results of each query to a manageable size. Otherwise, the performance gains possible with partial response might not be realized."
6,Patch (partial update)
6,"You can also avoid sending unnecessary data when modifying resources. To send updated data only for the specific fields that you’re changing, use the HTTP PATCH verb. The patch semantics described in this document are different (and simpler) than they were for the older, GData implementation of partial update."
6,The short example below shows how using patch minimizes the data you need to send to make a small update.
6,Example
6,"This example shows a simple patch request to update only the title of a generic (fictional) ""Demo"" API resource. The"
6,"resource also has a comment, a set of characteristics, status, and many other fields, but this request only sends the title field, since that's the only field being modified:"
6,PATCH https://www.googleapis.com/demo/v1/324
6,Authorization: Bearer your_auth_token
6,Content-Type: application/json
6,"""title"": ""New title"""
6,Response:
6,200 OK
6,"""title"": ""New title"","
6,"""comment"": ""First comment."","
6,"""characteristics"": {"
6,"""length"": ""short"","
6,"""accuracy"": ""high"","
6,"""followers"": [""Jo"", ""Will""],"
6,"""status"": ""active"","
6,...
6,"The server returns a 200 OK status code, along with the full representation of the updated resource."
6,"Since only the title field was included in the patch request, that's the only value that is different from before."
6,"Note: If you use the partial response fields parameter in combination with patch, you can increase the efficiency of your update requests even further. A patch request only reduces the size of the request. A partial response reduces the size of the response. So to reduce the amount of data sent in both directions, use a patch request with the fields parameter."
6,Semantics of a patch request
6,"The body of the patch request includes only the resource fields you want to modify. When you specify a field, you must include any enclosing parent objects, just as the enclosing parents are returned with a partial response. The modified data you send is merged into the data for the parent object, if there is one."
6,"Add: To add a field that doesn't already exist, specify the new field and its value."
6,"Modify: To change the value of an existing field, specify the field and set it to the new value."
6,"Delete: To delete a field, specify the field and set it to null. For example, ""comment"": null. You can also delete an entire object (if it is mutable) by setting it to null. If you are using the"
6,"Java API Client Library, use Data.NULL_STRING instead; for"
6,"details, see JSON null."
6,"Note about arrays: Patch requests that contain arrays replace the existing array with the one you provide. You cannot modify, add, or delete items in an array in a piecemeal fashion."
6,Using patch in a read-modify-write cycle
6,It can be a useful practice to start by retrieving a partial response with the data you want to modify. This is especially important for resources that
6,"use ETags, since you must provide the current ETag value in the If-Match HTTP header in order to update the resource successfully. After you get the data, you can then modify the values you want to change and send the modified partial representation back with a patch request. Here is an example that assumes the Demo resource uses ETags:"
6,"GET https://www.googleapis.com/demo/v1/324?fields=etag,title,comment,characteristics"
6,Authorization: Bearer your_auth_token
6,This is the partial response:
6,200 OK
6,"""etag"": ""ETagString"""
6,"""title"": ""New title"""
6,"""comment"": ""First comment."","
6,"""characteristics"": {"
6,"""length"": ""short"","
6,"""level"": ""5"","
6,"""followers"": [""Jo"", ""Will""],"
6,"The following patch request is based on that response. As shown below, it also uses the fields parameter to limit the data returned in the patch response:"
6,"PATCH https://www.googleapis.com/demo/v1/324?fields=etag,title,comment,characteristics"
6,Authorization: Bearer your_auth_token
6,Content-Type: application/json
6,"If-Match: ""ETagString"""
6,"""etag"": ""ETagString"""
6,"""title"": """","
6,/* Clear the value of the title by setting it to the empty string. */
6,"""comment"": null,"
6,/* Delete the comment by replacing its value with null. */
6,"""characteristics"": {"
6,"""length"": ""short"","
6,"""level"": ""10"","
6,/* Modify the level value. */
6,"""followers"": [""Jo"", ""Liz""], /* Replace the followers array to delete Will and add Liz. */"
6,"""accuracy"": ""high"""
6,/* Add a new characteristic. */
6,"The server responds with a 200 OK HTTP status code, and the partial representation of the updated resource:"
6,200 OK
6,"""etag"": ""newETagString"""
6,"""title"": """","
6,/* Title is cleared; deleted comment field is missing. */
6,"""characteristics"": {"
6,"""length"": ""short"","
6,"""level"": ""10"","
6,/* Value is updated.*/
6,"""followers"": [""Jo"" ""Liz""], /* New follower Liz is present; deleted Will is missing. */"
6,"""accuracy"": ""high"""
6,/* New characteristic is present. */
6,Constructing a patch request directly
6,"For some patch requests, you need to base them on the data you previously retrieved. For example, if you want to add an item to an array and don't want to lose any of the existing array elements, you must get the existing data first. Similarly, if an API uses ETags, you need to send the previous ETag value with your request in order to update the resource successfully."
6,"Note: You can use an ""If-Match: *"" HTTP header to force a patch to go through when ETags are in use.  If you do this, you don't need to do the read before the write."
6,"For other situations, however, you can construct the patch request directly, without first retrieving the existing data. For example, you can easily set up a patch request that updates a field to a new value or adds a new field. Here is an example:"
6,"PATCH https://www.googleapis.com/demo/v1/324?fields=comment,characteristics"
6,Authorization: Bearer your_auth_token
6,Content-Type: application/json
6,"""comment"": ""A new comment"","
6,"""characteristics"": {"
6,"""volume"": ""loud"","
6,"""accuracy"": null"
6,"With this request, if the comment field has an existing value, the new value overwrites it; otherwise it is set to the new value. Similarly, if there was a volume characteristic, its value is overwritten; if not, it is created. The accuracy field, if set, is removed."
6,Handling the response to a patch
6,"After processing a valid patch request, the API returns a 200 OK HTTP response code along with the complete representation of the modified resource. If ETags are used by the API, the server updates ETag values when it successfully processes a patch request, just as it does with PUT."
6,The patch request returns the entire resource representation unless you use the fields parameter to reduce the amount of data it returns.
6,"If a patch request results in a new resource state that is syntactically or semantically invalid, the server returns a 400 Bad Request or 422 Unprocessable Entity HTTP status code, and the resource state remains unchanged. For example, if you attempt to delete the value for a required field, the server returns an error."
6,Alternate notation when PATCH HTTP verb is not supported
6,"If your firewall does not allow HTTP PATCH requests, then do an HTTP POST request and set the override header to PATCH, as shown below:"
6,POST https://www.googleapis.com/...
6,X-HTTP-Method-Override: PATCH
6,...
6,Difference between patch and update
6,"In practice, when you send data for an update request that uses the HTTP PUT verb, you only need to send those fields which are either required or optional; if you send values for fields that are set by the server, they are ignored. Although this might seem like another way to do a partial update, this approach has some limitations. With updates that use the HTTP PUT verb, the request fails if you don't supply required parameters, and it clears previously set data if you don't supply optional parameters."
6,"It's much safer to use patch for this reason. You only supply data for the fields you want to change; fields that you omit are not cleared. The only exception to this rule occurs with repeating elements or arrays: If you omit all of them, they stay just as they are; if you provide any of them, the whole set is replaced with the set that you provide."
6,Send feedback
6,"Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates."
6,Last updated 2020-11-16 UTC.
6,Why Google
6,Choosing Google Cloud
6,Trust and security
6,Open cloud
6,Global infrastructure
6,Customers and case studies
6,Analyst reports
6,Whitepapers
6,Products and pricing
6,Google Cloud pricing
6,Google Workspace pricing
6,Maps Platform pricing
6,See all products
6,Solutions
6,Application modernization
6,Artificial Intelligence
6,Business application platform
6,Database solutions
6,Infrastructure modernization
6,Productivity & collaboration
6,Security
6,Smart analytics
6,DevOps
6,Industries
6,Small business
6,See all solutions
6,Resources
6,Google Cloud documentation
6,Google Cloud quickstarts
6,Google Cloud Marketplace
6,Google Workspace Marketplace
6,Support
6,Code samples
6,Tutorials
6,Training
6,Certifications
6,Google Developers
6,Google Cloud for Startups
6,System status
6,Release Notes
6,Engage
6,Contact sales
6,Find a Partner
6,Become a Partner
6,Blog
6,Events
6,Podcast
6,Community
6,Press center
6,Google Cloud on YouTube
6,Google Cloud Platform on YouTube
6,Google Workspace on YouTube
6,Follow on Twitter
6,Join User Research
6,We're hiring. Join Google Cloud!
6,About Google
6,Privacy
6,Site terms
6,Google Cloud terms
6,Carbon neutral since 2007
6,Sign up for the Google Cloud newsletter
6,Subscribe
6,Language
6,English
6,Deutsch
6,Español – América Latina
6,Français
6,Português – Brasil
6,中文 – 简体
6,日本語
6,한국어
8,MySQL 8.0.20: Index-Level Optimizer Hints — Jesper's MySQL Blog
8,↓ Skip to Main Content
8,Jesper's MySQL Blog
8,Main Navigation
8,Menu
8,Home
8,Recent Activity
8,Latest Posts
8,Top Posts
8,Latest Tweets
8,MySQL Shell Blogs
8,My Books
8,MySQL Concurrency
8,MySQL 8 Query Performance Tuning
8,MySQL Connector/Python Revealed
8,Pro MySQL NDB Cluster
8,MySQL Books
8,My Books
8,Books by Oracle Employees
8,Who Am I?
8,Privacy Policy
8,Home › MySQL › MySQL 8.0 › MySQL 8.0.20: Index-Level Optimizer Hints
8,MySQL 8.0.20: Index-Level Optimizer Hints
8,Jesper Krogh
8,"Posted on 28 April, 2020"
8,"Posted in MySQL 8.0, Optimizer, Performance"
8,No Comments
8,Table Of Contents The Short StoryThe DetailsHow About USE INDEX?RelatedTweet
8,MySQL introduced optimizer hints in version 5.7 and greatly extended the feature in MySQL 8. One thing that has been missing though is the ability to specify index hints using the syntax of optimizer hints. This has been improved of in MySQL 8.0.20 with the introduction of index-level optimizer hints for the FORCE and IGNORE versions of the index hints. This blog will look at the new index hint syntax.
8,"WarningDo not add index hints – neither using the old or new style – unless you really need them. When you add index hints, you limit the options of the optimizer which can prevent the optimizer obtaining the optimal query plan as new optimizer improvements are implemented or the data changes.On the other hand, if you really have a query where ANALYZE TABLE and increasing the number of pages analyzed in the random index dives do not help you, index hints can be very useful to ensure optimal performance."
8,The Short Story
8,"To make a long story short, consider this query in 8.0.19 and earlier:"
8,"SELECT ci.CountryCode, co.Name AS Country, ci.Name AS City, ci.District"
8,FROM world.country co IGNORE INDEX (Primary)
8,INNER JOIN world.city ci FORCE INDEX FOR ORDER BY (CountryCode)
8,ON ci.CountryCode = co.Code
8,WHERE co.Continent = 'Asia'
8,"ORDER BY ci.CountryCode, ci.ID;"
8,"This query has two index hints, IGNORE INDEX in the second line and USE INDEX FOR ORDER BY in the third line."
8,"In MySQL 8.0.20, you can write the query as:"
8,SELECT /*+ NO_INDEX(co PRIMARY) ORDER_INDEX(ci CountryCode) */
8,"ci.CountryCode, co.Name AS Country, ci.Name AS City, ci.District"
8,FROM world.country co
8,INNER JOIN world.city ci
8,ON ci.CountryCode = co.Code
8,WHERE co.Continent = 'Asia'
8,"ORDER BY ci.CountryCode, ci.ID;"
8,"InformationHowever, note that there seems to be a bug, so the ORDER_INDEX() hint makes the optimizer choose a plan like NO_JOIN_INDEX() for the same index is also specified. The workaround is to also add the JOIN_INDEX()."
8,Let's take a look which index-level optimizer hints that have been added and how they map to the old index hints.
8,The Details
8,There are four pairs of new index hints which all maps back to the old style hints as in the below table.
8,"New HintOld HintJOIN_INDEXNO_JOIN_INDEXFORCE INDEX FOR JOINIGNORE INDEX FOR JOINGROUP_INDEXNO_GROUP_INDEXFORCE INDEX FOR GROUP BYIGNORE INDEX FOR GROUP BYORDER_INDEXNO_ORDER_INDEXFORCE INDEX FOR ORDER BYIGNORE INDEX FOR ORDER BYINDEXNO_INDEXFORCE INDEXIGNORE INDEXThe new hints support all of the usual features of optimizer hints such as specifying the query block for a hint, adding them inline in subqueries, etc. For the full details, see the manual and the release notes (which has an extensive description of the new hints)."
8,"If you need to specify multiple index hints, there are two ways to accomplish it depending on whether the indexes are on the same table or not. Consider a query on the world.city table where you will not allow neither the primary key nor the CountryCode index to be used. You can accomplish that as in this example:"
8,"SELECT /*+ NO_INDEX(ci PRIMARY, CountryCode) */"
8,"ID, CountryCode, Name, District, Population"
8,FROM world.city ci
8,WHERE Population > 1000000;
8,"On the other hand, if the indexes are on different tables, then you will have to specify multiple hints. Let's say you want to force the optimizer to choose the hash join algorithm when joining the country and city tables in the world database by ignoring the primary key on the country table and the CountryCode index on the city table (effectively forcing the join not to use an index irrespective of the join order). In this case, you can use the NO_INDEX() hint twice, once on each table:"
8,SELECT /*+ NO_INDEX(co PRIMARY) NO_INDEX(ci CountryCode) */
8,"ci.CountryCode, co.Name AS Country, ci.Name AS City, ci.District"
8,FROM world.country co
8,INNER JOIN world.city ci ON ci.CountryCode = co.Code
8,WHERE co.Continent = 'Asia';
8,How About USE INDEX?
8,"The old index hint syntax also includes the USE INDEX variant, but none of the new index-level optimizer hints corresponds to it. So, what do you do if you want to specify the softer USE INDEX rather than FORCE INDEX? First of all, in that case there is a good chance, you do not need the index hint at all, so try to remove it and verify whether the optimizer uses the same query plan. If so, it is better to remove it, so the optimizer can use the optimal join strategy as new optimizer features become available or the data changes."
8,"If you really need the USE INDEX hint, the simplest is to continue to use the old syntax. However, be aware that you cannot mix the old and new syntax hints (in that case, the old hints are ignored)."
8,"Alternative, you can simulate USE INDEX by specifying all other applicable indexes in a NO_INDEX() optimizer hint. For example, consider the world.countrylanguage table:"
8,mysql> SHOW CREATE TABLE countrylanguage\G
8,*************************** 1. row ***************************
8,Table: countrylanguage
8,Create Table: CREATE TABLE `countrylanguage` (
8,"`CountryCode` char(3) NOT NULL DEFAULT '',"
8,"`Language` char(30) NOT NULL DEFAULT '',"
8,"`IsOfficial` enum('T','F') NOT NULL DEFAULT 'F',"
8,"`Percentage` float(4,1) NOT NULL DEFAULT '0.0',"
8,"PRIMARY KEY (`CountryCode`,`Language`),"
8,"KEY `CountryCode` (`CountryCode`),"
8,CONSTRAINT `countryLanguage_ibfk_1` FOREIGN KEY (`CountryCode`) REFERENCES `country` (`Code`)
8,) ENGINE=InnoDB DEFAULT CHARSET=latin1
8,1 row in set (0.0008 sec)
8,"If you want to find all languages spoken in Australia and for some reason want to either use the CountryCode index or no index at all, then you can tell the optimizer to ignore the primary key:"
8,SELECT /*+ NO_INDEX(cl PRIMARY) */
8,"CountryCode, Language, IsOfficial, Percentage"
8,FROM world.countrylanguage cl
8,WHERE CountryCOde = 'AUS';
8,That is equivalent of the old syntax hints with USE INDEX (CountryCode):
8,"SELECT CountryCode, Language, IsOfficial, Percentage"
8,FROM world.countrylanguage cl USE INDEX (CountryCode)
8,WHERE CountryCOde = 'AUS';
8,"While this works the same and is simple enough with just one alternative index, it is not so easy in the general case. To be sure your NO_INDEX() optimizer hint is the same as the USE INDEX index hint, you will need to fetch the list of indexes and add them to the NO_INDEX() hint each time you execute the query. That is the reason for the suggestion to keep using the old style index hint if you rely on USE INDEX."
8,Tweet
8,Related
8,About Jesper Krogh
8,"I have worked with MySQL databases since 2006 both as an SQL developer, a database administrator, and for more than eight years as part of the Oracle MySQL Support team. I have spoken at MySQL Connect and Oracle OpenWorld on several occasions. I have contributed to the sys schema and four Oracle Certified Professional (OCP) exams for MySQL 5.6 to 8.0."
8,"I have written four books, all published at Apress."
8,Post navigation
8,Previous Post is
8,‹ New Book: MySQL 8 Query Performance TuningNext Post is Apress Blog: MySQL Performance Tuning Best Practices ›
8,Leave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment Name *
8,E-mail *
8,Website
8,"Save my name, email, and website in this browser for the next time I comment."
8,Notify me of follow-up comments by email. Notify me of new posts by email.
8,This site uses Akismet to reduce spam. Learn how your comment data is processed.
8,Search
8,Search for:
8,TwitterFollow @JWKroghBooks
8,Order it from:
8,📚 Apress
8,📚 Amazon
8,📚 Barnes & Noble
8,📚 Saxo
8,Order it from:
8,📚 Apress
8,📚 Amazon
8,📚 Barnes & Noble
8,📚 Saxo
8,Order it from:
8,📚 Apress
8,📚 Amazon
8,📚 Barnes & Noble
8,📚 Saxo
8,Order it from:
8,📚 Apress
8,📚 Amazon
8,📚 Barnes & Noble
8,📚 Saxo
8,Recent Posts
8,MySQL Query Attributes
8,New Book: MySQL Concurrency
8,The MySQL X DevApi: Working with NULL Values
8,I Am Speaking at Oracle Developer Live – MySQL 2020
8,Happy Birthday MySQL
8,Recent CommentsMysql Best Practices - Mysql Backup Best Practices Â€“ - Jesper'S Mysql Blog on MySQL Backup Best PracticesNew Book: MySQL Concurrency – Jesper's MySQL Blog on New Book: MySQL 8 Query Performance TuningJesper Krogh on NoSQL/X DevAPI Tutorial with MySQL Connector/Python 8.0The MySQL X DevApi: Working with NULL Values – Jesper's MySQL Blog on NoSQL/X DevAPI Tutorial with MySQL Connector/Python 8.0francis on NoSQL/X DevAPI Tutorial with MySQL Connector/Python 8.0Archives
8,February 2021
8,January 2021
8,October 2020
8,May 2020
8,April 2020
8,March 2020
8,December 2019
8,October 2019
8,September 2019
8,July 2019
8,May 2019
8,April 2019
8,March 2019
8,February 2019
8,January 2019
8,December 2018
8,November 2018
8,October 2018
8,September 2018
8,August 2018
8,July 2018
8,June 2018
8,March 2018
8,February 2018
8,November 2017
8,May 2017
8,January 2014
8,November 2013
8,October 2013
8,September 2013
8,August 2013
8,June 2013
8,May 2013
8,February 2013
8,December 2012
8,October 2012
8,September 2012
8,August 2012
8,Categories
8,Backup
8,binlog_transaction_compression
8,binlog_transaction_compression_level_zstd
8,Book
8,Certification
8,Character Set
8,Cloud
8,Common Table Expression (CTE)
8,Conference
8,Connector/Python
8,Contribution
8,Curses
8,dbdeployer
8,Django
8,Encryption
8,Foreign Keys
8,InnoDB
8,iOS
8,JavaScript
8,JSON
8,lower_case_table_names
8,Monitoring
8,Mutex
8,MySQL
8,MySQL 5.6
8,MySQL 5.7
8,MySQL 8.0
8,MySQL Cluster
8,MySQL Cluster 7.3
8,MySQL Cluster 7.4
8,MySQL Cluster 7.5
8,MySQL Cluster 7.6
8,MySQL Connect 2012
8,MySQL Connect 2013
8,MySQL Enterprise Backup
8,MySQL Enterprise Monitor
8,MySQL Shell
8,MySQL Workbench
8,node.js
8,Node.js
8,NoSQL
8,Optimizer
8,Options
8,Oracle Code One
8,Oracle OpenWorld
8,Performance
8,Performance Schema
8,Python
8,Pythonista
8,Query Attributes
8,Replication
8,Slides
8,SQLAlchemy
8,sys Schema
8,Uncategorized
8,Utilities
8,Variables
8,X DevAPI
8,"Who am I? I have worked with MySQL databases since 2006 both as an SQL developer, a database administrator, and for more than eight years as part of the Oracle MySQL Support team. I have spoken at MySQL Connect and Oracle OpenWorld on several occasions. I have contributed to the sys schema and four Oracle Certified Professional (OCP) exams for MySQL 5.6 to 8.0."
8,"I have written four books, all published at Apress."
8,Comments and opinions are my own and do not necessarily reflect the views of my employer.
8,Meta
8,Log in
8,Entries feed
8,Comments feed
8,WordPress.org
8,"Privacy & Cookies: This site uses cookies. By continuing to use this website, you agree to their use."
8,"To find out more, including how to control cookies, see here:"
8,Cookie Policy
8,2021
8,Jesper's MySQL Blog
8,| Powered by Responsive Theme
9,MySQL :: MySQL 8.0 Reference Manual :: 8.2.1.4 Hash Join Optimization
9,Contact MySQL
9,Login  |
9,Register
9,The world's most popular open source database
9,MySQL.com
9,Downloads
9,Documentation
9,Developer Zone
9,Developer Zone
9,Downloads
9,MySQL.com
9,Documentation
9,MySQL Server
9,MySQL Enterprise
9,Workbench
9,InnoDB Cluster
9,MySQL NDB Cluster
9,Connectors
9,More
9,MySQL.com
9,Downloads
9,Developer Zone
9,Section Menu:
9,Documentation Home
9,MySQL 8.0 Reference Manual
9,Preface and Legal Notices
9,General Information
9,Installing and Upgrading MySQL
9,Tutorial
9,MySQL Programs
9,MySQL Server Administration
9,Security
9,Backup and Recovery
9,Optimization
9,Optimization Overview
9,Optimizing SQL Statements
9,Optimizing SELECT Statements
9,WHERE Clause Optimization
9,Range Optimization
9,Index Merge Optimization
9,Hash Join Optimization
9,Engine Condition Pushdown Optimization
9,Index Condition Pushdown Optimization
9,Nested-Loop Join Algorithms
9,Nested Join Optimization
9,Outer Join Optimization
9,Outer Join Simplification
9,Multi-Range Read Optimization
9,Block Nested-Loop and Batched Key Access Joins
9,Condition Filtering
9,Constant-Folding Optimization
9,IS NULL Optimization
9,ORDER BY Optimization
9,GROUP BY Optimization
9,DISTINCT Optimization
9,LIMIT Query Optimization
9,Function Call Optimization
9,Window Function Optimization
9,Row Constructor Expression Optimization
9,Avoiding Full Table Scans
9,"Optimizing Subqueries, Derived Tables, View References, and Common Table"
9,Expressions
9,Optimizing IN and EXISTS Subquery Predicates with Semijoin
9,Transformations
9,Optimizing Subqueries with Materialization
9,Optimizing Subqueries with the EXISTS Strategy
9,"Optimizing Derived Tables, View References, and Common Table Expressions"
9,with Merging or Materialization
9,Derived Condition Pushdown Optimization
9,Optimizing INFORMATION_SCHEMA Queries
9,Optimizing Performance Schema Queries
9,Optimizing Data Change Statements
9,Optimizing INSERT Statements
9,Optimizing UPDATE Statements
9,Optimizing DELETE Statements
9,Optimizing Database Privileges
9,Other Optimization Tips
9,Optimization and Indexes
9,How MySQL Uses Indexes
9,Primary Key Optimization
9,SPATIAL Index Optimization
9,Foreign Key Optimization
9,Column Indexes
9,Multiple-Column Indexes
9,Verifying Index Usage
9,InnoDB and MyISAM Index Statistics Collection
9,Comparison of B-Tree and Hash Indexes
9,Use of Index Extensions
9,Optimizer Use of Generated Column Indexes
9,Invisible Indexes
9,Descending Indexes
9,Indexed Lookups from TIMESTAMP Columns
9,Optimizing Database Structure
9,Optimizing Data Size
9,Optimizing MySQL Data Types
9,Optimizing for Numeric Data
9,Optimizing for Character and String Types
9,Optimizing for BLOB Types
9,Optimizing for Many Tables
9,How MySQL Opens and Closes Tables
9,Disadvantages of Creating Many Tables in the Same Database
9,Internal Temporary Table Use in MySQL
9,Limits on Number of Databases and Tables
9,Limits on Table Size
9,Limits on Table Column Count and Row Size
9,Optimizing for InnoDB Tables
9,Optimizing Storage Layout for InnoDB Tables
9,Optimizing InnoDB Transaction Management
9,Optimizing InnoDB Read-Only Transactions
9,Optimizing InnoDB Redo Logging
9,Bulk Data Loading for InnoDB Tables
9,Optimizing InnoDB Queries
9,Optimizing InnoDB DDL Operations
9,Optimizing InnoDB Disk I/O
9,Optimizing InnoDB Configuration Variables
9,Optimizing InnoDB for Systems with Many Tables
9,Optimizing for MyISAM Tables
9,Optimizing MyISAM Queries
9,Bulk Data Loading for MyISAM Tables
9,Optimizing REPAIR TABLE Statements
9,Optimizing for MEMORY Tables
9,Understanding the Query Execution Plan
9,Optimizing Queries with EXPLAIN
9,EXPLAIN Output Format
9,Extended EXPLAIN Output Format
9,Obtaining Execution Plan Information for a Named Connection
9,Estimating Query Performance
9,Controlling the Query Optimizer
9,Controlling Query Plan Evaluation
9,Switchable Optimizations
9,Optimizer Hints
9,Index Hints
9,The Optimizer Cost Model
9,Optimizer Statistics
9,Buffering and Caching
9,InnoDB Buffer Pool Optimization
9,The MyISAM Key Cache
9,Shared Key Cache Access
9,Multiple Key Caches
9,Midpoint Insertion Strategy
9,Index Preloading
9,Key Cache Block Size
9,Restructuring a Key Cache
9,Caching of Prepared Statements and Stored Programs
9,Optimizing Locking Operations
9,Internal Locking Methods
9,Table Locking Issues
9,Concurrent Inserts
9,Metadata Locking
9,External Locking
9,Optimizing the MySQL Server
9,Optimizing Disk I/O
9,Using Symbolic Links
9,Using Symbolic Links for Databases on Unix
9,Using Symbolic Links for MyISAM Tables on Unix
9,Using Symbolic Links for Databases on Windows
9,Optimizing Memory Use
9,How MySQL Uses Memory
9,Enabling Large Page Support
9,Measuring Performance (Benchmarking)
9,Measuring the Speed of Expressions and Functions
9,Using Your Own Benchmarks
9,Measuring Performance with performance_schema
9,Examining Server Thread (Process) Information
9,Accessing the Process List
9,Thread Command Values
9,General Thread States
9,Replication Source Thread States
9,Replication I/O Thread States
9,Replication SQL Thread States
9,Replication Connection Thread States
9,NDB Cluster Thread States
9,Event Scheduler Thread States
9,Language Structure
9,"Character Sets, Collations, Unicode"
9,Data Types
9,Functions and Operators
9,SQL Statements
9,MySQL Data Dictionary
9,The InnoDB Storage Engine
9,Alternative Storage Engines
9,Replication
9,Group Replication
9,MySQL Shell
9,Using MySQL as a Document Store
9,InnoDB Cluster
9,InnoDB ReplicaSet
9,MySQL NDB Cluster 8.0
9,Partitioning
9,Stored Objects
9,INFORMATION_SCHEMA Tables
9,MySQL Performance Schema
9,MySQL sys Schema
9,Connectors and APIs
9,MySQL Enterprise Edition
9,MySQL Workbench
9,MySQL on the OCI Marketplace
9,MySQL 8.0 Frequently Asked Questions
9,Error Messages and Common Problems
9,Indexes
9,MySQL Glossary
9,Related Documentation
9,MySQL 8.0 Release Notes
9,MySQL 8.0 Source Code Documentation
9,Download
9,this Manual
9,PDF (US Ltr)
9,- 40.9Mb
9,PDF (A4)
9,- 41.0Mb
9,PDF (RPM)
9,- 39.7Mb
9,HTML Download (TGZ)
9,- 9.5Mb
9,HTML Download (Zip)
9,- 9.6Mb
9,HTML Download (RPM)
9,- 8.1Mb
9,Man Pages (TGZ)
9,- 256.7Kb
9,Man Pages (Zip)
9,- 366.8Kb
9,Info (Gzip)
9,- 3.9Mb
9,Info (Zip)
9,- 3.9Mb
9,Excerpts from this Manual
9,MySQL Backup and Recovery
9,MySQL Globalization
9,MySQL Information Schema
9,MySQL Installation Guide
9,Security in MySQL
9,Starting and Stopping MySQL
9,MySQL and Linux/Unix
9,MySQL and Windows
9,MySQL and OS X
9,MySQL and Solaris
9,Building MySQL from Source
9,MySQL Restrictions and Limitations
9,MySQL Partitioning
9,MySQL Tutorial
9,MySQL Performance Schema
9,MySQL Replication
9,Using the MySQL Yum Repository
9,MySQL NDB Cluster 8.0
9,version 8.0
9,5.7
9,5.6
9,5.6
9,Japanese
9,MySQL 8.0 Reference Manual  /
9,...  /
9,Optimization  /
9,Optimizing SQL Statements  /
9,Optimizing SELECT Statements  /
9,Hash Join Optimization
9,8.2.1.4 Hash Join Optimization
9,"Beginning with MySQL 8.0.18, MySQL employs a hash join for any"
9,"query for which each join has an equi-join condition, and in"
9,which there are no indexes that can be applied to any join
9,"conditions, such as this one:"
9,SELECT *
9,FROM t1
9,JOIN t2
9,ON t1.c1=t2.c1;
9,A hash join can also be used when there are one or more
9,indexes that can be used for single-table predicates.
9,A hash join is usually faster than and is intended to be used
9,in such cases instead of the block nested loop algorithm (see
9,Block Nested-Loop Join Algorithm) employed
9,"in previous versions of MySQL. Beginning with MySQL 8.0.20,"
9,"support for block nested loop is removed, and the server"
9,employs a hash join wherever a block nested loop would have
9,been used previously.
9,In the example just shown and the remaining examples in this
9,"section, we assume that the three tables"
9,"t1, t2, and"
9,t3 have been created using the following
9,statements:
9,"CREATE TABLE t1 (c1 INT, c2 INT);"
9,"CREATE TABLE t2 (c1 INT, c2 INT);"
9,"CREATE TABLE t3 (c1 INT, c2 INT);"
9,You can see that a hash join is being employed by using
9,"EXPLAIN,"
9,like this:
9,mysql> EXPLAIN
9,-> SELECT * FROM t1
9,JOIN t2 ON t1.c1=t2.c1\G
9,*************************** 1. row ***************************
9,id: 1
9,select_type: SIMPLE
9,table: t1
9,partitions: NULL
9,type: ALL
9,possible_keys: NULL
9,key: NULL
9,key_len: NULL
9,ref: NULL
9,rows: 1
9,filtered: 100.00
9,Extra: NULL
9,*************************** 2. row ***************************
9,id: 1
9,select_type: SIMPLE
9,table: t2
9,partitions: NULL
9,type: ALL
9,possible_keys: NULL
9,key: NULL
9,key_len: NULL
9,ref: NULL
9,rows: 1
9,filtered: 100.00
9,Extra: Using where; Using join buffer (hash join)
9,"(Prior to MySQL 8.0.20, it was necessary to include the"
9,FORMAT=TREE option to see whether hash
9,joins were being used for a given join.)
9,EXPLAIN ANALYZE also displays
9,information about hash joins used.
9,The hash join is used for queries involving multiple joins as
9,"well, as long as at least one join condition for each pair of"
9,"tables is an equi-join, like the query shown here:"
9,SELECT * FROM t1
9,JOIN t2 ON (t1.c1 = t2.c1 AND t1.c2 < t2.c2)
9,JOIN t3 ON (t2.c1 = t3.c1);
9,"In cases like the one just shown, which makes use of an inner"
9,"join, any extra conditions which are not equi-joins are"
9,applied as filters after the join is executed. (For outer
9,"joins, such as left joins, semijoins, and antijoins, they are"
9,printed as part of the join.) This can be seen here in the
9,output of EXPLAIN:
9,mysql> EXPLAIN FORMAT=TREE
9,-> SELECT *
9,FROM t1
9,JOIN t2
9,ON (t1.c1 = t2.c1 AND t1.c2 < t2.c2)
9,JOIN t3
9,ON (t2.c1 = t3.c1)\G
9,*************************** 1. row ***************************
9,EXPLAIN: -> Inner hash join (t3.c1 = t1.c1)
9,(cost=1.05 rows=1)
9,-> Table scan on t3
9,(cost=0.35 rows=1)
9,-> Hash
9,-> Filter: (t1.c2 < t2.c2)
9,(cost=0.70 rows=1)
9,-> Inner hash join (t2.c1 = t1.c1)
9,(cost=0.70 rows=1)
9,-> Table scan on t2
9,(cost=0.35 rows=1)
9,-> Hash
9,-> Table scan on t1
9,(cost=0.35 rows=1)
9,"As also can be seen from the output just shown, multiple hash"
9,joins can be (and are) used for joins having multiple
9,equi-join conditions.
9,"Prior to MySQL 8.0.20, a hash join could not be used if any"
9,pair of joined tables did not have at least one equi-join
9,"condition, and the slower block nested loop algorithm was"
9,"employed. In MySQL 8.0.20 and later, the hash join is used in"
9,"such cases, as shown here:"
9,mysql> EXPLAIN FORMAT=TREE
9,-> SELECT * FROM t1
9,JOIN t2 ON (t1.c1 = t2.c1)
9,JOIN t3 ON (t2.c1 < t3.c1)\G
9,*************************** 1. row ***************************
9,EXPLAIN: -> Filter: (t1.c1 < t3.c1)
9,(cost=1.05 rows=1)
9,-> Inner hash join (no condition)
9,(cost=1.05 rows=1)
9,-> Table scan on t3
9,(cost=0.35 rows=1)
9,-> Hash
9,-> Inner hash join (t2.c1 = t1.c1)
9,(cost=0.70 rows=1)
9,-> Table scan on t2
9,(cost=0.35 rows=1)
9,-> Hash
9,-> Table scan on t1
9,(cost=0.35 rows=1)
9,(Additional examples are provided later in this section.)
9,A hash join is also applied for a Cartesian product—that
9,"is, when no join condition is specified, as shown here:"
9,mysql> EXPLAIN FORMAT=TREE
9,-> SELECT *
9,FROM t1
9,JOIN t2
9,WHERE t1.c2 > 50\G
9,*************************** 1. row ***************************
9,EXPLAIN: -> Inner hash join
9,(cost=0.70 rows=1)
9,-> Table scan on t2
9,(cost=0.35 rows=1)
9,-> Hash
9,-> Filter: (t1.c2 > 50)
9,(cost=0.35 rows=1)
9,-> Table scan on t1
9,(cost=0.35 rows=1)
9,"In MySQL 8.0.20 and later, it is no longer necessary for the"
9,join to contain at least one equi-join condition in order for
9,a hash join to be used. This means that the types of queries
9,which can be optimized using hash joins include those in the
9,following list (with examples):
9,Inner non-equi-join:
9,mysql> EXPLAIN FORMAT=TREE SELECT * FROM t1 JOIN t2 ON t1.c1 < t2.c1\G
9,*************************** 1. row ***************************
9,EXPLAIN: -> Filter: (t1.c1 < t2.c1)
9,(cost=4.70 rows=12)
9,-> Inner hash join (no condition)
9,(cost=4.70 rows=12)
9,-> Table scan on t2
9,(cost=0.08 rows=6)
9,-> Hash
9,-> Table scan on t1
9,(cost=0.85 rows=6)
9,Semijoin:
9,mysql> EXPLAIN FORMAT=TREE SELECT * FROM t1
9,WHERE t1.c1 IN (SELECT t2.c2 FROM t2)\G
9,*************************** 1. row ***************************
9,EXPLAIN: -> Nested loop inner join
9,-> Filter: (t1.c1 is not null)
9,(cost=0.85 rows=6)
9,-> Table scan on t1
9,(cost=0.85 rows=6)
9,-> Single-row index lookup on <subquery2> using <auto_distinct_key> (c2=t1.c1)
9,-> Materialize with deduplication
9,-> Filter: (t2.c2 is not null)
9,(cost=0.85 rows=6)
9,-> Table scan on t2
9,(cost=0.85 rows=6)
9,Antijoin:
9,mysql> EXPLAIN FORMAT=TREE SELECT * FROM t2
9,WHERE NOT EXISTS (SELECT * FROM t1 WHERE t1.col1 = t2.col1)\G
9,*************************** 1. row ***************************
9,EXPLAIN: -> Nested loop antijoin
9,-> Table scan on t2
9,(cost=0.85 rows=6)
9,-> Single-row index lookup on <subquery2> using <auto_distinct_key> (c1=t2.c1)
9,-> Materialize with deduplication
9,-> Filter: (t1.c1 is not null)
9,(cost=0.85 rows=6)
9,-> Table scan on t1
9,(cost=0.85 rows=6)
9,Left outer join:
9,mysql> EXPLAIN FORMAT=TREE SELECT * FROM t1 LEFT JOIN t2 ON t1.c1 = t2.c1\G
9,*************************** 1. row ***************************
9,EXPLAIN: -> Left hash join (t2.c1 = t1.c1)
9,(cost=3.99 rows=36)
9,-> Table scan on t1
9,(cost=0.85 rows=6)
9,-> Hash
9,-> Table scan on t2
9,(cost=0.14 rows=6)
9,Right outer join (observe that MySQL
9,rewrites all right outer joins as left outer joins):
9,mysql> EXPLAIN FORMAT=TREE SELECT * FROM t1 RIGHT JOIN t2 ON t1.c1 = t2.c1\G
9,*************************** 1. row ***************************
9,EXPLAIN: -> Left hash join (t1.c1 = t2.c1)
9,(cost=3.99 rows=36)
9,-> Table scan on t2
9,(cost=0.85 rows=6)
9,-> Hash
9,-> Table scan on t1
9,(cost=0.14 rows=6)
9,"By default, MySQL 8.0.18 and later employs hash joins whenever"
9,possible. It is possible to control whether hash joins are
9,employed using one of the
9,BNL and
9,NO_BNL optimizer hints.
9,(MySQL 8.0.18 supported
9,hash_join=on or
9,hash_join=off as part of the
9,setting for the
9,optimizer_switch server
9,system variable as well as the optimizer hints
9,HASH_JOIN or
9,NO_HASH_JOIN. In MySQL
9,"8.0.19 and later, these no longer have any effect.)"
9,Memory usage by hash joins can be controlled using the
9,join_buffer_size system
9,variable; a hash join cannot use more memory than this amount.
9,When the memory required for a hash join exceeds the amount
9,"available, MySQL handles this by using files on disk. If this"
9,"happens, you should be aware that the join may not succeed if"
9,a hash join cannot fit into memory and it creates more files
9,than set for
9,open_files_limit. To avoid
9,"such problems, make either of the following changes:"
9,Increase join_buffer_size so that the
9,hash join does not spill over to disk.
9,Increase open_files_limit.
9,"Beginning with MySQL 8.0.18, join buffers for hash joins are"
9,"allocated incrementally; thus, you can set"
9,join_buffer_size higher
9,"without small queries allocating very large amounts of RAM,"
9,but outer joins allocate the entire buffer. In MySQL 8.0.20
9,"and later, hash joins are used for outer joins (including"
9,"antijoins and semijoins) as well, so this is no longer an"
9,issue.
9,PREV
9,HOME
9,NEXT
9,Related Documentation
9,MySQL 8.0 Release Notes
9,MySQL 8.0 Source Code Documentation
9,Download
9,this Manual
9,PDF (US Ltr)
9,- 40.9Mb
9,PDF (A4)
9,- 41.0Mb
9,PDF (RPM)
9,- 39.7Mb
9,HTML Download (TGZ)
9,- 9.5Mb
9,HTML Download (Zip)
9,- 9.6Mb
9,HTML Download (RPM)
9,- 8.1Mb
9,Man Pages (TGZ)
9,- 256.7Kb
9,Man Pages (Zip)
9,- 366.8Kb
9,Info (Gzip)
9,- 3.9Mb
9,Info (Zip)
9,- 3.9Mb
9,Excerpts from this Manual
9,MySQL Backup and Recovery
9,MySQL Globalization
9,MySQL Information Schema
9,MySQL Installation Guide
9,Security in MySQL
9,Starting and Stopping MySQL
9,MySQL and Linux/Unix
9,MySQL and Windows
9,MySQL and OS X
9,MySQL and Solaris
9,Building MySQL from Source
9,MySQL Restrictions and Limitations
9,MySQL Partitioning
9,MySQL Tutorial
9,MySQL Performance Schema
9,MySQL Replication
9,Using the MySQL Yum Repository
9,MySQL NDB Cluster 8.0
9,Contact MySQL Sales
9,USA/Canada: +1-866-221-0634
9,(More Countries »)
9,"© 2021, Oracle Corporation and/or its affiliates"
9,Products
9,MySQL Database Service
9,MySQL Enterprise Edition
9,MySQL Standard Edition
9,MySQL Classic Edition
9,MySQL Cluster CGE
9,MySQL Embedded (OEM/ISV)
9,Services
9,Training
9,Certification
9,Consulting
9,Support
9,Downloads
9,MySQL Community Server
9,MySQL NDB Cluster
9,MySQL Shell
9,MySQL Router
9,MySQL Workbench
9,Documentation
9,MySQL Reference Manual
9,MySQL Workbench
9,MySQL NDB Cluster
9,MySQL Connectors
9,Topic Guides
9,About MySQL
9,Contact Us
9,How to Buy
9,Partners
9,Job Opportunities
9,Site Map
9,"© 2021, Oracle Corporation and/or its affiliates"
9,Legal Policies |
9,Your Privacy Rights |
9,Terms of Use |
9,Trademark Policy |
9,Contributor Agreement |
12,Server tuning — Nextcloud latest Administration Manual latest documentation
12,Introduction
12,Release notes
12,Maintenance and release schedule
12,Installation and server configuration
12,System requirements
12,Deployment recommendations
12,Installation on Linux
12,Installation wizard
12,Installing from command line
12,Supported apps
12,SELinux configuration
12,Nginx configuration
12,Hardening and security guidance
12,Server tuning
12,Using cron to perform background jobs
12,Reducing system load
12,Caching
12,Using MariaDB/MySQL instead of SQLite
12,Using Redis-based transactional file locking
12,TLS / encryption app
12,Enable HTTP/2 for faster loading
12,Tune PHP-FPM
12,Enable PHP OPcache
12,Example installation on Ubuntu 20.04 LTS
12,Example installation on CentOS 8
12,Example installation on OpenBSD
12,Nextcloud configuration
12,Apps management
12,User management
12,File sharing and management
12,File workflows
12,Groupware
12,Database configuration
12,Mimetypes management
12,Maintenance
12,Issues and troubleshooting
12,GDPR
12,Nextcloud latest Administration Manual
12,Installation and server configuration »
12,Server tuning
12,Edit on GitHub
12,Server tuning¶
12,Using cron to perform background jobs¶
12,See Background jobs for a description and the
12,benefits.
12,Reducing system load¶
12,High system load will slow down Nextcloud and might also lead to other unwanted
12,side effects. To reduce load you should first identify the source of the problem.
12,"Tools such as htop, iotop, netdata or"
12,glances
12,will help to identify the process or the drive that slows down your system. First
12,you should make sure that you installed/assigned enough RAM. Swap usage should be
12,"prevented by all means. If you run your database inside a VM, you should not"
12,store it inside a VM image file. Better put it on a dedicated block device to
12,reduce latency due to multiple abstraction layers.
12,Caching¶
12,"Caching improves performance by storing data, code, and other objects in memory."
12,Memory cache configuration for the Nextcloud server must be installed and configured.
12,See Memory caching.
12,Using MariaDB/MySQL instead of SQLite¶
12,MySQL or MariaDB are preferred because of the performance limitations of
12,"SQLite with highly concurrent applications, like Nextcloud."
12,See the section Database configuration for how to
12,configure Nextcloud for MySQL or MariaDB. If your installation is already running on
12,SQLite then it is possible to convert to MySQL or MariaDB using the steps provided
12,in Converting database type.
12,"For more details and help tuning your database, check this article at MariaDB."
12,Using Redis-based transactional file locking¶
12,"File locking is enabled by default, using the database locking backend. This"
12,places a significant load on your database. See the section
12,Transactional file locking for how to
12,configure Nextcloud to use Redis-based Transactional File Locking.
12,TLS / encryption app¶
12,TLS (HTTPS) and file encryption/decryption can be offloaded to a processor’s
12,AES-NI extension. This can both speed up these operations while lowering
12,processing overhead. This requires a processor with the AES-NI instruction set.
12,Here are some examples how to check if your CPU / environment supports the
12,AES-NI extension:
12,For each CPU core present: grep flags /proc/cpuinfo or as a summary for
12,all cores: grep -m 1 '^flags' /proc/cpuinfo If the result contains any
12,"aes, the extension is present."
12,Search eg. on the Intel web if the processor used supports the extension
12,Intel Processor Feature Filter You may set a filter by
12,"""AES New Instructions"" to get a reduced result set."
12,"For versions of openssl >= 1.0.1, AES-NI does not work via an engine and"
12,will not show up in the openssl engine command. It is active by default
12,on the supported hardware. You can check the openssl version via openssl
12,version -a
12,If your processor supports AES-NI but it does not show up eg via grep or
12,"coreinfo, it is maybe disabled in the BIOS."
12,"If your environment runs virtualized, check the virtualization vendor for"
12,support.
12,Enable HTTP/2 for faster loading¶
12,HTTP/2 has huge speed improvements over HTTP with multiple request. Most browsers already support HTTP/2 over TLS (HTTPS). Refer to your web server manual for guides on how to enable HTTP/2.
12,Tune PHP-FPM¶
12,If you are using a default installation of PHP-FPM you might have noticed
12,excessive load times on the web interface or even sync issues. This is due
12,to the fact that each simultaneous request of an element is handled by a
12,separate PHP-FPM process. So even on a small installation you should allow
12,more processes to run in parallel to handle the requests.
12,This link can help you calculate the good values for your system.
12,Enable PHP OPcache¶
12,The OPcache improves the performance of PHP applications by caching precompiled bytecode. We recommend at least the following settings:
12,opcache.enable = 1
12,opcache.interned_strings_buffer = 8
12,opcache.max_accelerated_files = 10000
12,opcache.memory_consumption = 128
12,opcache.save_comments = 1
12,opcache.revalidate_freq = 1
12,For more details check out the official documentation or this blog post about some recommended settings.
12,Next
12,Previous
12,© Copyright 2021 Nextcloud GmbH.
12,Read the Docs
12,v: latest
12,Versions
12,stable
12,latest
12,Downloads
12,On Read the Docs
12,Project Home
12,Builds
13,"Optimizing Drupal to load faster (Server, MySQL, caching, theming, HTML) | Managing site performance and scalability | Drupal Wiki guide on Drupal.org"
13,Skip to main content
13,Skip to search
13,Drupal.org home
13,Why Drupal?For developers
13,For marketers
13,For agencies
13,Case studies
13,About Drupal
13,Drupal 9
13,In the news
13,BuildDownload & Extend
13,Browse Repository
13,Documentation
13,Modules
13,Themes
13,Distributions
13,Issue queues
13,SolutionsBy industry
13,By feature
13,Case studies
13,For hosting
13,ServicesMarketplace
13,Hosting
13,Training
13,CommunityPortal
13,Contributor guide
13,Organizations
13,Forum
13,Promote Drupal
13,Community Case Studies
13,Drupal Swag
13,Core Development & Strategic Initiatives
13,ResourcesUser guide
13,Documentation
13,Support
13,Security
13,Jobs
13,Events
13,Newsletter
13,Project News
13,Partner Press
13,Drupal 9
13,"Diversity, Equity, and Inclusion Resources"
13,GiveDrupal Association
13,Supporters
13,Promote Drupal
13,Join us
13,Contributor guide
13,About Drupal.org
13,EventsDrupalCon North America
13,Community Events
13,Drupical
13,DrupalFest
13,Try DrupalDemo online
13,Download
13,Return to content
13,Search form
13,Search
13,Log in
13,Create account
13,Documentation
13,Search
13,Drupal WikiDrupal 7Managing site performance and scalability
13,"Celebrate 20 years of Drupal with us! April is DrupalFest, a month-long series of virtual events focused on community, contribution, and the positive impacts made possible with Drupal."
13,Add your DrupalFest event to the calendar
13,Advertising sustains the DA. Ads are hidden for members. Join today
13,On this page
13,Basic settings
13,Theme optimization
13,Coding standard and proper use of already existing core API
13,Secure codes
13,DB Query optimization in codes
13,DB table optimization
13,Disable unnecessary modules
13,Remove unnecessary contents and others
13,Cache modules
13,Make changes according to Google Pagespeed and yahoo YSlow suggestions
13,MySQL Settings
13,Apache settings
13,Also we can check this options :
13,1) Turn Page Caching On
13,2) Turn Views caching on
13,Managing site performance and scalability
13,Planning for Performance
13,Caching to improve performance
13,Changing PHP memory limits
13,Content Delivery Network [CDN]
13,Design for Low Bandwidth
13,Increase upload size in your php.ini
13,"Optimizing Drupal to load faster (Server, MySQL, caching, theming, HTML)"
13,Optimizing MySQL
13,Randomizing MySQL Users For Exceeded max_questions Error
13,Server tuning considerations
13,Tuning php.ini for Drupal
13,"Optimizing Drupal to load faster (Server, MySQL, caching, theming, HTML)"
13,Last updated on
13,25 March 2021
13,Basic settings
13,Configure cron job (for Drupal 6 http://drupal.org/project/poormanscron)
13,Make sure all cache tables are clearing properly especially cache_form
13,Enable cache options in performance page
13,"(For Drupal 6, http://drupal.org/project/advagg )"
13,Theme optimization
13,Manually Remove blankspaces and comments from .tpl
13,No indentation in .tpl
13,Turn on CSS and JS aggregation in the performance page
13,Manually reduce css file size by removing duplicate and combine similar together
13,Move codes to functions which should be in a custom common module. Use functions for similar problems instead of coding separately. Refer core API
13,Coding standard and proper use of already existing core API
13,http://drupal.org/coding-standards
13,https://drupalize.me/videos/understanding-drupal-coding-standards?p=2012
13,Secure codes
13,http://drupal.org/writing-secure-code
13,DB Query optimization in codes
13,Join db queries whenever possible
13,"For Db update and insert, use core API"
13,Use drupal standard http://drupal.org/coding-standards
13,DB table optimization
13,http://drupal.org/project/db_maintenance
13,Disable unnecessary modules
13,Devel
13,Statistics
13,Update status
13,Use syslog instead of Database logging
13,Remove unnecessary contents and others
13,Cache modules
13,"Make use of object caches to reduce database overhead, e.g. Memcache, Redis or APC"
13,https://drupal.org/project/authcache
13,Some module may help improve
13,http://drupal.org/project/ajaxblocks (not available with Drupal 8 & 9 )
13,Make changes according to Google Pagespeed and yahoo YSlow suggestions
13,MySQL Settings
13,Cache Size say 32MB in MySQL
13,Use https://github.com/initlabopen/mysqlconfigurer for fully automated MySQL performance tuning
13,Apache settings
13,DNS lookup : OFF
13,Set FollowSymLinks everywhere and never set SymLinksIfOwnerMatch
13,Avoid content negotiation. Or use type-map files rather than Options MultiViews directive
13,"KeepAlive on, and KeepAliveTimeout very low (1 or 2 sec)"
13,Disable or comment access.log settings
13,Enable mod_deflate or mod_gzip
13,Install APC server with higher memory limit apc.shm_size = 64
13,Also we can check this options :
13,1) Turn Page Caching On
13,"What page caching does is that instead of using a bunch of database queries to get the data used in making a typical web page, the rendered contents of the web page are stored in a separate database cache table so that it can be recalled quicker. If you have 10 people visiting the site from different computers, Drupal first looks into the database cache table to see if the page is there, if it is, it just gives them the page. Think of saving the output of 50 separate queries so that is accessible with a single query. You obviously are reducing the SQL queries required by a lot. What the page cache table actually stores is HTML content."
13,"Page Caching is that it only works to optimize the page load time for Anonymous users. This is because when you are logged in, you might have blocks that show up on the page that are customized for you, if it served everybody the same page, they would see your customized information (think of a My Recent Posts block), so Drupal does not use the Page Cache for Authenticated users automatically. This allows you to turn Page Caching on and still get the benefit for Anonymous user page load times, but does not break the site for Authenticated users. There are other caching options that will help with Authenticated user page performance, we will talk about those later."
13,"To enable Page Caching, you go to Configuration | Development and select the checkbox next to ""Cache pages for anonymous users""."
13,2) Turn Views caching on
13,"As mentioned when talking about Page Caching only working for anonymous users above, there are other caching options for helping with Authenticated user page performance. One of those options is to turn on caching for blocks and pages that you create using the Views module. This allows you to cache the output of the query used to generate the view, or the end HTML output of your View, and you can tune the cache for them separately. And realize too that this means you can cache portions of a page if you are using one or several Views blocks in the page, it will just cache that block in the page, not the whole page."
13,See more @ https://www.lullabot.com/articles/a-beginners-guide-to-caching-data-in-d...
13,Help improve this page
13,Page status:
13,No known problems
13,You can:
13,"Log in, click Edit, and edit this page"
13,"Log in, click Discuss, update the Page status value, and suggest an improvement"
13,Log in and create a Documentation issue with your suggestion
13,"Drupal’s online documentation is © 2000-2021 by the individual contributors and can be used in accordance with the Creative Commons License, Attribution-ShareAlike 2.0. PHP code is distributed under the GNU General Public License."
13,Thank you to these Drupal contributors
13,Top Drupal contributor Acquia would like to thank their partners for their contributions to Drupal.
13,Infrastructure management for Drupal.org provided by
13,News itemsNews
13,Planet Drupal
13,Social media
13,Sign up for Drupal news
13,Security advisories
13,Jobs
13,Our communityCommunity
13,"Services, Training & Hosting"
13,Contributor guide
13,Groups & meetups
13,DrupalCon
13,Code of conduct
13,DocumentationDocumentation
13,Drupal Guide
13,Drupal User Guide
13,Developer docs
13,API.Drupal.org
13,Drupal code baseDownload & Extend
13,Drupal core
13,Modules
13,Themes
13,Distributions
13,Governance of communityAbout
13,Web accessibility
13,Drupal Association
13,About Drupal.org
13,Terms of service
13,Privacy policy
13,Drupal is a registered trademark of Dries Buytaert.
14,ZFSTuningGuide - FreeBSD Wiki
14,Search:
14,Login
14,ZFSTuningGuide
14,RecentChangesFindPageHelpContentsZFSTuningGuide
14,Immutable PageCommentsInfoAttachments
14,More Actions:
14,Raw Text
14,Print View
14,Render as Docbook
14,Delete Cache
14,------------------------
14,Check Spelling
14,Like Pages
14,Local Site Map
14,------------------------
14,Rename Page
14,Delete Page
14,------------------------
14,Subscribe User
14,------------------------
14,Remove Spam
14,Revert to this revision
14,Package Pages
14,Sync Pages
14,------------------------
14,Load
14,Save
14,SlideShow
14,Contents
14,ZFS Tuning Guide
14,i386
14,amd64
14,Generic ARC discussion
14,L2ARC discussion
14,Application Issues
14,General Tuning
14,Deduplication
14,Suggestions
14,References
14,NFS tuning
14,MySQL
14,Scrub and Resilver Performance
14,"See also: Solaris: ZFS Evil Tuning Guide, loader.conf(5), sysctl(8)."
14,ZFS Tuning Guide
14,"(Work in Progress) To use ZFS, at least 1 GB of memory is recommended (for all architectures) but more is helpful as ZFS needs *lots* of memory."
14,"Depending on your workload, it may be possible to use ZFS on systems with less memory, but it requires careful tuning to avoid panics from memory exhaustion in the kernel. A 64-bit system is preferred due to its larger address space and better performance on 64-bit variables, which are used extensively by ZFS. 32-bit systems are supported though, with sufficient tuning. History of FreeBSD releases with ZFS is as follows: 7.0+ - original ZFS import, ZFS v6; requires significant tuning for stable operation (no longer supported) 7.2 - still ZFS v6, improved memory handling, amd64 may need no memory tuning (no longer supported) 7.3+ - backport of new ZFS v13 code, similar to the 8.0 code 8.0 - new ZFS v13 code, lots of bug fixes - recommended over all past versions. (no longer supported) 8.1+ - ZFS v14 8.2+ - ZFS v15 8.3+ - ZFS v28 9.0+ - ZFS v28"
14,i386
14,"Typically you need to increase vm.kmem_size_max and vm.kmem_size (with vm.kmem_size_max >= vm.kmem_size) to not get kernel panics (kmem too small). The value depends upon the workload. If you need to extend them beyond 512M, you need to recompile your kernel with increased KVA_PAGES option, e.g. add the following line to your kernel configuration file to increase available space for vm.kmem_size beyond 1 GB: options KVA_PAGES=512 To chose a good value for KVA_PAGES read the explanation in the sys/i386/conf/NOTES file. By default the kernel receives 1 GB of the 4 GB of address space available on the i386 architecture, and this is used for all of the kernel address space needs, not just the kmem map."
14,"By increasing KVA_PAGES you can allocate a larger proportion of the 4 GB address space to the kernel (2 GB in the above example), allowing more room to increase vm.kmem_size."
14,"The trade-off is that user applications have less address space available, and some programs (e.g. those that rely on mapping data at a fixed address that is now in the kernel address space, or which require close to the full 3 GB of address space themselves) may no longer run. If you change KVA_PAGES and the system reboots (no panic) after running a while this may be because the address space for userland applications is too small now. For *really* memory constrained systems it is also recommended to strip out as many unused drivers and options from the kernel (which will free a couple of MB of memory). A stable configuration with vm.kmem_size=""1536M"" has been reported using an unmodified 7.0-RELEASE kernel, relatively sparse drivers as required for the hardware and options KVA_PAGES=512. Some workloads need greatly reduced ARC size and the size of VDEV cache. ZFS manages the ARC through a multi-threaded process. If it requires more memory for ARC ZFS will allocate it. Previously it exceeded arc_max (vfs.zfs.arc_max) from time to time, but with 7.3 and 8-stable as of mid-January 2010 this is not the case anymore. On memory constrained systems it is safer to use an arbitrarily low arc_max. For example it is possible to set vm.kmem_size and vm.kmem_size_max to 512M, vfs.zfs.arc_max to 160M, keeping vfs.zfs.vdev.cache.size to half its default size of 10 Megs (setting it to 5 Megs can even achieve better stability, but this depends upon your workload)."
14,"There is one example (CySchubert) of ZFS running nicely on a laptop with 768 Megs of physical RAM with the following settings in /boot/loader.conf: vm.kmem_size=""330M"" vm.kmem_size_max=""330M"" vfs.zfs.arc_max=""40M"" vfs.zfs.vdev.cache.size=""5M"" Kernel memory should be monitored while tuning to ensure a comfortable amount of free kernel address space. The following script will summarize kernel memory utilization and assist in tuning arc_max and VDEV cache size. #!/bin/sh -"
14,"TEXT=`kldstat | awk 'BEGIN {print ""16i 0"";} NR>1 {print toupper($4) ""+""} END {print ""p""}' | dc`"
14,DATA=`vmstat -m | sed -Ee '1s/.*/0/;s/.* ([0-9]+)K.*/\1+/;$s/$/1024*p/' | dc`
14,TOTAL=$((DATA + TEXT))
14,"echo TEXT=$TEXT, `echo $TEXT | awk '{print $1/1048576 "" MB""}'`"
14,"echo DATA=$DATA, `echo $DATA | awk '{print $1/1048576 "" MB""}'`"
14,"echo TOTAL=$TOTAL, `echo $TOTAL | awk '{print $1/1048576 "" MB""}'`"
14,"Note: Perhaps there is a more precise way to calculate / measure how large of a vm.kmem_size setting can be used with a particular kernel, but the authors of this wiki do not know it."
14,Experimentation does work.
14,"However, if you set vm.kmem_size too high in loader.conf, the kernel will panic on boot."
14,"You can fix this by dropping to the boot loader prompt and typing set vm.kmem_size=""512M"" (or a similar smaller number known to work.) The vm.kmem_size_max setting is not used directly during the system operation (i.e. it is not a limit which kmem can ""grow"" into) but for initial autoconfiguration of various system settings, the most important of which for this discussion is the ARC size. If kmem_size and arc_max are tuned manually, kmem_size_max will be ignored, but it is still required to be set. The issue of kernel memory exhaustion is a complex one, involving the interaction between disk speeds, application loads and the special caching ZFS does. Faster drives will write the cached data faster but will also fill the caches up faster. Generally, larger and faster drives will need more memory for ZFS. To increase performance, you may increase kern.maxvnodes (in /etc/sysctl.conf) way up if you have the RAM for it (e.g. 400000 for a 2GB system). On i386, keep an eye on vfs.numvnodes during production to see where it stabilizes. (AMD64 uses direct mapping for vnodes, so you don't have to worry about address space for vnodes on this architecture)."
14,amd64
14,"NOTE (gcooper): this blanket statement is far from true 100% of the time, depending on how the system with ZFS is being used. FreeBSD 7.2+ has improved kernel memory allocation strategy and no tuning may be necessary on systems with more than 2 GB of RAM."
14,Generic ARC discussion
14,"The value for vfs.zfs.arc_max needs to be smaller than the value for vm.kmem_size (not only ZFS is using the kmem). To monitor the ARC, you should install the sysutils/zfs-stats port;"
14,"the port is an evolution of the arc_stat.pl script available in Solaris that was ported to FreeBSD by FreeBSD contributor, jhell. To improve the random read performance, a separate L2ARC device can be used (zpool add <pool> cache <device>). A cheap solution is to add an USB memory stick (see http://www.leidinger.net/blog/2010/02/10/making-zfs-faster/). The high performance solution is to add a SSD. Using a L2ARC device will increase the amount of memory ZFS needs to allocate, see http://www.mail-archive.com/zfs-discuss@opensolaris.org/msg34674.html for more info."
14,L2ARC discussion
14,"ZFS has the ability to extend the ARC with one or more L2ARC devices, which provides the best benefit for random read workloads."
14,These L2ARC devices should be faster and/or lower latency than the storage pool.
14,Generally speaking this limits the useful choices to flash based devices.
14,In very large pools the ability to have devices faster than the pool may be problematic.
14,In smaller pools it may be tempting to use a spinning disk as a dedicated L2ARC device.
14,Generally this will result in lower pool performance (and definitely capacity) than if it was just placed in the pool.
14,"There may be scenarios in lower memory systems where a single 15K SAS disk can improve the performance of a small pool of 5.4k or 7.2 drives, but this is not a typical case. By default the L2ARC does not attempt to cache prefetched/streaming workloads, on the assumption that most data of this type is sequential and the combined throughput of your pool disks exceeds the throughput of the L2ARC devices, and therefore, this workload is best left for the pool disks to serve. This is usually the case. If you believe otherwise (number of L2ARC devices X their max throughput > number of pool disks X their max throughput, or you are not doing large amounts of sequential access), then this can be toggled with the following sysctl: vfs.zfs.l2arc_noprefetchThe default value of 1 does not allow caching of streaming and/or sequential workloads, and will not read from L2ARC when prefetching blocks."
14,"Switching it to 0 will allow prefetched/streaming reads to be cached, and may significantly improve performance if you are storing many small files in a large directory hierarchy (since many metadata blocks are read via the prefetcher and would ordinarily always be read from pool disks). The default throttling of loading the L2ARC device is 8 Mbytes/sec, on the assumption that the L2ARC is warming up from a random read workload from spinning disks, for which 8 Mbytes/sec is usually more than the spinning disks can provide. For example, at a 4 Kbyte I/O size, this is 2048 random disk IOPS, which may take at least 20 pool disks to drive. Should the L2ARC throttling be increased from 8 Mbytes, it would make no difference in many configurations, which cannot provide more random IOPS. The downside of increasing the throttling is CPU consumption: the L2ARC periodically scans the ARC to find buffers to cache, based on the throttling size. If you increase the throttling but the pool disks cannot keep up, you burn CPU needlessly. In extreme cases of tuning, this can consume an entire CPU for the ARC scan. If you are using the L2ARC in its typical use case: say, fewer than 30 pool disks, and caching a random read workload for ~4 Kbyte I/O which is mostly being pulled from the pool disks, then 8 Mbytes is usually sufficient. If you are not this typical use case: say, you are caching streaming workloads, or have several dozens of disks, then you may want to consider tuning the rate. Modern L2ARC devices (SSDs) can handle an order of magnitude higher than the default. It can be tuned by setting the following sysctls: vfs.zfs.l2arc_write_max"
14,vfs.zfs.l2arc_write_boostThe former value sets the runtime max that data will be loaded into L2ARC.
14,"The latter can be used to accelerate the loading of a freshly booted system. Note that the same caveats apply about these sysctls and pool imports as the previous one. While you can improve the L2ARC warmup rate, keep an eye on increased CPU consumption due to scanning by the l2arc_feed_thread(). Eg, use DTrace to profile on-CPU thread names (see DTrace One-Liners). The known caveats: There's no free lunch."
14,"A properly tuned L2ARC will increase read performance, but it comes at the price of decreased write performance. The pool essentially magnifies writes by writing them to the pool as well as the L2ARC device."
14,Another interesting effect that's been observed is a falloff in L2ARC performance when doing a streaming read from L2ARC while simultaneously doing a heavy write workload.
14,My conjecture is that the write can cause cache thrashing but this hasn't been confirmed at this time. Given a working set close to ARC size an L2ARC can actually hurt performance.
14,"If a system has a 14GB ARC and a 13GB working set, adding an L2ARC device will rob ARC space to map the L2ARC."
14,"If the reduced ARC size is smaller than the working set reads will be evicted from the ARC into the (ostensibly slower) L2ARC. Multiple L2ARC devices are concatenated, there's no provision for mirroring them."
14,If a heavily used L2ARC device fails the pool will continue to operate with reduced performance.
14,There's also no provision for striping reads across multiple devices.
14,If the blocks for a file end up in multiple devices you'll see striping but there's no way to force this behavior. Be very careful when adding devices to a production pool.
14,By default zpool add stripes vdevs to the pool.
14,"If you do this you'll end up striping the device you intended to add as an L2ARC to the pool, and the only way to remove it will be backing up the pool, destroying it, and recreating it. Many SSDs benefit from 4K alignment."
14,Using gpart and gnop on L2ARC devices can help with accomplishing this.
14,Because the pool ID isn't stored on hot spare or L2ARC devices they can get lost if the system changes device names. The caveat about only giving ZFS full devices is a solarism that doesn't apply to FreeBSD.
14,On Solaris write caches are disabled on drives if partitions are handed to ZFS.
14,On FreeBSD this isn't the case.
14,Application Issues
14,"ZFS is a copy-on-write filesystem. As such metadata from the top of the hierarchy is copied in order to maintain consistency in case of sudden failure, i.e. loss of power during a write operation. This obviates the need for an fsck-like requirement of ZFS filesystems at boot. However the downside to this is that applications which perform updates in place to large files, e.g. databases, will likely perform poorly in this application of the filesystem due to excessive I/O from copy-on-write (a fast SLOG device -- e.g. a SSD -- can help regarding the write performance of databases or any application which is doing synchronous writes (e.g. open with O_FSYNC) to the FS to make sure the data is on non-volatile storage when the write-call returns). Additionally, database applications, such as Oracle, maintain a large cache (called the SGA in Oracle) in memory will perform poorly due to double caching of data in the ARC and in the application's own cache. Reducing the ARC to a minimum can improve performance of applications which maintain their own cache. At ZFS Best Practices Guide there are some generic recommendations for ZFS on Solaris which mostly apply to FreeBSD too."
14,General Tuning
14,There are some changes that can be made to improve performance in certain situations and avoid the bursty IO that's often seen with ZFS. Loader tunables (in /boot/loader.conf): # Disable ZFS prefetching
14,# http://southbrain.com/south/2008/04/the-nightmare-comes-slowly-zfs.html
14,"# Increases overall speed of ZFS, but when disk flushing/writes occur,"
14,# system is less responsive (due to extreme disk I/O).
14,# NOTE: Systems with 4 GB of RAM or more have prefetch enabled by default.
14,"vfs.zfs.prefetch_disable=""1"""
14,# Decrease ZFS txg timeout value from 30 (default) to 5 seconds.
14,This
14,"# should increase throughput and decrease the ""bursty"" stalls that"
14,# happen during immense I/O with ZFS.
14,# http://lists.freebsd.org/pipermail/freebsd-fs/2009-December/007343.html
14,# http://lists.freebsd.org/pipermail/freebsd-fs/2009-December/007355.html
14,# default in FreeBSD since ZFS v28
14,"vfs.zfs.txg.timeout=""5"""
14,Sysctl variables (/etc/sysctl.conf):
14,"# Increase number of vnodes; we've seen vfs.numvnodes reach 115,000"
14,# at times.
14,"Default max is a little over 200,000."
14,Playing it safe...
14,# If numvnodes reaches maxvnode performance substantially decreases.
14,kern.maxvnodes=250000
14,# Set TXG write limit to a lower threshold.
14,"This helps ""level out"""
14,"# the throughput rate (see ""zpool iostat"")."
14,A value of 256MB works well
14,"# for systems with 4 GB of RAM, while 1 GB works well for us w/ 8 GB on"
14,# disks which have 64 MB cache. <<BR>>
14,"# NOTE: in <v28, this tunable is called 'vfs.zfs.txg.write_limit_override'."
14,vfs.zfs.write_limit_override=1073741824
14,Be aware that the vfs.zfs.write_limit_override tuning you see above
14,may need to be adjusted for your system.
14,It's up to you to figure out
14,what works best in your environment.
14,Deduplication
14,"Deduplication is a misunderstood feature in ZFS v21+; some users see it as a silver bullet for increasing capacity by reducing redundancies in data. Here are the author's (gcooper's) observations: There are some resources that suggest that one needs 2GB per TB of storage with deduplication [i] (in fact this is a misinterpretation of the text). In practice with FreeBSD, based on empirical testing and additional reading, it's closer to 5GB per TB. Using deduplication is slower than not running it. Deduplication [on 8.x/9.x at least] lies via stat(2) / statvfs(2); it reports the theoretical used space -- not the actual used space -- which can confuse scripts that look at df output, etc (TODO: find PR that mentions this)."
14,Suggestions
14,"If you are going to use deduplication and your machine is underspec'ed, you must set vfs.zfs.arc_max to a sane value or ZFS will wire down as much available memory as possible, which can create memory starvation scenarios. It's a much better idea in general to use compression -- instead of deduplication -- if you're trying to save space, and you know that you can benefit from compression. When in doubt, check how much you would actually gain from deduplication via zdb -S <zpool> instead of just turning it on. Please note that this will take a while to run, depending on the dataset/zpool selected."
14,References
14,http://blogs.oracle.com/roch/entry/dedup_performance_considerations1
14,NFS tuning
14,"The combination of ZFS and NFS stresses the ZIL to the point that performance falls significantly below expected levels. The best solution is to put the ZIL on a fast SSD (or a pair of SSDs in a mirror, for added redundancy). You can now enable/disable ZIL on a per-dataset basis (as of ZFS version 28 / FreeBSD 8.3+).  zfs set sync=disabled tank/dataset  The next best solution is to disable ZIL with the following setting in loader.conf (up to ZFS version 15): vfs.zfs.zil_disable=""1"" the vfs.zfs.zil_disable loader tunable was replaced with the ""sync"" dataset property."
14,"Disabling ZIL is not recommended where data consistency is required (such as database servers) but will not result in file system corruption. See ZFS Evil Tuning Guide, section ""Disabling the ZIL (Don't)"". ZFS is designed to be used with ""raw"" drives - i.e. not over already created hardware RAID volumes (this is sometimes called ""JBOD"" or ""passthrough"" mode when used with RAID controllers), but can benefit greatly from good and fast controllers."
14,MySQL
14,This assumes lots of RAM Tweaks for MySQL innodb_flush_log_at_trx_commit=2 skip-innodb_doublewrite Tweaks for ZFS zfs set primarycache=metadata tank/db zfs set atime=off tank/db zfs set recordsize=16k tank/db/innodb zfs set recordsize=128k tank/db/logs zfs set zfs:zfs_nocacheflush = 1 zfs set sync=disabled tank/db Note: MySQL 5.6.6 and newer (and related MariaDB / Percona forks)
14,"has innodb_file_per_table = on as default, so IBD files are not created under tank/db/innodb (defined by innodb_data_home_dir in your my.cnf), they are created under tank/db/<db_name>/ and you should use recordsize=16k on this dataset too or switch back to innodb_file_per_table = off References MySQL Innodb ZFS Best Practices (Oracle)"
14,Scrub and Resilver Performance
14,"If you're getting horrible performance during a scrub or resilver, the following sysctls can be set: vfs.zfs.scrub_delay=0"
14,vfs.zfs.top_maxinflight=128
14,vfs.zfs.resilver_min_time_ms=5000
14,vfs.zfs.resilver_delay=0Setting those sysctls to those values increased my (Shawn Webb's) resilver performance from 7MB/s to 230MB/s.
14,CategoryZfs CategoryHowTo ZFSTuningGuide
14,(last edited 2021-01-16 19:54:35 by MateuszPiotrowski)
14,Immutable PageCommentsInfoAttachments
14,More Actions:
14,Raw Text
14,Print View
14,Render as Docbook
14,Delete Cache
14,------------------------
14,Check Spelling
14,Like Pages
14,Local Site Map
14,------------------------
14,Rename Page
14,Delete Page
14,------------------------
14,Subscribe User
14,------------------------
14,Remove Spam
14,Revert to this revision
14,Package Pages
14,Sync Pages
14,------------------------
14,Load
14,Save
14,SlideShow
14,MoinMoin PoweredPython PoweredGPL licensedValid HTML 4.01
16,MySQL server tuning
16,You are not logged in. Click here
16,to log in.
16,codeBeamer Application Lifecycle Management (ALM)
16,Last ModifiedRecently Visited Items
16,Ever
16,In 7 days
16,In 30 days
16,In 90 days
16,Search In Project
16,Search inClear
16,Work Items
16,Branched Work Items
16,Trackers & Branches & Baselines
16,Documents
16,Wikis & Dashboards
16,Item Attachments & Comments
16,Baselines
16,Source code Commits
16,anonymous
16,Anonymous User
16,HelpLogin
16,Login
16,Projects
16,Wiki
16,Last ModifiedRecently Visited Items
16,Ever
16,In 7 days
16,In 30 days
16,In 90 days
16,Search In Project
16,Search inClear
16,Work Items
16,Branched Work Items
16,Trackers & Branches & Baselines
16,Documents
16,Wikis & Dashboards
16,Item Attachments & Comments
16,Baselines
16,Source code Commits
16,anonymous
16,Anonymous User
16,HelpLogin
16,codebeamer»Wiki»Administrator's Guide»Tests & Audits»Performance Tests»Optimizing codeBeamer PerformanceMySQL server tuning
16,#937831/HEAD / v150
16,Menu is not available…
16,Tags:
16,not added yet
16,MySQL Server Tuning
16,The default MySql-5.7 values fit normally for minor/middle installations.
16,For larger installations it is recommended checking the MySql values below that have the largest impact on codeBeamer & MySql performance:
16,query_cache_size = 0
16,query_cache_type = 0
16,innodb_buffer_pool_instances = 8 (or 1 if innodb_buffer_pool_size < 1GB)
16,innodb_file_per_table=ON
16,innodb_stats_on_metadata = OFF
16,The values below should be adjusted depending on the available physical memory (RAM) of the MySql server:
16,RAM: 4GB
16,innodb_buffer_pool_size = 2G
16,innodb_log_file_size = 256M
16,RAM: 8GB
16,innodb_buffer_pool_size = 5G
16,innodb_log_file_size = 512M
16,RAM 16GB
16,innodb_buffer_pool_size = 10G
16,innodb_log_file_size = 1GB
16,The values above are examples for dedicated MySql servers (no other services are running on the server).
16,"If codeBeamer and MySql run on the same server, it is recommended that codeBeamer gets 50-60% and MySql 20-25% of the available RAM."
16,Parameter values can be changed in MySQL server configuration file. File location can be different depending on the specific installation.
16,MySQL server configuration file location examples:
16,Windows: C:\ProgramData\MySQL\MySQL Server 5.7\my.ini
16,CentOS: /etc/my.cnf
16,Ubuntu Linux: /etc/mysql/mysql.conf.d/mysqld.cnf
16,An example:
16,[mysqld]
16,# some other variables here
16,query_cache_size = 0
16,query_cache_type = 0
16,innodb_buffer_pool_instances = 8
16,innodb_buffer_pool_size = 5G
16,innodb_file_per_table = ON
16,innodb_log_file_size = 512M
16,innodb_stats_on_metadata = OFF
16,Please refer to https://dev.mysql.com/doc/refman/5.7/en/optimization.html for further configuration options.
16,"Occasionally it is advised to check and reorganizes the physical storage of table data and associated index data, to reduce storage space and improve I/O efficiency when accessing the table. MySQL OPTIMIZE statement can be used on tables for achieving this. It is advised to do it offline, without running codeBeamer instance."
16,An example in MySQL console:
16,mysql> OPTIMIZE TABLE object_reference;
16,mysql> OPTIMIZE TABLE object;
16,mysql> OPTIMIZE TABLE object_revision;
16,mysql> OPTIMIZE TABLE task;
16,mysql> OPTIMIZE TABLE task_type;
16,Please refer to https://dev.mysql.com/doc/refman/5.7/en/optimize-table.html for further options.
16,Fast Links
16,Menu is not available…
16,codebeamer Overview
16,What is new?
16,How-to videos
16,codebeamer Knowledge Base
16,User's Guide (user manual)
16,Administrator's Guide
16,Installation and Configuration Guide
16,Developer's Guide
16,Localization Guide
16,Services by Intland Software
16,Product Support
16,Consulting
16,Training
16,Integrations
16,This site is powered by codebeamer Dorothy-RC1 (mysql). |
16,Incident / Question |
16,Knowledge Base
16,Hotkeys
16,Licensed by
16,Intland Software GmbH
16,This website uses cookies
16,"This website stores cookies on your computer. These cookies are used to improve your browsing experience, constantly optimize the functionality and content of our website, furthermore helps us to understand your interests and provide more personalized services to you, both on this website and through other media. With your permission we and our partners may use precise geolocation data and identification through device scanning. You may click accept to consent to our and our partners’ processing as described above. Please be aware that some processing of your personal data may not require your consent, but you have a right to object to such processing. By using our website, you acknowledge this notice of our cookie practices. By accepting and continuing to browse this site, you agree to this use. For more information about the cookies we use, please visit our Privacy Policy.Your preferences will apply to this website only."
16,Accept
17,"�      ��Ks#I�&��#E�?X�N%ɓ���"
17,2:��Q'^
17,"dVtIVN��� x��tw���J����Y�͕^��]͈\�;2�+wV=2�������"
17,�3ws��H�٧+@ws353��T�T�
17,~�����?�{.���?��!�#|���TP��dKL#5�>m��(�L���Ѵ:Q�A�{1
17,",>�~��u)�"
17,�w�)���b�
17,"m�p������""�_"
17,"7Z�ԛ�""	��T)W�0��}�8�T4"
17,%v~x�����g���Ӈ���H�G[@��r�-1������Ū�̧��K��J����@N��ֹ�.�a�׉
17,�M�G�:�
17,U�?��x�'�J�H_
17,"Ց���y�Dđs������l���)���,tY�H��_Oui?���_���s �h�į���:2q�;j������&����D}N�?�s�O�����I�0SAO^���=gW�U�ˈ&X"
17,�`�����\?vC�&��&���T&c
17,"�C��TD��_ů���`�ᐿ;:[��	���-E*�E��P~���C�B-T��ɓ�_�d U��Y8��-R=��/�_\\T/�5���N�/����A�i�A�l��UT=������V�"
17,�j��
17,"_�J݌�#�0�`α��Cz�,��0 �	�_"
17,�乯�g�d~*Go`�v������~��75�x�UT}��1>��[l��������>N����
17,H��NI��	\�-���0�v��7�z�K
17,>����
17,�t��0i�C\'q�+x;�I�
17,"Z\�u�""N�^,b/Q�"
17,"��7�~��}�%c����s(�D�<+`�O��DL�L�8oT�ն�3�s,���}.��*���A^ԮbJ�vk]?"
17,�<��
17,$�5�
17,"b6Ň�8H�#�""������@'�"
17,�U{��*�;��4�y��=DD���;E�6�&�
17,�[�y���
17,"�[T�d�}��~�N����""�t�^0҄,~��ܢ����,�����*Lg��u3�ꌅ"
17,H��2�z�����c�N��Q��m��z��i��Z����z�Z�Uk�Oav�lF�QM&s��p)�ʚ|�ͱ�4B�UVF����KJ�Vf��~���?
17,"aI �-�o�'��c?�³�����+=���� ���3&+�����,�W�ϕ;����+�"
17,$1�N`��j��Z���{����ȑZq��]l6E�%�o����Z^�HϦ4��i0��chK�
17,82rmx�M&2��e4R��%_��a�%
17,"�����j�U���n�h��圿䓿�,c�H�$�,��B9c�$Q%���!�� ��B�6o�������9�b��v~kY�a5�N�8�V�$�^T�hT(�%}�p qY��ө�����' ��@�L�o""��,</p��z7"
17,"�m�7m�i8!�wU"""
17,]��C�)��|X�P����.��'�H�����9�
17,�ں}�l��
17,f���X��:
17,\9���
17,"�P��D�֨��A�/�dJǠm���sk���a�Q�q� �iL��"
17,1�x
17,"H;O,s��0�/Gc蹑»�z��1�����$��>�$t"
17,"�	���zhxp����V��DV�!鿰����_=@�i�}<WQ�0l�;�L��aV�h���L!�@�A�� �Ĥ�l������r૓$�7#Y�*?<����z��mT��|����Q���\�('�=E�Y��"
17,"����h���8��H �t����%��C�Z?N5J�|O�کEܷb�{��+�AO�"
17,�jhz�Ų��'
17,"� Y\�P�=�s�L�Ki��] "
17,�ŏ?YÛ��V���xi��d��]{���b��
17,��t<�ͺx�
17,"X&��h4�}�?����C����}�Z!�����<�~��w��x/��E{ro��W��m^H�x;�,���Qt���t��󷿥��R}v�q��#��o����t�w@��!�wݣ��Q��u!����"
17,�#XN�*mh�	w���	�)�?��?�~:�U^Uu��v����V�7��
17,"�M�( .��]�:���q�33���Ku{o��:�i�Ok=��S"
17,%�	���u��z���
17,����5@
17,���J�-P먓�W��r�\�~��`Y���/�Chj�SZi��Sm�H%��
17,i��
17,bkK`_l��̂K����Ε�2vN�L�ߓ��V�����^��y�< d
17,�p:D�Zh��Y��ɯ�����5>�C��wS�B[
17,"#	��������,.}�u�!�mMY)]d�S��t-��eN%��DM��"
17,�R�f�
17,��~�/+��캴t(��
17,"�C ��GlfſL���K�J�@�Ӊm��"
17,"�m�	�4��2 ��|�h��n�*��+ʑ f����M��D@��*��lg�=�dd�n�����c>�&c��>S���5�X]аv)���"
17,.����
17,"�O3�~i�ǾY{?����.�sU`O��� ��o3� ����Z�;�Ƭ""��<�)q2�U��A8"
17,܏(-�G?��/[��/[���_���z[�?�����-_�O��p���_�*��D}D�ԁ��l��bI�m�Y���G㧟�q
17,���
17,"6�8��B^ܬQ)E%y_� ��>�7fȫP�?�k�8o����O�����m�9o�{��"
17,�.<�j�c��լv���jG[��U5���2�W������TZ���L�Szυ<�j�%�=U]
17,PTb����*�
17,"���}2�; O�����"
17,Zӏ�޹!�ue��
17,";�M�^���y�=�k�""�S�G���资��T�Sx �F�2�lE��۱6Ģ/Dj��:�"
17,?ܗ8�����)5n�
17,_^���)|
17,"}9&AE��g=�g v�=�Lw���( ��@8��<ʞ���ϩm0z.*�Ğ���U�Prˮh"
17,]���8��f�����
17,>Y	�(+$�����z�
17,"�� ��3�.x��b�1~,"
17,"z�ݡC	R#ʎ(iú�<Y{���#��aؽc����ˆKg�'�mY����έ�f�Y�Q��yWȐR�S�������Ը1I��Ƌt���W6޾q�� <"
17,"��/�$�uc@ƻpd\2'�͕D4oLH�Z\�┞_IC�Vh�%!,��"
17,"��*JZ�Sb4�""�Cf.��p�n���t�:�Aӕ�Ɖ� Ȩ�u��"
17,"�������7�-��߳�S%!��I��h��wW@_dG��x\�إ<����M	�O��&!t>�v��z��b�W�]g��6 �P�� l�Oune(U���g#��T5�"
17,.g�W��d�{�v�(I�\��אO�*Ną���
17,V�b�ͽ�5�~��
17,ѱ�5Wt�]s����'\��/ì�;
17,b��
17,���T��j� �
17,;^���D
17,��*iMS�bO�S��Mɫ:�Z&+���
17,~�aLݤ���Z?3�П_��s�K;����Wt�E��
17,b�}�����) �s�7��5��C�
17,�s����̹�KW�x�N%a�r�[���CedZ(��
17,)��:{X	��h���6�g���suZ�޵�&}��r�1�i� �D������sm���A�Eh�_J�	��
17,"Y��k˗L��T#�#�Q��xN%V%�ڊ��1����""\���]��k�L;9��$=�""�ͪ4][=b��L5�A����ş�@���e��u��O��x:ɸ�_���@ڵՅ�Ʊua��������;�a�9H#a�h9Z,vU��^9���2��]�3')n(lbz��|� ����U�<+��'d924��_����SX^��ōB��V��j8��؁�90���80�*��@"
17,��:�O�U��!�1�n��C�~��-�����
17,�\.����j��T4�
17,mA)c
17,3%��
17,�t�
17,�Q���χ�X�(M�١6����=�8�0�6��YBCV�ff{�����ЊO�Q�Eߒ	�VAZ�.�?[�dƆ�-
17,T��|�@�p8�C(C7
17,���[h~j�K�h�N4�T�>ND6��4��H����}��������vq�h
17,)	
17,"FM�[Z�y�rF�0�FE�`���hpέ�t�/G�rY�}����V�����W��ɦ,J>��I?��~��W�& ��"
17,�ă�T��rA/B]*���K�`�=x< . 
17,���%��K�W���*���4�_���J�'ZTk��[�]5ʇ��T=�jEZ�B��f���e�d6n�	/�C��
17,"�,�>������]j�y.cϟ"
17,��B��t'�Z��d�X����6�H[I�#����YO;�^��=
17,�.������ũ�L���%5
17,i�С��$�����
17,??.<�r=8>@_�H�4���_�O�*��
17,%�B���h�ɿD��N4�
17,�-��w��$}�ʋaџ��
17,��\ֱܖ9粫��Pn��ө�T��ǸJe�
17,(�`C�l>��42Q�|V�
17,A�y�.Dζ�����t��ʠt�
17,����'Q�(8aL'
17,"�]�-s&�c;�	���:�PJ׍""ELW��F#�uP����j��N�|�W>�%��w��泅3c��"
17,"��z��I�$`,��ϸn��"
17,b����Cآ^a��l�J���ChSY$�ס�
17,"=�{��� "
17,l�Q����Z�5
17,�9hu���j���4�s7`9L1Q�h-^�8
17,ӊ��N��a�0����xS@�4Ju
17,�đ�ǅ���BϽ����
17,":��d�)�B�? �I�N�8$�H�֓wMN�IB��B�M[ǍޙfƯ��$!��+�Ӳe0���nD""&��G��݅��N��G��k���׬��nUNu�#z�#GO@�"
17,g����yD����AM�?F!���*�;�p���@��M�A���t$
17,��a�)�H��xaz����R1
17,"a�9+cvy�l�p�t12O;�](�<�є�G"""
17,�#����h���[
17,�ob��
17,uD_s{� �
17,"�|k@�[�o��[�oS���@��?4: 3h>���o@eg���y|�|���7�<�c�͊�гJo�uMH�"
17,��f��
17,"{�ew���!��C�	""�ZO�"
17,¡m��
17,V=�Wʩ��+r*���1�
17,"V�A4e��""f�y�r�"
17," �Tmַ��Be��f����[�WD��`MxX"
17,�)��U�+=-H
17,����Ou�?ٿ��������ř�Q�L�S>
17,E����-��M�͑�ul�����>�
17,"H%IC��p��\�.%��-�u)SҿB专EV��Y""�,����"
17,ʝ�3�
17,]Ņ��S��>���
17,�!gs<;��n��̌�?��*�s��mU�
17,"`�=�C����,h���2ƀ6�3�,�$�O� YQ;'R�︶TQ��"
17,F����F��b��x�����|�$!�.�.
17,"Z�""g-��ld���F�r-��""��J���4������D�Qv�v����|%*U?>FR�0tR�9��f�����J�a>O�Q�r"
17,���d'FF�(�s�I�i����3�g
17,",p4��<��"
17,"�p"".�s=�"
17,"�~��p�X���ڔʖs>��)��!��X켁>�Xj����˝W�Vj��V�O5""��tYMF4EB\�!�%=�y$�/�\yJ����@�Z�""\�i�w��-@ο(5-N��#�c�uk0�"
17,"-Ź>=��;m -����gM\M?���\�����D�"
17,"�1���/�&��^bʀ��fI\@��8�~���L�4�Y��������%0��T<��^�O����Ô?�""�$�<�����#"
17,"^�L#�1��XxQ�A�*��k͹ ��"
17,-u��X����R�?�CR�;��s�F��%E3�C	N����U�K�LjWO��F��1���Jރ8� ��
17,����;�^�^V
17,���
17,X�Y
17,"��S��K�M�X,��F�T��s>H(E+�¿��@�~��l�b2)����LƗ��g�h^)��PJ�[��R��/s-3��05,{CX�]�`n���?��h8W~��������dJ�t�������d���xNOI��"
17,3�#.�9�d:���$��߿�DB
17,"�������|���'	,vX�1��ĭ���F�V��rpg��}e""D��"
17,�\�'a�����
17,"Ԛ)�O��""���.�8]�G&�aKnH)�����J�| $��Ƌvh=2�AWX�""��Ř`"
17,��Q2�.p
17,#3��C���=��	PD�9G�.U6
17,� �Ҁ�źf҂�!�
17,��P҈���3���$x�Rײ�?v
17,:_��'	y �
17,é�#U l��y&��<��aHO�/a���-�˲?� �H�C�N��H
17,W��A/�`@�u'�q}�q�����RX
17,"���� ��B��(,����Ԭ�J�l'�����C���lZ5a/�sCv#"
17,-�!�ai𪟦��vY�eZY���V�i�Z�[L�5e�i���f��b�d��ɸ5H�qr��Զ*�pYr$'wC-
17,"�` ��M�����<uհ�Ae 6"
17,{�3.��4
17,�������3A��gd[�g�=���41A�֕�A�ږ�3���L���Ѓ֙@WV�
17,j�Y8/
17,��~�W`��ht{;�ix�MpƋ
17,��%
17,��_�'$�d���Hq�w�4ta�?G�w��
17,��D�D����	����t�O�p$�$(Q��}up:�Q��.�b�������T�	bNFs4��
17,�.\�9b�bh�$u����
17,�V3
17,"��V,���V9NW~�0�H�"
17,��Zz0���F��>�.�!2��Ո<��Ö
17,"uNM �0��bpt���[i{/1G��������x�<;R�(��1�7��5�����"
17,�(���
17,/&�h���Aɬ�g纥-Z�L��<�/�ɇw)���
17,"�8]L9f�T�¶_l������Nd�>�� ���bHs���Q�Ȇ8a�""8�+��K�ti�pq�nR�{�G�̛�	XV�5Yf�eM�n���A�oo������D���'��=�Dl"
17,0G\�x���r��48?�q��̇��MpJe�
17,}R��a�଱ ��K�� �Y�
17,�mZ{�e�N`=䇪x�Bt��=#��֧
17,g��b�$�QB�
17,�4]��&�Y��j�h�$�	8
17,�WK2َ�v&��G�����P���Oa/��������SnSY���zo�N�P�f<��
17,B�2A�
17,"�Jyp�YT���j��*u��=�"""
17,�5��
17,�KY�Z�s�X�-�W ��L��·S{��ٓ���<0�$Sp.��P4Ay��@
17,+��9	���qɛ��/#uPP��I
17,2K�~��n}�r�%
17,KØ9�ј��r�y�z�Y�
17,RB�8�Eex]��c�S]
17,"M�""E/�e&YC"
17,E�a�
17,�Ѽ>c-
17,"y����M\N��$�YL㸰o⩊�""	#��Q�L@q�"
17,"��4T����Z����%6�i\��0�}��0p&���� ݩy�|ht}�Jw�i���ֵfLp�b ^dN�2��"
17,"L\BLj^��)*� �*���gpB�C.�N"
17,"�a<�ф�""1�naQ�ǜ3��g#(�_~r�J� ���"
17,"SH 0<]��e���0��j\אmc�bx6��R""�'�~Տ�$j��qfQ7!��0Ǔ�� HD�O�p�T�5f�#���,��������R��""BǪ�`u��qZ"
17,�:������\�~q�&I�N�±�+A�it���qw�4<
17,���C{r�pŕ�͂ �n�$-KM����|�3Qy�1�m�!��
17,"�L�]p�!f�'=�� "
17,�����`�
17,"�Za7��T""���h��J���$7�"
17,"�����h�6�iԠ�\J�MWo���a�O>}_�~��alN�ǰi�0��)��B�u�{E�!�Ja �KV�*+w�W�V�d�ⲕ<�؊��d[�"
17,��|�-P�N���ta!V��
17,"W��b��.���x�!,�.%N������/SH��<"
17,�C���t��@Db�=A�l1NB@�1�����D�b	���5y��iLwX�'��puz�x�*�a�Գ��§*�R�#ҋ��3�Y6�~�%gt��w���<��Nʸ�
17,2��Y�g
17,"��gmz��(�҄+ţ�{[�O� �.�QȈ��C8_c���'tR�fSv�O{��,�m�E��tn���ٶ2k-�H֌�^�E$���P���*"
17,��;�
17,"�M�tUMӍ,�H��ǲ�/�����9W"
17,�sֿ8�4�n��
17,"c: ��c^�l"
17,�+��eͶD
17,"��p|�5�L$����|%�� �}���C�⨊�	O,ڸ�jٗ���ׯ҃���D�ӑ!���Ld�a�V�V!�b{ 7�_]|�M����l�*�#�|"
17,"3�mI��>�}j�+E�+�SG���n�J��9�]�\���-�&�Zo��U�)f��\z>�>��� ��t��D��x:gju��2�/�tz>N+'�vt��]��3�G�{f蠱x�<��"")L��:�5w��`7�G3�|��7��N�F���#���"
17,#v�tO�5H�R�E��y�L�6�
17,"1�S����ͷ�om~�,��2��R�T:k�:BI"
17,"�<v0�먄�c�5M�w(�kCz�emm�Q��L%�EK�,��4Ȝ�sSS�-gճ��@�J�П��&�V���s""Z"
17,��<���
17,21yQ��Q�~��b.⭨�̖��25�Zɧh�C���ٰ�;(f_��V��􈘴sH��I
17,�Ś���vt���Ot����
17,��#W{�i^}
17,g���i
17,�t�;�9�������=�ժ�S
17,�'ߥ�Yp[_��˭�ؚ�g�I�h��s�
17,"�</�~�E��KfI_�i믖��N�J��F�{#J`��zN+""Noa,svG��3��p���4Y��3>s�rOS�Fdݻ�./J�W5Z|�i��"
17,��zXv�som�ƈe���1��rQ��݀@&}
17,Vt`�9�dW
17,%l�Յe�� c�?kA�2 CU^Dd
17,���dk
17,Z��+�ʓ��ɸ��5r��o���Pպ�T
17,-X��3��OAs�c��EW����
17,"{8���QЛ����͵l�cvC�5�L���uiZ�Yy�`����Se��K,]��5�;��������*5�خ����~|�{:��	��fH晡�Dd��.�E{��0� � ��W�ߵ����9(��/"
17," �C=2�G�� ��c�EC,z Q��8F��~�qW�N)�6��m&&~vi�Z<3�"
17,E&�0�Q\t���hXU��
17,)��/q5Q��
17,"�h5:r�""���W"
17,"���P.���p�\йX���*2+���{�f_�[A�U��L1�[�A���A����\�[��O����r��j[SZ���8�Av>����U��^f Vv�,7�""���R�s���X�h�Ct��!M.B��"
17,���wϿ#�߽�.��6{/:
17,使�*ݣ̎��{i����v���e��O�����C>;�O<4填���6�8�bvM���q�@?�u�e��a|��L�Dz>
17,��sY��R��
17,�#{��*��	Ю�2xH�ƅ�9
17,�=�J�9��J�[���\1]�QlM�����6��[�6F����oH�[7D6AG)7=׵�	�]�W�ݠ@�ҋW��̱�L��*��q(�`�4��
17,�r���C�WS/9�i}�c��_�+fn�9��
17,O䧐DC���D$�5T
17,"�Y�	ǝ1�""k��`�֏��t�[n�J�7�,֮JX�"
17,c�]��d�-wHe �&�n�u� `eT�퍺�N鎽�)�{�)��b��
17,f�X}v���3Z�Xc���>^��>Kڟ�}�t_��
17,"mc��,Z*�&�^RK'����U���+,�Cn*���/�\��|�E�,t�|����E�ܶ�x`�D"
17,�L�@�̠�E�8-x��\��?�F��w�
17,"��t'��L.���>s�,�|���x*:��L��Yظ��r��권N� ��L��jR�`d]<�$�0"
17,"`�%�>)�Rj�8��xk@�H��,���6w�ki�ˑ/�����i��`�6��"
17,Q�u.�������f��X-�֊�ܳ�h`��6�H�˭���$ѱ�#Č�>+��Vs
17,=���ݐ�p�:;$gl��
17,�f�|��˴�9+A�3O�����^Z�j
17,>�D|�AG#�X�l��ê�P��8
17,"�֚3.�����8�>�[0*�����꩷�)��8p?�$���)O��)E �	l�]]�9�����:Nٰ�����u1FS�F�\��Hڈt�=x��D%�=�PZӍ$z���d���&ʏ(z�����'遜-�k7T��R"
17," ����G\=��L�!����	}��G""�"
17,�v���v�2��ᬜ�![���s0� !�ܐ<�)
17,"��5""mlj6�Uv̶UNJ]��7h�Y(����dZ��""��R���[�J���"
17,��
17,���і��:'��`n���(�X���+3ѣ
17,T�%���N2����К�tx�!l�k/�8�
17,�?�{E��
17,b�cX���]�����
17,"����`�÷E����T��,��]l�|Z����y�|�0�FKb�/֑�����-��ͭ/���$a��`3�L%\�"
17,�ę�
17,��[����G(�@P���B��Ŝ7#�1�
17,"=FB�Ў�(� `>7�Q��I�\p/3�""x���6%�"
17,"�L7�%�E,�9��Է&u�Ip>!B������ ���"
17,"'̈́,���A""�"
17,"�,�"
17,��Q
17,/�9��B��������Y
17,��7�%�:K��Ҷ�Iݺ
17,��EҼ�)��sF�v/�Oױ�0k��.�6���@\ۢ�e�iDr������S��{5Q�
17,ySv/�
17,�Ӝtڲ�*Y�&���DHV���fK|���
17,�'
17,"� u����7�Slѓ�����@8�""uB�pĦ]CH�A�N���b�G��g�ʣ6f���'"
17,r��Ɓ��2E�J��2pE��4MO~.�OI��C]/�eʌ?h+��
17,"��C�n\��^""�cƒa>""�ʡu�	�k�HX�3J�gƇ\�S�t"
17,"o�]}��c��`�Q�	Fh��&>�7ƺb�,��%�^��?C���%"
17,�^�9�|zr����<��!{��&��7�y���i��ay�|��E�`�FI
17,"�=; �$j,� �4	\���z���qX��/�o]5;�D�:�t��:��� e��d�4S�=Y.��u"
17,�p�U�q��Ʀlk\8�y�.+�hI�q�����eV�O78�	��
17,"�3E�	T�����q�'^6gm�t�'""���t�1�����nE�(�s\�"
17,���M\��v*:-G���w�/ au�`�j
17,_�@��l7��T&�}�`)�z-?�C�D~^�
17,�V��
17,��g�t�Kg�ݝR���J��p�؅|>F��=�c1��
17,?{�T��#�2�Tke��M
17,���[��:�A���
17,"F�E`��t6�Y2Y���,Wl��W|tsQ�l�����>��y�/`"
17,"I� ����xd��|�:"
17,�R�C����E�)���p�j�m��:��O��s�|�
17,��@HԂ�r�H1�74h|���^����N9=T��v�Ե'��o�2+�u<������f(�B�;
17,:A 6�5n���~�Q[�&��
17,�Dς�ثX/K}^�
17,"�I��>n8��y��@A�Y�I��i����>��x��W,'YY�q����k�y��+ZK��;���ǚ�$E�d��|bQ6��"
17,"E""O��݅��[z{�8� ]��p�����Q]"
17,"�&_��7}�r-�P�*O4���(ߕC�,QfG�(�N��6e=N �-�|�������k��R_4�B���o7 #��7�6���W䐽��"
17,.^L��
17,�d>8�;}5�f٢�w���/�~N-���$�)BW�����Ls��F*ԝ
17,�UN��˨m����y<;�^I׵��Z�+�on
17,"��$ȡ��u$z��P�����G%��o��Wb�D��⿻Fwf�2r�4�JN��B*s����Z�Ŗ""�&x�(O,�Rd9L�]3/N��䆂�Z���/:I���{y�ޛ��`Wa�R���J��^���E�\��N�e�N�r���--��,�#���i��즉���9"
17,"P]/ ��w�6� �sU�"
17,"�ƃVG""��'��عGI�6�B�C2��r��"
17,;�ys=:-2��Cck6��u2̤Q
17,"�$93�&�|�v2�,��^�G"
17,1F��8*'w�<�@W(<�1.��4�T�b�e6�^����
17,ˀ�jB����
17,.�豿�@��	W����I�늼�Y
17,E�|�o�:�$\��6u�������
17,��.Z�I'�ء�.�ֲꭴ��S'
17,�3&.Y�6/���$_ɟw��A��
17,"'WT�>�����8C:��EvJ��GG��rx%��WeQ�iڅ9��0~�+�""���'��"
17,[�>�AuR��.亲��B]��K��L��r�cC
17,VT�
17,c&�I�����N���ݭp	��MNl����7��r�-�tW6]�ۭ���r��;{Y�PX����BC&hV&��
17,X���f�>�
17,"�D;���,�v�h1{�B���I�e�qy�8V""�xP�)	"
17,y������
17,]�ꢘ�nK�n
17,�E��X�-^V��U�F#�]b��έ�
17,"���Hv��ə�V��ѽ�S$!F��W/�Zt&�8�	�Q�i(�;�B?�qn��0�> �$uΡ���O�[�F'��p��+̓n�b m�\y"
17,%��2�ч��?�V�nc��
17,"6]S47ض[.9Q""�cn���e]:��an9X��4�6Uo�8+n������>�E�}+�����1/�#n�"
17,"|<�P�h��ŋ2�lN��^$�w����(��2�Ϯ��|3�)�PN�kJa0>2'a��0!��O\Kl������5��M'3 ^6��+C/�Gb���z��aZ2*%��v�\��sHq�y����!ũϟ��5�Hٟ�f�$$TgC��X�E�.I"
17,8�t�E��/
17,��r��X[�2�X��S�G&^�1
17,����
17,Z�^�\I�;y��[������]bs�9/F��`�Pǰ+:��|
17,J^֏�]�BS\5�G�or���\)���6�M��W��ԲҢh-����y������%�
17,����_H�b����j�Gl;��
17,"�0�Yh�	���{��:���-""����w߿[%=��r����1��+>��ҁ���pQ+���}vQ"
17,b�q��>�
17,�1�n`1:
17,-���bJ�
17,�{`#�Q�*H��
17,��k]�[��py�n��߂�q#�*�\��[+��^�W5�
17,=�It��@�e�y����t���/��j�w��
17,��S5Sl�g&3
17,"j	����@�,"
17,Jk �E�H΂
17,h��Z㝃�����
17,��%��/�D;�/��d	ť��ަ�I�p8�5�pj}X��9l�y5��o���������H�%��Qr�p��`�I���|�	�m��.Q�:$��R��*f��
17,I/�]�~E�!�aH;��u��R�u�4gyk
17,"9�k�N�	�,"
17,D�%�e���LǪ>�e.��][D��+X�Lw����ξ��F.Jo_ʓ��sn�At�k�K@M�D�qT� �Wi�77�o�:�J�B��(�z��n.�C!�
17,A�M��FS_�y��
17,�U�M�iZ6�N�\��
17,��
17,�����X���]�3�amf�0ς�t����j� *��
17,���<����߄k��kJ�#�3�ÄE
17,��a+4�O��gA:��kd�3�%;]
17,"%�J[�6N����H;�$��7�g�9$""=t̼c�2�uq"
17,"��7?��q-��P6*��Un���(�']OO���""�S���DE������"
17,"qH�""wa��a����O��,p�v ��:3��<"
17,"B��.��Hk�Ћ��Ε� ��<�. "
17,"=3�<�Ǘ�PzsWps�qT�""�[���V.�M�y""�rK٧���T��9 '"
17,}��V
17,01l�
17,���7�S�g^��Y�YBj5c-u�|��k/W]~��3&3�[�u)Y��aZn�Xr�O���
17,m�Y��!����\:5h��)��d?�0���C�t�kA�|�
17,w-�YF{;0��s}wLj�'�v����y~NN�_���N����I�/�
17,F�i
17,"VvBL�a�f�'���,`��'�]���#P�(��?�Ʊ��[���YZ�B}M�ug^�h��x��y �P�A�5j��w_a��d��?����y�djx�Iv�z������ζ.l�>h�3Xe�D�` ��{("
17,�����/�$��N��9��h2
17,Q����~�^�p¬��\���[
17,=Q9�O�&PQ�����;�ؙ�	|�m���
17,c_-�ׯ��]�L�B��
17,���
17,�p�&��1���h�ŬyG��p�<��	)
17,"��a��m:�*K�F�:��D+훾*��!�\j,"
17,����bǚ
17,r��]Q��S颉�:/����4yma�#����.�� �4��m�ռyz��0�4ɫ�ļ��p���o���$���
17,"F��#q��,�N_Tz��=�#0'��I��^Fd�"
17,Rٚ�Q�HE�챯��^�r��d���f��݁�+ct#��DV�
17,"�*���VR��K4������ܩ����iK��/�J�!1� +5�X�-Ml��A���$�� ů*9�i��k���|�Ҕ�1G"
17,%8�Ռ!���Q�=���q����Of�w>UN.0���yp囘�U.�Uo�
17,��^;��
17,A��8Э��jH�:�
17,�Mf�=h��/��89У�(�
17,�Mf!d��E\�!	���5�t�c�i���S�9�����C�\L�j-ȑ/
17,"�X�}װycS�ٴ2fzKoÖ�-q�""2��tm2�X��:��H�r���ͩ0ޛ����f�~^Ҿ��"
17,"�M��pl�VK�K��m�\����!p�>t��5�Nܛ����͠��>��;�Ӊ	�IG �rC�]��!)S�di��K��r��@�x�N��qz�϶	>K��/t�����<�/}�/��T7Tq��F�""�L}f#[~b���I�z��i2��՘"
17,��s�i�|�{�(�
17,@?�p6L�yM��Y��
17,"�}1���M�_6`��Q �#�Dt��jGlx���ۓӏ�����˷oN��DK�"
17,foKH
17,1�6wM%ԣ
17,"�| ���-kL�,���=��=vP�}���/��D�iob3�8L``�(����|}�>#j^)"
17,��?59/�2��˜
17,f��
17,"j-�lg�����`�ˮqJ3-�4GZ魅٭)�""���,/^�@��H��h�>%W�	%�()m���6`�{Z/c9����D�м!�M3�T���P������"
17,Y�A����(5鲁�c��R@W�(�5�uO3V�ʮ
17,���l��6�dR
17,IӜ�!ߊ�^�XS��
17,vd[Ou�H�Y��>���o`sH�'��!Ds���Y�W�ƙB�ݛ���7���Fi��x��(�$����B�w_�W��/���MG�4U��J�m�
17,w���F��ދ�W?	[!f��3;�_��L��\D��̀���'q���iiO7g�&p ;u6y	3Vl��i�������@[[�i�魦\�7B�b����ДL��֜�L����؉$��d��炧��
17,��}
17,"�Z��r���h��xq%�8,����z�"
17,Ow(�L{�p�g.ܣ$b1�ؚ>�
17,"�W��1'fHn8����lR1���-��U�,���g���Y+N2ߝ��"
17,���Z<���R#��Ts����������x�woHyK��
17,"bq�>�_�5$��x��M����_""|оF�5Ə'u�������A�Y�2��z� 6� �5h��ʵ�Iv�9[��rH���Z�,T=h�Yjf3ov)�@Q���v�ѷ�g��UD%95�}�Vu#�-#���e"
17,�����J��r%>�Gw�Z�zS�
17,e� 36!_�υ%�ԳŊ��9�IF@�$S��*2��Iq�Β������;�8;
17,ֈ��Q�1׸��!j[P�B��g�c�I`��߄���`�5�7�W\`8Ya��
17,"�鉝$_Ƕ�^�/$ɫ�I���	��z���� ��[r""�Ӥ:��j��ʜ��u��9�"
17,������b}�߈Hw
17,"�����,y0��:��"",.������������-C���O��i�|�"
17,"""��qҸr�c�pO��{�,μVh�g+?��h�k�z���|����"
17,"�39ǝ쟜����s��;���  L=�U��3��V�%`S;7�V���&"
17,"�L@�X[o��խ2��:�^4�H\�c�.��O/߾����_H�DލP�&R�,�y�m"
17,[���0���[n|����
17,"��� �I�G�u$�"
17,J!@�*}4A���K�
17,9A�5M��V�s��0
17,Yo�� �`\�W�O�n2�u�J�����
17,��
17,�Ȣ����hƮiN)��q%���ָG�^���偩�V�%�Flb��b��R��fmO2Sf/�O��3Ƽ��9Z��
17,{'��Ktͮ�9�6/;��#�2uC���4
17,�P����
17,Ig�;���<����+2/��Ӳ��
17,"��2�}ʑ��/���<ϖ�]-�#/:������!���q���3(��S�d���AF6�'�C�횘]@����}�	��c?3R�ɷU�(�޼��94Y�a8�+O'B  ������Y�c6�0��S�=�ݚG!��2#R}n��3w���٭�D�q�ٵa�l�"
17,����Ԭ�[m�(P�u�V�EhR2[^��
17,���)'�e.&맛����Wtd����\R)pҺ�&#���˳c�R�m�ŽZbh-'p1�+����Ct�ɖ��v���.��_�����y�]5����:��
17,�c�6����a�̠��$�R�<���x��JA�;��׺��9U����sɕ[��cEwQ��1e ��Cw'1��z�ۓ����
17,c<틩oh�����
17,��f��3���Ü�C���E���o��޸!eIY
17,��I1
17,"��$v����k~#�/$����)�2,!�h͖�ʾ�s�ܽf�"
17,I����Ԕ�s�.��M)
17,"+�� ��J�T]r�E[P>�[��7W+k����<�"
17,=������4X\�s�дlE��)�Z%@a�͍�f-�
17,"=d�� ��`�������Qq�����:m1/p�~��֏�^�}��W:��Ƀ)P�b�"
17,m�N�Br1Z
17,�8����Q�
17,?-t���l��Y���5j�ʤ�~�nۦN��p��ΏƊS����
17,U��-XP
17,W��<�'��
17,a�@F
17,^�:����
17,�C
17,�y{��~;-��U5�t�
17,"��o�[�nk���aK���vg��;r(݆T�G�Q����k�ޡ{4���hD-�/�-5Ro��""���Pms]���tk|�m���nm[�3��"
17,"O��B,��$�J"
17,��+k���
17,"k ��Lͽ����@Ôx�z:��$��!'�A�!0B	a�m>p�z4���B�x�d�2	ǉ�����:Sh�NN���韍=��_`?��ڢ^m�	�rh�S ��xIA���5#?�_=XQ9�VA�@�ӑ���Oɲ��}o���<�Pn�k�<�����l�{�O�h��w����?�t2X����1�$�7����ib�Qnf~�`v�U��K:�"
17,]}@+m
17,qJG�@���>]�{ISB-���S���ʄs]�-+��QQm�
17,"�6�L�b� �z��ü�F�T�# ���Z�m"
17,6d��B��;\n
17,�3
17,"�f,�|��ê-�5ӿ`y7MA��M��"
17,"U��kfk���8릜-۫�?̒W�+�IX��CZ���u�*�`�d���Wx��\���n�~S2�J=G|��l��Ց���z]je��}�ڀ3�9Ǵr�T�Sb��$�{�""q��\��u6���;n�Vkvj�A��W})�a��j���~�"
17,X߹�B#��zJgaC�dJ�6��x��0/'�À����x*ӽ=�s`F���Kx
17,���T(p�Z�J���t�(�J����L��m�
17,�����X�5
17,"��F�QǍ��8���t«��[�""{�E���wI�̮GuR�"
17,j��=к�Й�:I��c�գj@�^��#`�`}�!�*[A��m
17,���xm����
17,mG*���#]W/L��f\�
17,�'�����g���;uv��㞬�r�*�
17,"@\�}����f��!}8� "
17,c#T���`N.!y��|����C��Ō�F�F�Z�^�����vZ�n$�A}خu�Z��V�q{���� �
17,���Y���l��W7	�4���2���zB��}�K6�p6K��
17,�	ɒ����7�!���J
17,"2�""'T��A��� ��jy�`7�0`��S��o"
17,�4O��+O/�k�sux]d��sO�8�=���ix(%���ϗ}q�|��?���xQ�l�[�)i6�Өw��a�/�@���vC5j
17,W�֭��
17,��
17,"���a,N~���'�$(�,JP�ʠ�}Po�"
17,k��sax1'Q�����#%��{��4d�:i�Y�4?WG�
17,"�K��n��	v_�ύB<�SB���c4�w�b�O/�""k�i���l2���&k^b�l���Nm#A��{��l��ͺ�U�N]Jի"
17,�v���ڭ��
17,"��辐co�A8kX� �f8�8h6�gI���Z}��J/&:'b��{`��Z�� #�e?W�>�{)H��~#1� �A g�� �a	����"
17,��$If���2���EI�O%ӟ&J�d������Z���N��
17,Ȧ�m
17,"��v�/A��5ǩ����+4rW �85��p��M:*7L�&<|R^3�[���+G�y'J���,3�/T��b�* oqi�K�g�����9��U��sm��Ӏ���\"
17,�y��z�^o�7
17,������O� ޙ��E-�U���q�g�p5��a��%v�Z�۬m�aR�9l�]��출.ȃ��jw�H�
17,9pխ��
17,"�������I��aηN""�"
17,"j���]��c��/!w���T�xh���kL�I�@��x^ȡ�E����,��e.��(g? ��/��{�1���(�A����+A)�W�گ�C*O��X���6��:�j���+v���H�""�n�""����"
17,"A��u�6[�BT�O���säוL��n���r찮j����Á�f���F����u����1���ȝy���b5g""�j7��5�+�՞��U����}������|<��|��5�<��� �m7k_�/��G5��TA����l��q�|��#��4%� R�IR�s�D���~��in��8pڭ�;贚]��T��t:�v�!���ۑ�n����"
17,"����r,��3�m"
17,�.}<�q���qPk�
17,־���.��ѣ��f��*�0h(��?`�<�'+�G���9�һ
17,������
17,"8yp��M=% �ٯ�Qs�W�,������N�""�n6j6El�,�̓z��ܓ�ȡۅ�XNyN�_&��b��wڍ�f�5ۍ��L"
17,�e��m�9�P)����9���+4rW��<�
17,�'L�
17,"m�j��#""�zΘyWG�/uJ6�~����������v���GB��Kb,ͯՑ���(Rv��~}#��H�Q(Gl��r5���b��M�f�P���P�m�Py�e��k�73'R�"
17,��V�-{}�4;}ի�U��u;�z�ٻ��W7rg�M5����1]ݜ
17,��ݍ��R_
17,"<��y����a F��jR�~�eLo Nq�bQO�dL�^���á""��s���o�4�k�%O"
17,�e�k
17,"�&0�9k�֗�/""�=Eͦ����Yoݕ���&C3��.E��"
17,"�M""��/�*���	�z��8�]`G�6��U�o��㼾 I�C����a��J��f䟍��N�٩�"
17,"g����f�)���P�[�A�Q�RB���+,}���z�<{�x�0T�'bh��ah�qg�7bSkw��M_E�M�x���8$ܽ�(���J�m,v~#'�����~S$��^Iy�I�_�#iƵ�Q�lvAj�Ht�""inJ�Q���w�Ny9��"
17,�@O�;FO�k���K
17,"��V���M�#ccO�e��iևN��ջ�A�߿�넮l���d<��D<���MW�Qז��h܍i��A��5���Zw7$��[����QZ�hl	�̯����r�R~�Կ���""?�{�c6%��G��եB���0ai;xo����""��H�aOK�Y��9[pi�	pͯ"
17,�6W�z�:�~������t[}g�k��~g��Á�l��6t����\.tu#w����/R���D��ѹ��w6�
17,"����ܹz�����)���Y�{�%�$xư�""�"
17,"��|�4ƛh.@@u$,�`D��jPOfIU�İ��L?�ⱈ�b7����/���O����FI31:Q�X�A�	�i"
17,(�\�Uw�2H
17,"�X8�pb]D�o/6����- ��_��xɊ���3���%u����#4��""B���y��"
17,ϧ�p��xV�8�a��=�j��59G��
17,"�	� �=��"
17,"��{�����: �k���c% ��[�"
17,�9s�U����uխ��V�������j�j���B#w����e��Rs&���U?h��V�=U�� �q��%�ú?
17,�0ƫ�Fa��
17,5�H��ɞH.��S
17,"FB E�u���ȱ�E|,�<�k#&O"
17,"""f�ku�$>��Bd�Ӯm�w�!r�p��q�n'!.���)�Ƞ=KtH�� Uz�zx:J�<���0�C�k��` ��z���~�b�/��#���@HsA@h~m�踒��Uov{���m5�봛�^�_��ư�l�f��k��n�v���+�|퍽"
17,>�t�k9ig���
17,"�>�F����z�""���"
17,�:�
17,(e�D�
17,mĆ˪8A{�1c���H'��UU<
17,"�ߛGi��::I�6�A�F~m|��B|M���e�|��V���L��""wnv2�v6�A�wd�|cb��o>E<q��y$ �'15"
17,*'ə0���q���2�h����t��N��ޥu�L�YSi
17,�Y/1w6���f�;۲�4�l�v�Ѭ5:nP��N�
17,��N�~;�
17,���]a�)��ϰ��1���f��0Dv�ޝ��K�F����P����<��6͝G�Y�*��.�*��1�tv��MPE��+ŷ
17,�<��T+ը��;��
17,�ZU<�孉�-{��)���A��
17,"!��_C�Z\ e�Y�@�r"".�Gy�����{`� Y�����_��~~��G!��m��x |��=���"
17,��;J�|j��R�2���pk0
17,��Hc�EX
17,"�0tf1>�~�7���Hm�T܃u�""4Y1��x�(��$98�� �μ"
17,"�/�,|͵����=�љRS�Z�-Ǧ�x.�[�"
17,@��(�=�8�!ǍR���/@
17,� ��� �m�쾊�����74i�l��f]J��
17,ʞ��U�����Nߩ�n�Wh���]��ep�A6fÜ�~X���
17,"�-�T� �a���,N��$�n�M9S*QNv����xG}v`���a&a%��>�xw�P"
17,"P�Õv��k[T�!	nХp��A�v�DY<IU@7���Fd0������ �_��Yk�P|��j�8��_��"
17,"�vͦ�흜OZ�}Ƿ���j�&C�QD�D3�ȨJ�6��""���6L�^n����f��"
17,��[o��mUw��~�їr����
17,��:}y;`]����������/�|�$j͢o�s
17,6Q�sR��]f��œ�����4t�\P��w��
17,��	~�@	���c
17,��
17,�Q�~Lf���;KX�CȄ�|�d.�����u�0l�*4	��|���
17,k#=�TƳ�a�����X��j�M1�S�9]^+9�/9���~��o��3�6p3? p��Vn\$���f��G��	<����/R�!�fSB�v����f緻klO���y4\հ:@w�/�2��ژ�3�����0ay�����՛��TS*�
17,"�Ԡ,�v��f�����u��to�Wh�β��9`�"
17,"s���h��:�#ocN��ğ�1�$ⱌ@�̮���X"""
17,"C���H�{��\��4�t��3ʢ�:>��m��q�w>��,#�D"
17,"{�����`]H���Z#'�f�{(�Bw��tq�""�np���^;h�Q��?���&39���;%p"
17,"��	OǍ��u �� �� �m�L����o������z�3l����q���Z�E_�^C�;�׷B#w���/""/oL2{>����'�w��^�g������"
17,<F��ً9&j���h��E7ވ+	0��IoⱣp0�
17,`�@%�:�b�אP��Z�퀄
17,����X��9!9��B6(��xZO%�l
17,"׋0���Mr��K1Af��(�00����B��-��k�	hz��� �'cn�D����x�)8�ł<���,"
17,�u�d����>�#̯���l=�K1�S�}�3㋘|��ٴ���;�'r�
17,"ɉVE��<""�?G{A�?�h�$$s3��$�;r� ��D�""�E�#Q>�^������"
17,���+
17,�R�$�W��PS�L0ʜ����
17,41P�W���j5��A
17,D.��
17,"�H""o*]��0څ�?PgP_�g#��R:�!E*����ρǼq"
17,9��w�k�
17,x���侙�Er�$�g�6L��8�jl�+H�Wo�k�zK�[�N��jk�a�/
17,"��[J�B#w����r��/q&�o�y����ȼߡလ2��)`""I6CL��Y4"
17,�yx1�V�ވ�9�%���y��u�l�7��m
17,�\F��|�f
17,"��&""2��<�\Oq�O�����~"
17,��������
17,5�M����A�.����TK�hcų3+�?��
17,D�IeDտ6LB]�T��P��:]5��
17,9���un�W��
17,�Y��ޭ@�
17,����D�'�/�M��İgQ`mb.�;�W��@ƀ�����z
17,"Cڪv���@�D���ŉ���,�v�GU�N�>(��(��x�^�BXm^"
17,Z�)�6ׅ֌����lm$\~�X7
17,"6�M%�z����ebeNϴ�8����f�:�+L��^�׆I���W���fʬ��;t��z�Z���u�n�Q��ڲVk��ڭ��]��w��Y�U�L��zwkw#��R���0��aʱ!ˠ�A<���ۋ8�9��{�ŋ��D +Gs�S�./��q}�)"
17,�R�D
17,"�y�Ü=�]#PA�	�M�JF1�""�g��s�"
17,���j��Y���)�>H�8��C2�=A���:$�B��o���̼�_ލ^bSކ
17,��;
17,�.��
17,1�q`�
17,"�Ұb�uݐ������5�%0$""�t��i��,�p"
17,k��wٱ�|/p�8�S�8�m�躚���l��@W6���r�
17,��]�
17,lu�n_��֠}+��B#w�o})��x�x�����ɧ뽳S��j��k
17,�y��@&!ޘ
17,��?��on~�K+-�R�	�a�	{9�
17,��?u��d���Y�J�7��g�Lt(��Mۃ)E/�RÄ��QT�!�)�
17,/�M�tk)R�RG��D�z�t�e�����0�ܢ�$3�}�
17,�u�wb�qX@�
17,�N�
17,"�B�(u&�:D� dpދ�bt��"
17,�n�
17,�X�v�k�
17,�Z��R�n��_�z����}�`6]�-����-B���3��c&�cp�0��qBuW9�RMP�+��@��a����1K�(�#�8
17,��h�%ވ�аhG�O��b�4W��N�d˄�{��8q�#.M7!���a2�%��^4����a�ې-��:
17,�i;�'�N�t��6���-�Wwe#w��'p��%4k�D�]���ws�G�g��!u[Cѝ`-h�]
17,"� �=K	G�""�9"
17,���A`�
17,��9
17,j{����u�yi��FO�%D�����g�kw;_|#����
17,#�M�yw޼%W�e������hz��8�
17,��׆	�+�{�~w3Sڨ��
17,�z��k;�Ơ�z��j�
17,"ِ���\��B#w�od�I�[h�$еҴ��~�;q���c�+\*�z�0D��ȥDe�|2T��0_�dK�. ��H��!��e�`���8�D|1�@F{��"
17,���a��
17,b4B`�`h!�aq1žy�̗�?��Ĕ�8
17,�g4�D���
17,`�A!���[2o���[����k���
17,")���>K�(YV��9��a""�8�g�>��~(�2Ƽ��µ2�䀢�A�;���i��9@�'��o"
17,oq�(�2vA�v
17,�#`'�
17,"�0�1��{���""Xk��C[����ֆ0r?u�F�K��/��}߶�Mi�j���wv/�0�'a;Rpm���'��*-]�]a �D����kHd̵PB{�5���*߅��Y ������C���\�P��އQ4'� R���*lf2~0$�$�_C�@�`mC��0m��w�Y�m��{�j�:( �^GIY�"
17,�n��
17,���q��[���+�~���x�A����r�n
17,"4���'�]�9����_�_�Sv��,����oS�{���?��㣐�Xc�ҐB���"
17,�r�i�{l
17,�_�JA�D�� ��~?�HE:����)�2g؈�
17,-ac9���
17,"�,v-hNpO��yb��q��q=��:����>�۝f���!3_��BobS��s�v�qǶyJM�e��6�Ӝ\Jq&J��"
17,�W2ȷ��
17,M'����mu���juUsj�n�m�r8��뷂�+4rw�r��
17,"�Z͚�9�<�6��t��Q���,Ғ��\"
17,#�2s<�����'o6�Wj�0�
17,"�<���u$Md��(f����f&~�""fn����ͺ�;�3���"
17,lIj
17,o�9|��6��9_��%W\��r���ҋ�?��[�r���`aV��
17,"""������<="
17,���*��z?��#κ�n��Ȼ���a��%��V���)�Z�z7�m�Ss� �������^W��[�3Vh��
17,9�(w��	͙�hsْ��1�@>6O�p藡�礈}
17,UA��4�6�`n ��Q����
17,�E
17,}����H�x%�V������EB����lJ�׳��jw�'��
17,��!�ڍ@��@���oYه8뚴�P6]�r�KG�KW�M � {@gd}�y$5�6L�\�
17,�l�7�ݬ�m��^���c0��R��v�9��۹���F�
17,uQ*
17,�J%���[͠�����ݸ#���.��߽|����4V/wy�
17,&���Wh�E+�0
17,OL��0c�g����Wx b�݃�Vd��dAFg��B��NG
17,"����K��6��\""榿V�\�����l������_7"
17,O�M	Omսvм�
17,�+]4w!�/1z����8ǌ��׆ɶ�XM��~k3�r:���͡r�5<���-5p��>���ۉ�[��;��S�Mr+ЌI�۴-�;
17,"�~�F����� �p<��6E�6�k��/�{);6�͌"
17,"�"";n�1�.""X�y��8���#˄8"
17,��&�
17,"���a2�J��z���L-����a��H��4]G�=�i� ewp;�����]!��`��p��U���i$����A�w7�Jk�&�^��9U�̼[>�&"
17,pb�
17,"�4�k#;c�ku`d&��""e�ެ�7辈�zĦz�|\�"
17,"�(�F�#K�4!� Q�FQ�kä�K,��F���P�}g("
17,�I����6:m���w���5n�
17,}�F�L����
17,W͘�a`��M���	�~��Y!�x�O
17,l`�J�b�f��'�i�q~�v5K�U��������eF*�V���� ��}m
17,��B
17,M�!���G1�t���zߑ�ٔ��m[6[��Q�_�|��$�����@g�U0MP�\�W�B�����)���܋��
17,"�#���HD�h~m�������h�73[I��`�Y5lK���;��껽a�Wkt���ܰ�B#w���M:�֌I Z�+򝻺Mg�<<�DMx�$T��|9����~�Q����	�s:�t�?��O �&�ɷ�M� #&��=@Q"
17,"�k��8K,b�_��(r�1��ff�""bnB�B6r���;��择���0#��P�G�P�k�$��L��~}3��Fש�����T{8h�_=U����"
17,"����ڴB#w����/a,���&A,s'Bl�o_`����~��;	�J�*���3��8���b��~�9S��G�)�X�b����Jq�"
17,�?�*1{�@��A�A^5ij5ͯ�Q3���)~֚_2;}?�;62���qP�#���>s�p#.~O��(�o���X�t3I����MzI/=A�W����U�A�^�
17,'��-���(�⪎)T������
17,%�
17,"���sL0u��1�T*g	�����=""��x�XZ��d$i���&9J��Żt��#[\CFfbX��0x5kl���L�C"
17,"͡ݖ۬�] ܡr\��z��jv���	5Z��;;��`n6	�3	�-g��A�{�Q�U�d��D2)��ϋHɑ�,��_��`���>�!��5D["
17,km��5����X���0�/b�&
17,"�iQ��������a|�ȍ�m�$�J���T��4��v�̤�J2KB�?�1���\�*�N�M+#���""'�JT0���'S�����V��H�q�#�6jF�m�}zm�wu�L��@ol(�V��cx�`x��ax��b Ұ�� K�k�R#��ƺ�toD��""b�63V��Y��k-�W�vи�l�x�Gv����F���~k3�svj�N����N��T�>t:�+k��rn�J��+|"
17,"�c��7!hO�~̠�wr~��;���L����O��Ɏ�>�Yǋ@""����/�D��/mҜ��i~�.m���>�"
17,"v�z��E��""t�w�e6-"
17,�4���N�ߙ|�f�8�I
17,�Qq^�s�Tk�#�ڿÄ��C����թ
17,"鰋��G[��a��""���4��.���x��J�g�_)���zO��ʂ՛7\j~eq�����|q����܇"
17,���/�A�í�T;]�0g��e�D{��	���\�rx3�L�8�N��-
17,cOr{V�0D�0ϥ�@)�O�B�il
17,�B'A�[g
17,"���x��c�KW�;���ёs�O<㕃9���6� ���q`Tu��(��d�ND�����>ڲ�^p�����=:����e{>.S/< ���S1Pb:�^<��sݻg!��QG��x�;y�ϡ�|(�J%����?�ӌ�*��@���DI��ɖ ���UvC|	S�q�WC��i���{����,��A%��J"
17,)H_���C�Dv�M�g�#����%�>�q�ì
17,"m��콂���/��_�P���̹�|��,�U�~�y��֒h��	����6a����l����5�� Q�A�A3�r�~ȸT>R4ۗ��A�?y��W�sb���P�j�|�+"
17,�!�Fc�
17,�8D��-�_̢��K�����qT`�Ka�*B�:F#���na ����q�=��C������
17,�ճǬU>f�oӲE�������*0ͭg%JH�h>p�{P;��}z���ԧ��D�v=
17,"�9�C2���we$�c@a� F�m�""��Cj��C�i������������k�D�4�Xr�H��t��"
17,"�a�H%�}�?��_�;i�w� �?N�� � �x$t�v���v�~��U�zW�Z���֚���w:���"
17,.�ۚ�%է#�{lK�.c\��
17,�HeOf�|@:�S�üW~(1L
17,���{�F�]������wb{6w9���S�ot;���9qiU��
17,KO�$�Įw�^��i�S���}g�-+��^���
17,�1�X�(��rG�*�V��>~�I�3���å5�U�-����%Q���{��ck��-T�p��:o���Ѻ�m�i6���� W�;R(G���0'�/'J��Q
17,M�H��FN��qtv�mz�Fwຽ�`0l��i�[�~�q۲�
17,ʣX�B-h�}l
17,"�<&�`��K��R5؞���o���[[��&^��l�n��F�""��mTa����N�O��pS�^�i��i[s�&�#ʟ_>�63N�̹Pd��j�x�Y�"
17,�L7R�p�E�[h��ni�[�RЙ�����
17,�tkX�R�P�N�߭�ZF��V�?�Y{����VN1��֖��V��
17,�g�Ufr��@�uD�oey�ѐ�]���[���)�9ز�έw���eeS؇w����
17,��SZ
17,��xaɾ�H��t�e�.BO�|��ķ�9����
17,�Xߢ��_��B%b#6�@�4e԰!ՙ�����@[�c�D�j
17,�0��V�
17,"���k8��q+��棭�_�n��D�[`0�3���1��/ ��%d��v)��bh(v�/@}g���	�R�*�"
17,������
17,~���)W�����
17," 5.�"
17,T����>R�
17,�wTO�4Oʺc�_0���d�|�`�9;���
17,��3ܯ�
17,��Ǎ�
17,"�����~�)S�Z]+a�WT��`�O��k�g��򃅘�+�HZ��xr�3ik������iu� 8��Ko�(u��+�o�E"
17,-�%C�#C	�SQD?�Ohq�TpB�TU�+FK#�Tm`�Z��Pf�O
17,}�=�r-
17,��(3}�	O����	�;'����]�of@��$~��\�pM���Ⱦ��}�G�GV_u
17,[�K�a�A
17,�����cC�e��0
17,"��r�ĔK��~y;�K��~���;ۘ�+� ln�h��>(���xA"
17,1PTݪ�K�Q0
17,"MЇ�j5��ܷt��%;�:�X�RPI��D���q�.�5N����[��8s��O��4�zɼ�E�n����-�卅��<�F(ڇ�5��~aWd�V�����\��PF �^��9=^��%�����J�v"
17,�U9s����'3�'S�
17,����
17,",Mx�C�E��[H���((�""�""�R��꣄U.t"
17,����P�b�SF+QhϪ��+HD��M?�T����
17,��Q�����:U[�g���Y
17,C���Y۰�Ĳ�P=Yw����5Rf��Ţm1
17,"#���,I� 7F���^"
17,"#���|��ò=7C��h���k��%V�؜IZ�)�o���SB[�()j[83��/�g>+���O��>���,0s�ܭ`ж���9�M+�}���H���(�;��"
17,�(��ܬ���x�A��&L��w\P�>��*�ӟ�P�.�^�܍2�����Uj$
17,Z�$�քdd���6�z�-���_x��t� M<�j�����
17,��k�Ӱ
17,��K�G\!b�/�sԢ*��C�
17,7@�)��[��Q$�c�Y�5LՂf�7(H���:ݤ�@[�
17,G�ď/����L䙪����ۗ�ڬ�W�5p�����5���o��w�s��A��m�}��a���gc
17,F��y\FJYO��l�����S��h)�qU��:��P�5�5D8�|��g�
17,"��,"
17,"�Sb����}!jߛ�,���k""�2�}�Y��."
17,"Xh��Ve2 �ce�l_D�,����E�P�h��P��"
17,"�-a\�Q�k��k����E�,��@�M�-"
17,1D�^���vW`=(���+�흮����I[d�`���c��5��lCmކP�ىτ�9�u�6��i�����_���l^�ft�E��9��4�8/�Z���)���
17,"֠i}$v�,�K�^A}�=�'"
17,"C}�)jfk^I�.*n��D}Y�ɭK���f6��J��Q<���}���=ƴZ~��R�j�$>E75U�8fu#.��]햸M�>��)Z߄H-�� ��݃T�{�k�"
17,\2��9h9㧣h�(������UQRea�Q)���+7Ɍ�禒U�v���܉�
17,�E�ޢ�����ZR�w
17,��e�T�$�'2:S	�e��Z�e�{m>��?�Uv�~��������
17,"W\��""(�)���W�F������ҖD�d\���xL�K7Q��!�<�Z�c��)2?Xz��&��0cSv%U�*@�Y�8�sFSR�_sU����';�#�帰wQ�)�XNھr=>��Ȉ�"
17,"j�%Y��� �=xC$)5�N��V�zh;Y1��vr��%���y��vB�������<�ߒ�ο���{]�-�4�)cu��e��1�4����]]dѽ�t�Pl�{�?#�{�1�R"
17,Sא���T��ݸe���u>MW�
17,�Z	H�w'���
17,�H�xl��t�:�\-bҧ�	!��!�����?
17,��vv�$
17,"Sc1a���._�d�N�y���Wb�q��x^���W�+�9�d� ��-���+/�%jM�����/Ƀ���x����$,�1���z���a1^�0h�oZV{lj��o�x�l�]fyBT���1x�\Y	B��� ���^-3;Q>qT8����'�&��zoў��V��l,ɋ��K��AIz""A�ܢ��V���;�ΉP�gژ�k����m_=؜�R�ij����_�6:οl����B�X� �ȏ݄/����0Dш�h+��ȧ�\^������2�N"
17,yoP�@4����4��Z��#���I�ۮ(:
17,"?�Z�|�j��l8� ����8"
17,W-��$R�8P���F��U}p�
17,�d~'4!y��
17,L��?(�T�	�P2��!R���J�%uja�I�l�_���C���-��g0��$��p�vEZ19t�ڮ��wF�º��
17,�F���5�p����r�?{��3�tx.�Ћ+�{AZ���b�
17,㵆J��?
17,�t�6S���+
17,h5����xO$����cI�P\��]2q
17,^�8��+����V�	����߉w�_�+�N��0N�0
17,��3���y��Ӄ�q�@�DM��;W��
17,3�Gga���at ~?
17,v`~����^��H�
17,"�U���2�ǆ�\�~��C� �vD��^s��F��q0C%��Z�dV\/���挔�8}*�����F� T�ҙ�i�����5�dL�J8��b�K�O�O\ ��}�"
17,3��@����d��'��:��H_��O���&
17,ӆ�
17,/��G��
17,T�+x1˓����Ҥ�`a�hbѠ|��-9���vP�>a�+�EW�(�������k�:^0b�Z�r�G�|n�Y_�2/�2P
17,"�89�^^�B��K��!rA%`��y$� &��}}%�S�(�k"
17,!�縬F�ğ�:Q�UF-V
17,VY�E�	|�>2�<
17,'t������b
17,"�B	�փp�!��z!6�qr�T=�, ������p3WQ���WS�ٴ2E��;��Q����6!�z|��WOd<"
17,"�p�M~���/i�M1���S�r�\E���X�k���m����w���""Q�����os^\"
17,�/�k���a�o�ݟPrx� �ҋ��1ΎT����
17,���W
17,"v�,���ǣ����x���2q;���]�#z��s8����~^��@M�6�	k)��"
17,�xTZ��g�M2IP�@kX���ql�%�9^+��N�>*�7�x��W9}5
17,�?�uN�D�P�
17,"#��a~Q��K�3��Ĉ�IEA,X&f���T��S��@�L �og^""vި�]���s���;��_�1��v;2d�,;�\t{�0�r��B��mx�K""	Z�DN�r�j&����㟰V?�����s����@����ףmo���<�Pn�k�<�����m�$��<ڮo�V����T�p��F!n{���OUh^�SLD��᯻;&;'n�0��1��Z�|]�vbG�S�w1�e�ʿ��^�����YE�'"
17,�`��?��͓�P�PE�p����h�+�>:�h����n�]�W*��
17,�C�حb�y6_����D���1��1�t|Ą.;i
17,y�>~
17,"I""XE�T}�����e��""���H���1[ЙBI�Qj�y)�~"
17,Aql���33l��_�����Ⱦ����
17,"s�Y����d4,%[Ye����r��,"
17,�<���	#XjH|���C�U��.���-�H����ݳ�r�;����'�)O��sLoE��Gb(�X�e@�J�y���=:��|���⟵�g?ڷK�S��4u�ɹ���pR�ȫ�Ͻ�ķ��N���x����Wa������{���	�@�3�z$���'�=�u!c�Z�����=A�{B��û[
17,"��w��R(��:��Gh���`�%��P�w �1�~���+DϞ�,d�_Y��L��R��r�[�'`OD"
17,"G��@�k_��""��J����^�"
17,"Q~��Y�����깊���7]���֔���l�0�����ĈB��W� �2@J��������@�ah\������nP��O�T�k؝a��I����x�H��_�¯�^.�_"
17,eO�Q�$�J�K
17,/���|�
17,q�HU?Ţ^mW�h�9�ߟ��>QPy�Ў�KF�J�Vo�?�E6�q<>S��y�MO�p���ep�)�F���
17,O��s��]˨�b����Ӱ�\�h��ͮ i�x��T�8�����%z���'.�W!�!)���dO�&#�Ġ�
17,K��-
17,�`��
17,��hCg��&�/Q��.ٞ
17,X_�9�:����	�գM˟���T���_Z�XJ��Ţ�0aC��=
17,�����Y/
17,�8
17,��`�����
17,�����}���HY��C��ŵ�GZ�<�;z����e#���;/~䍕�5�)Ψ��PN쵗��e��.�D�(b���h�]e!.�������E�¾e��T
17,"��/�R~�����Ae�ffTE0��:��	 2��z��گ>*{x`L��*-�^Giy~qNah�i�{�~���"
17,"lf%%�1.����rݿ��S���b%϶D��ʢl��f�_-{�]�����_�gY��x����	�w�\'�""�ɒ�����e�]�jU�J�"
17,",�X�\ o��Y����Վ��F{�����u���W�y"
17,&{��腄̒����	]e�K�
17,'-�L[����W�>��.
17,��2>���>�
17,"��45Px����G�*:jVk��`�8#�7'�I4������cl�e}e��H91""Dz�G�0�nD�=��)y�:���й�"
17,#�����AS���YY�4
17,�{K2�W��
17,����kM���Rt�ɥ_���'4����Щ>���?����a�*Ԏ�!ܧ�qe�?%��X緔�q��oa�U�Y�˾�Z�����X|��_��+U���X�S�e$a4�
17,���V��ʿ	�»��U_Ș*�O(t
17,��1�蝅�@��2�?ncR\���ɜ�_��T�J�
17,���k�@FW���O�R�[��ﰣ�� �^R���0�
17,�tiމ�>9�
17,�ĕYꚳ���
17,<[���*�y���0v��/uۄ��j�Z�o���I_������)�jӦd%�]�<�
17,}�q$�*�'���r��
17,ðO�[����G|���q%�iUGuP�� ����ܞ�v�!����n�I�[��[���0yf���W~mDVO���z~����5�}�p�.���w[?����i�G��:�۟�'gjda��mz|m������1V����� t�x[*��1�^�����8#���\|h��0�� �0^�)#tĊ`Zr�ݬ(w�t�P��
17,"�Q�Ӟ�9�A��v�.��X�(��:@�Ԧ?w���@�`�d�k��dm)�\&�y]Wj�J�.����Zs����R9����g8 "
19,SQL Server Performance Tuning Tips
19,IOT Virtual Conference - Register now to book your ticket and get updates x
19,"CONGRATULATIONS! C# Corner Q1, 2021 MVPs Announced"
19,Why Join Become a member Login
19,No unread comment.
19,View All Comments
19,No unread message.
19,View All Messages
19,No unread notification.
19,View All Notifications
19,C# Corner
19,Post
19,An Article
19,A Blog
19,A News
19,A Video
19,An EBook
19,An Interview Question
19,Ask Question
19,TECHNOLOGIES
19,ANSWERSLEARNNEWSBLOGSVIDEOSINTERVIEW PREPBOOKSEVENTSCAREERMEMBERSJOBS
19,SQL Server Performance Tuning Tips
19,Pankaj
19,Kumar Choudhary
19,Updated date
19,"Feb 08, 2021"
19,64.1k
19,"In this article, you will learn the tips of fine tuning the performance of SQL Server."
19,facebook twitter linkedIn Reddit WhatsApp
19,Email Bookmark Print Other Artcile
19,Expand
19,"Introduction   In this article, we will learn about SQL Server performance tuning tips with examples."
19,Database
19,"The Database is the most important and powerful part of any application. If your database is not working properly and taking a long time to compute the result, this means something is going wrong in the database. Here, database tune-up is required, otherwise, the performance of the application will degrade.       I know a lot of articles already published on this topic. But in this article, I tried to provide a list of database tune-up tips that will cover all the aspects of the database. Database tuning is a very critical and fussy process. It is true that database tuning is a database admin task but we should have the basic level of knowledge for doing this. Because, if we are working on a project where there is no role of admin, then it is our responsibility to maintain the performance of the database. If the performance of the database is degraded, then it will cause the worst effect on the whole system."
19,"In this article, I will explain some basic database tuning tips that I learned from my experience and from my friends who are working as a database administrator. Using these tips, you can maintain or upgrade the performance of your database system. Basically, these tips are written for SQL Server but we can implement these into another database too, like Oracle and MySQL. Please read these tips carefully and at the end of the article, let me know if you find something wrong or incorrect.   Avoid Null value in the fixed-length field   We should avoid the Null value in fixed-length fields because if we insert the NULL value in a fixed-length field, then it will take the same amount of space as the desired input value for that field. So, if we require a null value in a field, then we should use a variable-length field that takes lesser space for NULL. The use of NULLs in a database can reduce the database performance, especially,  in WHERE clauses. For example, try to use varchar instead of char and nvarchar."
19,Never use Select * Statement:
19,"When we require all the columns of a table, we usually use a “Select *” statement. Well, this is not a good approach because when we use the “select *” statement, the SQL Server converts * into all column names before executing the query, which takes extra time and effort. So, always provide all the column names in the query instead of “select *”.   Normalize tables in a database   Normalized and managed tables increase the performance of a database. So,  always try to perform at least 3rd normal form. It is not necessary that all tables require a 3NF normalization form, but if any table contains 3NF form normalization, then it can be called well-structured tables.   Keep Clustered Index Small   Clustered index stores data physically into memory. If the size of a clustered index is very large, then it can reduce the performance. Hence, a large clustered index on a table with a large number of rows increases the size significantly. Never use an index for frequently changed data because when any change in the table occurs, the index is also modified, and that can degrade performance.   Use Appropriate Datatype   If we select an inappropriate data type, it will reduce the space and enhance the performance; otherwise, it generates the worst effect. So, select an appropriate data type according to the requirement. SQL contains many data types that can store the same type of data but you should select an appropriate data type because each data type has some limitations and advantages upon another one.   Store image path instead of the image itself   I found that many developers try to store the image into the database instead of the image path. It may be possible that it is a requirement of the application to store images into a database. But generally, we should use an image path, because storing image in a database increases the database size and reduces performance.   USE Common Table Expressions (CTEs) instead of Temp table   We should prefer a CTE over the temp table because temp tables are stored physically in a TempDB which is deleted after the session ends. While CTEs are created within memory. Execution of a CTE is very fast as compared to the temp tables and very lightweight too.   Use Appropriate Naming Convention   The main goal of adopting a naming convention for database objects is to make it easily identifiable by the users, their type, and the purpose of all objects contained in the database. A good naming convention decreases the time required in searching for an object. A good name clearly indicates the action name of any object that it will perform."
19,* tblEmployees // Name of table   * vw_ProductDetails // Name of View   * PK_Employees // Name of Primary Key
19,"Use UNION ALL instead of UNION   We should prefer UNION ALL instead of UNION because UNION always performs sorting that increases the time. Also, UNION can't work with text datatype because text datatype doesn't support sorting. So, in that case, UNION can't be used. Thus, always prefer UNION All.   Use Small data type for Index   It is very important to use a Small data type for the index. Because the bigger size of the data type reduces the performance of the Index. For example, nvarhcar(10) uses  20 bytes of data, and varchar(10) uses 10 bytes of the data. So, the index for the varchar data type works better. We can also take another example of DateTime and int. Datetime data type takes 8 Bytes and int takes 4 bytes. A small datatype means less I/O overhead that increases the performance of the index."
19,Use Count(1) instead of Count(*) and Count(Column_Name):
19,"There is no difference in the performance of these three expressions; but, the last two expressions are not well considered to be a good practice. So, always use count(10) to get the numbers of records from a table.   Use Stored Procedure   Instead of using the row query, we should use the stored procedure because stored procedures are fast and easy to maintain for security and large queries.   Use Between instead of In   If Between can be used instead of IN, then always prefer Between. For example, you are searching for an employee whose id is either 101, 102, 103, or 104. Then, you can write the query using the In operator like this:"
19,"Select * From Employee Where EmpId In (101,102,103,104)"
19,You can also use Between operator for the same query.
19,Select * from Employee Where EmpId Between 101 And 104
19,"Use If Exists to determine the record   It has been seen many times that developers use ""Select Count(*)"" to get the existence of records. For example"
19,Declare @Count int;   Set @Count=(Select * From Employee Where EmpName Like '%Pan%')   If @Count>0   Begin   //Statement   End
19,"But, this is not a proper way for such type of queries. Because, the above query performs the complete table scan, so you can use If Exists for the same query. That will increase the performance of your query, as below."
19,IF Exists(Select Emp_Name From Employee Where EmpName Like '%Pan%')   Begin   //Statements   End
19,"Never Use ” Sp_” for User Define Stored Procedure   Most programmers use “sp_” for user-defined Stored Procedures. I suggest to never use “sp_” for user-defined Stored Procedure because in SQL Server, the master database has a Stored Procedure with the ""sp_"" prefix. So, when we create a Stored Procedure with the ""sp_"" prefix, the SQL Server always looks first in the Master database, then in the user-defined database, which takes some extra time.   Practice to use Schema Name   A schema is an organization or structure for a database. We can define a schema as a collection of database objects that are owned by a single principle and form a single namespace. Schema name helps the SQL Server finding that object in a specific schema. It increases the speed of the query execution. For example, try to use [dbo] before the table name.   Avoid Cursors   A cursor is a temporary work area created in the system memory when a SQL statement is executed. A cursor is a set of rows together with a pointer that identifies the current row. It is a database object to retrieve the data from a result set one row at a time. But, the use of a cursor is not good because it takes a long time because it fetches data row by row. So, we can use a replacement of cursors. A temporary table for or While loop may be a replacement of a cursor in some cases.   SET NOCOUNT ON   When an INSERT, UPDATE, DELETE, or SELECT command is executed, the SQL Server returns the number affected by the query. It is not good to return the number of rows affected by the query. We can stop this by using NOCOUNT ON.   Use Try–Catch   In T-SQL, a Try-Catch block is very important for exception handling. A best practice and use of a Try-Catch block in SQL can save our data from undesired changes. We can put all T-SQL statements in a TRY BLOCK and the code for exception handling can be put into a CATCH block.   Remove Unused Index   Remove all unused indexes because indexes are always updated when the table is updated so the index must be maintained even if not used.   Always create an index on the table   An index is a data structure to retrieve fast data. Indexes are special lookup tables that the database search engine can use to speed up data retrieval. Simply an index is a pointer to data in a table. Mainly an index increases the speed of data retrieval. So always try to keep a minimum of one index on each table it may be either clustered or non-clustered index.   Use Foreign Key with the appropriate action   A foreign key is a column or combination of columns that is the same as the primary key, but in a different table. Foreign keys are used to define a relationship and enforce integrity between two tables. In addition to protecting the integrity of our data, FK constraints also help document the relationships between our tables within the database itself. Also, define an action rule for the delete and update command, you can select any action among the No Action, Set NULL, Cascade, and set default.   Use Alias Name   Aliasing renames a table or a column temporarily by giving another name. The use of table aliases means to rename a table in a specific SQL statement. Using aliasing, we can provide a small name to a large name that will save our time."
19,"Use Transaction Management   A transaction is a unit of work performed against the database. A transaction is a set of work (T-SQL statements) that execute together like a single unit in a specific logical order as a single unit. If all the statements are executed successfully then the transaction is complete and the transaction is committed and the data will be saved in the database permanently. If any single statement fails then the entire transaction will fail and then the complete transaction is either canceled or rolled back.   Use Index Name in Query   Although in most cases the query optimizer will pick the appropriate index for a specific table based on statistics, sometimes it is better to specify the index name in your SELECT query.   Example"
19,"SELECT   e.Emp_IId,   e.First_Name,   e.Last_Name   FROM dbo.EMPLOYEE e   WITH (INDEX (Clus_Index))   WHERE e.Emp_IId > 5   Select Limited Data"
19,"We should retrieve only the required data and ignore the unimportant data. The fewer data retrieved, the faster the query will run. Rather than filtering on the client, push as much filtering as possible on the server-end. This will result in less data being sent on the wire and you will see results much faster.   Drop Index before Bulk Insertion of Data   We should drop the index before the insertion of a large amount of data. This makes the insert statement run faster. Once the inserts are completed, you can recreate the index again.   Use Unique Constraint and Check Constraint   A Check constraint checks for a specific condition before inserting data into a table. If the data passes all the Check constraints then the data will be inserted into the table otherwise the data for insertion will be discarded. The CHECK constraint ensures that all values in a column satisfy certain conditions.   A Unique Constraint ensures that each row for a column must have a unique value. It is like a Primary key but it can accept only one null value. In a table, one or more column can contain a Unique Constraint. So we should use a Check Constraint and Unique Constraint because it maintains the integrity in the database.   Importance of Column Order in index   If we are creating a Non-Clustered index on more than one column then we should consider the sequence of the columns. The order or position of a column in an index also plays a vital role in improving SQL query performance. An index can help to improve the SQL query performance if the criteria of the query match the columns that are left most in the index key. So we should place the most selective column on left most side of a non-clustered index.   Recompiled Stored Procedure   We all know that Stored Procedures execute T-SQL statements in less time than a similar set of T-SQL statements are executed individually. The reason is that the query execution plan for the Stored Procedures is already stored in the ""sys. procedures"" system-defined view. We all know that recompilation of a Stored Procedure reduces SQL performance. But in some cases, it requires recompilation of the Stored Procedure. Dropping and altering of a column, index, and/or trigger of a table. Updating the statistics used by the execution plan of the Stored Procedure. Altering the procedure will cause the SQL Server to create a new execution plan.   Use Sparse Column   Sparse columns provide better performance for NULL and Zero data. If you have any column that contains large amounts numbers of NULL and Zero then prefer Sparse Column instead of the default column of SQL Server. The sparse column takes lesser space than the regular column (without the SPARSE clause).   Example"
19,"Create Table Table_Name   (   Id int, //Default Column   Group_Id int Sparse // Sparse Column   )"
19,Avoid Loops In Coding   Suppose you want to insert 10 records into the table then instead of using a loop to insert the data into the table you can insert all data using a single insert query.
19,"declare @int int;   set @int=1;   while @int<=10   begin   Insert Into Tab values(@int,'Value'+@int);   set @int=@int+1;   end"
19,The above method is not a good approach to insert the multiple records instead of this you can use another method like below.
19,"Insert Into Tab values(1,'Value1'),(2,'Value2'),(3,'Value3'),(4,'Value4'),(5,'Value5'),(6,'Value6'),(7,'Value7'),(8,'Value8'),(9,'Value9'),(10,'Value10');"
19,"Avoid Correlated Queries   In A Correlated query inner query take input from the outer(parent) query, this query runs for each row that reduces the performance of the database."
19,"Select Name, City, (Select Company_Name   from   Company where companyId=cs.CustomerId) from Customer cs"
19,The best method is that we should prefer the join instead of the correlated query as below.
19,"Select cs.Name, cs.City, co.Company_Name   from Customer cs   Join   Company co   on   cs.CustomerId=co.CustomerId"
19,"Avoid index and join hints   In some cases, index and join hint may increase the performance of a database, but if you provide any join or index hint then the server always tries to use the hint provided by you although it has a better execution plan, so such type of approach may reduce the database performance. Use Join or index hint if you are confident that there is not any better execution plan. If you have any doubt then make the server free to choose an execution plan.   Avoid Use of Temp table   Avoid the use of a temp table as much as you can because a temp table is created into a temp database like any basic table structure. After completion of the task, we require to drop the temp table. That raises the load on the database. You can prefer the table variable instead of this.   Use Index for required columns   The index should be created for all columns which are using the Where, Group By, Order By, Top, and Distinct command.   Don't use Index   It is true that the use of an index makes the fast retrieval of the result. But, it is not always true. In some cases, the use of index doesn't affect the performance of the query. In such cases, we can avoid the use of the index."
19,"When the size of the table is very small. The index is not used in the query optimizer DML(insert, Update, Delete) operations are frequently used. Column contains TEXT, nText type of data."
19,"Use View for complex queries   If you are using join on two or more tables and the result of queries is frequently used, then it will be better to make a View that will contain the result of the complex query. Now, you can use this View multiple times, so that you don't have to execute the query multiple times to get the same result.   Make Transaction short   It will be better to keep the transaction as short as we can. Because the big size of transactions makes the table locked and reduces the database concurrency. So, always try to make shorter transactions.   Use Full-text Index   If your query contains multiple wild card searches using LIKE(%%), then the use of Full-text Index can increase the performance. Full-text queries can include simple words and phrases or multiple forms of a word or phrase. A full-text query returns any document that contains at least one match (also known as a hit). A match occurs when a target document contains all the terms specified in the Full-text query and meets any other search conditions, such as the distance between the matching terms.   Thanks for reading the article. As I have asked in the starting, if you have any doubt or I wrote something wrong, then write me back in the comments section."
19,Read More>>
19,SQL Server Performance Tuning Tips Transact SQL Query Performance Tuning Tips Transact SQL Query Performance Tuning Tips Improve Store Procedure Performance In SQL Server/ Store Procedure Performance Tuning Tips to Increase SQL Server Query Performance: Part 1 Tips to Increase SQL Server Query Performance: Part 2 Tips to Improve SQL Database Performance Transact SQL Query Performance Tuning Tips Tips To Increase SQL Server Stored Procedure Performance SQL Server Performance Tuning: Data Compression How To Optimize SQL Queries Tips And Tricks To Improve WEB API Performance
19,"Watch here a full video for more information about MS SQL New Functions, Syntaxes, Performance Tuning Tips & Tricks."
19,"Summary   In this article, we learned about SQL Server Performance Tuning Tips with examples."
19,SQL ServerSQL Server Performance
19,Next Recommended Article
19,FEATURED ARTICLES
19,View All
19,TRENDING UP
19,01Classes And Objects 02Blazor Server App CRUD With Entity Framework Core In .Net 5 03Create A C# Azure Function Using Visual Studio 2019 04Get Notified Via Azure Event Grid Whenever Azure Blob Is Updated 05A Detailed View At Data Binding in Blazor 06Upload And Download Multiple Files Using Web API 07What Is Deployment Slots In Azure App Service 08Unit Testing Using XUnit And MOQ In ASP.NET Core 09Bring Azure Blob Objects Back To Life 10.Net 5 Blazor WASM - Calling JavaScript From C# And Vice Versa
19,View All
19,About Us Contact Us Privacy Policy Terms Media Kit Sitemap Report a Bug FAQ Partners
19,C# Tutorials Common Interview Questions Stories Consultants Ideas Certifications
19,©2021 C# Corner. All contents are copyright of their authors.
20,How to tune Nextcloud on-premise cloud server for better performance - TechRepublic
20,Join / Log In
20,View full profile
20,Preferences
20,Newsletters
20,Community
20,Upgrade to Premium
20,Log out
20,More
20,TechRepublic Premium
20,COVID-19
20,Developer
20,IT Policy Downloads
20,Security
20,Top DaaS providers
20,Excel tips
20,Cloud
20,Big Data
20,Learn Python: Online training
20,Top 2020 DevOps trends
20,Top IT salaries
20,TechRepublic Academy
20,Best VPN Services
20,See All Topics
20,Editions:
20,United States
20,Australia
20,United Kingdom
20,Japan
20,Newsletters
20,Forums
20,Resource Library
20,Search
20,Close
20,What are you looking for?
20,COVID-19
20,Developer
20,IT Policy Downloads
20,Security
20,Top DaaS providers
20,Excel tips
20,Cloud
20,Big Data
20,Learn Python: Online training
20,Top 2020 DevOps trends
20,Top IT salaries
20,TechRepublic Academy
20,Best VPN Services
20,How to tune Nextcloud on-premise cloud server for better performance
20,Jack Wallen
20,Cloud
20,"on December 3, 2020, 7:52 AM PST"
20,Jack Wallen offers tips on configuring Nextcloud for performance that should better meet your demands.
20,Image: Getty Images/iStockphoto
20,"Nextcloud is, hands down, the best on-premise cloud server platform on the market. Not only is it easy to deploy, it's also reliable and expandable. However, out of the box, you might find the platform's performance doesn't quite meet your expectations. Fear not, there are ways you can eke out a bit more performance from the platform. Let me offer you up a few tips. SEE: Managing the multicloud (ZDNet/TechRepublic special feature) | Download the free PDF version (TechRepublic) 	What you'll need"
20,"In order to tune Nextcloud, you'll need a running instance of the platform. These tips should work with any iteration of Nextcloud from 16 on. You'll also need a user with sudo privileges."
20,Must-read cloud
20,Top cloud trends for 2021
20,5 programming languages cloud engineers should learn
20,AWS: 9 pro tips and best practices (free PDF)
20,Cloud computing policy (TechRepublic Premium)
20,How to enable caching
20,"The first tip is to enable caching on your Nextcloud server. I've already covered this in my piece ""How to enable caching on Nextcloud 16."" With caching enabled, you increase performance by storing frequently requested objects in memory for faster retrieval. This should be one of the first configurations you take care of, in order to squeeze out as much performance as possible. Do take care, as enabling caching is a bit tricky."
20,"How to enable HTTP2 HTTP2 is the latest HTTP protocol and allows web browsers to send multiple, simultaneous requests to a server. HTTP2 is well supported by Apache, so it's not at all challenging to enable. This is another issue I've tackled before, so you can easily enable HTTP2 on Apache by following the tutorial, ""How to enable HTTP/2 Protocol on Ubuntu 16.04."" How to optimize PHP-FPM The default PHP-FPM installation on your server might be the cause of excessive load times for Nextcloud--each request for an element is handled by a separate PHP-FPM process. By allowing numerous PHP-FPM processes to run, you should gain a considerable increase in performance. Here's what you need to do. Open the necessary configuration file for editing with the command: sudo nano /etc/php/7.X/fpm/pool.d/www.conf"
20,"Where X is your current installed version of PHP. In that file, you'll need to look for the following lines: pm = dynamic"
20,pm.max_children = 120
20,pm.start_servers = 12
20,pm.min_spare_servers = 6
20,pm.max_spare_servers = 18
20,The above configuration changes will work for a server that has 4 GB of RAM and a 1 GB  MySQL cache. Edit the above options and save and close the file. Restart apache with the command: sudo systemctl restart apache2
20,"Once you've taken care of the above optimizations, you should find that Nextcloud performs considerably better. As my last bit of advice, you might want to run these on a non-production environment, to ensure you nail the configurations. Once you see the increase in performance, you can then make the changes on your production environment. Subscribe to TechRepublic's How To Make Tech Work on YouTube for all the latest tech advice for business pros from Jack Wallen."
20,Cloud and Everything as a Service Newsletter
20,"This is your go-to resource for XaaS, AWS, Microsoft Azure, Google Cloud Platform, cloud engineering jobs, and cloud security news and tips."
20,Delivered Mondays
20,Sign up today
20,"Also see Multicloud: A cheat sheet (TechRepublic) 		Top IT certifications to increase your salary (free PDF) (TechRepublic) 		Power checklist: Local email server-to-cloud migration (TechRepublic Premium) 		Top cloud providers in 2020: AWS, Microsoft Azure, and Google Cloud, hybrid, SaaS players (ZDNet) 		Cloud computing: More must-read coverage (TechRepublic on Flipboard)"
20,Editor's Picks
20,"TechRepublic Premium: The best IT policies, templates, and tools, for today and tomorrow."
20,"MXLinux is the most downloaded Linux desktop distribution, and now I know why"
20,Windows 10: How to flush the DNS cache to improve network performance
20,SpaceX Starlink beta: What it is and when you can get it
20,IBM report: Four things that will never be the same post-pandemic
20,Photos: 82 coolest virtual backgrounds to use in Zoom or Teams meetings
20,Comment and share: How to tune Nextcloud on-premise cloud server for better performance
20,By Jack Wallen
20,"Jack Wallen is an award-winning writer for TechRepublic, The New Stack, and Linux New Media. He's covered a variety of topics for over twenty years and is an avid promoter of open source. For more news about Jack Wallen, visit his website jackwallen...."
20,Full Bio
20,See all of Jack's content
20,Related Topics:
20,Cloud
20,Open Source
20,Enterprise Software
20,Security
20,Networking
20,Data Centers
20,Microsoft
20,Cloud on ZDNet
20,Show Comments
20,Hide Comments
20,LOG IN TO COMMENT
20,My Profile
20,Log out
20,| Commenting FAQs | Community Guidelines
20,Join Discussion
20,LOG IN TO COMMENT
20,Add your Comment
20,5 Linux server distributions you should be using
20,Scheduled systems outage checklist
20,A guide to The Open Source Index and GitHub projects checklist
20,Offshore work policy
20,Services
20,About Us
20,Manage Profile
20,Membership
20,Newsletters
20,RSS Feeds
20,Site Map
20,Site Help & Feedback
20,FAQ
20,Advertise
20,Reprint Policy
20,Do Not Sell My Information
20,Explore
20,Blogs
20,Downloads
20,TechRepublic Forums
20,Meet the Team
20,TechRepublic Academy
20,TechRepublic Premium
20,TechRepublic Premium
20,Resource Library
20,Photos
20,Videos
20,"© 2021 ZDNET, A RED VENTURES COMPANY. ALL RIGHTS RESERVED."
20,Privacy Policy
20,| Cookie Settings
20,| Terms of Use
20,| A ZDNet site
21,Optimizer Hints | PingCAP DocsTiDBToolsCloudContact UsDoc Menu
21,About TiDB
21,TiDB Introduction
21,What's New in TiDB 5.0
21,Basic Features
21,Experimental Features
21,Benchmarks
21,v5.0 Sysbench Performance Test Report
21,v5.0 TPC-C Performance Test Report
21,v5.0 MPP mode TPC-H 100GB Performance Test
21,MySQL Compatibility
21,TiDB Limitations
21,TiDB Adopters
21,Credits
21,Quick Start
21,Try Out TiDB
21,Learn TiDB SQL
21,Import Example Database
21,Deploy
21,Software and Hardware Requirements
21,Environment Configuration Checklist
21,Plan Cluster Topology
21,Minimal Topology
21,TiFlash Topology
21,TiCDC Topology
21,TiDB Binlog Topology
21,TiSpark Topology
21,Cross-DC Topology
21,Hybrid Topology
21,Install and Start
21,Use TiUP (Recommended)
21,Deploy in Kubernetes
21,Verify Cluster Status
21,Test Cluster Performance
21,Test TiDB Using Sysbench
21,Test TiDB Using TPC-C
21,Migrate
21,Overview
21,Migrate from MySQL
21,Migrate from Amazon Aurora MySQL Using TiDB Lightning
21,Migrate from MySQL SQL Files Using TiDB Lightning
21,Migrate from Amazon Aurora MySQL Using DM
21,Migrate from CSV Files
21,Use TiDB Lightning
21,Use LOAD DATA Statement
21,Migrate from SQL Files
21,Maintain
21,Upgrade
21,Use TiUP (Recommended)
21,Use TiDB Operator
21,Scale
21,Use TiUP (Recommended)
21,Use TiDB Operator
21,Backup and Restore
21,Use BR Tool (Recommended)
21,BR Tool Overview
21,Use BR Command-line
21,BR Use Cases
21,Read Historical Data
21,Configure Time Zone
21,Daily Checklist
21,Maintain TiFlash
21,Maintain TiDB Using TiUP
21,Modify Configuration Online
21,Monitor and Alert
21,Monitoring Framework Overview
21,Monitoring API
21,Deploy Monitoring Services
21,Export Grafana Snapshots
21,TiDB Cluster Alert Rules
21,TiFlash Alert Rules
21,Troubleshoot
21,TiDB Troubleshooting Map
21,Identify Slow Queries
21,Analyze Slow Queries
21,SQL Diagnostics
21,Identify Expensive Queries
21,Statement Summary Tables
21,Troubleshoot Hotspot Issues
21,Troubleshoot Increased Read and Write Latency
21,Troubleshoot Cluster Setup
21,Troubleshoot High Disk I/O Usage
21,Troubleshoot Lock Conflicts
21,Troubleshoot TiFlash
21,Troubleshoot Write Conflicts in Optimistic Transactions
21,Performance Tuning
21,System Tuning
21,Operating System Tuning
21,Software Tuning
21,Configuration
21,Tune TiDB Memory
21,Tune TiKV Threads
21,Tune TiKV Memory
21,TiKV Follower Read
21,TiFlash Tuning
21,Coprocessor Cache
21,SQL Tuning
21,Overview
21,Understanding the Query Execution Plan
21,Overview
21,EXPLAIN Walkthrough
21,Indexes
21,Joins
21,MPP Queries
21,Subqueries
21,Aggregation
21,Views
21,Partitions
21,SQL Optimization Process
21,Overview
21,Logic Optimization
21,Overview
21,Subquery Related Optimizations
21,Column Pruning
21,Decorrelation of Correlated Subquery
21,Eliminate Max/Min
21,Predicates Push Down
21,Partition Pruning
21,TopN and Limit Push Down
21,Join Reorder
21,Physical Optimization
21,Overview
21,Index Selection
21,Statistics
21,Wrong Index Solution
21,Distinct Optimization
21,Prepare Execution Plan Cache
21,Control Execution Plans
21,Overview
21,Optimizer Hints
21,SQL Plan Management
21,The Blocklist of Optimization Rules and Expression Pushdown
21,Tutorials
21,Multiple Data Centers in One City Deployment
21,Three Data Centers in Two Cities Deployment
21,Best Practices
21,Use TiDB
21,Java Application Development
21,Use HAProxy
21,Highly Concurrent Write
21,Grafana Monitoring
21,PD Scheduling
21,TiKV Performance Tuning with Massive Regions
21,Three-node Hybrid Deployment
21,Use Placement Rules
21,Use Load Base Split
21,Use Store Limit
21,TiDB Ecosystem Tools
21,Overview
21,Use Cases
21,Download
21,Backup & Restore (BR)
21,BR Tool Overview
21,Use BR Command-line for Backup and Restoration
21,BR Use Cases
21,External Storages
21,BR FAQ
21,TiDB Binlog
21,Overview
21,Quick Start
21,Deploy
21,Maintain
21,Configure
21,Pump
21,Drainer
21,Upgrade
21,Monitor
21,Reparo
21,binlogctl
21,Binlog Consumer Client
21,TiDB Binlog Relay Log
21,Bidirectional Replication Between TiDB Clusters
21,Glossary
21,Troubleshoot
21,Troubleshoot
21,Handle Errors
21,FAQ
21,TiDB Lightning
21,Overview
21,Tutorial
21,Deploy
21,Configure
21,Key Features
21,Checkpoints
21,Table Filter
21,CSV Support
21,Backends
21,Web Interface
21,Monitor
21,FAQ
21,Glossary
21,TiCDC
21,Overview
21,Deploy
21,Maintain
21,Troubleshoot
21,Monitor
21,TiCDC Open Protocol
21,Integrate TiDB with Confluent Platform
21,Glossary
21,Dumpling
21,sync-diff-inspector
21,Overview
21,Data Check for Tables with Different Schema/Table Names
21,Data Check in Sharding Scenarios
21,Data Check for TiDB Upstream/Downstream Clusters
21,TiSpark
21,Quick Start
21,User Guide
21,Reference
21,Cluster Architecture
21,Overview
21,Storage
21,Computing
21,Scheduling
21,Key Monitoring Metrics
21,Overview
21,TiDB
21,TiKV
21,TiFlash
21,TiCDC
21,Secure
21,Enable TLS Between TiDB Clients and Servers
21,Enable TLS Between TiDB Components
21,Generate Self-signed Certificates
21,Encryption at Rest
21,Enable Encryption for Disk Spill
21,Log Redaction
21,Privileges
21,Security Compatibility with MySQL
21,Privilege Management
21,User Account Management
21,Role-Based Access Control
21,Certificate-Based Authentication
21,SQL
21,SQL Language Structure and Syntax
21,Attributes
21,AUTO_INCREMENT
21,AUTO_RANDOM
21,SHARD_ROW_ID_BITS
21,Literal Values
21,Schema Object Names
21,Keywords and Reserved Words
21,User-Defined Variables
21,Expression Syntax
21,Comment Syntax
21,SQL Statements
21,ADD COLUMN
21,ADD INDEX
21,ADMIN
21,ADMIN CANCEL DDL
21,ADMIN CHECKSUM TABLE
21,ADMIN CHECK [TABLE|INDEX]
21,ADMIN SHOW DDL [JOBS|QUERIES]
21,ALTER DATABASE
21,ALTER INDEX
21,ALTER INSTANCE
21,ALTER TABLE
21,ALTER USER
21,ANALYZE TABLE
21,BACKUP
21,BEGIN
21,CHANGE COLUMN
21,COMMIT
21,CHANGE DRAINER
21,CHANGE PUMP
21,CREATE [GLOBAL|SESSION] BINDING
21,CREATE DATABASE
21,CREATE INDEX
21,CREATE ROLE
21,CREATE SEQUENCE
21,CREATE TABLE LIKE
21,CREATE TABLE
21,CREATE USER
21,CREATE VIEW
21,DEALLOCATE
21,DELETE
21,DESC
21,DESCRIBE
21,DROP [GLOBAL|SESSION] BINDING
21,DROP COLUMN
21,DROP DATABASE
21,DROP INDEX
21,DROP ROLE
21,DROP SEQUENCE
21,DROP STATS
21,DROP TABLE
21,DROP USER
21,DROP VIEW
21,EXECUTE
21,EXPLAIN ANALYZE
21,EXPLAIN
21,FLASHBACK TABLE
21,FLUSH PRIVILEGES
21,FLUSH STATUS
21,FLUSH TABLES
21,GRANT <privileges>
21,GRANT <role>
21,INSERT
21,KILL [TIDB]
21,LOAD DATA
21,LOAD STATS
21,MODIFY COLUMN
21,PREPARE
21,RECOVER TABLE
21,RENAME INDEX
21,RENAME TABLE
21,REPLACE
21,RESTORE
21,REVOKE <privileges>
21,REVOKE <role>
21,ROLLBACK
21,SELECT
21,SET DEFAULT ROLE
21,SET [NAMES|CHARACTER SET]
21,SET PASSWORD
21,SET ROLE
21,SET TRANSACTION
21,SET [GLOBAL|SESSION] <variable>
21,SHOW ANALYZE STATUS
21,SHOW [BACKUPS|RESTORES]
21,SHOW [GLOBAL|SESSION] BINDINGS
21,SHOW BUILTINS
21,SHOW CHARACTER SET
21,SHOW COLLATION
21,SHOW [FULL] COLUMNS FROM
21,SHOW CONFIG
21,SHOW CREATE SEQUENCE
21,SHOW CREATE TABLE
21,SHOW CREATE USER
21,SHOW DATABASES
21,SHOW DRAINER STATUS
21,SHOW ENGINES
21,SHOW ERRORS
21,SHOW [FULL] FIELDS FROM
21,SHOW GRANTS
21,SHOW INDEX [FROM|IN]
21,SHOW INDEXES [FROM|IN]
21,SHOW KEYS [FROM|IN]
21,SHOW MASTER STATUS
21,SHOW PLUGINS
21,SHOW PRIVILEGES
21,SHOW [FULL] PROCESSSLIST
21,SHOW PROFILES
21,SHOW PUMP STATUS
21,SHOW SCHEMAS
21,SHOW STATS_HEALTHY
21,SHOW STATS_HISTOGRAMS
21,SHOW STATS_META
21,SHOW STATUS
21,SHOW TABLE NEXT_ROW_ID
21,SHOW TABLE REGIONS
21,SHOW TABLE STATUS
21,SHOW [FULL] TABLES
21,SHOW [GLOBAL|SESSION] VARIABLES
21,SHOW WARNINGS
21,SHUTDOWN
21,SPLIT REGION
21,START TRANSACTION
21,TABLE
21,TRACE
21,TRUNCATE
21,UPDATE
21,USE
21,Data Types
21,Overview
21,Default Values
21,Numeric Types
21,Date and Time Types
21,String Types
21,JSON Type
21,Functions and Operators
21,Overview
21,Type Conversion in Expression Evaluation
21,Operators
21,Control Flow Functions
21,String Functions
21,Numeric Functions and Operators
21,Date and Time Functions
21,Bit Functions and Operators
21,Cast Functions and Operators
21,Encryption and Compression Functions
21,Information Functions
21,JSON Functions
21,Aggregate (GROUP BY) Functions
21,Window Functions
21,Miscellaneous Functions
21,Precision Math
21,Set Operations
21,List of Expressions for Pushdown
21,Clustered Indexes
21,Constraints
21,Generated Columns
21,SQL Mode
21,Transactions
21,Overview
21,Isolation Levels
21,Optimistic Transactions
21,Pessimistic Transactions
21,Garbage Collection (GC)
21,Overview
21,Configuration
21,Views
21,Partitioning
21,Character Set and Collation
21,System Tables
21,mysql
21,INFORMATION_SCHEMA
21,Overview
21,ANALYZE_STATUS
21,CLIENT_ERRORS_SUMMARY_BY_HOST
21,CLIENT_ERRORS_SUMMARY_BY_USER
21,CLIENT_ERRORS_SUMMARY_GLOBAL
21,CHARACTER_SETS
21,CLUSTER_CONFIG
21,CLUSTER_HARDWARE
21,CLUSTER_INFO
21,CLUSTER_LOAD
21,CLUSTER_LOG
21,CLUSTER_SYSTEMINFO
21,COLLATIONS
21,COLLATION_CHARACTER_SET_APPLICABILITY
21,COLUMNS
21,DDL_JOBS
21,ENGINES
21,INSPECTION_RESULT
21,INSPECTION_RULES
21,INSPECTION_SUMMARY
21,KEY_COLUMN_USAGE
21,METRICS_SUMMARY
21,METRICS_TABLES
21,PARTITIONS
21,PROCESSLIST
21,SCHEMATA
21,SEQUENCES
21,SESSION_VARIABLES
21,SLOW_QUERY
21,STATISTICS
21,TABLES
21,TABLE_CONSTRAINTS
21,TABLE_STORAGE_STATS
21,TIDB_HOT_REGIONS
21,TIDB_INDEXES
21,TIDB_SERVERS_INFO
21,TIFLASH_REPLICA
21,TIKV_REGION_PEERS
21,TIKV_REGION_STATUS
21,TIKV_STORE_STATUS
21,USER_PRIVILEGES
21,VIEWS
21,METRICS_SCHEMA
21,TiDB Dashboard
21,Overview
21,Maintain
21,Deploy
21,Reverse Proxy
21,Secure
21,Access
21,Overview Page
21,Cluster Info Page
21,Key Visualizer Page
21,Metrics Relation Graph
21,SQL Statements Analysis
21,SQL Statements Page
21,SQL Details Page
21,Slow Queries Page
21,Cluster Diagnostics
21,Access Cluster Diagnostics Page
21,View Diagnostics Report
21,Use Diagnostics
21,Search Logs Page
21,Profile Instances Page
21,FAQ
21,CLI
21,tikv-ctl
21,pd-ctl
21,tidb-ctl
21,pd-recover
21,Command Line Flags
21,tidb-server
21,tikv-server
21,tiflash-server
21,pd-server
21,Configuration File Parameters
21,tidb-server
21,tikv-server
21,tiflash-server
21,pd-server
21,System Variables
21,Storage Engines
21,TiKV
21,TiKV Overview
21,RocksDB Overview
21,Titan Overview
21,Titan Configuration
21,TiFlash
21,Overview
21,Use TiFlash
21,TiUP
21,Documentation Guide
21,Overview
21,Terminology and Concepts
21,Manage TiUP Components
21,FAQ
21,Troubleshooting Guide
21,TiUP Components
21,tiup-playground
21,tiup-cluster
21,tiup-mirror
21,tiup-bench
21,Telemetry
21,Errors Codes
21,Table Filter
21,Schedule Replicas by Topology Labels
21,FAQs
21,TiDB FAQs
21,SQL FAQs
21,Deploy and Maintain FAQs
21,Upgrade FAQs
21,High Availability FAQs
21,High Reliability FAQs
21,Migration FAQs
21,Glossary
21,Release Notes
21,All Releases
21,v5.0
21,5.0 GA
21,5.0.0-rc
21,v4.0
21,4.0.12
21,4.0.11
21,4.0.10
21,4.0.9
21,4.0.8
21,4.0.7
21,4.0.6
21,4.0.5
21,4.0.4
21,4.0.3
21,4.0.2
21,4.0.1
21,4.0 GA
21,4.0.0-rc.2
21,4.0.0-rc.1
21,4.0.0-rc
21,4.0.0-beta.2
21,4.0.0-beta.1
21,4.0.0-beta
21,v3.1
21,3.1.2
21,3.1.1
21,3.1.0 GA
21,3.1.0-rc
21,3.1.0-beta.2
21,3.1.0-beta.1
21,3.1.0-beta
21,v3.0
21,3.0.20
21,3.0.19
21,3.0.18
21,3.0.17
21,3.0.16
21,3.0.15
21,3.0.14
21,3.0.13
21,3.0.12
21,3.0.11
21,3.0.10
21,3.0.9
21,3.0.8
21,3.0.7
21,3.0.6
21,3.0.5
21,3.0.4
21,3.0.3
21,3.0.2
21,3.0.1
21,3.0 GA
21,3.0.0-rc.3
21,3.0.0-rc.2
21,3.0.0-rc.1
21,3.0.0-beta.1
21,3.0.0-beta
21,v2.1
21,2.1.19
21,2.1.18
21,2.1.17
21,2.1.16
21,2.1.15
21,2.1.14
21,2.1.13
21,2.1.12
21,2.1.11
21,2.1.10
21,2.1.9
21,2.1.8
21,2.1.7
21,2.1.6
21,2.1.5
21,2.1.4
21,2.1.3
21,2.1.2
21,2.1.1
21,2.1 GA
21,2.1 RC5
21,2.1 RC4
21,2.1 RC3
21,2.1 RC2
21,2.1 RC1
21,2.1 Beta
21,v2.0
21,2.0.11
21,2.0.10
21,2.0.9
21,2.0.8
21,2.0.7
21,2.0.6
21,2.0.5
21,2.0.4
21,2.0.3
21,2.0.2
21,2.0.1
21,2.0
21,2.0 RC5
21,2.0 RC4
21,2.0 RC3
21,2.0 RC1
21,1.1 Beta
21,1.1 Alpha
21,v1.0
21,1.0.8
21,1.0.7
21,1.0.6
21,1.0.5
21,1.0.4
21,1.0.3
21,1.0.2
21,1.0.1
21,1.0
21,Pre-GA
21,RC4
21,RC3
21,RC2
21,RC1
21,"Optimizer HintsTiDB supports optimizer hints, which are based on the comment-like syntax introduced in MySQL 5.7. For example, one of the common syntaxes is /*+ HINT_NAME([t1_name [, t2_name] ...]) */. Use of optimizer hints is recommended in cases where the TiDB optimizer selects a less optimal query plan.Note:MySQL command-line clients earlier than 5.7.7 strip optimizer hints by default. If you want to use the Hint syntax in these earlier versions, add the --comments option when starting the client. For example: mysql -h 127.0.0.1 -P 4000 -uroot --comments.SyntaxOptimizer hints are case insensitive and specified within /*+ ... */ comments following the SELECT, UPDATE or DELETE keyword in a SQL statement. Optimizer hints are not currently supported for INSERT statements. Multiple hints can be specified by separating with commas. For example, the following query uses three different hints:CopySELECT /*+ USE_INDEX(t1, idx1), HASH_AGG(), HASH_JOIN(t1) */ count(*) FROM t t1, t t2 WHERE t1.a = t2.b;How optimizer hints affect query execution plans can be observed in the output of EXPLAIN and EXPLAIN ANALYZE.An incorrect or incomplete hint will not result in a statement error. This is because hints are intended to have only a hint (suggestion) semantic to query execution. Similarly, TiDB will at most return a warning if a hint is not applicable.Note:If the comments do not follow behind the specified keywords, they will be treated as common MySQL comments. The comments do not take effect, and no warning is reported.Currently, TiDB supports two categories of hints, which are different in scope. The first category of hints takes effect in the scope of query blocks, such as /*+ HASH_AGG() */; the second category of hints takes effect in the whole query, such as /*+ MEMORY_QUOTA(1024 MB)*/.Each query or sub-query in a statement corresponds to a different query block, and each query block has its own name. For example:CopySELECT * FROM (SELECT * FROM t) t1, (SELECT * FROM t) t2;The above query statement has three query blocks: the outermost SELECT corresponds to the first query block, whose name is sel_1; the two SELECT sub-queries correspond to the second and the third query block, whose names are sel_2 and sel_3, respectively. The sequence of the numbers is based on the appearance of SELECT from left to right. If you replace the first SELECT with DELETE or UPDATE, then the corresponding query block names are del_1 or upd_1.Hints that take effect in query blocksThis category of hints can follow behind any SELECT, UPDATE or DELETE keywords. To control the effective scope of the hint, use the name of the query block in the hint. You can make the hint parameters clear by accurately identifying each table in the query (in case of duplicated table names or aliases). If no query block is specified in the hint, the hint takes effect in the current block by default.For example:CopySELECT /*+ HASH_JOIN(@sel_1 t1@sel_1, t3) */ * FROM (SELECT t1.a, t1.b FROM t t1, t t2 WHERE t1.a = t2.a) t1, t t3 WHERE t1.b = t3.b;This hint takes effect in the sel_1 query block, and its parameters are the t1 and t3 tables in sel_1 (sel_2 also contains a t1 table).As described above, you can specify the name of the query block in the hint in the following ways:Set the query block name as the first parameter of the hint, and separate it from other parameters with a space. In addition to QB_NAME, all the hints listed in this section also have another optional hidden parameter @QB_NAME. By using this parameter, you can specify the effective scope of this hint.Append @QB_NAME to a table name in the parameter to explicitly specify which query block this table belongs to.Note:You must put the hint in or before the query block where the hint takes effect. If the hint is put after the query block, it cannot take effect.QB_NAMEIf the query statement is a complicated statement that includes multiple nested queries, the ID and name of a certain query block might be mistakenly identified. The hint QB_NAME can help us in this regard.QB_NAME means Query Block Name. You can specify a new name to a query block. The specified QB_NAME and the previous default name are both valid. For example:CopySELECT /*+ QB_NAME(QB1) */ * FROM (SELECT * FROM t) t1, (SELECT * FROM t) t2;This hint specifies the outer SELECT query block's name to QB1, which makes QB1 and the default name sel_1 both valid for the query block.Note:In the above example, if the hint specifies the QB_NAME to sel_2 and does not specify a new QB_NAME for the original second SELECT query block, then sel_2 becomes an invalid name for the second SELECT query block.MERGE_JOIN(t1_name [, tl_name ...])The MERGE_JOIN(t1_name [, tl_name ...]) hint tells the optimizer to use the sort-merge join algorithm for the given table(s). Generally, this algorithm consumes less memory but takes longer processing time. If there is a very large data volume or insufficient system memory, it is recommended to use this hint. For example:Copyselect /*+ MERGE_JOIN(t1, t2) */ * from t1，t2 where t1.id = t2.id;Note:TIDB_SMJ is the alias for MERGE_JOIN in TiDB 3.0.x and earlier versions. If you are using any of these versions, you must apply the TIDB_SMJ(t1_name [, tl_name ...]) syntax for the hint. For the later versions of TiDB, TIDB_SMJ and MERGE_JOIN are both valid names for the hint, but MERGE_JOIN is recommended.INL_JOIN(t1_name [, tl_name ...])The INL_JOIN(t1_name [, tl_name ...]) hint tells the optimizer to use the index nested loop join algorithm for the given table(s). This algorithm might consume less system resources and take shorter processing time in some scenarios and might produce an opposite result in other scenarios. If the result set is less than 10,000 rows after the outer table is filtered by the WHERE condition, it is recommended to use this hint. For example:Copyselect /*+ INL_JOIN(t1, t2) */ * from t1，t2 where t1.id = t2.id;The parameter(s) given in INL_JOIN() is the candidate table for the inner table when you create the query plan. For example, INL_JOIN(t1) means that TiDB only considers using t1 as the inner table to create a query plan. If the candidate table has an alias, you must use the alias as the parameter in INL_JOIN(); if it does not has an alias, use the table's original name as the parameter. For example, in the select /*+ INL_JOIN(t1) */ * from t t1, t t2 where t1.a = t2.b; query, you must use the t table's alias t1 or t2 rather than t as INL_JOIN()'s parameter.Note:TIDB_INLJ is the alias for INL_JOIN in TiDB 3.0.x and earlier versions. If you are using any of these versions, you must apply the TIDB_INLJ(t1_name [, tl_name ...]) syntax for the hint. For the later versions of TiDB, TIDB_INLJ and INL_JOIN are both valid names for the hint, but INL_JOIN is recommended.INL_HASH_JOINThe INL_HASH_JOIN(t1_name [, tl_name]) hint tells the optimizer to use the index nested loop hash join algorithm. The conditions for using this algorithm are the same with the conditions for using the index nested loop join algorithm. The difference between the two algorithms is that INL_JOIN creates a hash table on the joined inner table, but INL_HASH_JOIN creates a hash table on the joined outer table. INL_HASH_JOIN has a fixed limit on memory usage, while the memory used by INL_JOIN depends on the number of rows matched in the inner table.HASH_JOIN(t1_name [, tl_name ...])The HASH_JOIN(t1_name [, tl_name ...]) hint tells the optimizer to use the hash join algorithm for the given table(s). This algorithm allows the query to be executed concurrently with multiple threads, which achieves a higher processing speed but consumes more memory. For example:Copyselect /*+ HASH_JOIN(t1, t2) */ * from t1，t2 where t1.id = t2.id;Note:TIDB_HJ is the alias for HASH_JOIN in TiDB 3.0.x and earlier versions. If you are using any of these versions, you must apply the TIDB_HJ(t1_name [, tl_name ...]) syntax for the hint. For the later versions of TiDB, TIDB_HJ and HASH_JOIN are both valid names for the hint, but HASH_JOIN is recommended.HASH_AGG()The HASH_AGG() hint tells the optimizer to use the hash aggregation algorithm in all the aggregate functions in the specified query block. This algorithm allows the query to be executed concurrently with multiple threads, which achieves a higher processing speed but consumes more memory. For example:Copyselect /*+ HASH_AGG() */ count(*) from t1，t2 where t1.a > 10 group by t1.id;STREAM_AGG()The STREAM_AGG() hint tells the optimizer to use the stream aggregation algorithm in all the aggregate functions in the specified query block. Generally, this algorithm consumes less memory but takes longer processing time. If there is a very large data volume or insufficient system memory, it is recommended to use this hint. For example:Copyselect /*+ STREAM_AGG() */ count(*) from t1，t2 where t1.a > 10 group by t1.id;USE_INDEX(t1_name, idx1_name [, idx2_name ...])The USE_INDEX(t1_name, idx1_name [, idx2_name ...]) hint tells the optimizer to use only the given index(es) for a specified t1_name table. For example, applying the following hint has the same effect as executing the select * from t t1 use index(idx1, idx2); statement.CopySELECT /*+ USE_INDEX(t1, idx1, idx2) */ * FROM t1;Note:If you specify only the table name but not index name in this hint, the execution does not consider any index but scan the entire table.IGNORE_INDEX(t1_name, idx1_name [, idx2_name ...])The IGNORE_INDEX(t1_name, idx1_name [, idx2_name ...]) hint tells the optimizer to ignore the given index(es) for a specified t1_name table. For example, applying the following hint has the same effect as executing the select * from t t1 ignore index(idx1, idx2); statement.Copyselect /*+ IGNORE_INDEX(t1, idx1, idx2) */ * from t t1;AGG_TO_COP()The AGG_TO_COP() hint tells the optimizer to push down the aggregate operation in the specified query block to the coprocessor. If the optimizer does not push down some aggregate function that is suitable for pushdown, then it is recommended to use this hint. For example:Copyselect /*+ AGG_TO_COP() */ sum(t1.a) from t t1;READ_FROM_STORAGE(TIFLASH[t1_name [, tl_name ...]], TIKV[t2_name [, tl_name ...]])The READ_FROM_STORAGE(TIFLASH[t1_name [, tl_name ...]], TIKV[t2_name [, tl_name ...]]) hint tells the optimizer to read specific table(s) from specific storage engine(s). Currently, this hint supports two storage engine parameters - TIKV and TIFLASH. For example:Copyselect /*+ READ_FROM_STORAGE(TIFLASH[t1], TIKV[t2]) */ t1.a from t t1, t t2 where t1.a = t2.a;Note:If you want the optimizer to use a table from another schema, you need to explicitly specify the schema name. For example:CopySELECT /*+ READ_FROM_STORAGE(TIFLASH[test1.t1,test2.t2]) */ t1.a FROM test1.t t1, test2.t t2 WHERE t1.a = t2.a;USE_INDEX_MERGE(t1_name, idx1_name [, idx2_name ...])The USE_INDEX_MERGE(t1_name, idx1_name [, idx2_name ...]) hint tells the optimizer to access a specific table with the index merge method. The given list of indexes are optional parameters. If you explicitly specify the list, TiDB selects indexes from the list to build index merge; if you do not give the list of indexes, TiDB selects indexes from all available indexes to build index merge. For example:CopySELECT /*+ USE_INDEX_MERGE(t1, idx_a, idx_b, idx_c) */ * FROM t1 WHERE t1.a > 10 OR t1.b > 10;When multiple USE_INDEX_MERGE hints are made to the same table, the optimizer tries to select the index from the union of the index sets specified by these hints.Note:The parameters of USE_INDEX_MERGE refer to index names, rather than column names. The index name of the primary key is primary.This hint takes effect on strict conditions, including:If the query can select a single index scan in addition to full table scan, the optimizer does not select index merge.If the query is in an explicit transaction, and if the statements before this query has already written data, the optimizer does not select index merge.Hints that take effect in the whole queryThis category of hints can only follow behind the first SELECT, UPDATE or DELETE keyword, which is equivalent to modifying the value of the specified system variable when this query is executed. The priority of the hint is higher than that of existing system variables.Note:This category of hints also has an optional hidden variable @QB_NAME, but the hint takes effect in the whole query even if you specify the variable.NO_INDEX_MERGE()The NO_INDEX_MERGE() hint disables the index merge feature of the optimizer.For example, the following query will not use index merge:Copyselect /*+ NO_INDEX_MERGE() */ * from t where t.a > 0 or t.b > 0;In addition to this hint, setting the tidb_enable_index_merge system variable also controls whether to enable this feature.Note:NO_INDEX_MERGE has a higher priority over USE_INDEX_MERGE. When both hints are used, USE_INDEX_MERGE does not take effect.USE_TOJA(boolean_value)The boolean_value parameter can be TRUE or FALSE. The USE_TOJA(TRUE) hint enables the optimizer to convert an in condition (containing a sub-query) to join and aggregation operations. Comparatively, the USE_TOJA(FALSE) hint disables this feature.For example, the following query will convert in (select t2.a from t2) subq to corresponding join and aggregation operations:Copyselect /*+ USE_TOJA(TRUE) */ t1.a, t1.b from t1 where t1.a in (select t2.a from t2) subq;In addition to this hint, setting the tidb_opt_insubq_to_join_and_agg system variable also controls whether to enable this feature.MAX_EXECUTION_TIME(N)The MAX_EXECUTION_TIME(N) hint places a limit N (a timeout value in milliseconds) on how long a statement is permitted to execute before the server terminates it. In the following hint, MAX_EXECUTION_TIME(1000) means that the timeout is 1000 milliseconds (that is, 1 second):Copyselect /*+ MAX_EXECUTION_TIME(1000) */ * from t1 inner join t2 where t1.id = t2.id;In addition to this hint, the global.max_execution_time system variable can also limit the execution time of a statement.MEMORY_QUOTA(N)The MEMORY_QUOTA(N) hint places a limit N (a threshold value in MB or GB) on how much memory a statement is permitted to use. When a statement's memory usage exceeds this limit, TiDB produces a log message based on the statement's over-limit behavior or just terminates it.In the following hint, MEMORY_QUOTA(1024 MB) means that the memory usage is limited to 1024 MB:Copyselect /*+ MEMORY_QUOTA(1024 MB) */ * from t;In addition to this hint, the tidb_mem_quota_query system variable can also limit the memory usage of a statement.READ_CONSISTENT_REPLICA()The READ_CONSISTENT_REPLICA() hint enables the feature of reading consistent data from the TiKV follower node. For example:Copyselect /*+ READ_CONSISTENT_REPLICA() */ * from t;In addition to this hint, setting the tidb_replica_read environment variable to 'follower' or 'leader' also controls whether to enable this feature.IGNORE_PLAN_CACHE()The IGNORE_PLAN_CACHE() hint reminds the optimizer not to use the Plan Cache when handling the current prepare statement.This hint is used to temporarily disable the Plan Cache for a certain type of queries when prepare-plan-cache is enabled.In the following example, the Plan Cache is forcibly disabled when executing the prepare statement.Copyprepare stmt from 'select"
21,"/*+ IGNORE_PLAN_CACHE() */ * from t where t.id = ?';NTH_PLAN(N)The NTH_PLAN(N) hint reminds the optimizer to select the Nth physical plan found during the physical optimization. N must be a positive integer.If the specified N is beyond the search range of the physical optimization, TiDB will return a warning and select the optimal physical plan based on the strategy that ignores this hint.This hint does not take effect when the cascades planner is enabled.In the following example, the optimizer is forced to select the third physical plan found during the physical optimization:CopySELECT /*+ NTH_PLAN(3) */ count(*) from t where a > 5;Note:NTH_PLAN(N) is mainly used for testing, and its compatibility is not guaranteed in later versions. Use this hint with caution.Download PDFEdit this pageRequest docs changesWhat’s on this pageOptimizer HintsSyntaxHints that take effect in query blocksQB_NAMEMERGE_JOIN(t1_name , tl_name ...)INL_JOIN(t1_name , tl_name ...)INL_HASH_JOINHASH_JOIN(t1_name , tl_name ...)HASH_AGG()STREAM_AGG()USE_INDEX(t1_name, idx1_name , idx2_name ...)IGNORE_INDEX(t1_name, idx1_name , idx2_name ...)AGG_TO_COP()READ_FROM_STORAGE(TIFLASH[t1_name , tl_name ...], TIKV[t2_name , tl_name ...])USE_INDEX_MERGE(t1_name, idx1_name , idx2_name ...)Hints that take effect in the whole queryNO_INDEX_MERGE()USE_TOJA(boolean_value)MAX_EXECUTION_TIME(N)MEMORY_QUOTA(N)READ_CONSISTENT_REPLICA()IGNORE_PLAN_CACHE()NTH_PLAN(N)Was this page helpful?Open Source EcosystemTiDBTiKVTiSparkChaos Mesh®ResourcesQuick StartDocumentationBlogCommunityGitHubUse CasesInternetGamingFinancial servicesCompanyAboutContactCareersCookie PolicyPrivacy Policy LanguageEnglish简体中文©2021 PingCAP. All Rights Reserved. LanguageEnglish简体中文©2021 PingCAP. All Rights Reserved."
22,MySQL InnoDB performance improvement: InnoDB buffer pool instances - Updated! - Sysadmins of the North
22,Skip to content
22,"Sysadmins of the NorthTechnical blog, where topics include: computer, server, web, sysadmin, MySQL, database, virtualization, optimization and security"
22,Home
22,Welcome
22,Privacy Policy
22,Code base
22,"PowerShellPowerShell code snippets, examples and info for Windows Server administrators. Maybe some AppCmd and DISM as well."
22,GNU Linux
22,MySQLMySQL performance tuning and optimization: optimize MySQL server and database
22,Security
22,Windows Server
22,WordPress
22,Donate
22,Search for...
22,"Sysadmins of the NorthTechnical blog, where topics include: computer, server, web, sysadmin, MySQL, database, virtualization, optimization and security"
22,Toggle Navigation
22,Search for...
22,Toggle Navigation
22,Home
22,Welcome
22,Privacy Policy
22,Code base
22,"PowerShellPowerShell code snippets, examples and info for Windows Server administrators. Maybe some AppCmd and DISM as well."
22,GNU Linux
22,MySQLMySQL performance tuning and optimization: optimize MySQL server and database
22,Security
22,Windows Server
22,WordPress
22,Donate
22,You are here:
22,Saotn.org » MySQL » MySQL InnoDB performance improvement: InnoDB buffer pool instances – Updated!MySQL InnoDB performance improvement: InnoDB buffer pool instances – Updated! Jan ReilinkMySQL10 June 20163 June 2020
22,"Are you running into MySQL load problems? Learn how to tune MySQL servers for a heavy InnoDB workload, by configuring innodb_buffer_pool_instances. Dividing the InnoDB buffer pool into multiple instances improves Disk I/O. By doing so, you run your database and website more efficiently and faster. Here is a little help for you."
22,Tune MySQL InnoDB buffer pool instances & size for heavy workloads#
22,All for more InnoDB Disk I/O performance on MySQL 5.5+.
22,"Tuning MySQL servers is an ever ongoing process. Every new MySQL version brings new configuration settings you can use to improve its performance. As a MySQL DBA you want your database server and databases to perform better than well, don’t you?"
22,"MariaDB/MySQL 5.5.4 introduces new configuration settings for the InnoDB storage engine. This can greatly improve MySQL’s InnoDB performance, both in read and write operations."
22,"One of those settings is innodb_buffer_pool_instances. The innodb_buffer_pool_instances divides the InnoDB buffer pool into separate instances. Dividing your buffer pool into separate instances can improve concurrency, by reducing contention as different threads read and write to cached pages. Multiple buffer pool instances are configured using the innodb_buffer_pool_instances configuration option."
22,You might also want to adjust the innodb_buffer_pool_size value:
22,"The larger the InnoDB buffer pool, the more InnoDB acts like an in-memory database. It reads data from disk once and then accesses the data from memory during subsequent reads. Buffer pool size is configured using the innodb_buffer_pool_size configuration option."
22,Back to increasing innodb_buffer_pool_instances:
22,The innodb_buffer_pool_instances divides the InnoDB buffer pool in a number of regions.
22,"The number of regions that the InnoDB buffer pool is divided into. For systems with buffer pools in the multi-gigabyte range, dividing the buffer pool into separate instances can improve concurrency, by reducing contention as different threads read and write to cached pages. Each page that is stored in or read from the buffer pool is assigned to one of the buffer pool instances randomly, using a hashing function. Each buffer pool manages its own free lists, flush lists, LRUs, and all other data structures connected to a buffer pool, and is protected by its own buffer pool mutex.This option takes effect only when you set the innodb_buffer_pool_size to a size of 1 gigabyte or more. The total size you specify is divided among all the buffer pools. For best efficiency, specify a combination of innodb_buffer_pool_instances and innodb_buffer_pool_size so that each buffer pool instance is at least 1 gigabyte."
22,In MySQL versions prior to 5.5.4 this was not configurable and thus set to just one instance. Now
22,"you can increase innodb_buffer_pool_size, and you can divide the InnoDB buffer pool into multiple regions by setting innodb_buffer_pool_instances to 2, 3, 4 or 8. As long as innodb_buffer_pool_size is set high enough, and you have enough memory available in your MySQL database server. This increases"
22,InnoDB read/write threads.
22,"To enable multiple buffer pool instances, set the innodb_buffer_pool_instances configuration option to a value greater than 1 (the default) up to 64 (the maximum)."
22,"For example, you can set innodb_buffer_pool_size to 6 GB and innodb_buffer_pool_instances to 4 in your my.cnf MySQL configuration file:"
22,"; InnoDB buffer pool size in bytes. The primary value to adjust on a database server,"
22,; can be set up to 80% of the total memory in these environments
22,innodb_buffer_pool_size = 6000M
22,"If innodb_buffer_pool_size is set to more than 1GB, innodb_buffer_pool_instances"
22,; divides the InnoDB buffer pool into this many instances.
22,"innodb_buffer_pool_instances = 4Code language: TOML, also INI (ini)"
22,"In this example, I’ve used an innodb_buffer_pool_size of 6000M (6 GB), so there is 1500M available per innodb_buffer_pool_instance, which is more than the minimum 1 GB. As a rule of thumb, set your innodb_buffer_pool_size to approximately 70 – 80% of the RAM available."
22,Innodb_buffer_pool_instances defaults#
22,"Various MySQL versions have different innodb_buffer_pool_instances default values, here is an overview – listing – for you:"
22,MySQL version# InnoDB buffer pool instancesNotesMySQL 5.5 (<= 5.5.4)1not configurableMySQL 5.51–MySQL 5.6 (<= 5.6.5)1–MySQL 5.6 (>= 5.6.6)8or 1 if innodb_buffer_pool_size < 1GBMySQL 5.78or 1 if innodb_buffer_pool_size < 1GBMariaDB 10 (<= MariaDB 10.0.3)1–MariaDB 10 (>= MariaDB 10.0.4)8–InnoDB read and write I/O threads in MySQL#
22,"Besides innodb_buffer_pool_instances, you can also increase the number of InnoDB read I/O threads and write I/O threads. These are configured with innodb_write_io_threads and innodb_read_io_threads."
22,"Both settings default to 4 threads. We can increase these to, for example, 8:"
22,; Number of I/O threads for writes
22,innodb_write_io_threads = 8
22,; Number of I/O threads for reads
22,"innodb_read_io_threads = 8Code language: TOML, also INI (ini)"
22,innodb_read_io_threads
22,The number of I/O threads for read operations in InnoDB. The default value is 4.
22,innodb_write_io_threads
22,The number of I/O threads for write operations in InnoDB. The default value is 4.
22,"When should you increase the number of innodb_read_io_threads? When you see more than 64 × innodb_read_io_threads pending read requests in SHOW ENGINE INNODB STATUS, you might gain by increasing the value of innodb_read_io_threads."
22,Optimizing InnoDB Disk I/O#
22,"If you follow the best practices for database design and the tuning techniques for SQL operations, but your database is still slowed by heavy disk I/O activity, explore these low-level techniques related to disk I/O. If the Unix top tool or the Windows Task Manager shows that the CPU usage percentage with your workload is less than 70%, your workload is probably disk-bound, Optimizing InnoDB Disk I/O."
22,"Starting from MariaDB 10.0, the default number of innodb_buffer_pool_instances is 8. This means you have to configure your innodb_buffer_pool_size to at least 8 GB, see the defaults above."
22,"Protip, don’t over optimize: never make too many configuration changes at once. After changing one or two settings, let the server run for a few days so you can learn the implications of the changes. Then, if necessary, make additional changes to the configuration."
22,"Convert MyISAM to InnoDB tables for WordPress using a pluginFor WordPress, I created a plugin to convert MyISAM tables to InnoDB, that now is incorporated into the Vevida Optimizer WordPress plugin. The Vevida Optimizer plugin extends the automatic update feature already present in WordPress. WordPress core updates can be switched on or off, themes and translations can be automatically updated, and the plugin updates can be configured on a per-plugin basis."
22,Extra tips for MySQL performance tuning#
22,"Besides optimizing InnoDB for a high-performance workload, there is more you can do to tune MySQL server and database performance. Here are some extra MySQL configuration tips for you."
22,Some information might be outdated and obsolete but may hold valuable information for tuning your MySQL server.
22,"Note: this is a translation and rewrite of my older Dutch post “MySQL performance en optimalisatie tips“, which is now deleted and links to here. Just in case you were wondering why you arrived here instead of the Dutch post after clicking a link :-)"
22,1: No two MySQL servers are the same#
22,"When optimizing MySQL database servers, keep in mind that no server is equal to another. Settings that work well on one server, may degrade performance on a second. If you manage multiple servers with its configuration under version control (e.g almost -or exactly- the same MySQL configuration for all servers), choose what works best on all servers."
22,"To determine what you can improve, you first need to know how the server performs now. You can use some MySQL commands for this on your MySQL cli (data comes from my very old post)."
22,mysql> SHOW STATUS LIKE '%key_read%';
22,+-------------------+-------------+
22,| Variable_name
22,| Value
22,+-------------------+-------------+
22,| Key_read_requests | 11810240259 |
22,| Key_reads
22,| 9260357
22,+-------------------+-------------+Code language: SQL (Structured Query Language) (sql)
22,These two variables and values relate to the configured key_buffer_size
22,"In this old example, the database server has 4 GB of RAM and a configured key_buffer_size of 512 MB. The ratio (Key_read_requests / Key_reads) is approximately 1/1275, which is good but the key_buffer_size value may be increased to 768 MB. Even though this is not yet necessary."
22,mysql> SHOW STATUS LIKE 'thread%';
22,+-------------------+---------+
22,| Variable_name
22,| Value
22,+-------------------+---------+
22,| Threads_cached
22,| 0
22,| Threads_connected | 76
22,| Threads_created
22,| 6234040 |
22,| Threads_running
22,| 2
22,+-------------------+---------+Code language: SQL (Structured Query Language) (sql)
22,"These Threads_* variable values show you there are currently 76 connected threads, of which only 2 are really running a thread (executing a statement). This means 74 connections are idle."
22,Here you can also see that there is no “thread cache” set up for MySQL: Threads_cached | 0
22,"You can use the MySQL Server System variable thread_cache_size to configure how many threads must be cached by MySQL. This is one of those configuration settings that, probably, provides the least performance gain, but still…"
22,"Don’t set this one too high, somewhere between 20 and 40 is often good enough:"
22,"thread_cache_size = 20Code language: TOML, also INI (ini)"
22,"When you execute the previous statement again, the values will be:"
22,mysql> SHOW STATUS LIKE 'thread%';
22,+-------------------+-------+
22,| Variable_name
22,| Value |
22,+-------------------+-------+
22,| Threads_cached
22,| 14
22,| Threads_connected | 98
22,| Threads_created
22,| 2896
22,| Threads_running
22,| 1
22,+-------------------+-------+Code language: SQL (Structured Query Language) (sql)
22,You now have 14 threads cached :)
22,2: Miscellaneous MySQL configuration settings#
22,A few words on some miscellaneous configuration settings.
22,"2.1: tmp_table_size and max_heap_table_sizeThe default tmp_table_size and max_heap_table_size values are 16M. These two have to be equal in size! It sets the maximum size for internal in-memory tables, resulting in less creation of temporarily MyISAM tables on the file system. That in return, results in less disk I/O."
22,"https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_tmp_table_sizehttps://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_max_heap_table_size2.2: join_buffer_sizeThe join_buffer_size sets a maximum buffer size for plain index scans, range index scans and joins without indices (and therefore perform full table scans). Keep this one low, 1M for example."
22,3. Use Diagnostics for improvements#
22,It is important to frequently run diagnostics and/or look up diagnostic data (for example in your information_scheme table). Percona has a lot of information about some key metrics:
22,"https://www.percona.com/doc/percona-server/8.0/index.html#diagnostics-improvementsMySQL tuning, the conclusion#"
22,"Tuning MySQL and the InnoDB storage engine is an important step in further optimizing your hosting environment. Every new MySQL version brings new settings to improve your MySQL configuration, so be sure to read those changelogs."
22,In this article we went over InnoDB Buffer Pool Size and InnoDB Buffer Pool Instances. Setting these properly greatly improves your MySQL server’s performance!
22,"But never (ever, ever) over-optimize! Please don’t make too many configuration changes at once. Make one or two and restart mysqld. After monitoring your system for a few days, running with the new configuration, you have data available to further optimize other MySQL settings."
22,"With InnoDB being the default storage engine, you also have to make sure you make use of this storage engine in MySQL. Therefore it is important to convert old MyISAM tables to InnoDB."
22,Tags:InnoDBlinuxMySQLoptimizationquery_cache
22,4 thoughts on “MySQL InnoDB performance improvement: InnoDB buffer pool instances – Updated!”
22,Dmitry Kapustin
22,2 June 2020 at 23:30
22,"The use of Query Cache was justified when using MyISAM tables, which were completely locked during update/insert/delete data. When using InnoDB tables on the highload progect, the Query Cache on the contrary, only interferes and dramatically degrades server performance. Therefore, Query Cache is disabled by default in new versions of the MySQL Server."
22,Reply
22,Jan Reilink
22,3 June 2020 at 08:45
22,"Hi Dmitry, thank you for your comment."
22,"I thought I removed all references to query_cache. Nowadays Query Cache is removed from MySQL, because it’s only for the MyISAM table engine and not for InnoDB. Unfortunately I forgot one, and that’s gone now."
22,Reply
22,Unca.Alby
22,14 October 2019 at 21:32
22,"I understand you used “courier” font for the configuration parameters so that they will display differently and be obvious that it’s a “computer term”, but you also need to be careful about the size of that font. As it is now, the “computer terms” are HALF the size of the normal text! It makes reading your post quite a bit more difficult. It’s like walking down a road with holes to trip in every few feet."
22,Reply
22,Jan Reilink
22,15 October 2019 at 14:26
22,"Thanks for your constructive criticism, I’ve increased the pre and code font-size to 16px (which was 14). Is this better?"
22,Reply
22,Leave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment Name *
22,Email *
22,Website
22,"Save my name, email, and website in this browser for the next time I comment."
22,previousKMS Migration from 2008 R2 to Windows Server 2012 R2 and KMS Activation Known IssuesnextSSL in WordPress: how to move WordPress to HTTPS? The definitive guide
22,NavigationHome
22,Welcome
22,Privacy Policy
22,Code base
22,PowerShell
22,GNU Linux
22,MySQL
22,Security
22,Windows Server
22,WordPress
22,Donate
22,Recent Posts
22,Getting more out your Windows Performance Counters monitoring for web applications – part 3
22,"ASP.NET web application monitoring in Zabbix, part 2"
22,"Monitor IIS application pools in Zabbix, part 1"
22,Disable Joomla Contacts component (com_contact) in MySQL / phpMyAdmin
22,Disable WordPress XCloner Plugin logger in MySQL / phpMyAdmin
22,Force HSTS in Apache .htaccess
22,".NET Core 2.1, 3.1, and .NET 5.0 updates are coming to Microsoft Update"
22,Proudly hosted by
22,Tags.htaccess (18)
22,AppCmd (10)
22,ASP.NET (15)
22,Bash (19)
22,IIS (61)
22,linux (21)
22,MySQL (34)
22,OPcache (8)
22,optimization (18)
22,performance (14)
22,PHP (55)
22,plugin (14)
22,PowerShell (49)
22,security (44)
22,SQL Server (10)
22,SSL (15)
22,URL Rewrite (17)
22,web.config (21)
22,web application security (17)
22,website (17)
22,WinCache (9)
22,Windows (17)
22,Windows 10 (9)
22,Windows Server 2016 (8)
22,WordPress (30)
22,Tip: also visit
22,Sysadmins of the North
22,"ITFAQ-nl, internet en computers uitgelegd in eenvoudig Nederlands!"
22,WordPress hosting
22,ASP.NET & ASP.NET Core hosting – @Vevida
22,Reilink.nl
22,Search for:
22,Thanks! Thank you for your visit!
22,"Don’t forget to share this site with your family, friends and co-workers :-)"
22,Donations are more than welcome and will be used for research new posts and hosting.
22,"If you like this site or encourage its development, please use the form above. Or you can transfer a direct donation via Paypal or bank wire-transfer IBAN: NL31 ABNA 0432217258 (Jan Reilink). Thanks! :-)"
22,Search
22,Search for:
22,Archives Archives
22,Select Month
22,April 2021  (1)
22,February 2021  (2)
22,January 2021  (2)
22,December 2020  (2)
22,November 2020  (1)
22,October 2020  (2)
22,September 2020  (1)
22,August 2020  (1)
22,July 2020  (1)
22,June 2020  (1)
22,May 2020  (3)
22,March 2020  (1)
22,February 2020  (1)
22,January 2020  (3)
22,December 2019  (2)
22,November 2019  (1)
22,October 2019  (1)
22,September 2019  (1)
22,August 2019  (1)
22,June 2019  (1)
22,April 2019  (1)
22,March 2019  (1)
22,December 2018  (1)
22,November 2018  (3)
22,October 2018  (4)
22,August 2018  (2)
22,March 2018  (2)
22,February 2018  (3)
22,January 2018  (1)
22,September 2017  (1)
22,August 2017  (4)
22,July 2017  (1)
22,June 2017  (1)
22,May 2017  (7)
22,March 2017  (1)
22,December 2016  (1)
22,October 2016  (5)
22,September 2016  (2)
22,August 2016  (4)
22,July 2016  (2)
22,June 2016  (2)
22,May 2016  (6)
22,April 2016  (4)
22,March 2016  (1)
22,February 2016  (4)
22,January 2016  (1)
22,December 2015  (4)
22,November 2015  (2)
22,October 2015  (2)
22,September 2015  (1)
22,August 2015  (1)
22,July 2015  (3)
22,June 2015  (2)
22,April 2015  (5)
22,March 2015  (4)
22,February 2015  (2)
22,January 2015  (4)
22,December 2014  (4)
22,November 2014  (3)
22,October 2014  (4)
22,September 2014  (4)
22,August 2014  (3)
22,July 2014  (5)
22,June 2014  (9)
22,May 2014  (9)
22,April 2014  (12)
22,March 2014  (2)
22,February 2014  (1)
22,January 2014  (3)
22,November 2013  (3)
22,October 2013  (1)
22,September 2013  (2)
22,July 2013  (4)
22,June 2013  (3)
22,May 2013  (1)
22,April 2013  (1)
22,March 2013  (2)
22,February 2013  (6)
22,January 2013  (1)
22,September 2012  (1)
22,August 2012  (1)
22,July 2012  (2)
22,May 2012  (1)
22,March 2012  (1)
22,January 2012  (1)
22,December 2011  (1)
22,November 2011  (3)
22,October 2011  (1)
22,September 2011  (1)
22,August 2011  (2)
22,Neve | Powered by WordPress
22,Home
22,Code base
22,GNU Linux
22,Security
22,Windows Server
22,WordPress
22,Donate
23,Teaching new Presto performance tricks to the Old-School DBA - Engineering Blog
23,Skip to content
23,Engineering Blog
23,Menu
23,Home
23,Front-End
23,Performance
23,Mobile
23,Conferences
23,Jobs
23,Teaching new Presto performance tricks to the Old-School DBA
23,"Posted on June 29, 2020 by Ed Presz"
23,Stories You will love
23,MySQL High Availability at Eventbrite
23,Leveraging AWS “spot” instances to drive down costs
23,"Open Data: The what, why and how to get started"
23,Boosting Big Data workloads with Presto Auto Scaling
23,"I’ve spent much of my career working with relational databases such as Oracle and MySQL, and SQL performance has always been an area of focus for me. I’ve spent countless hours reviewing EXPLAIN plans, rewriting subqueries, adding new indexes, and chasing down table-scans. I’ve been trained to make performance improvements such as:  only choose columns in a SELECT that are absolutely necessary, stay away from LIKE clauses, review the cardinality of columns before adding indexes, and always JOIN on indexed columns."
23,It’s been an instinctual part of my life as a Database Administrator who supports OLTP databases that have sold in excess of 20K tickets per minute to your favorite events. I remember a specific situation where a missing index caused our production databases to get flooded with table-scans that brought a world-wide on-sale to an immediate halt. I had a lot of explaining to do that day as the missing index made it to QA and Stage but not Production!
23,"In recent years, I’ve transitioned to Data Engineering and began supporting Big Data environments.  Specifically, I’m supporting Eventbrite’s Data Warehouse which leverages Presto and Apache Hive using the Presto/Hive connector. The data files can be of different formats, but we’re using HDFS and S3.  The Hive metadata describes how data stored in HDFS/S3 maps to schemas, tables, and columns to be queried via SQL. We persist this metadata information in Amazon Aurora and access it through the Presto/Hive connector via the Hive Metastore Service (HMS)."
23,"The stakes have changed and so have the skill-sets required. I’ve needed to retrain myself in how to write optimal SQL for Presto. Some of the best practices for Presto are the same as relational databases and others are brand new to me. This blog post summarizes some of the similarities and some of the differences with writing efficient SQL on MySQL vs Presto/Hive. Along the way I’ve had to learn new terms such as “federated queries”, “broadcast joins”, “reshuffling”, “join reordering”, and “predicate pushdown”."
23,Let’s start with the basics:
23,"What is MySQL? The world’s most popular open source database. The MySQL software delivers a fast, multi-threaded, multi-user, and robust SQL (Structured Query Language) database server. MySQL is intended for mission-critical, heavy-load production database usage."
23,"What is Presto? Presto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes. Presto doesn’t use the map reduce framework for its execution. Instead, Presto directly accesses the data through a specialized distributed query engine that is very similar to those found in commercial parallel relational databases."
23,"Presto uses ANSI SQL syntax/semantics to build its queries. The advantage of this is that analysts with experience with relational databases will find it very easy and straightforward to write Presto queries! That said, the best practices for developing efficient SQL via Presto/Hive are different from those used to query standard RDBMS databases."
23,Let’s transition to Presto performance tuning tips and how they compare to standard best practices with MySQL.
23,1. Only specify the columns you need
23,Restricting columns for SELECTs can improve your query performance significantly. Specify only needed columns instead of using a wildcard (*). This applies to Presto as well as MySQL!
23,2. Consider the cardinality within GROUP BY
23,"When using GROUP BY, order the columns by the highest cardinality (that is, most number of unique values) to the lowest."
23,"The GROUP BY operator distributes rows based on the order of the columns to the worker nodes, which hold the GROUP BY values in memory. As rows are being ingested, the GROUP BY columns are looked up in memory and the values are compared. If the GROUP BY columns match, the values are then aggregated together."
23,3. Use LIMIT with ORDER BY
23,"The ORDER BY clause returns the results of a query in sort order. To  process the sort, Presto must send all rows of data to a single worker and then sort them. This sort can be a very memory-intensive operation for large datasets that will put strain on the Presto workers. The end result will be long execution times and/or memory errors."
23,"If you are using the ORDER BY clause to look at the top N values, then use a LIMIT clause to reduce the cost of the sort significantly by pushing the sorting/limiting to individual workers, rather than the sorting being done by a single worker."
23,I highly recommend you use the LIMIT clause not just for SQL with ORDER BY but in any situation when you’re validating new SQL. This is good practice for MySQL as well as Presto!
23,4. Using approximate aggregate functions
23,When exploring large datasets often an approximation (with standard deviation of 2.3%) is more than good enough! Presto has approximate aggregation functions that give you significant performance improvements. Using the approx_distinct(x) function on large data sets vs COUNT(DISTINCT x) will result in performance gains.
23,"When an exact number may not be required―for instance, if you are looking for a rough estimate of the number of New Years events in the Greater New York area then consider using approx_distinct(). This function minimizes the memory usage by counting unique hashes of values instead of entire strings. The drawback is that there is a small standard deviation."
23,5. Aggregating a series of LIKE clauses in one single regexp_like clause
23,The LIKE operation is well known to be slow especially when not anchored to the left (i.e. the search text is surrounded by ‘%’ on both sides) or when used with a series of OR conditions. So it is no surprise that Presto’s query optimizer is unable to improve queries that contain many LIKE clauses.
23,"We’ve found improved  LIKE performance on Presto by  substituting the LIKE/OR  combination with a single REGEXP_LIKE clause, which is Presto native.  Not only is it easier to read but it’s also more performant. Based on some quick performance tests, we see ~30% increase in run-times with REGEXP_LIKE vs comparable LIKE/OR combination."
23,For example:
23,SELECT  ...FROM zoo
23,WHERE method LIKE '%monkey%' OR
23,method LIKE '%hippo%' OR
23,method LIKE '%tiger%' OR
23,method LIKE '%elephant%'
23,can be optimized by replacing the four LIKE clauses with a single REGEXP_LIKE clause:
23,SELECT  ...FROM zoo
23,"WHERE REGEXP_LIKE(method, 'monkey|hippo|tiger|elephant')"
23,6. Specifying large tables first in join clause
23,"When joining tables, specify the largest table first in the join. The default join algorithm of Presto is broadcast join, which partitions the left-hand side table of a join and sends (broadcasts) a copy of the entire right-hand side table to all of the worker nodes that have the partitions. If the right-hand side table is “small” then it can be replicated to all the join workers which will save CPU and network costs.  This type of join will be most efficient when the right-hand side table is small enough to fit within one node."
23,"If you receive an ‘Exceeded max memory’ error, then the right-hand side table is too large. Presto does not perform automatic join-reordering, so make sure your largest table is the first table in your sequence of joins."
23,"This was an interesting performance tip for me. As we know, SQL is a declarative language and the ordering of tables used in joins in MySQL, for example,  is *NOT* particularly important. The MySQL optimizer will re-order to choose the most efficient path. With Presto, the join order matters. You’ve been WARNED! Presto does not perform automatic join-reordering unless using the Cost Based Optimizer!"
23,7. Turning on the distributed hash join
23,"If you’re battling with memory errors then try a distributed hash join. This algorithm partitions both the left and right tables using the hash values of the join keys. So the distributed join works even if the right-hand side table is large, but the performance might be slower because the join increases the number of network data transfers."
23,At Eventbrite we have the distributed_join variable set to ‘true’. (SHOW SESSION). Also it can be enabled by setting a session property (set session distributed_join = ‘true’).
23,8. Partition your data
23,"Partitioning divides your table into parts and keeps the related data together based on column values such as date or country.  You define partitions at table creation, and they help reduce the amount of data scanned per query, thereby improving performance."
23,Here are some hints on partitioning:
23,Columns that are used as WHERE filters are good candidates for partitioning.
23,"Partitioning has a cost. As the number of partitions in your table increases, the higher the overhead of retrieving and processing the partition metadata, and the smaller your files. Use caution when partitioning and make sure you don’t partition too finely."
23,"If your data is heavily skewed to one partition value, and most queries use that value, then the overhead may wipe out the initial benefit."
23,A key partition column at Eventbrite is transaction date (txn_date).
23,CREATE TABLE IF NOT EXISTS fact_ticket_purchase
23,"ticket_id STRING,"
23,....
23,"create_date STRING,"
23,update_date STRING
23,PARTITIONED BY (trx_date STRING)
23,STORED AS PARQUET
23,TBLPROPERTIES ('parquet.compression'='SNAPPY')
23,9. Optimize columnar data store generation
23,"Apache Parquet and Apache ORC are popular columnar data stores. They provide features that store data efficiently by using column-wise compression based on data type, special encoding, and predicate pushdown. At Eventbrite, we define Hive tables as PARQUET using compression equal to SNAPPY…."
23,CREATE TABLE IF NOT EXISTS dim_event
23,"dim_event_id STRING,"
23,....
23,"create_date STRING,"
23,"update_date STRING,"
23,STORED AS PARQUET
23,TBLPROPERTIES ('parquet.compression'='SNAPPY')
23,"Apache Parquet is an open-source, column-oriented data storage format. Snappy is designed for speed and will not overload your CPU cores. The downside of course is that it does not compress as well as gzip or bzip2."
23,10. Presto’s Cost-Based Optimizer/Join Reordering
23,"We’re not currently using Presto’s Cost-Based Optimizer (CBO)! Eventbrite data engineering released Presto 330 in March 2020, but we haven’t tested CBO yet."
23,"CBO inherently requires the table stats be up-to-date which we only calculate for a small subset of tables! Using the CBO, Presto will be able to intelligently decide the best sequence based on the statistics stored in the Hive Metastore."
23,"As mentioned above, the order in which joins are executed in a query can have a big impact on performance. If we collect table statistics then the CBO can automatically pick the join order with the lowest computed costs. This is governed by the join_reordering_strategy (=AUTOMATIC) session property and I’m really excited to see this feature in action."
23,"Another interesting join optimization is dynamic filtering. It relies on the stats estimates of the CBO to correctly convert the join distribution type to “broadcast” join. By using dynamic filtering via run-time predicate pushdown, we can squeeze out more performance gains for highly-selective inner-joins.  We look forward to using this feature in the near future!"
23,11. Using WITH Clause
23,"The WITH clause is used to define an inline view within a single query.  It allows for flattening nested subqueries. I find it hugely helpful for simplifying SQL, and making it more readable and easier to support."
23,12. Use Presto Web Interface
23,Presto provides a web interface for monitoring queries (https://prestodb.io/docs/current/admin/web-interface.html).
23,"The main page has a list of queries along with information like unique query ID, query text, query state, percentage completed, username and source from which this query originated. If Presto cluster is having any performance-related issues, this web interface is a good place to go to identify and capture slow running SQL!"
23,13. Explain plan with Presto/Hive (Sample)
23,EXPLAIN is an invaluable tool for showing the logical or distributed execution plan of a statement and to validate the SQL statements.
23,— Logical Plan with Presto
23,"explain select SUBSTRING(last_modified,1,4) ,count(*)  from hive.df_machine_learning.event_text where lower(name) like ‘%wilbraham%’ or (REGEXP_LIKE(lower(name), ‘.*wilbraham.*’)) group by 1 order by 1;"
23,14. Explain plan with MySQL (Sample)
23,In this particular case you can see that the primary key is used on the ‘ejp_events’ table and the non-primary key on the “ejp_orders’ table. This query is going to be fast!
23,Conclusion
23,"Presto is the “SQL-on-Anything” solution that powers Eventbrite’s data warehouse. It’s been very rewarding for me as the “Old School DBA” to learn new SQL tricks related to a distributed query engine such as Presto. In most cases, my SQL training on MySQL/Oracle has served me well but there are some interesting differences which I’ve attempted to call-out above. Thanks for reading and making it to the end. I appreciate it!"
23,We look forward to giving Presto’s Cost-Based Optimizer a test drive and kicking the tires on new features such as dynamic filtering & partition pruning!
23,"All comments are welcome, or you can message me at ed@eventbrite.com. You can learn more about Eventbrite’s use of Presto by checking out my previous post at Boosting Big Data workloads with Presto Auto Scaling."
23,"Special thanks to Eventbrite’s Data Foundry team (Jeremy Bakker,  Alex Meyer, Jasper Groot, Rainu Ittycheriah, Gray Pickney, and Beck Cronin-Dixon) for the world-class Presto support, and Steven Fast for reviewing this blog post. Eventbrite’s Data Teams rock!"
23,"CategoriesAnalytics, Architecture, Data Stores"
23,Leave a Reply Cancel reply
23,Your email address will not be published. Required fields are marked *Comment Name *
23,Email *
23,Website
23,Notify me of follow-up comments by email. Notify me of new posts by email.
23,Post navigation
23,Previous PostPrevious
23,Create Meaningful (and Fun!) Remote CommunityNext PostNext How are you building/maintaining team cohesion?
23,"Our Writing TeamAndrew Smelser8 Simple Tips for better Communication with Customer-Facing TeamsRethinking quality and the engineers who protect itBartek OgryczakPackaging and Releasing Private Python Code (Pt.2)Packaging and Releasing Private Python Code (Pt.1)Beck Cronin-DixonEventbrite and SEO: How does Google find our pages?Eventbrite and SEO: The BasicsBen IlegboduWhy Would Webpack Stop Re-compiling? (The Quest for Micro-Apps)The Quest for React Micro-Apps: Single App ModeThe Quest for React Micro-Apps: The BeginningLearning ES6: Generators as IteratorsLearning ES6: Iterators & iterablesBryan MayesSoftware Developers to Nashville, “Stop calling us IT”Daniel CarterCreating Flexible and Reusable React File UploadersDelaine WendlingBriteBytes: Maddie CousensBriteBytes: Nam-Chi VanEd PreszMySQL High Availability at EventbriteBuilding a Protest Map: A Behind the Scenes Look!Teaching new Presto performance tricks to the Old-School DBALeveraging AWS “spot” instances to drive down costsBoosting Big Data workloads with Presto Auto ScalingElizabeth Viera & Loretta StokesWhat the Top Minds in Tech Communicated at Hopperx1 SeattleeventbriteIsomorphic React Sans NodeReact + ES.next = ❤Engineering + Accounting for Marketplace BusinessesEscapándome de las Software FactoryEventbrite’s Search-Based Approach to RecommendationsEyal ReuveniReplayable Pub/Sub Queues with Cassandra and ZooKeeperSmarter Unit Testing with nose-knowsWatching Metadata Changes in a Distributed Application Using ZooKeeperGagoThe Realistic Code Reviewer, Part IIThe Realistic Code Reviewer, Part ICode Review: The art of writing code for othersDiego Girotti8 Reasons Why Manual Testing is Still ImportantHanahCreate Meaningful (and Fun!) Remote CommunityHow to be a Successful Junior EngineerJay ChanMulti-Index Locality Sensitive Hashing for Fun and ProfitJessica KatzThe Truth about Boundaries, Curiosity, and Requests (Part 2 of 2)The Truth about Boundaries, Curiosity, and Requests (Part 1 of 2)The Lies We Tell OurselvesJiangyue ZhuA Story of a React Re-Rendering BugJohn BerrymanThe Fundamental Problem of SearchCowboys and Consultants Don’t Need Unit TestsSearch Precision and Recall By ExampleBuilding a Marketplace — Search and Recommendation at EventbriteLoretta StokesGrace Hopper 2018: Five Unforgettable ExperiencesMalina WiesenMother May I?The Lifecycle of an Eventbrite WebhookMarcos IglesiasDiscover “Pro D3.js”, a new book to improve your JavaScript data visualizationsSimple and Easy Mentorship with a Mentoring AgreementBritecharts v2.0 ReleasedIntroducing Britecharts: Eventbrite’s Reusable Charting Library Based on D3Leveling Up D3: Events and RefactoringsMartin BrambatiHow are you building/maintaining team cohesion?Matthew HimelsteinThe 63-point Plan for Helping Your Remote Team SucceedHow to Make Your Next Event App Remarkable with these 4 Mobile Navigation GesturesMaria EguiluzDesign System Wednesday: A Supportive Professional CommunityHow to Make Swift Product Changes Using a Design SystemMelisa PiccinettiBe the changeMiguel HernandezEventbrite Engineering at PyConESGetting started with Unit TestsNatalia CorteseOpen Data: The what, why and how to get startedPat PoelsThe “Aha” Moments of Becoming an Engineering ManagerRandall KannaHow Your Company Can Support Junior EngineersRashad Russell6 Unsuspecting Problems in HTML, CSS, and JavaScript – Part 26 Unsuspecting Problems in HTML, CSS, and JavaScript – Part 16 Unsuspecting Problems in HTML, CSS, and JavaScript – Part 3Santiago Hollmann5 Good Practices I Follow When I Code Using GitSahar BalaHow to fix the ugly focus ring and not break accessibility in ReactWhat is the best way to hire QA Engineers?How To Move From Customer Support to Engineering in 5 StepsStephanie PiGetting the most out of React AlicanteSteve FrenchHeavy Hitters in RedisTamara ChuBriteBytes: Diego “Kartones” MuñozStyleguide-Driven Development at Eventbrite: IntroductionTom InsamSetting the title of AirDrop shares under iOS 7Toph BurnsHow to Craft a Successful Engineering InterviewVictoria ZhangThe Elevator Pitch from a Data StrategistVincent BudrovichVarnish and A-B Testing: How to Play Nice"
23,Proudly powered by WordPress
24,Optimization guide for Power BI - Power BI | Microsoft Docs
24,Skip to main content
24,Contents
24,Exit focus mode
24,Bookmark
24,Feedback
24,Edit
24,Share
24,Twitter
24,LinkedIn
24,Facebook
24,Email
24,Table of contents
24,Optimization guide for Power BI
24,04/02/2021
24,6 minutes to read
24,In this article
24,This article provides guidance that enables developers and administrators to produce and maintain optimized Power BI solutions. You can optimize your solution at different architectural layers. Layers include:
24,The data source(s)
24,The data model
24,"Visualizations, including dashboards, Power BI reports, and Power BI paginated reports"
24,"The environment, including capacities, data gateways, and the network"
24,Optimizing the data model
24,"The data model supports the entire visualization experience. Data models are either external-hosted or internal-hosted, and in Power BI they are referred to as datasets. It's important to understand your options, and to choose the appropriate dataset type for your solution. There are three dataset modes: Import, DirectQuery, and Composite. For more information, see Datasets in the Power BI service, and"
24,Dataset modes in the Power BI service.
24,"For specific dataset mode guidance, see:"
24,Data reduction techniques for Import modeling
24,DirectQuery model guidance in Power BI Desktop
24,Composite model guidance in Power BI Desktop
24,Optimizing visualizations
24,"Power BI visualizations can be dashboards, Power BI reports, or Power BI paginated reports. Each has different architectures, and so each has their own guidance."
24,Dashboards
24,"It's important to understand that Power BI maintains a cache for your dashboard tiles—except live report tiles, and streaming tiles. For more information, see Data refresh in Power BI (Tile refresh). If your dataset enforces dynamic row-level security (RLS), be sure to understand performance implications as tiles will cache on a per-user basis."
24,"When you pin live report tiles to a dashboard, they're not served from the query cache. Instead, they behave like reports, and make queries to back-end cores on the fly."
24,"As the name suggests, retrieving the data from the cache provides better and more consistent performance than relying on the data source. One way to take advantage of this functionality is to have dashboards be the first landing page for your users. Pin often-used and highly requested visuals to the dashboards. In this way, dashboards become a valuable ""first line of defense"", which delivers consistent performance with less load on the capacity. Users can still click through to a report to analyze details."
24,"For DirectQuery and live connection datasets, the cache is updated on a periodic basis by querying the data source. By default, it happens every hour, though you can configure a different frequency in the dataset settings. Each cache update will send queries to the underlying data source to update the cache. The number of queries that generate depends on the number of visuals pinned to dashboards that rely on the data source. Notice that if row-level security is enabled, queries are generated for each different security context. For example, consider there are two different roles that categorize your users, and they have two different views of the data. During query cache refresh, Power BI generates two sets of queries."
24,Power BI reports
24,There are several recommendations for optimizing Power BI report designs.
24,Note
24,"When reports are based on a DirectQuery dataset, for additional report design optimizations, see DirectQuery model guidance in Power BI Desktop (Optimize report designs)."
24,Apply the most restrictive filters
24,"The more data that a visual needs to display, the slower that visual is to load. While this principle seems obvious, it's easy to forget. For example: suppose you have a large dataset. Atop of that dataset, you build a report with a table. End users use slicers on the page to get to the rows they want—typically, they're only interested in a few dozen rows."
24,"A common mistake is to leave the default view of the table unfiltered—that is, all 100M+ rows. The data for these rows loads into memory and is uncompressed at every refresh. This processing creates huge demands for memory. The solution: use the ""Top N"" filter to reduce the max number of items that the table displays. You can set the max item to larger than what users would need, for example, 10,000. The result is the end-user experience doesn't change, but memory use drops greatly. And most importantly, performance improves."
24,"A similar design approach to the above is suggested for every visual in your report. Ask yourself, is all the data in this visual needed? Are there ways to filter the amount of data shown in the visual with minimal impact to the end-user experience? Remember, tables in particular can be expensive."
24,Limit visuals on report pages
24,The above principle applies equally to the number of visuals added to a report page. It's highly recommended you limit the number of visuals on a particular report page to only what is necessary. Drillthrough pages and report page tooltips are great ways to provide additional details without jamming more visuals onto the page.
24,Evaluate custom visual performance
24,Be sure to put each custom visual through its paces to ensure high performance. Poorly optimized Power BI visuals can negatively affect the performance of the entire report.
24,Power BI paginated reports
24,"Power BI paginated report designs can be optimized by applying best practice design to the report's data retrieval. For more information, see Data retrieval guidance for paginated reports."
24,"Also, ensure your capacity has sufficient memory allocated to the paginated reports workload."
24,Optimizing the environment
24,"You can optimize the Power BI environment by configuring capacity settings, sizing data gateways, and reducing network latency."
24,Capacity settings
24,"When using capacities—available with Power BI Premium (P SKUs), Premum Per User (PPU) licenses, or"
24,"Power BI Embedded (A SKUs, A4-A6)—you can manage capacity settings. For more information, see Managing Premium capacities. For guidance on how to optimize your capacity, see Optimizing Premium capacities."
24,Gateway sizing
24,"A gateway is required whenever Power BI must access data that isn't accessible directly over the Internet. You can install the on-premises data gateway on a server on-premises, or VM-hosted Infrastructure-as-a-Service (IaaS)."
24,"To understand gateway workloads and sizing recommendations, see On-premises data gateway sizing."
24,Network latency
24,"Network latency can impact report performance by increasing the time required for requests to reach the Power BI service, and for responses to be delivered. Tenants in Power BI are assigned to a specific region."
24,Tip
24,"To determine where your tenant is located, see Where is my Power BI tenant located?"
24,"When users from a tenant access the Power BI service, their requests always route to this region. As requests reach the Power BI service, the service may then send additional requests—for example, to the underlying data source, or a data gateway—which are also subject to network latency."
24,"Tools such as Azure Speed Test provide an indication of network latency between the client and the Azure region. In general, to minimize the impact of network latency, strive to keep data sources, gateways, and your Power BI cluster as close as possible. Preferably, they reside within the same region. If network latency is an issue, try locating gateways and data sources closer to your Power BI cluster by placing them inside cloud-hosted virtual machines."
24,Monitoring performance
24,"You can monitor performance to identify bottlenecks. Slow queries—or report visuals—should be a focal point of continued optimization. Monitoring can be done at design time in Power BI Desktop, or on production workloads in Power BI Premium capacities. For more information, see Monitoring report performance in Power BI."
24,Next steps
24,"For more information about this article, check out the following resources:"
24,Power BI guidance
24,Monitoring report performance
24,Whitepaper: Planning a Power BI Enterprise Deployment
24,Questions? Try asking the Power BI Community
24,Suggestions? Contribute ideas to improve Power BI
24,Is this page helpful?
24,Yes
24,Any additional feedback?
24,Skip
24,Submit
24,Thank you.
24,Feedback
24,Submit and view feedback for
24,This product
24,This page
24,View all page feedback
24,Theme
24,Light
24,Dark
24,High contrast
24,Previous Version Docs
24,Blog
24,Contribute
24,Privacy & Cookies
24,Terms of Use
24,Trademarks
24,© Microsoft 2021
24,Is this page helpful?
24,Yes
24,Any additional feedback?
24,Skip
24,Submit
24,Thank you.
24,In this article
24,Theme
24,Light
24,Dark
24,High contrast
24,Previous Version Docs
24,Blog
24,Contribute
24,Privacy & Cookies
24,Terms of Use
24,Trademarks
24,© Microsoft 2021
25,Best Practices of Liferay Performance Tuning - Anblicks
25,Services
25,Data Analytics
25,Data Analytics Strategy & Assessment
25,Data Platform Modernization
25,Advanced Data Analytics
25,Data Platform Management
25,CloudOps
25,Cloud Strategy and Assessment
25,Cloud Automation
25,DevOps Automation
25,Infrastructure Automation
25,Cloud Infrastructure Management
25,Modern Apps
25,Cloud Native Applications
25,Microservices & API
25,Intelligent Apps
25,Accelerators
25,Hadoop to Snowflake Migration
25,Cloud Cost Management
25,CapptixAI
25,LendingAI
25,CustomerAI
25,SalesAI
25,Industries
25,Healthcare
25,Retail
25,Logistics
25,Financial Services
25,Real Estate
25,Automotive
25,Partnerships
25,Talend
25,Databricks
25,Snowflake
25,RapidMiner
25,Liferay
25,Amazon Web Services
25,Microsoft Azure
25,Insights
25,Blog
25,Success Stories
25,Data Sheets
25,Webinars
25,Presentations
25,About Us
25,Company Overview
25,Leadership Team
25,Ignite Program
25,Media
25,Press Releases
25,Events
25,In The News
25,Careers
25,Global Presence
25,Clients
25,Inquire Now
25,Best Practices of Liferay Performance Tuning
25,Blogs
25,Best Practices of Liferay Performance Tuning
25,admin
25,"November 6, 2020"
25,"1,746 views"
25,"Liferay Performance Tuning is aimed to collect one of the most important aspects to deliver user experience of any website or portal with help of performance enhancing tweaks, tips and techniques. We have hereby provided optimization areas and performance tuning tips for Liferay as well as Web, Application and Database layer."
25,"You may need to fine tune Liferay Portal performance, especially if your site traffic shoots up than you’d expected. Though having tons of rich UI/features users wouldn’t like a portal with slow response time."
25,"Best Practices importance may vary however it is essential to measure various key performance areas including server metrics, tweak configuration based on load tests, and verify that the changes were helpful to achieve optimum performance."
25,1 Apache
25,1.1 Enable Keep Alive in httpd.conf
25,This parameter is to allow more than 1 request per TCP connection. MaxKeepAliveRequest sets the maximum number of requests allowed per persistent connection.
25,(a) /etc/httpd/conf/httpd.conf
25,KeepAlive ON
25,MaxKeepAliveRequests 100
25,KeepAliveTimeout 15
25,1.2 Enable MPM worker
25,"This module implements a hybrid multi-process multi-threaded server. By using threads to serve requests, it is able to serve a large number of requests with fewer system resources than a process-based server. However, it retains much of the stability of a process-based server by keeping multiple processes available, each with many threads."
25,(a) Enable line in /etc/sysconfig/httpd
25,HTTPD=/usr/sbin/httpd.worker
25,1.3 Configure MPM worker
25,Configuration parameters will be dependent upon concurrency expected.
25,(a) /etc/httpd/conf/httpd.conf
25,StartServers 2
25,ServerLimit 32
25,MaxClients 800
25,MinSpareThreads 25
25,MaxSpareThreads 75
25,ThreadsPerChild 25
25,MaxRequestsPerChild 0
25,1.4 Change LogLevel
25,a) /etc/httpd/conf/httpd.conf
25,LogLevel error
25,(b) /etc/httpd/conf.d/ssl.conf
25,LogLevel error
25,1.5 Configure ETag
25,(a) /etc/httpd/conf/httpd.conf
25,FileETag none
25,1.6 Use mod_jk instead of mod_proxy for connecting to Liferay tomcat from Apache.
25,"Ensure JK module is enabled/configured in Apache. Using mod_jk is preferred to use by Liferay for clustering as mod_proxy module has its own performance issues. Along with this we are configuring static contents to be served from Apache, hence static contents should be placed on Apache Web Server local file-system."
25,(a) /etc/httpd/conf/httpd.conf
25,Include conf/mod-jk.conf
25,(b) /etc/httpd/conf/worker.properties
25,"worker.list=loadbalancer,status #JVM Host Settings"
25,worker.jvm1.port=8009
25,worker.jvm1.host=IPAddress
25,worker.jvm1.type=ajp13
25,worker.jvm1.lbfactor=1
25,worker.jvm1.socket_timeout=60
25,worker.jvm1.socket_keepalive=1
25,worker.jvm1.connection_pool_timeout=60
25,worker.jvm1.ping_mode=A
25,worker.jvm1.ping_timeout=20000
25,worker.jvm1.connect_timeout=20000
25,worker.jvm2.port=8009
25,worker.jvm2.host=IPAddress
25,worker.jvm2.type=ajp13
25,worker.jvm2.lbfactor=1
25,worker.jvm2.socket_timeout=60
25,worker.jvm2.socket_keepalive=1
25,worker.jvm2.connection_pool_timeout=60
25,worker.jvm2.ping_mode=A
25,worker.jvm2.ping_timeout=20000
25,worker.jvm2.connect_timeout=20000
25,# Load-balancing behaviour
25,worker.loadbalancer.method=B
25,worker.loadbalancer.type=lb
25,"worker.loadbalancer.balance_workers=jvm1,jvm2"
25,worker.loadbalancer.sticky_session=1
25,# Status worker for managing load balancer
25,worker.status.type=status
25,(c) /etc/httpd/conf.d/mod-jk.conf
25,LoadModule jk_module modules/mod_jk.so
25,JkWorkersFile conf/worker.properties
25,JkLogFile logs/mod_jk.log
25,JkLogLevel error
25,JkLogStampFormat “[%a %b %d %H:%M:%S %Y]”
25,JkUnMount /liferay-theme/js/*.js loadbalancer
25,JkUnMount /liferay-theme/css/*.css loadbalancer
25,JkUnMount /liferay-theme/images/*.gif loadbalancer
25,JkUnMount /liferay-theme/images/*.png loadbalancer
25,JkUnMount /liferay-theme/images/*.jpg loadbalancer
25,JkUnMount /liferay-theme/images/*.ico loadbalancer
25,JkUnMount /html/js/barebone.jsp loadbalancer
25,JkUnMount /html/js/everything.jsp loadbalancer
25,JkMount /* loadbalancer
25,1.7 Gzip on Apache
25,Ensure Deflate module is enabled in Apache
25,(a) /etc/httpd/conf/httpd.conf
25,SetOutputFilter DEFLATE
25,SetEnvIfNoCase Request_URI \.(?:exe|t?gz|zip|bz2|sit|rar)$ no-gzip dont-vary
25,SetEnvIfNoCase Request_URI \.(?:gif|jpe?g|png)$ no-gzip dont-vary
25,1.8 Caching on Apache
25,Ensure Expiry module is enabled in Apache. We have enabled caching for JSP.
25,(a) /etc/httpd/conf/httpd.conf
25,# Turn on Expires and set default to 0
25,ExpiresActive On
25,ExpiresDefault A0
25,# Set up caching on media files for 1 year (forever?)
25,<filesmatch “\.(flv|ico|pdf|avi|mov|ppt|doc|mp3|wmv|wav)$”=””>
25,ExpiresDefault A29030400
25,Header append Cache-Control “public”
25,# Set up caching on media files for 1 week
25,<filesmatch “\.(gif|jpg|jpeg|png|swf)$”=””>
25,ExpiresDefault A604800
25,Header append Cache-Control “public”
25,# Set up caching on media files for 1 week
25,<filesmatch “\.(xml|txt|html|js|jsp|css)$”=””>
25,ExpiresDefault A604800
25,Header append Cache-Control “proxy-revalidate”
25,# Force no caching for dynamic files
25,<filesmatch “\.(php|cgi|pl|htm)$”=””>
25,ExpiresActive Off
25,"Header set Cache-Control “private, no-cache, no-store, proxy-revalidate, no-transform” Header set Pragma “no-cache”"
25,2 Tomcat
25,2.1 Optimize thread pool in Tomcat
25,"Each incoming request to the application server consumes a worker thread for the duration of the request. When no threads are available to process requests, the request will be queued waiting for the next available worker thread. In a finely tuned system, the number of threads in the thread pool should be relatively balanced with the total number of concurrent requests. There should not be a significant amount of threads.Liferay Engineering recommends setting this initially to 50 threads and then monitoring it within your application server’s monitoring consoles. You may wish to use a higher number (e.g. 250) if your average page times are in the 2-3s range"
25,Apache should be configured to connect on AJP port i.e. 8009 with help of topic 1.6defined in the guide
25,(a) $CATALINA_HOME /conf/server.xml
25,acceptCount=”100″
25,connectionTimeout=”20000″
25,enableLookups=”false”
25,maxThreads=”450″
25,minSpareThreads=”50″
25,port=”8009″
25,protocol=”AJP/1.3″
25,redirectPort=”8443″
25,disableUploadTimeout=”true”
25,maxHttpHeaderSize=”8192″/>
25,2.2 Fine tune JVM settings as suggested by Liferay.
25,Tuning the JVM primarily focuses on tuning the garbage collector and the Java memory heap. These parameters look to otimize the throughput of your application.
25,(a) $CATALINA_HOME /bin/setenv.sh
25,JAVA_OPTS=”$JAVA_OPTS -Dfile.encoding=UTF8 -Dorg.apache.catalina.loader.WebappClassLoader.ENABLE_CLEAR_REFERENCES=false -Duser.timezone=GMT -server -d64 -XX:NewSize=2048m -XX:MaxNewSize=2048m -Xms6144m -Xmx6144m -XX:PermSize=200m -XX:MaxPermSize=512m -XX:SurvivorRatio=20 -XX:TargetSurvivorRatio=90 -XX:MaxTenuringThreshold=15 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:ParallelGCThreads=8 -XX:ReservedCodeCacheSize=512m -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+CMSCompactWhenClearAllSoftRefs -XX:CMSInitiatingOccupancyFraction=85 -XX:+CMSScavengeBeforeRemark -XX:+CMSConcurrentMTEnabled -XX:ParallelCMSThreads=2 -XX:+UseCompressedOops -XX:+DisableExplicitGC -XX:-UseBiasedLocking -XX:+BindGCTaskThreadsToCPUs -XX:+UseFastAccessorMethods -Djava.net.preferIPv4Stack=true -Djava.rmi.server.hostname=10.154.14.71 -Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.port=5000 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -XX:+UseLargePages”
25,2.3 Remove Unwanted Applications from Tomcat
25,(a) We suggest removing unwanted applications which are deployed in the server.
25,2.4 Optimize Database Thread pool
25,"The database connection pool is generally sized at roughly 20-30% of the thread pool size. The connection pool provides a connection whenever LPEE needs to retrieve data from the database (e.g. user login, etc). If this size is too small, requests will queue in the server waiting for database connections. However, too large a setting will mean wasting resources with idle database connections."
25,(a) $CATALINA_HOME /conf/context.xml
25,testOnBorrow=”true”
25,testWhileIdle=”true”
25,numTestsPerEvictionRun=”10″
25,timeBetweenEvictionRunsMillis=”1800000″
25,minEvictableIdleTimeMillis=”3600000″
25,logAbandoned=”true”
25,removeAbandonedTimeout=”20″
25,removeAbandoned=”true”
25,url=”jdbc:mysql://192.168.100.1/db_name?autoReconnect=true&useUnicode=true&characterEncoding=UTF-8&useFastDateParsing=false”
25,driverClassName=”com.mysql.jdbc.Driver”
25,username=”user”
25,password=”password”
25,maxWait=”20000″
25,maxIdle=”120″
25,minIdle=”3″
25,validationQuery=”select 1″
25,maxActive=”750″
25,type=”javax.sql.DataSource”
25,auth=”Container”
25,name=”jdbc/LiferayPool” />
25,3 Liferay
25,3.1 Optimize web.xml
25,a) $CATALINA_HOME /conf/web.xml
25,jsp
25,org.apache.jasper.servlet.JspServlet
25,fork
25,false
25,development
25,false
25,mappedFile
25,false
25,genStrAsCharArray
25,true
25,xpoweredBy
25,false
25,3.2 Portal-ext.properties
25,jdbc.default.jndi.name=jdbc/LiferayPool
25,theme.images.fast.load = true
25,theme.css.fast.load = true
25,layout.template.cache.enable = true
25,javascript.fast.load = true
25,com.liferay.portal.servlet.filters.gzip.GZipFilter=false
25,com.liferay.portal.servlet.filters.strip.StripFilter=false
25,com.liferay.portal.servlet.filters.sso.cas.CASFilter=false
25,com.liferay.portal.servlet.filters.sso.ntlm.NtlmFilter=false
25,com.liferay.portal.servlet.filters.sso.opensso.OpenSSOFilter=false
25,com.liferay.portal.sharepoint.SharepointFilter=false
25,com.liferay.portal.servlet.filters.validhtml.ValidHtmlFilter=false
25,session.tracker.memory.enabled=false
25,counter.increment=2000
25,portlet.css.enabled=false
25,javadoc.manager.enabled=false
25,direct.servlet.context.reload=false
25,blogs.pingback.enabled=false
25,blogs.trackback.enabled=false
25,blogs.ping.google.enabled=false
25,message.boards.pingback.enabled=false
25,permissions.inline.sql.check.enabled=false
25,look.and.feel.modifiable=false
25,layout.user.public.layouts.enabled=false
25,dl.file.rank.enabled=false
25,dl.file.entry.read.count.enabled=false
25,4 Database
25,4.1 My.cnf
25,#Each session that needs to do a sort allocates a buffer of this size
25,sort_buffer_size=16M
25,#Each thread that does a sequential scan for a MyISAM table allocates a buffer of this size (in bytes) for each table it scans
25,read_buffer_size=8M
25,"#When reading rows from a MyISAM table in sorted order following a key-sorting operation, the rows are read through this buffer to avoid disk seeks"
25,read_rnd_buffer_size=8M
25,#Do not cache results that are larger than this number of bytes
25,query_cache_limit=4M
25,#The size in bytes of the memory buffer InnoDB uses to cache data and indexes of its tables.
25,innodb_buffer_pool_size = 2048M
25,#Increase the value of join_buffer_size to get a faster full join when adding indexes is not possible.
25,join_buffer_size = 256M
25,#The number of open tables for all threads.
25,table_open_cache = 400
25,#The amount of memory allocated for caching query results.
25,query_cache_size=32M
25,* Reference: http://www.packtpub.com/liferay-portal-performance-best-practices/book
25,admin
25,Share this post
25,Integrate Liferay With Varnish
25,Javascript MVC Frameworks
25,Search for:
25,Search
25,Recent Posts
25,8 Ways to Improve Decision Making and Cut Cost with Better Data Quality
25,"Advantages, Challenges And Features Of Partner Portal"
25,10 Common Misconceptions About eCommerce Development
25,9 Benefits of Intelligent Apps to Increase User Engagement
25,4 Reasons Why Your Business Needs Digital Transformation
25,Talk to an Expert
25,Get a 30-Minute FREE Strategy Consultation
25,Global Footprint
25,Subscribe to Newsletter
25,Leave this field empty if you're human:
25,Company
25,About Us
25,Career Overview
25,Blog
25,Services
25,Data Analytics
25,CloudOps
25,Modern Apps
25,Latest Tweets
25,Is larger volumes of #Data really an effective differentiator? Would the large variety yield significantly better Insights? Does #realtimeanalytics offer value as promised? Check out this blog to understand the underpinnings of the effectiveness of Data. https://hubs.la/H0L29pf0
25,Connect with Us
25,© Copyright 2021 Anblicks. All rights reserved. Various trademarks held by their respective owners.
25,Privacy policy
25,Terms of use
25,admin
25,Search for:
25,Search
25,Services
25,Data Analytics
25,Data Analytics Strategy & Assessment
25,Data Platform Modernization
25,Advanced Data Analytics
25,Data Platform Management
25,CloudOps
25,Cloud Strategy and Assessment
25,Cloud Automation
25,DevOps Automation
25,Infrastructure Automation
25,Cloud Infrastructure Management
25,Modern Apps
25,Cloud Native Applications
25,Microservices & API
25,Intelligent Apps
25,Accelerators
25,Hadoop to Snowflake Migration
25,Cloud Cost Management
25,CapptixAI
25,LendingAI
25,CustomerAI
25,SalesAI
25,Industries
25,Healthcare
25,Retail
25,Logistics
25,Financial Services
25,Real Estate
25,Automotive
25,Partnerships
25,Talend
25,Databricks
25,Snowflake
25,RapidMiner
25,Liferay
25,Amazon Web Services
25,Microsoft Azure
25,Insights
25,Blog
25,Success Stories
25,Data Sheets
25,Webinars
25,Presentations
25,About Us
25,Company Overview
25,Leadership Team
25,Ignite Program
25,Media
25,Press Releases
25,Events
25,In The News
25,Careers
25,Global Presence
25,Clients
25,Inquire Now
26,Hibernate - Vlad Mihalcea
26,Vlad Mihalcea
26,Home
26,Blog
26,Store
26,Books
26,Courses
26,Hypersistence Optimizer
26,Documentation
26,Installation Guide
26,User Guide
26,Examples
26,Release Notes
26,Issue Tracker
26,Trial Version
26,Full Version
26,Training
26,High-Performance SQL
26,High-Performance Java Persistence
26,Consulting
26,Tutorials
26,Hibernate
26,SQL
26,Spring
26,Videos
26,Talks
26,Hibernate
26,Last modified:
26,Follow @vlad_mihalcea
26,Imagine having a tool that can automatically detect JPA and Hibernate performance issues.
26,Hypersistence Optimizer is that tool!
26,High-Performance Hibernate Tutorial
26,"I’ve been using Hibernate for almost a decade and I admit it was not an easy journey. These tutorials are snippets from my High-Performance Java Persistence book, whose main goal is to show you how to make your data access layer run a high-speeds."
26,"This material is useful for both beginners and experienced developers, so enjoy reading it."
26,"The best Tutorials on High-Performance Hibernate #Hibernate #Java #Tutorials #NewYearsResolution #Career https://t.co/wQNjz6kK24 pic.twitter.com/4wDmQYvswq— Java (@java) January 1, 2019"
26,Tips and Best Practices
26,Why and when you should use JPA
26,The best way to prevent JPA and Hibernate performance issues
26,How to detect JPA and Hibernate performance issues automatically using Hypersistence Optimizer
26,Tuning Spring Petclinic JPA and Hibernate configuration with Hypersistence Optimizer
26,Hibernate Query Performance Tuning
26,Spring Boot performance tuning
26,Spring Boot performance monitoring
26,A beginner’s guide to the high-performance-java-persistence GitHub repository
26,Hibernate performance tuning tips
26,14 High-Performance Java Persistence tips
26,9 High-Performance Tips when using MySQL with JPA and Hibernate
26,9 High-Performance Tips when using PostgreSQL with JPA and Hibernate
26,How to detect the Hibernate N+1 query problem during testing
26,Hibernate slow query log
26,A beginner’s guide to SQL injection and how you should prevent it
26,"How to store date, time, and timestamps in UTC time zone with JDBC and Hibernate"
26,The fastest way to update a table row when using Hibernate and Oracle
26,How to use database-specific or Hibernate-specific features without sacrificing portability
26,How to use the Hibernate Session doWork and doReturningWork methods
26,JPA providers market share
26,Bootstrapping
26,A beginner’s guide to JPA persistence.xml file
26,How to bootstrap Hibernate without the persistence.xml configuration file
26,How to bootstrap JPA programmatically without the persistence.xml configuration file
26,JDBC Driver Connection URL strings
26,JDBC Driver Maven dependency list
26,How to get access to database table metadata with Hibernate 5
26,How to get the entity mapping to database table binding metadata from Hibernate
26,Schema Management
26,Flyway Database Schema Migrations
26,Hibernate hbm2ddl.auto schema generation
26,Mappings
26,Basic Types
26,A beginner’s guide to Hibernate Types
26,How to implement a custom basic type using Hibernate UserType
26,JPA AttributeConverter – A Beginner’s Guide
26,How to map calculated properties with JPA and Hibernate @Formula annotation
26,How to map calculated properties with Hibernate @Generated annotation
26,How to emulate @CreatedBy and @LastModifiedBy from Spring Data using the @GeneratorType Hibernate annotation
26,How to map Date and Timestamp with JPA and Hibernate
26,What’s new in JPA 2.2 – Java 8 Date and Time Types
26,The best way to map a Java 1.8 Optional entity attribute with JPA and Hibernate
26,The best way to map an Enum Type with JPA and Hibernate
26,How to map a JPA entity to a View or SQL query using Hibernate
26,How to map the PostgreSQL inet type with JPA and Hibernate
26,How to map a PostgreSQL Range column type with JPA and Hibernate
26,How to map the Java YearMonth type with JPA and Hibernate
26,How to map java.time.Year and java.time.Month with JPA and Hibernate
26,How to map a PostgreSQL Interval to a Java Duration with Hibernate
26,How to escape SQL reserved keywords with JPA and Hibernate
26,JSON
26,"The hibernate-types open-source project offers extra Hibernate Types (e.g. JSON, ARRAY)"
26,How to map JSON objects using generic Hibernate Types
26,How to map Oracle JSON columns using JPA and Hibernate
26,How to map SQL Server JSON columns using JPA and Hibernate
26,How to store schema-less EAV (Entity-Attribute-Value) data using JSON and Hibernate
26,How to map a String JPA property to a JSON column using Hibernate
26,How to map JSON collections using JPA and Hibernate
26,Java Map to JSON mapping with JPA and Hibernate
26,How to map Java Records to JSON columns using Hibernate
26,How to encrypt and decrypt JSON properties with JPA and Hibernate
26,How to customize the Jackson ObjectMapper used by Hibernate-Types
26,How to customize the JSON Serializer used by Hibernate-Types
26,How to fix the Hibernate “No Dialect mapping for JDBC type” issue
26,How to fix the Hibernate “column is of type jsonb but expression is of type bytes” issue
26,ARRAY
26,How to map a PostgreSQL ARRAY to a Java List with JPA and Hibernate
26,How to map Java and SQL arrays with JPA and Hibernate
26,How to map a PostgreSQL Enum ARRAY to a JPA entity property using Hibernate
26,Multidimensional array mapping with JPA and Hibernate
26,Hibernate HSQLDB ARRAY Type
26,Equals and HashCode
26,"The best way to implement equals, hashCode, and toString with JPA and Hibernate"
26,How to implement equals and hashCode using the entity identifier (primary key)
26,How to implement equals and hashCode using the entity natural identifier
26,Relationships
26,A beginner’s guide to database table relationships
26,ManyToOne JPA and Hibernate association best practices
26,The best way to map a @OneToOne relationship with JPA and Hibernate
26,How to change the @OneToOne shared primary key column name with JPA and Hibernate
26,The best way to map a @OneToMany relationship with JPA and Hibernate
26,The best way to use the @ManyToMany annotation with JPA and Hibernate
26,The best way to map a many-to-many association with extra columns when using JPA and Hibernate
26,The best way to map a Composite Primary Key with JPA and Hibernate
26,How to map a composite identifier using an automatically @GeneratedValue with JPA and Hibernate
26,How to synchronize bidirectional entity associations with JPA and Hibernate
26,How to map a @ManyToOne association using a non-Primary Key column
26,How to customize an entity association JOIN ON clause with Hibernate @JoinFormula
26,How to map a JPA @ManyToOne relationship to a SQL query using the Hibernate @JoinFormula annotation
26,How to optimize unidirectional collections with JPA and Hibernate
26,How do Set and List collections behave with JPA and Hibernate
26,Advanced mapping techniques
26,Fluent API entity building with JPA and Hibernate
26,How to map an immutable entity with JPA and Hibernate
26,How to map the latest child of a parent entity using Hibernate @JoinFormula
26,How to map multiple JPA entities to one database table with Hibernate
26,How to update only a subset of entity attributes using JPA and Hibernate @DynamicUpdate
26,How to use external XML mappings files with JPA and Hibernate
26,How to encrypt and decrypt data with Hibernate
26,The best way to soft delete with Hibernate
26,How to fix “wrong column type encountered” schema-validation errors with JPA and Hibernate
26,"How to audit entity modifications using the JPA @EntityListeners, @Embedded, and @Embeddable annotations"
26,How to use @PrePersist and @PreUpdate on Embeddable with JPA and Hibernate
26,How to map camelCase properties to snake_case column names with Hibernate
26,Identifiers
26,A beginner’s guide to natural and surrogate database keys
26,"Hibernate Identity, Sequence, and Table (Sequence) generator"
26,How to generate JPA entity identifier values using a database sequence
26,The hi/lo algorithm
26,Hibernate pooled and pooled-lo identifier generators
26,A beginner’s guide to Hibernate enhanced identifier generators
26,How to migrate the hilo Hibernate identifier optimizer to the pooled strategy
26,Why you should never use the TABLE identifier generator with JPA and Hibernate
26,Why should not use the AUTO JPA GenerationType with MySQL and Hibernate
26,How to replace the TABLE identifier generator with either SEQUENCE or IDENTITY in a portable way
26,PostgreSQL SERIAL column and Hibernate IDENTITY generator
26,How to combine the Hibernate assigned generator with a sequence or an identity column
26,How to implement a custom String-based sequence identifier generator with Hibernate
26,MariaDB 10.3 supports database sequences
26,Hibernate and UUID identifiers
26,How to use a JVM or database auto-generated UUID identifier with JPA and Hibernate
26,The best way to map a @NaturalId business key with JPA and Hibernate
26,Inheritance
26,The best way to use entity inheritance with JPA and Hibernate
26,The best way to map the SINGLE_TABLE inheritance with JPA and Hibernate
26,MySQL 8 support for custom SQL CHECK constraints simplifies SINGLE_TABLE inheritance data integrity validation rules
26,The best way to map the @DiscriminatorColumn with JPA and Hibernate
26,How to inherit properties from a base class entity using @MappedSuperclass with JPA and Hibernate
26,How to order entity subclasses by their class type using JPA and Hibernate
26,Connection Management
26,The simple scalability equation
26,The anatomy of Connection Pooling
26,Why you should use FlexyPool
26,How to monitor your connection pool with FlexyPool
26,How to monitor a Java EE DataSource
26,Why you should always use hibernate.connection.provider_disables_autocommit for resource-local JPA transactions
26,How does aggressive connection release work in Hibernate
26,Persistence Context
26,The JPA and Hibernate first-level cache
26,A beginner’s guide to JPA/Hibernate entity state transitions
26,A beginner’s guide to JPA and Hibernate Cascade Types
26,How does orphanRemoval work with JPA and Hibernate
26,A beginner’s guide to flush strategies in JPA and Hibernate
26,How does persist and merge work in JPA
26,"How do JPA persist, merge and Hibernate save, update, saveOrUpdate work"
26,How to merge entity collections with JPA and Hibernate
26,How does AUTO flush strategy work in Hibernate
26,How to override the default Hibernate Session FlushMode
26,How do JPA and Hibernate define the AUTO flush mode
26,A beginner’s guide to Hibernate flush operation order
26,The best way to clone or duplicate an entity with JPA and Hibernate
26,How to intercept entity changes with Hibernate event listeners
26,The anatomy of Hibernate dirty checking mechanism
26,How to customize Hibernate dirty checking mechanism
26,Fetching
26,Pagination best practices
26,JPA Default Fetch Plan
26,How do find and getReference EntityManager methods work when using JPA and Hibernate
26,N+1 query problem with JPA and Hibernate
26,The best way to fetch multiple entities by id using JPA and Hibernate
26,A beginner’s guide to Hibernate fetching strategies
26,EAGER fetching is a code smell
26,The best way to map a projection query to a DTO (Data Transfer Object) with JPA and Hibernate
26,How to write a compact DTO projection query with JPA
26,How to fetch a one-to-many DTO projection with JPA and Hibernate
26,The best way to handle the LazyInitializationException
26,The best way to lazy load entity attributes using JPA and Hibernate
26,Hibernate LazyToOne annotation
26,The best way to initialize LAZY entity and collection proxies with JPA and Hibernate
26,Why you should avoid EXTRA Lazy Collections with Hibernate
26,ResultSet statement fetching with JDBC and Hibernate
26,The best way to fix the Hibernate MultipleBagFetchException
26,How to fetch entities multiple levels deep with Hibernate
26,The Open Session In View Anti-Pattern
26,The hibernate.enable_lazy_load_no_trans Anti-Pattern
26,How does MySQL result set streaming perform vs fetching the whole JDBC ResultSet at once
26,How does a JPA Proxy work and how to unproxy it with Hibernate
26,The best way to fix the Hibernate “HHH000104: firstResult/maxResults specified with collection fetch; applying in memory!” warning message
26,How to detect HHH000104 issues with the hibernate.query.fail_on_pagination_over_collection_fetch configuration property
26,Bytecode Enhancement
26,Maven and Gradle Hibernate Enhance Plugin
26,How to enable bytecode enhancement dirty checking in Hibernate
26,How does the bytecode enhancement dirty checking mechanism work in Hibernate 4.3
26,Concurrency Control
26,Data knowledge stack
26,A beginner’s guide to ACID and database transactions
26,Optimistc vs. Pessimistic Locking
26,A beginner’s guide to database deadlock
26,How does the 2PL (Two-Phase Locking) algorithm work
26,How does MVCC (Multi-Version Concurrency Control) work
26,How to get the current database transaction id
26,How to log the database transaction id using MDC (Mapped Diagnostic Context)
26,How does the entity version property work when merging with JPA and Hibernate
26,Optimistic locking version property with JPA and Hibernate
26,The best way to map an entity version property with JPA and Hibernate
26,How do PostgreSQL advisory locks work
26,How to implement a database job queue using SKIP LOCKED
26,A beginner’s guide to database locking and the lost update phenomena
26,How to prevent lost updates in long conversations
26,A beginner’s guide to Dirty Read anomaly
26,A beginner’s guide to Non-Repeatable Read anomaly
26,A beginner’s guide to Phantom Read anomaly
26,A beginner’s guide to Read and Write Skew phenomena
26,"A beginner’s guide to the Write Skew anomaly, and how it differs between 2PL and MVCC"
26,"How does database pessimistic locking interact with INSERT, UPDATE, and DELETE SQL statements"
26,"How do UPSERT and MERGE work in Oracle, SQL Server, PostgreSQL, and MySQL"
26,Logical vs physical clock optimistic locking
26,How to retry JPA transactions after an OptimisticLockException
26,How does Hibernate guarantee application-level repeatable reads
26,Hibernate collections optimistic locking
26,How to address the OptimisticLockException in JPA and Hibernate
26,How to prevent OptimisticLockException with Hibernate versionless optimistic locking
26,A beginner’s guide to transaction isolation levels in enterprise Java
26,A beginner’s guide to Java Persistence locking
26,How does LockModeType.OPTIMISTIC work in JPA and Hibernate
26,How to fix optimistic locking race conditions with pessimistic locking
26,How does LockModeType.OPTIMISTIC_FORCE_INCREMENT work in JPA and Hibernate
26,How does LockModeType.PESSIMISTIC_FORCE_INCREMENT work in JPA and Hibernate
26,How do LockModeType.PESSIMISTIC_READ and LockModeType.PESSIMISTIC_WRITE work in JPA and Hibernate
26,How does CascadeType.LOCK works in JPA and Hibernate
26,How to increment the parent entity version whenever a child entity gets modified with JPA and Hibernate
26,Spring read-only transaction Hibernate optimization
26,Read-write and read-only transaction routing with Spring
26,Batching
26,Batch processing best practices
26,The best way to do batch processing with JPA and Hibernate
26,How to batch INSERT and UPDATE statements with Hibernate
26,How to batch DELETE statements with Hibernate
26,How to customize the JDBC batch size for each Persistence Context with Hibernate
26,How to find which statement failed in a JDBC Batch Update
26,How to optimize the merge operation using update while batching with JPA and Hibernate
26,How to enable multi-row inserts with the PostgreSQL reWriteBatchedInserts configuration property
26,Queries
26,A beginner’s guide to JPA and Hibernate Query setParameter method
26,A beginner’s guide to JPA and Hibernate query hints
26,Query timeout with JPA and Hibernate
26,The best way to use the JPA SqlResultSetMapping
26,How to return a Map result from a JPA or Hibernate query
26,How to improve statement caching efficiency with IN clause parameter padding
26,A beginner’s guide to the Hibernate JPQL and Native Query Plan Cache
26,How to optimize JPQL and Criteria API query plans with Hibernate Statistics
26,How to intercept and modify SQL queries with the Hibernate StatementInspector
26,The best way to use the JPQL DISTINCT keyword with JPA and Hibernate
26,How to JOIN unrelated entities with JPA and Hibernate
26,How to resolve the Hibernate global database schema and catalog for native SQL queries
26,How to map table rows to columns using SQL PIVOT or CASE expressions
26,The JPA EntityManager createNativeQuery is a Magic Wand
26,Why you should use the Hibernate ResultTransformer to customize result set mappings
26,The best way to use a Hibernate ResultTransformer
26,Why you should definitely learn SQL Window Functions
26,Query pagination with JPA and Hibernate
26,What’s new in JPA 2.2 – Stream the result of a Query execution
26,How to get the actual execution plan for an Oracle SQL query using Hibernate query hints
26,How to solve the PostgreSQL :: cast operator issue with JPA and Hibernate
26,The best way to use SQL functions in JPQL or Criteria API queries with JPA and Hibernate
26,How to execute SQL functions with multiple parameters in a JPQL query with Hibernate
26,How to query parent rows when all children must match the filtering criteria with SQL and Hibernate
26,How to bind custom Hibernate parameter types to JPA queries
26,Statement Caching
26,How does a relational database execute SQL statements and prepared statements
26,MySQL JDBC Statement Caching
26,Bulk Processing
26,Bulk Update and Delete with JPA and Hibernate
26,JPA Criteria API Bulk Update and Delete
26,Bulk Update optimistic locking with JPA and Hibernate
26,Criteria Queries
26,How to write JPA Criteria API queries using Codota
26,JPA Criteria Metamodel Generation and Usage Guide
26,How to query by entity type using JPA Criteria API
26,Why you should always check the SQL statements generated by Criteria API
26,The performance penalty of Class.forName when parsing JPQL and Criteria queries
26,How does Hibernate handle JPA Criteria API literals
26,Stored Procedures
26,The best way to call a stored procedure with JPA and Hibernate
26,How to call Oracle stored procedures and functions with JPA and Hibernate
26,How to call SQL Server stored procedures and functions with JPA and Hibernate
26,How to call PostgreSQL functions (stored procedures) with JPA and Hibernate
26,How to call MySQL stored procedures and functions with JPA and Hibernate
26,Caching
26,A beginner’s guide to Cache synchronization strategies
26,Things to consider before jumping to enterprise caching
26,Caching best practices
26,How does Hibernate store second-level cache entries
26,How does Hibernate READ_ONLY CacheConcurrencyStrategy work
26,How does Hibernate NONSTRICT_READ_WRITE CacheConcurrencyStrategy work
26,How does Hibernate READ_WRITE CacheConcurrencyStrategy work
26,How does Hibernate TRANSACTIONAL CacheConcurrencyStrategy work
26,How does Hibernate Collection Cache work
26,How does Hibernate Query Cache work
26,How to use the Hibernate Query Cache for DTO projections
26,How to avoid the Hibernate Query Cache N+1 issue
26,How to cache non-existing entity fetch results with JPA and Hibernate
26,Statistics
26,A beginner’s guide to Hibernate Statistics
26,How to expose Hibernate Statistics via JMX
26,Audit Logging
26,A beginner’s guide to CDC (Change Data Capture)
26,MySQL audit logging using triggers
26,The best way to implement an audit log using Hibernate Envers
26,How to extract change data events from MySQL to Kafka using Debezium
26,Multitenancy
26,A beginner’s guide to database multitenancy
26,Hibernate database catalog multitenancy
26,Hibernate database schema multitenancy
26,Testing
26,The minimal configuration for testing Hibernate
26,Hibernate integration testing strategies
26,How to run database integration tests 20 times faster
26,How to run integration tests at warp-speed using Docker and tmpfs
26,The best way to log JDBC statements
26,The best way to detect database connection leaks
26,How to install DB2 Express-C on Docker and set up the JDBC connection properties
26,How to get started with CockroachDB and Hibernate
26,Online Workshops
26,"If you enjoyed this article, I bet you are going to love my upcoming 4-day x 4 hours High-Performance Java Persistence Online Workshop"
26,Follow @vlad_mihalcea
26,Insert details about how the information is going to be processedDOWNLOAD NOW
26,Leave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment Name *
26,Email *
26,Website
26,This site uses Akismet to reduce spam. Learn how your comment data is processed.
26,Let’s connect
26,Twitter
26,YouTube
26,LinkedIn
26,Email
26,Facebook
26,Amazon
26,GitHub
26,Find Article
26,Search
26,Book
26,Video Course
26,Hypersistence Optimizer
26,Training
26,ERP Contact
26,TutorialsHibernate
26,SQL
26,Spring
26,Git
26,FlexyPool
26,Social MediaTwitter
26,Facebook
26,YouTube
26,GitHub
26,LinkedIn
26,AboutAbout
26,FAQ
26,Archive
26,Privacy Policy
26,Terms of Service
26,Meta
26,Log in
26,Entries feed
26,Comments feed
26,WordPress.org
26,"Privacy & Cookies: This site uses cookies. By continuing to use this website, you agree to their use."
26,"To find out more, including how to control cookies, see here:"
26,Our Cookie Policy
26,Vlad Mihalcea
26,Powered by WordPress.com.
26,High-Performance Java Persistence19th - 22nd of April
26,Prepare yourself to be amazed!The best way to map JPA and Hibernate entities and associationsBatch processing best practicesThe best way to fetch data with JPA and HibernateTransactions and Concurrency ControlCaching best practices
26,Save Your Seat!
27,Performance tuning - OpenZFS
27,Performance tuning
27,From OpenZFS
27,"Jump to:					navigation, 					search"
27,This page was moved to: https://openzfs.github.io/openzfs-docs/Performance%20and%20Tuning/Workload%20Tuning.html
27,"Retrieved from ""https://openzfs.org/w/index.php?title=Performance_tuning&oldid=2948"""
27,Navigation menu
27,Personal tools
27,Create accountLog in
27,Namespaces
27,Page
27,Discussion
27,Variants
27,Views
27,Read
27,View source
27,View history
27,More
27,Search
27,Navigation
27,Developer SummitCode of ConductDistributionsCompaniesParticipateNewcomer resourcesDeveloper resourcesDocumentation
27,Tools
27,What links hereRelated changesSpecial pagesPrintable versionPermanent linkPage information
27,"This page was last modified on 21 September 2020, at 16:35."
27,Content is available under Creative Commons Attribution-ShareAlike license unless otherwise noted.
27,About OpenZFS
27,Mobile view
28,How to Tune SQL with SEMI JOIN by Hints INDEX_DESC Injection for Oracle? - Tosska Technologies Limited
28,Skip to content
28,Tosska Technologies Limited
28,Alternative Wisdom
28,Menu
28,MENUMENUHomeProducts
28,Tosska SQL Tuning Expert (TSES™) for SQL Server®
28,View DetailsFree DownloadBuy License
28,Tosska SQL Tuning Expert (TSEM™) for MySQL®
28,View DetailsFree DownloadBuy License
28,Tosska SQL Tuning Expert (TSE™) for Oracle®
28,View DetialsFree DownloadBuy License
28,Tosska SQL Tuning Expert Pro (TSE Pro™) for Oracle®
28,View DetailsDownload Free TrialBuy License
28,Tosska In-Memory Maestro (TIM™) for Oracle®
28,View DetailsDownload Free TrialBuy License
28,Services
28,Tosska Maestros' Club (TMC™)
28,TMC™ Membership (Maintenance)Product Updates
28,Support
28,Frequently Asked Questions (FAQ)Submit Support Request
28,Resources
28,DocumentationsTips & Tricks
28,Company
28,Our CompanyPrivacy Policy & CopyrightTerms Of UseContact Us
28,News
28,Posted on 2021-01-252021-01-24 by Ka Ming NgHow to Tune SQL with SEMI JOIN by Hints INDEX_DESC Injection for Oracle?
28,"Semi-join is introduced in Oracle 8.0. It provides an efficient method of performing a WHERE EXISTS or WHERE IN sub-queries. A semi-join returns one copy of each row in first table for which at least one match is found in second table, there is no need of further scanning of the second table once a record is found."
28,SELECT *
28,FROM DEPARTMENT
28,where dpt_id
28,in (select emp_dept from EMPLOYEE
28,where emp_id >3300000)
28,"Here the following is the query plan of this SQL, it takes 13.59 seconds to finish. The query shows a “NESTED LOOPS SEMI” from DEPARTMENT to EMPLOYEE table."
28,"Basically, this SQL is difficult to optimize by just syntax rewrite due to the simplicity of the SQL syntax that Oracle is easily transformed into a canonical syntax internally, so not much alternative query plan can be triggered by syntax rewrite."
28,"Let’s use Hints injection to the SQL and see if there any brutal force of hints injection can trigger a better performance plan. With our A.I. Hints Injection algorithm applying to the SQL, it comes up with a SQL with extraordinary performance improvement that even I cannot understand at the first glance."
28,SELECT  /*+ INDEX_DESC(@SEL$2 EMPLOYEE) */ *
28,FROM     department
28,WHERE  dpt_id IN (SELECT emp_dept
28,FROM     employee
28,WHERE  emp_id > 3300000)
28,Here is the query plan of the hints injected SQL and it is now running much faster. The new query plan shows that the “INDEX RANGE SCAN” of EMP_DPT_INX to EMPLOYEE table is changed to “INDEX RANGE SCAN DESCENDING” and the estimated cost is the same as the Original SQL.
28,"The Hints /*+ INDEX_DESC(@SEL$2 EMPLOYEE) */  injected SQL takes only 0.05 second, it is much faster than the original SQL, the reason behind is the employee records creation order in EMPLOYEE table, the higher the emp_id will be created later, so the corresponding records will be inserted into the right hand side of the EMP_DPT_INX index tree nodes. The “INDEX RANGE SCAN” in the original SQL plan that needs to scan a lot of records from left to right direction before it can hit one record for  the condition “WHERE  emp_id > 3300000”.  In contrast, the Hints injected SQL with the “INDEX RANGE SCAN DESCENDING” operation that can evaluate the WHERE condition with only one scan from right to left on EMP_DPT_INX index tree nodes. That explains why the Hints injected SQL outperformed the original SQL by more than 270 times."
28,"It is common that we employ “transaction id”, “serial no” or “creation date” in our application design, this kind of the records are normally created alone with an increasing sequence order, there may be some SQL in your system can be improved by this technique."
28,"This kind of rewrites or Hints injection can be achieved by Tosska SQL Tuning Expert for Oracle automatically, it shows that the Hints injected SQL is more than 270 times faster than the original SQL."
28,https://tosska.com/tosska-sql-tuning-expert-pro-tse-pro-for-oracle/
28,CategoriesBlogs
28,"Tagsdatabase, hints, improve oracle database performance, optimize, oracle database and sql, oracle database performance tuning, SQL Optimization, sql performance tuning, SQL with SEMI JOIN, TSE, TSE Pro"
28,Post navigation
28,Previous PostPrevious
28,How to Tune SQL Statement with Multiple Union in Subquery for MySQL?Next PostNext How to Tune SQL with Exists Operator in Certain Environment for Oracle?
28,Search this site
28,Search for:
28,Search
28,Recent Posts
28,How to Tune SQL Statement with IN Operator with an Expression List for SQL Server?
28,2021-04-08
28,How to Tune SQL Statements with CONCAT Operator for MySQL?
28,2021-04-08
28,Tosska SQL Tuning Expert
28,(TSES™) for SQL Server 1.0.5 Release Notes
28,2021-03-31
28,6 Query Related Problems to Avoid in Oracle Database
28,2021-03-24
28,MySQL SQL Performance Tuning: 8 Great Monitoring Practices
28,2021-02-26
28,Address
28,Suite 2512 Langham Place
28,"Office Tower,"
28,"8 Argyle Street, Mongkok, Kowloon, Hong Kong."
28,Phone
28,+852-2150-1987
28,Email
28,support@tosska.com
28,enquire@tosska.com
28,Search this site
28,Search for:
28,Search
28,All Trademarks mentioned on this Site are the property of their respective owners.
28,©2016-2021 Tosska Technologies Limited. All rights reserved.
28,Privacy Policy & Copyright
28,Proudly powered by WordPress
28,Tosska SQL Tuning Expert (TSES™) for SQL Server® is now available ! View Details
31,How to Analyze and Tune MySQL Queries for Better Performance - MySQL Tutorial - YouTubeAboutPressCopyrightContact usCreatorsAdvertiseDevelopersTermsPrivacyPolicy & SafetyHow YouTube worksTest new features© 2021 Google LLC
32,"Performance | GORM - The fantastic ORM library for Golang, aims to be developer friendly."
32,GORM
32,DocsCommunityAPIContribute
32,English
32,English
32,简体中文
32,Deutsch
32,Español
32,bahasa Indonesia
32,Italiano
32,日本語
32,Русский
32,French
32,한국어
32,Performance
32,"GORM optimizes many things to improve the performance, the default performance should good for most applications, but there are still some tips for how to improve it for your application."
32,"Disable Default TransactionGORM perform write (create/update/delete) operations run inside a transaction to ensure data consistency, which is bad for performance, you can disable it during initialization"
32,"db, err := gorm.Open(sqlite.Open(""gorm.db""), &gorm.Config{"
32,"SkipDefaultTransaction: true,})"
32,Caches Prepared StatementCreates a prepared statement when executing any SQL and caches them to speed up future calls
32,"// Globally modedb, err := gorm.Open(sqlite.Open(""gorm.db""), &gorm.Config{"
32,"PrepareStmt: true,})// Session modetx := db.Session(&Session{PrepareStmt: true})tx.First(&user, 1)tx.Find(&users)tx.Model(&user).Update(""Age"", 18)"
32,NOTE Also refer how to enable interpolateparams for MySQL to reduce roundtrip https://github.com/go-sql-driver/mysql#interpolateparams
32,"SQL Builder with PreparedStmtPrepared Statement works with RAW SQL also, for example:"
32,"db, err := gorm.Open(sqlite.Open(""gorm.db""), &gorm.Config{"
32,"PrepareStmt: true,})db.Raw(""select sum(age) from users where role = ?"", ""admin"").Scan(&age)"
32,"You can also use GORM API to prepare SQL with DryRun Mode, and execute it with prepared statement later, checkout Session Mode for details"
32,"Select FieldsBy default GORM select all fields when querying, you can use Select to specify fields you want"
32,"db.Select(""Name"", ""Age"").Find(&Users{})"
32,Or define a smaller API struct to use the smart select fields feature
32,type User struct {
32,uint
32,Name
32,string
32,Age
32,int
32,Gender string
32,// hundreds of fields}type APIUser struct {
32,uint
32,"Name string}// Select `id`, `name` automatically when querydb.Model(&User{}).Limit(10).Find(&APIUser{})// SELECT `id`, `name` FROM `users` LIMIT 10"
32,Iteration / FindInBatchesQuery and process records with iteration or in batches
32,"Index HintsIndex is used to speed up data search and SQL query performance. Index Hints gives the optimizer information about how to choose indexes during query processing, which gives the flexibility to choose a more efficient execution plan than the optimizer"
32,"import ""gorm.io/hints""db.Clauses(hints.UseIndex(""idx_user_name"")).Find(&User{})// SELECT * FROM `users` USE INDEX (`idx_user_name`)db.Clauses(hints.ForceIndex(""idx_user_name"", ""idx_user_id"").ForJoin()).Find(&User{})// SELECT * FROM `users` FORCE INDEX FOR JOIN (`idx_user_name`,`idx_user_id`)""db.Clauses("
32,"hints.ForceIndex(""idx_user_name"", ""idx_user_id"").ForOrderBy(),"
32,"hints.IgnoreIndex(""idx_user_name"").ForGroupBy(),).Find(&User{})// SELECT * FROM `users` FORCE INDEX FOR ORDER BY (`idx_user_name`,`idx_user_id`) IGNORE INDEX FOR GROUP BY (`idx_user_name`)"""
32,"Read/Write SplittingIncrease data throughput through read/write splitting, check out Database Resolver"
32,Last updated: 2021-04-14
32,PrevNext
32,Platinum Sponsors
32,Become a Sponsor!
32,Platinum Sponsors
32,Become a Sponsor!
32,OpenCollective Sponsors
32,Contents
32,Disable Default TransactionCaches Prepared StatementSQL Builder with PreparedStmtSelect FieldsIteration / FindInBatchesIndex HintsRead/Write Splitting
32,Improve this page
32,Back to Top
32,Getting StartedOverviewDeclaring ModelsConnecting to DatabaseCRUD InterfaceCreateQueryAdvanced QueryUpdateDeleteRaw SQL & SQL BuilderAssociationsBelongs ToHas OneHas ManyMany To ManyAssociation ModePreloading (Eager Loading)TutorialsContextError HandlingMethod ChainingSessionHooksTransactionsMigrationLoggerGeneric Database InterfacePerformanceCustomize Data TypesScopesConventionsSettingsAdvanced TopicsDatabase ResolverPrometheusHintsIndexesConstraintsComposite Primary KeySecurityGORM ConfigWrite PluginsWrite DriverChangeLogCommunityContributeTranslate current site
32,© 2013~2021 Jinzhu
32,Documentation licensed under CC BY 4.0.
32,感谢 七牛云 对 CDN 的赞助，无闻 对域名 gorm.cn 的捐赠
32,浙ICP备2020033190号-1
32,Home
32,DocsCommunityAPIContribute
32,Getting StartedOverviewDeclaring ModelsConnecting to DatabaseCRUD InterfaceCreateQueryAdvanced QueryUpdateDeleteRaw SQL & SQL BuilderAssociationsBelongs ToHas OneHas ManyMany To ManyAssociation ModePreloading (Eager Loading)TutorialsContextError HandlingMethod ChainingSessionHooksTransactionsMigrationLoggerGeneric Database InterfacePerformanceCustomize Data TypesScopesConventionsSettingsAdvanced TopicsDatabase ResolverPrometheusHintsIndexesConstraintsComposite Primary KeySecurityGORM ConfigWrite PluginsWrite DriverChangeLogCommunityContributeTranslate current site
32,English
32,English
32,简体中文
32,Deutsch
32,Español
32,bahasa Indonesia
32,Italiano
32,日本語
32,Русский
32,French
32,한국어
33,Top 10 performance tuning techniques for Amazon Redshift | AWS Big Data Blog
33,Click here to return to Amazon Web Services homepage
33,Contact Sales
33,Support
33,English
33,My Account
33,Create an AWS Account
33,Products
33,Solutions
33,Pricing
33,Documentation
33,Learn
33,Partner Network
33,AWS Marketplace
33,Customer Enablement
33,Events
33,Explore More
33,عربي
33,Bahasa Indonesia
33,Deutsch
33,English
33,Español
33,Français
33,Italiano
33,Português
33,Tiếng Việt
33,Türkçe
33,Ρусский
33,ไทย
33,日本語
33,한국어
33,中文 (简体)
33,中文 (繁體)
33,AWS Management Console
33,Account Settings
33,Billing & Cost Management
33,Security Credentials
33,AWS Personal Health Dashboard
33,Support Center
33,Knowledge Center
33,AWS Support Overview
33,Click here to return to Amazon Web Services homepage
33,Products
33,Solutions
33,Pricing
33,Introduction to AWS
33,Getting Started
33,Documentation
33,Training and Certification
33,Developer Center
33,Customer Success
33,Partner Network
33,AWS Marketplace
33,Support
33,Log into Console
33,Download the Mobile App
33,Blog Home
33,Category
33,Edition
33,Follow
33,Architecture
33,AWS Cost Management
33,AWS Partner Network
33,AWS Podcast
33,AWS Marketplace
33,AWS News
33,Big Data
33,Business Productivity
33,Compute
33,Contact Center
33,Containers
33,Database
33,Desktop & Application Streaming
33,Developer
33,DevOps
33,Enterprise Strategy
33,Front-End Web & Mobile
33,Game Tech
33,HPC
33,Infrastructure & Automation
33,Industries
33,Internet of Things
33,Machine Learning
33,Management & Governance
33,Media
33,Messaging & Targeting
33,Modernizing with AWS
33,Networking & Content Delivery
33,Open Source
33,Public Sector
33,Quantum Computing
33,Robotics
33,SAP
33,"Security, Identity, & Compliance"
33,Startups
33,Storage
33,Training & Certification
33,中国版
33,Édition Française
33,Deutsche Edition
33,日本版
33,한국 에디션
33,Edição em Português
33,Edición en Español
33,English Edition
33,Версия на русском
33,Edisi Bahasa Indonesia
33,Mailing List
33,RSS Feed
33,AWS Big Data Blog
33,Top 10 performance tuning techniques for Amazon Redshift
33,"Matt Scaer,"
33,"Manish Vazirani, and"
33,Tarun Chaudhary | on
33,28 AUG 2020 | in
33,"Amazon Redshift, Amazon Redshift, Analytics, AWS Big Data, Database |"
33,Permalink |
33,Comments |
33,Share
33,"Customers use Amazon Redshift for everything from accelerating existing database environments, to ingesting weblogs for big data analytics. Amazon Redshift is a fully managed, petabyte-scale, massively parallel data warehouse that offers simple operations and high performance. Amazon Redshift provides an open standard JDBC/ODBC driver interface, which allows you to connect your existing business intelligence (BI) tools and reuse existing analytics queries."
33,"Amazon Redshift can run any type of data model, from a production transaction system third-normal-form model to star and snowflake schemas, data vault, or simple flat tables."
33,This post takes you through the most common performance-related opportunities when adopting Amazon Redshift and gives you concrete guidance on how to optimize each one.
33,What’s new
33,"This post refreshes the Top 10 post from early 2019. We’re pleased to share the advances we’ve made since then, and want to highlight a few key points."
33,Query throughput is more important than query concurrency.
33,"Configuring concurrency, like memory management, can be relegated to Amazon Redshift’s internal ML models through Automatic WLM with Query Priorities. On production clusters across the fleet, we see the automated process assigning a much higher number of active statements for certain workloads, while a lower number for other types of use-cases. This is done to maximize throughput, a measure of how much work the Amazon Redshift cluster can do over a period of time. Examples are 300 queries a minute, or 1,500 SQL statements an hour. It’s recommended to focus on increasing throughput over concurrency, because throughput is the metric with much more direct impact on the cluster’s users."
33,"In addition to the optimized Automatic WLM settings to maximize throughput, the concurrency scaling functionality in Amazon Redshift extends the throughput capability of the cluster to up to 10 times greater than what’s delivered with the original cluster. The tenfold increase is a current soft limit, you can reach out to your account team to increase it."
33,Investing in the Amazon Redshift driver.
33,"AWS now recommends the Amazon Redshift JDBC or ODBC driver for improved performance. Each driver has optional configurations to further tune it for higher or lower number of statements, with either fewer or greater row counts in the result set."
33,Ease of use by automating all the common DBA tasks.
33,"In 2018, the SET DW “backronym” summarized the key considerations to drive performance (sort key, encoding, table maintenance, distribution, and workload management). Since then, Amazon Redshift has added automation to inform 100% of SET DW, absorbed table maintenance into the service’s (and no longer the user’s) responsibility, and enhanced out-of-the-box performance with smarter default settings. Amazon Redshift Advisor continuously monitors the cluster for additional optimization opportunities, even if the mission of a table changes over time. AWS publishes the benchmark used to quantify Amazon Redshift performance, so anyone can reproduce the results."
33,Scaling compute separately from storage with RA3 nodes and Amazon Redshift Spectrum.
33,"Although the convenient cluster building blocks of the Dense Compute and Dense Storage nodes continue to be available, you now have a variety of tools to further scale compute and storage separately. Amazon Redshift Managed Storage (the RA3 node family) allows for focusing on using the right amount of compute, without worrying about sizing for storage. Concurrency scaling lets you specify entire additional clusters of compute to be applied dynamically as-needed. Amazon Redshift Spectrum uses the functionally-infinite capacity of Amazon Simple Storage Service (Amazon S3) to support an on-demand compute layer up to 10 times the power of the main cluster, and is now bolstered with materialized view support."
33,Pause and resume feature to optimize cost of environments
33,"All Amazon Redshift clusters can use the pause and resume feature. For clusters created using On Demand, the per-second grain billing is stopped when the cluster is paused. Reserved Instance clusters can use the pause and resume feature to define access times or freeze a dataset at a point in time."
33,Tip #1: Precomputing results with Amazon Redshift materialized views
33,"Materialized views can significantly boost query performance for repeated and predictable analytical workloads such as dash-boarding, queries from BI tools, and extract, load, transform (ELT) data processing. Data engineers can easily create and maintain efficient data-processing pipelines with materialized views while seamlessly extending the performance benefits to data analysts and BI tools."
33,"Materialized views are especially useful for queries that are predictable and repeated over and over. Instead of performing resource-intensive queries on large tables, applications can query the pre-computed data stored in the materialized view."
33,"When the data in the base tables changes, you refresh the materialized view by issuing the Amazon Redshift SQL statement “refresh materialized view“. After issuing a refresh statement, your materialized view contains the same data as a regular view. Refreshes can be incremental or full refreshes (recompute). When possible, Amazon Redshift incrementally refreshes data that changed in the base tables since the materialized view was last refreshed."
33,"To demonstrate how it works, we can create an example schema to store sales information, each sale transaction and details about the store where the sales took place."
33,"To view the total amount of sales per city, we create a materialized view with the create materialized view SQL statement (city_sales) joining records from two tables and aggregating sales amount (sum(sales.amount)) per city (group by city):"
33,CREATE MATERIALIZED VIEW city_sales AS
33,"SELECT st.city, SUM(sa.amount) as total_sales"
33,"FROM sales sa, store st"
33,WHERE sa.store_id = st.id
33,GROUP BY st.city
33,"Now we can query the materialized view just like a regular view or table and issue statements like “SELECT city, total_sales FROM city_sales” to get the following results. The join between the two tables and the aggregate (sum and group by) are already computed, resulting in significantly less data to scan."
33,"When the data in the underlying base tables changes, the materialized view doesn’t automatically reflect those changes. You can refresh the data stored in the materialized view on demand with the latest changes from the base tables using the SQL refresh materialized view command. For example, see the following code:"
33,!-- let's add a row in the sales base table
33,"INSERT INTO sales (id, item, store_id, customer_id, amount)"
33,"VALUES(8, 'Gaming PC Super ProXXL', 1, 1, 3000);"
33,"SELECT city, total_sales FROM city_sales WHERE city = 'Paris'"
33,|city |total_sales|
33,|-----|-----------|
33,|Paris|
33,690|
33,!-- the new sale is not taken into account !!
33,-- let's refresh the materialized view
33,REFRESH MATERIALIZED VIEW city_sales;
33,"SELECT city, total_sales FROM city_sales WHERE city = 'Paris'"
33,|city |total_sales|
33,|-----|-----------|
33,|Paris|
33,3690|
33,!-- now the view has the latest sales data
33,The full code for this use case is available as a gist in GitHub.
33,"You can also extend the benefits of materialized views to external data in your Amazon S3 data lake and federated data sources. With materialized views, you can easily store and manage the pre-computed results of a SELECT statement referencing both external tables and Amazon Redshift tables. Subsequent queries referencing the materialized views run much faster because they use the pre-computed results stored in Amazon Redshift, instead of accessing the external tables. This also helps you reduce the associated costs of repeatedly accessing the external data sources, because you can only access them when you explicitly refresh the materialized views."
33,Tip #2: Handling bursts of workload with concurrency scaling and elastic resize
33,"The legacy, on-premises model requires you to estimate what the system will need 3-4 years in the future to make sure you’re leasing enough horsepower at the time of purchase. But the ability to resize a cluster allows for right-sizing your resources as you go. Amazon Redshift extends this ability with elastic resize and concurrency scaling."
33,"Elastic resize lets you quickly increase or decrease the number of compute nodes, doubling or halving the original cluster’s node count, or even change the node type. You can expand the cluster to provide additional processing power to accommodate an expected increase in workload, such as Black Friday for internet shopping, or a championship game for a team’s web business. Choose classic resize when you’re resizing to a configuration that isn’t available through elastic resize. Classic resize is slower but allows you to change the node type or expand beyond the doubling or halving size limitations of an elastic resize."
33,"Elastic resize completes in minutes and doesn’t require a cluster restart. For anticipated workload spikes that occur on a predictable schedule, you can automate the resize operation using the elastic resize scheduler feature on the Amazon Redshift console, the AWS Command Line Interface (AWS CLI), or API."
33,Concurrency scaling allows your Amazon Redshift cluster to add capacity dynamically in response to the workload arriving at the cluster.
33,"By default, concurrency scaling is disabled, and you can enable it for any workload management (WLM) queue to scale to a virtually unlimited number of concurrent queries, with consistently fast query performance. You can control the maximum number of concurrency scaling clusters allowed by setting the “max_concurrency_scaling_clusters” parameter value from 1 (default) to 10 (contact support to raise this soft limit). The free billing credits provided for concurrency scaling is often enough and the majority of customers using this feature don’t end up paying extra for it. For more information about the concurrency scaling billing model see Concurrency Scaling pricing."
33,"You can monitor and control the concurrency scaling usage and cost by creating daily, weekly, or monthly usage limits and instruct Amazon Redshift to automatically take action (such as logging, alerting or disabling further usage) if those limits are reached. For more information, see Managing usage limits in Amazon Redshift."
33,"Together, these options open up new ways to right-size the platform to meet demand. Before these options, you needed to size your WLM queue, or even an entire Amazon Redshift cluster, beforehand in anticipation of upcoming peaks."
33,Tip #3: Using the Amazon Redshift Advisor to minimize administrative work
33,Amazon Redshift Advisor offers recommendations specific to your Amazon Redshift cluster to help you improve its performance and decrease operating costs.
33,"Advisor bases its recommendations on observations regarding performance statistics or operations data. Advisor develops observations by running tests on your clusters to determine if a test value is within a specified range. If the test result is outside of that range, Advisor generates an observation for your cluster. At the same time, Advisor creates a recommendation about how to bring the observed value back into the best-practice range. Advisor only displays recommendations that can have a significant impact on performance and operations. When Advisor determines that a recommendation has been addressed, it removes it from your recommendation list. In this section, we share some examples of Advisor recommendations:"
33,Distribution key recommendation
33,"Advisor analyzes your cluster’s workload to identify the most appropriate distribution key for the tables that can significantly benefit from a KEY distribution style. Advisor provides ALTER TABLE statements that alter the DISTSTYLE and DISTKEY of a table based on its analysis. To realize a significant performance benefit, make sure to implement all SQL statements within a recommendation group."
33,The following screenshot shows recommendations regarding distribution keys.
33,"If you don’t see a recommendation, that doesn’t necessarily mean that the current distribution styles are the most appropriate. Advisor doesn’t provide recommendations when there isn’t enough data or the expected benefit of redistribution is small."
33,Sort key recommendation
33,"Sorting a table on an appropriate sort key can accelerate query performance, especially queries with range-restricted predicates, by requiring fewer table blocks to be read from disk."
33,Advisor analyzes your cluster’s workload over several days to identify a beneficial sort key for your tables. See the following screenshot.
33,"If you don’t see a recommendation for a table, that doesn’t necessarily mean that the current configuration is the best. Advisor doesn’t provide recommendations when there isn’t enough data or the expected benefit of sorting is small."
33,Table compression recommendation
33,"Amazon Redshift is optimized to reduce your storage footprint and improve query performance by using compression encodings. When you don’t use compression, data consumes additional space and requires additional disk I/O. Applying compression to large uncompressed columns can have a big impact on your cluster."
33,The compression analysis in Advisor tracks uncompressed storage allocated to permanent user tables. It reviews storage metadata associated with large uncompressed columns that aren’t sort key columns.
33,The following screenshot shows an example of table compression recommendation.
33,Table statistics recommendation
33,"Maintaining current statistics helps complex queries run in the shortest possible time. The Advisor analysis tracks tables whose statistics are out-of-date or missing. It reviews table access metadata associated with complex queries. If tables that are frequently accessed with complex patterns are missing statistics, Amazon Redshift Advisor creates a critical recommendation to run ANALYZE. If tables that are frequently accessed with complex patterns have out-of-date statistics, Advisor creates a suggested recommendation to run ANALYZE."
33,The following screenshot shows a table statistics recommendation.
33,Tip #4: Using Auto WLM with priorities to increase throughput
33,"Auto WLM simplifies workload management and maximizes query throughput by using ML to dynamically manage memory and concurrency, which ensures optimal utilization of the cluster resources"
33,Amazon Redshift runs queries using the queuing system (WLM). You can define up to eight queues to separate workloads from each other.
33,Amazon Redshift Advisor automatically analyzes the current WLM usage and can make recommendations to get more throughput from your cluster. Periodically reviewing the suggestions from Advisor helps you get the best performance.
33,"Query priorities is a feature of Auto WLM that lets you assign priority ranks to different user groups or query groups, to ensure that higher priority workloads get more resources for consistent query performance, even during busy times. It is a good practice to set up query monitoring rules (QMR) to monitor and manage resource intensive or runaway queries. QMR also enables you to dynamically change a query’s priority based on its runtime performance and metrics-based rules you define."
33,"For more information on migrating from manual to automatic WLM with query priorities, see Modifying the WLM configuration."
33,"It’s recommended to take advantage of Amazon Redshift’s short query acceleration (SQA). SQA uses ML to run short-running jobs in their own queue. This keeps small jobs processing, rather than waiting behind longer-running SQL statements. SQA is enabled by default in the default parameter group and for all new parameter groups. You can enable and disable SQA via a check box on the Amazon Redshift console, or by using the Amazon Redshift CLI."
33,"If you enable concurrency scaling, Amazon Redshift can automatically and quickly provision additional clusters should your workload begin to back up. This is an important consideration when deciding the cluster’s WLM configuration."
33,"A common pattern is to optimize the WLM configuration to run most SQL statements without the assistance of supplemental memory, reserving additional processing power for short jobs. Some queueing is acceptable because additional clusters spin up if your needs suddenly expand. To enable concurrency scaling on a WLM queue, set the concurrency scaling mode value to AUTO. You can best inform your decisions by reviewing the concurrency scaling billing model. You can also monitor and control the concurrency scaling usage and cost by using the Amazon Redshift usage limit feature."
33,"In some cases, unless you enable concurrency scaling for the queue, the user or query’s assigned queue may be busy, and you must wait for a queue slot to open. During this time, the system isn’t running the query at all. If this becomes a frequent problem, you may have to increase concurrency."
33,"First, determine if any queries are queuing, using the queuing_queries.sql admin script. Review the maximum concurrency that your cluster needed in the past with wlm_apex.sql, or get an hour-by-hour historical analysis with wlm_apex_hourly.sql. Keep in mind that increasing concurrency allows more queries to run, but each query gets a smaller share of the memory. You may find that by increasing concurrency, some queries must use temporary disk storage to complete, which is also sub-optimal."
33,Tip #5: Taking advantage of Amazon Redshift data lake integration
33,Amazon Redshift is tightly integrated with other AWS-native services such as Amazon S3 which let’s the Amazon Redshift cluster interact with the data lake in several useful ways.
33,"Amazon Redshift Spectrum lets you query data directly from files on Amazon S3 through an independent, elastically sized compute layer. Use these patterns independently or apply them together to offload work to the Amazon Redshift Spectrum compute layer, quickly create a transformed or aggregated dataset, or eliminate entire steps in a traditional ETL process."
33,"Use the Amazon Redshift Spectrum compute layer to offload workloads from the main cluster, and apply more processing power to the specific SQL statement. Amazon Redshift Spectrum automatically assigns compute power up to approximately 10 times the processing power of the main cluster. This may be an effective way to quickly process large transform or aggregate jobs."
33,"Skip the load in an ELT process and run the transform directly against data on Amazon S3. You can run transform logic against partitioned, columnar data on Amazon S3 with an INSERT … SELECT statement. It’s easier than going through the extra work of loading a staging dataset, joining it to other tables, and running a transform against it."
33,"Use Amazon Redshift Spectrum to run queries as the data lands in Amazon S3, rather than adding a step to load the data onto the main cluster. This allows for real-time analytics."
33,"Land the output of a staging or transformation cluster on Amazon S3 in a partitioned, columnar format. The main or reporting cluster can either query from that Amazon S3 dataset directly or load it via an INSERT … SELECT statement."
33,"Within Amazon Redshift itself, you can export the data into the data lake with the UNLOAD command, or by writing to external tables. Both options export SQL statement output to Amazon S3 in a massively parallel fashion. You can do the following:"
33,"Using familiar CREATE EXTERNAL TABLE AS SELECT and INSERT INTO SQL commands, create and populate external tables on Amazon S3 for subsequent use by Amazon Redshift or other services participating in the data lake without the need to manually maintain partitions. Materialized views can also cover external tables, further enhancing the accessibility and utility of the data lake."
33,"Using the UNLOAD command, Amazon Redshift can export SQL statement output to Amazon S3 in a massively parallel fashion. This technique greatly improves the export performance and lessens the impact of running the data through the leader node. You can compress the exported data on its way off the Amazon Redshift cluster. As the size of the output grows, so does the benefit of using this feature. For writing columnar data to the data lake, UNLOAD can write partition-aware Parquet data."
33,Tip #6: Improving the efficiency of temporary tables
33,"Amazon Redshift provides temporary tables, which act like normal tables but have a lifetime of a single SQL session. The proper use of temporary tables can significantly improve performance of some ETL operations. Unlike regular permanent tables, data changes made to temporary tables don’t trigger automatic incremental backups to Amazon S3, and they don’t require synchronous block mirroring to store a redundant copy of data on a different compute node. Due to these reasons, data ingestion on temporary tables involves reduced overhead and performs much faster. For transient storage needs like staging tables, temporary tables are ideal."
33,"You can create temporary tables using the CREATE TEMPORARY TABLE syntax, or by issuing a SELECT … INTO #TEMP_TABLE query. The CREATE TABLE statement gives you complete control over the definition of the temporary table. The SELECT … INTO and C(T)TAS commands use the input data to determine column names, sizes and data types, and use default storage properties. Consider default storage properties carefully, because they may cause problems. By default, for temporary tables, Amazon Redshift applies EVEN table distribution with no column encoding (such as RAW compression) for all columns. This data structure is sub-optimal for many types of queries."
33,"If you employ the SELECT…INTO syntax, you can’t set the column encoding, column distribution, or sort keys. The CREATE TABLE AS (CTAS) syntax instead lets you specify a distribution style and sort keys, and Amazon Redshift automatically applies LZO encoding for everything other than sort keys, Booleans, reals, and doubles. You can exert additional control by using the CREATE TABLE syntax rather than CTAS."
33,"If you create temporary tables, remember to convert all SELECT…INTO syntax into the CREATE statement. This ensures that your temporary tables have column encodings and don’t cause distribution errors within your workflow. For example, you may want to convert a statement using this syntax:"
33,"SELECT column_a, column_b INTO #my_temp_table FROM my_table;"
33,You need to analyze the temporary table for optimal column encoding:
33,Master=# analyze compression #my_temp_table;
33,Table | Column | Encoding
33,----------------+----------+---------
33,#my_temp_table | columb_a | lzo
33,#my_temp_table | columb_b | bytedict
33,(2 rows)
33,You can then convert the SELECT INTO a statement to the following:
33,BEGIN;
33,CREATE TEMPORARY TABLE my_temp_table(
33,"column_a varchar(128) encode lzo,"
33,column_b char(4) encode bytedict)
33,distkey (column_a) -- Assuming you intend to join this table on column_a
33,sortkey (column_b) -- Assuming you are sorting or grouping by column_b
33,"INSERT INTO my_temp_table SELECT column_a, column_b FROM my_table;"
33,COMMIT;
33,"If you create a temporary staging table by using a CREATE TABLE LIKE statement, the staging table inherits the distribution key, sort keys, and column encodings from the parent target table. In this case, merge operations that join the staging and target tables on the same distribution key performs faster because the joining rows are collocated. To verify that the query uses a collocated join, run the query with EXPLAIN and check for DS_DIST_NONE on all the joins."
33,"You may also want to analyze statistics on the temporary table, especially when you use it as a join table for subsequent queries. See the following code:"
33,ANALYZE my_temp_table;
33,"With this trick, you retain the functionality of temporary tables but control data placement on the cluster through distribution key assignment. You also take advantage of the columnar nature of Amazon Redshift by using column encoding."
33,Tip #7: Using QMR and Amazon CloudWatch metrics to drive additional performance improvements
33,"In addition to the Amazon Redshift Advisor recommendations, you can get performance insights through other channels."
33,"The Amazon Redshift cluster continuously and automatically collects query monitoring rules metrics, whether you institute any rules on the cluster or not. This convenient mechanism lets you view attributes like the following:"
33,The CPU time for a SQL statement (query_cpu_time)
33,The amount of temporary space a job might ‘spill to disk’ (query_temp_blocks_to_disk)
33,The ratio of the highest number of blocks read over the average (io_skew)
33,"It also makes Amazon Redshift Spectrum metrics available, such as the number of Amazon Redshift Spectrum rows and MBs scanned by a query (spectrum_scan_row_count and spectrum_scan_size_mb, respectively). The Amazon Redshift system view SVL_QUERY_METRICS_SUMMARY shows the maximum values of metrics for completed queries, and STL_QUERY_METRICS and STV_QUERY_METRICS carry the information at 1-second intervals for the completed and running queries respectively."
33,"The Amazon Redshift CloudWatch metrics are data points for use with Amazon CloudWatch monitoring. These can be cluster-wide metrics, such as health status or read/write, IOPS, latency, or throughput. It also offers compute node–level data, such as network transmit/receive throughput and read/write latency. At the WLM queue grain, there are the number of queries completed per second, queue length, and others. CloudWatch facilitates monitoring concurrency scaling usage with the metrics ConcurrencyScalingSeconds and ConcurrencyScalingActiveClusters."
33,"It’s recommended to consider the CloudWatch metrics (and the existing notification infrastructure built around them) before investing time in creating something new. Similarly, the QMR metrics cover most metric use cases and likely eliminate the need to write custom metrics."
33,"Tip #8: Federated queries connect the OLAP, OLTP and data lake worlds"
33,"The new Federated Query feature in Amazon Redshift allows you to run analytics directly against live data residing on your OLTP source system databases and Amazon S3 data lake, without the overhead of performing ETL and ingesting source data into Amazon Redshift tables. This feature gives you a convenient and efficient option for providing realtime data visibility on operational reports, as an alternative to micro-ETL batch ingestion of realtime data into the data warehouse. By combining historical trend data from the data warehouse with live developing trends from the source systems, you can gather valuable insights to drive real-time business decision making."
33,"For example, consider sales data residing in three different data stores:"
33,Live sales order data stored on an Amazon RDS for PostgreSQL database (represented as “ext_postgres” in the following external schema)
33,Historical sales data warehoused in a local Amazon Redshift database (represented as “local_dwh”)
33,"Archived, “cold” sales data older than 5 years stored on Amazon S3 (represented as “ext_spectrum”)"
33,We can create a late binding view in Amazon Redshift that allows you to merge and query data from all three sources. See the following code:
33,CREATE VIEW store_sales_integrated AS
33,SELECT * FROM ext_postgres.store_sales_live
33,UNION ALL
33,SELECT * FROM local_dwh.store_sales_current
33,UNION ALL
33,"SELECT ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_customer_sk, ss_cdemo_sk,"
33,"ss_hdemo_sk, ss_addr_sk, ss_store_sk, ss_promo_sk, ss_ticket_number, ss_quantity,"
33,"ss_wholesale_cost, ss_list_price, ss_sales_price, ss_ext_discount_amt,"
33,"ss_ext_sales_price, ss_ext_wholesale_cost, ss_ext_list_price, ss_ext_tax,"
33,"ss_coupon_amt, ss_net_paid, ss_net_paid_inc_tax, ss_net_profit"
33,FROM ext_spectrum.store_sales_historical
33,WITH NO SCHEMA BINDING
33,"Currently, direct federated querying is supported for data stored in Amazon Aurora PostgreSQL and Amazon RDS for PostgreSQL databases, with support for other major RDS engines coming soon. You can also use the federated query feature to simplify the ETL and data-ingestion process. Instead of staging data on Amazon S3, and performing a COPY operation, federated queries allow you to ingest data directly into an Amazon Redshift table in one step, as part of a federated CTAS/INSERT SQL query."
33,"For example, the following code shows an upsert/merge operation in which the COPY operation from Amazon S3 to Amazon Redshift is replaced with a federated query sourced directly from PostgreSQL:"
33,BEGIN;
33,CREATE TEMP TABLE staging (LIKE ods.store_sales);
33,-- replace the following COPY from S3:
33,/*COPY staging FROM 's3://yourETLbucket/daily_store_sales/'
33,IAM_ROLE 'arn:aws:iam::<account_id>:role/<s3_reader_role>'
33,DELIMITER '|' COMPUPDATE OFF; */
33,-- with this federated query to load staging data directly from PostgreSQL source
33,INSERT INTO staging SELECT * FROM pg.store_sales p
33,WHERE p.last_updated_date > (SELECT MAX(last_updated_date) FROM ods.store_sales);
33,DELETE FROM ods.store_sales USING staging s WHERE ods.store_sales.id = s.id;
33,INSERT INTO ods.store_sales SELECT * FROM staging;
33,DROP TABLE staging;
33,COMMIT;
33,"For more information about setting up the preceding federated queries, see Build a Simplified ETL and Live Data Query Solution using Redshift Federated Query. For additional tips and best practices on federated queries, see Best practices for Amazon Redshift Federated Query."
33,Tip #9: Maintaining efficient data loads
33,"Amazon Redshift best practices suggest using the COPY command to perform data loads of file-based data. Single-row INSERTs are an anti-pattern. The COPY operation uses all the compute nodes in your cluster to load data in parallel, from sources such as Amazon S3, Amazon DynamoDB, Amazon EMR HDFS file systems, or any SSH connection."
33,"When performing data loads, compress the data files whenever possible. For row-oriented (CSV) data, Amazon Redshift supports both GZIP and LZO compression. It’s more efficient to load a large number of small files than one large one, and the ideal file count is a multiple of the cluster’s total slice count. Columnar data, such as Parquet and ORC, is also supported. You can achieve best performance when the compressed files are between 1MB-1GB each."
33,"The number of slices per node depends on the cluster’s node size (and potentially elastic resize history). By ensuring an equal number of files per slice, you know that the COPY command evenly uses cluster resources and complete as quickly as possible. Query for the cluster’s current slice count with SELECT COUNT(*) AS number_of_slices FROM stv_slices;."
33,"Another script in the amazon-redshift-utils GitHub repo, CopyPerformance, calculates statistics for each load. Amazon Redshift Advisor also warns of missing compression or too few files based on the number of slices (see the following screenshot):"
33,"Conducting COPY operations efficiently reduces the time to results for downstream users, and minimizes the cluster resources utilized to perform the load."
33,Tip #10: Using the latest Amazon Redshift drivers from AWS
33,"Because Amazon Redshift is based on PostgreSQL, we previously recommended using JDBC4 PostgreSQL driver version 8.4.703 and psql ODBC version 9.x drivers. If you’re currently using those drivers, we recommend moving to the new Amazon Redshift–specific drivers. For more information about drivers and configuring connections, see JDBC and ODBC drivers for Amazon Redshift in the Amazon Redshift Cluster Management Guide."
33,"While rarely necessary, the Amazon Redshift drivers do permit some parameter tuning that may be useful in some circumstances. Downstream third-party applications often have their own best practices for driver tuning that may lead to additional performance gains."
33,"For JDBC, consider the following:"
33,"To avoid client-side out-of-memory errors when retrieving large data sets using JDBC, you can enable your client to fetch data in batches by setting the JDBC fetch size parameter or BlockingRowsMode."
33,"Amazon Redshift doesn’t recognize the JDBC maxRows parameter. Instead, specify a LIMIT clause to restrict the result set. You can also use an OFFSET clause to skip to a specific starting point in the result set."
33,"For ODBC, consider the following:"
33,A cursor is enabled on the cluster’s leader node when useDelareFecth is enabled. The cursor fetches up to fetchsize/cursorsize and then waits to fetch more rows when the application request more rows.
33,"The CURSOR command is an explicit directive that the application uses to manipulate cursor behavior on the leader node. Unlike the JDBC driver, the ODBC driver doesn’t have a BlockingRowsMode mechanism."
33,It’s recommended that you do not undertake driver tuning unless you have a clear need. AWS Support is available to help on this topic as well.
33,Conclusion
33,"Amazon Redshift is a powerful, fully managed data warehouse that can offer increased performance and lower cost in the cloud. As Amazon Redshift grows based on the feedback from its tens of thousands of active customers world-wide, it continues to become easier to use and extend its price-for-performance value proposition. Staying abreast of these improvements can help you get more value (with less effort) from this core AWS service."
33,We hope you learned a great deal about making the most of your Amazon Redshift account with the resources in this post.
33,"If you have questions or suggestions, please leave a comment."
33,About the Authors
33,"Matt Scaer is a Principal Data Warehousing Specialist Solution Architect, with over 20 years of data warehousing experience, with 11+ years at both AWS and Amazon.com."
33,Manish Vazirani is an Analytics Specialist Solutions Architect at Amazon Web Services.
33,Tarun Chaudhary is an Analytics Specialist Solutions Architect at AWS.
33,TAGS:
33,Amazon Redshift
33,View Comments
33,Resources
33,Amazon Athena
33,Amazon EMR
33,Amazon Kinesis
33,Amazon MSK
33,Amazon QuickSight
33,Amazon Redshift
33,AWS Glue
33,Follow
33,Twitter
33,Facebook
33,LinkedIn
33,Twitch
33,Email Updates
33,Sign In to the Console
33,Learn About AWS
33,What Is AWS?
33,What Is Cloud Computing?
33,"AWS Inclusion, Diversity & Equity"
33,What Is DevOps?
33,What Is a Container?
33,What Is a Data Lake?
33,AWS Cloud Security
33,What's New
33,Blogs
33,Press Releases
33,Resources for AWS
33,Getting Started
33,Training and Certification
33,AWS Solutions Portfolio
33,Architecture Center
33,Product and Technical FAQs
33,Analyst Reports
33,AWS Partner Network
33,Developers on AWS
33,Developer Center
33,SDKs & Tools
33,.NET on AWS
33,Python on AWS
33,Java on AWS
33,PHP on AWS
33,Javascript on AWS
33,Help
33,Contact Us
33,AWS Careers
33,File a Support Ticket
33,Knowledge Center
33,AWS Support Overview
33,Legal
33,Create an AWS Account
33,Amazon is an Equal Opportunity Employer:
33,Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.
33,Language
33,عربي
33,Bahasa Indonesia
33,Deutsch
33,English
33,Español
33,Français
33,Italiano
33,Português
33,Tiếng Việt
33,Türkçe
33,Ρусский
33,ไทย
33,日本語
33,한국어
33,中文 (简体)
33,中文 (繁體)
33,Privacy
33,Site Terms
33,Cookie Preferences
33,"© 2021, Amazon Web Services, Inc. or its affiliates. All rights reserved."
34,Performance Tuning Guide for Cisco UCS M5 Servers White Paper - Cisco
34,Skip to content
34,Skip to footer
34,Cisco.com Worldwide
34,MENU
34,CLOSE
34,Products
34,Support & Learn
34,Partners
34,Events & Videos
34,Search
34,Log In
34,Log Out
34,Choose Language Selection
34,More
34,Log In
34,Log Out
34,Log In
34,Have an account?
34,Personalized content
34,Your products and support
34,Log In
34,Forgot your user ID and/or password?
34,Manage account
34,Need an account?
34,Create an account
34,Help
34,Choose Language Selection
34,Choose Language Selection
34,Products
34,Support
34,Partners
34,Events & Videos
34,Products & ServicesServers - Unified ComputingCisco UCS B-Series Blade ServersWhite Papers
34,Performance Tuning Guide for Cisco UCS M5 Servers White Paper
34,White Paper
34,Download
34,Print
34,Available Languages
34,Download Options
34,PDF
34,(3.7 MB)
34,View with Adobe Reader on a variety of devices
34,"Updated:February 4, 2021"
34,See Revisions
34,Contact Cisco
34,Get a call from Sales
34,Product / Technical Support
34,Training & Certification
34,1-800-553-6387
34,US/CAN | 5am-5pm PT
34,Download
34,Print
34,Available Languages
34,Download Options
34,PDF
34,(3.7 MB)
34,View with Adobe Reader on a variety of devices
34,"Updated:February 4, 2021"
34,See Revisions
34,Table of Contents
34,Purpose and scope
34,What you will learn
34,BIOS tuning scenarios
34,High performance
34,Low latency
34,Cisco UCS BIOS options
34,Processor configuration
34,Power technology
34,Energy performance
34,Workload configuration
34,Memory settings
34,Configuring the BIOS for optimized CPU hardware power management
34,Operating system tuning guidance for best performance
34,BIOS recommendations for various workload types
34,Online transaction processing workloads
34,Virtualization workloads
34,High-performance computing workloads
34,Java Enterprise Edition application server workloads
34,Analytics database decision-support system workloads
34,Conclusion
34,For more information
34,Purpose and scope
34,The Basic Input and Output System (BIOS) tests and initializes the hardware components of a system and boots the operating system from a storage device. A typical computational system has several BIOS settings that control the system’s behavior. Some of these settings are directly related to the performance of the system.
34,"This document explains the BIOS settings that are valid for the Cisco Unified Computing System™ (Cisco UCS®) M5 server generation (Cisco UCS B200 and B480 M5 Blade Servers and C220, C240, and C480 M5 Rack Servers) using Intel® Xeon® Scalable processor family CPUs. It describes how to optimize the BIOS settings to meet requirements for the best performance and energy efficiency for the Cisco UCS M5 generation of blade and rack servers."
34,This document also discusses the BIOS settings that can be selected for various workload types on Cisco UCS M5 servers that use Intel Xeon Scalable processor family CPUs. Understanding the BIOS options will help you select appropriate values to achieve optimal system performance.
34,This document does not discuss the BIOS options for specific firmware releases of Cisco UCS servers. The settings demonstrated here are generic.
34,What you will learn
34,"The process of setting performance options in your system BIOS can be daunting and confusing, and some of the options you can choose are obscure. For most options, you must choose between optimizing a server for power savings or for performance. This document provides some general guidelines and suggestions to help you achieve optimal performance from your Cisco UCS blade and rack servers that use Intel Xeon Scalable processor family CPUs."
34,BIOS tuning scenarios
34,This document focuses on two main scenarios: how to tune the BIOS for high performance and how to tune it for low latency.
34,High performance
34,"With the latest multiprocessor, multicore, and multithreading technologies in conjunction with current operating systems and applications, today's Cisco UCS servers based on the Intel Xeon Scalable processor deliver the highest levels of performance, as demonstrated by the numerous industry-standard benchmark publications from the Standard Performance Evaluation Corporation (SPEC), SAP, and the Transaction Processing Performance Council (TPC)."
34,"Cisco UCS servers with standard settings already provide an optimal ratio of performance to energy efficiency. However, through BIOS settings you can further optimize the system with higher performance and less energy efficiency. Basically, this optimization operates all the components in the system at the maximum speed possible and prevents the energy-saving options from slowing down the system. In general, optimization to achieve greater performance is associated with increased consumption of electrical power. This document explains how to configure the BIOS settings to achieve optimal computing performance."
34,Low latency
34,"The BIOS offers a variety of options to reduce latency. In some cases, the corresponding application does not make efficient use of all the threads available in the hardware. To improve performance, you can disable threads that are not needed (hyperthreading) or even cores in the BIOS to reduce the small fluctuations in the performance of computing operations that especially occur in some High-Performance Computing (HPC) applications and analytical database applications. Furthermore, by disabling cores that are not needed, you can improve turbo-mode performance in the remaining cores under certain operating conditions."
34,"However, other scenarios require performance that is as constant as possible. Although the current generation of Intel processors delivers better turbo-mode performance than the preceding generation, the maximum turbo-mode frequency is not guaranteed under certain operating conditions. In such cases, disabling the turbo mode can help prevent changes in frequency."
34,"Energy-saving functions, whose aim is to save energy whenever possible through frequency and voltage reduction and through the disabling of certain function blocks and components, also have a negative impact on response time. The higher the settings for the energy saving modes, the lower the performance. Furthermore, in each energy-saving mode, the processor requires a certain amount of time to change back from reduced performance to maximum performance."
34,"This document explains how to configure the power and energy saving modes to reduce system latency. The optimization of server latency, particularly in an idle state, results in substantially greater consumption of electrical power."
34,Cisco UCS BIOS options
34,This section describes the options you can configure in the Cisco UCS BIOS.
34,Processor configuration
34,This section describes processor options you can configure.
34,Enhanced Intel SpeedStep Technology
34,Intel SpeedStep Technology is designed to save energy by adjusting the CPU clock frequency up or down depending on how busy the system is. Intel Turbo Boost Technology provides the capability for the CPU to adjust itself to run higher than its stated clock speed if it has enough power to do so.
34,"You can specify whether the processor uses Enhanced Intel SpeedStep Technology, which allows the system to dynamically adjust processor voltage and core frequency. This technology can result in decreased average power consumption and decreased average heat production."
34,The setting can be one of the following:
34,●     Disabled: The processor never dynamically adjusts its voltage or frequency.
34,●     Enabled: The processor uses Enhanced Intel SpeedStep Technology and enables all supported processor sleep states to further conserve power.
34,Intel Turbo Boost Technology
34,"Intel Turbo Boost Technology depends on Intel SpeedStep: if you want to enable Intel Turbo Boost, you must enable Intel SpeedStep first. If you disable Intel SpeedStep, you lose the capability to use Intel Turbo Boost."
34,"Intel Turbo Boost is especially useful for latency-sensitive applications and for scenarios in which the system is nearing saturation and would benefit from a temporary increase in the CPU speed. If your system is not running at this saturation level and you want the best performance at a utilization rate of less than 90 percent, you should disable Intel SpeedStep to help ensure that the system is running at its stated clock speed at all times."
34,Intel Hyper-Threading Technology
34,"You can specify whether the processor uses Intel Hyper-Threading Technology, which allows multithreaded software applications to process threads in parallel within each processor. You should test the CPU hyperthreading option both enabled and disabled in your specific environment. If you are running a single-threaded application, you should disable hyperthreading."
34,The setting can be one of the following:
34,●     Disabled: The processor does not permit hyperthreading.
34,●     Enabled: The processor allows parallel processing of multiple threads.
34,Core multiprocessing and latency-sensitive single-threaded applications
34,The core multiprocessing option is designed to enable the user to disable cores. This option may affect the pricing of certain software packages that are licensed by the core. You should consult your software license and software vendor about whether disabling cores qualifies you for any particular pricing policies. Set core multiprocessing to All if pricing policy is not an issue for you.
34,"For latency-sensitive single-threaded applications, you can optimize performance by disabling unnecessary cores, disabling hyperthreading, enabling all C-states, enabling Intel SpeedStep, and enabling Intel Turbo Boost. With this configuration, the remaining cores often will benefit from higher turbo speeds and better use of the shared Layer 3 cache."
34,CPU performance
34,"Intel Xeon processors have several layers of cache. Each core has a tiny Layer 1 cache, sometimes referred to as the Data Cache Unit (DCU), that has 32 KB for instructions and 32 KB for data. Slightly bigger is the Layer 2 cache, with 256 KB shared between data and instructions for each core. In addition, all cores on a chip share a much larger Layer 3 cache, which is about 10 to 45 MB in size (depending on the processor model and number of cores)."
34,The prefetcher settings provided by Intel primarily affect the Layer 1 and Layer 2 caches on a processor core (Table 1). You will likely need to perform some testing with your individual workload to find the combination that works best for you. Testing on the Intel Xeon Scalable processor has shown that most applications run best with all prefetchers enabled. See Tables 2 and 3 for guidance.
34,Table 1.           CPU performance and prefetch options from Intel
34,Performance option
34,Cache affected
34,Hardware prefetcher
34,Layer 2
34,Adjacent-cache-line prefetcher
34,Layer 2
34,DCU prefetcher
34,Layer 1
34,DCU instruction pointer (DCU-IP) prefetcher
34,Layer 1
34,Table 2.           Cisco UCS CPU performance options
34,Option
34,Description
34,CPU performance
34,Sets the CPU performance profile for the server. This can be one of the following:
34,●  Enterprise: All prefetchers are enabled.
34,"●  High throughput: DCU IP prefetcher are enabled, and all other prefetchers are disabled."
34,●  HPC: All prefetchers are enabled. This setting is also known as HPC.
34,●  Platform default: The BIOS uses the value for this attribute contained in the BIOS defaults for the server type and vendor.
34,Table 3.           Cisco UCS CPU prefetcher options and target benchmarks and workloads
34,Prefetchers
34,Target benchmarks and workloads
34,All enabled
34,"HPC benchmarks, web server, SAP application server, virtualization, and TPC-E"
34,DCU-IP enabled; all others disabled
34,SPECjbb2005 and certain server-side Java application-server applications
34,Hardware prefetcher
34,"The hardware prefetcher prefetches additional streams of instructions and data into the Layer 2 cache upon detection of an access stride. This behavior is more likely to occur during operations that sort through sequential data, such as database table scans and clustered index scans, or that run a tight loop in code."
34,You can specify whether the processor allows the Intel hardware prefetcher to fetch streams of data and instructions from memory into the unified second-level cache when necessary.
34,The setting can be one of the following:
34,●     Disabled: The hardware prefetcher is not used.
34,●     Enabled: The processor uses the hardware prefetcher when cache problems are detected.
34,Adjacent-cache-line prefetcher
34,"The adjacent-cache-line prefetcher always prefetches the next cache line. Although this approach works well when data is accessed sequentially in memory, it can quickly litter the small Layer 2 cache with unneeded instructions and data if the system is not accessing data sequentially, causing frequently accessed instructions and code to leave the cache to make room for the adjacent-line data or instructions."
34,You can specify whether the processor fetches cache lines in even or odd pairs instead of fetching just the required line.
34,The setting can be one of the following:
34,●     Disabled: The processor fetches only the required line.
34,●     Enabled: The processor fetches both the required line and its paired line.
34,DCU streamer prefetcher
34,"Like the hardware prefetcher, the DCU streamer prefetcher prefetches additional streams of instructions or data upon detection of an access stride; however, it stores the streams in the tiny Layer 1 cache instead of the Layer 2 cache."
34,"This prefetcher is a Layer 1 data cache prefetcher. It detects multiple loads from the same cache line that occur within a time limit. Making the assumption that the next cache line is also required, the prefetcher loads the next line in advance to the Layer 1 cache from the Layer 2 cache or the main memory."
34,The setting can be one of the following:
34,●     Disabled: The processor does not try to anticipate cache read requirements and fetches only explicitly requested lines.
34,●     Enabled: The DCU prefetcher analyzes the cache read pattern and prefetches the next line in the cache if it determines that it may be needed.
34,DCU-IP prefetcher
34,The DCU-IP prefetcher predictably prefetches data into the Layer 1 cache on the basis of the recent instruction pointer load instruction history.
34,You can specify whether the processor uses the DCU-IP prefetch mechanism to analyze historical cache access patterns and preload the most relevant lines in the Layer 1 cache.
34,The setting can be one of the following:
34,●     Disabled: The processor does not preload any cache data.
34,●     Enabled: The DCU-IP prefetcher preloads the Layer 1 cache with the data it determines to be the most relevant.
34,Low-level cache prefetch
34,"This BIOS option configures the processor’s Last-Level Cache (LLC) prefetch feature as a result of the noninclusive cache architecture. The LLC prefetcher exists on top of other prefetchers that can prefetch data into the core DCU and Mid-Level Cache (MLC). In some cases, setting this option to disabled can improve performance."
34,The setting for this BIOS option can be one of the following:
34,●     Disabled: The LLC prefetcher is disabled. The other core prefetchers are not affected.
34,●     Enabled: The core prefetcher can prefetch data directly to the LLC.
34,"By default, the LLC prefetch option is disabled."
34,Direct cache access
34,"The Direct-Cache Access (DCA) mechanism is a system-level protocol in a multiprocessor system to improve I/O network performance, thereby providing higher system performance. The basic goal is to reduce cache misses when a demand read operation is performed. This goal is accomplished by placing the data from the I/O devices directly into the CPU cache through hints to the processor to perform a data prefetch operation and install the data in its local caches."
34,Execute Disable Bit feature
34,"The Intel Execute Disable Bit feature classifies memory areas on the server to specify where the application code can run. As a result of this classification, the processor disables code processing if a malicious worm attempts to insert code in the buffer. This setting helps prevent damage, worm propagation, and certain classes of malicious buffer overflow attacks."
34,The setting can be one of the following:
34,●     Disabled: The processor does not classify memory areas.
34,●     Enabled: The processor classifies memory areas.
34,Intel VT for Directed I/O
34,"You can specify whether the processor uses Intel Virtualization Technology (VT) for Directed I/O (VT-d), which allows a platform to run multiple operating systems and applications in independent partitions."
34,The setting can be one of the following:
34,●     Disabled: The processor does not permit virtualization.
34,●     Enabled: The processor allows multiple operating systems in independent partitions.
34,"Note:      If you change this option, you must power the server off and on before the setting takes effect."
34,Power technology
34,Enables you to configure the CPU power management settings for the following options:
34,●     Enhanced Intel Speedstep Technology
34,●     Intel Turbo Boost Technology
34,●     Processor Power State C6
34,"For best performance, set the power technology option to Performance or Custom. If it is not set to Custom, the individual settings for Intel SpeedStep and Turbo Boost and the C6 power state are ignored."
34,This option can be set to one of the following:
34,●     Custom: The server uses the individual settings for the BIOS parameters in the preceding section. You must select this option if you want to change any of these BIOS parameters.
34,●     Performance: The server determines the best settings for the BIOS parameters and provides optimal CPU power performance in the preceding section.
34,"●     Disabled: The server does not perform any CPU power management, and any settings for the BIOS parameters in the preceding section are ignored."
34,●     Energy Efficient: The server determines the best settings for the BIOS parameters in the preceding section and ignores the individual settings for these parameters.
34,You can set the processor C-states.
34,Processor C1E
34,"Enabling the C1E option allows the processor to transition to its minimum frequency upon entering the C1 state. This setting does not take effect until after you have rebooted the server. When this option is disabled, the CPU continues to run at its maximum frequency in the C1 state. Users should disable this option to perform application benchmarking."
34,You can specify whether the CPU transitions to its minimum frequency when entering the C1 state.
34,The setting can be one of the following:
34,●     Disabled: The CPU continues to run at its maximum frequency in the C1 state.
34,●     Enabled: The CPU transitions to its minimum frequency. This option saves the maximum amount of power in the C1 state.
34,Processor C3 report
34,"You can specify whether the BIOS sends the C3 report to the operating system. When the OS receives the report, it can transition the processor into the lower C3 power state to decrease energy use while maintaining optimal processor performance."
34,The setting can be one of the following:
34,●     Disabled: The BIOS does not send the C3 report.
34,"●     Enabled: The BIOS sends the C3 report, allowing the OS to transition the processor to the C3 low-power state."
34,Processor C6 report
34,"The C6 state is power-saving halt and sleep state that a CPU can enter when it is not busy. Unfortunately, it can take some time for the CPU to leave these states and return to a running condition. If you are concerned about performance (for all but latency-sensitive single-threaded applications), and if you can do so, disable anything related to C-states."
34,"You can specify whether the BIOS sends the C6 report to the operating system. When the OS receives the report, it can transition the processor into the lower C6 power state to decrease energy use while maintaining optimal processor performance."
34,The setting can be one of the following:
34,●     Disabled: The BIOS does not send the C6 report.
34,"●     Enabled: The BIOS sends the C6 report, allowing the OS to transition the processor to the C6 low-power state."
34,P-state coordination
34,"You can define the way that the BIOS communicates the P-state support model to the operating system. Three models are available, as defined by the Advanced Configuration and Power Interface (ACPI) specification:"
34,●     HW_ALL: The processor hardware is responsible for coordinating the P-state among logical processors with dependencies (all the logical processors in a package).
34,●     SW_ALL: The OS Power Manager (OSPM) is responsible for coordinating the P-state among logical processors with dependencies (all the logical processors in a physical package) and must initiate the transition on all the logical processors.
34,●     SW_ANY: The OSPM is responsible for coordinating the P-state among logical processors with dependencies (all the logical processors in a package) and can initiate the transition on any of the logical processors in the domain.
34,"Note:      The power technology option must be set to Custom; otherwise, the server ignores the setting for this parameter."
34,Package C-state limit
34,"When power technology is set to Custom, use this option to configure the lowest processor idle power state (C-state). The processor automatically transitions into package C-states based on the core C-states to which cores on the processor have transitioned. The higher the package C-state, the lower the power use of that idle package state. The default setting, Package C6 (nonretention), is the lowest power idle package state supported by the processor."
34,You can specify the amount of power available to the server components when they are idle.
34,The possible settings are as follows:
34,"●     C0/C1 State: When the CPU is idle, the system slightly reduces the power consumption. This option requires less power than C0 and allows the server to return quickly to high-performance mode."
34,"●     C2 State: When the CPU is idle, the system reduces power consumption more than with the C1 option. This option requires less power than C1 or C0, but the server takes slightly longer to return to high-performance mode."
34,"●     C6 Nonretention: When the CPU is idle, the system reduces the power consumption more than with the C3 option. This option saves more power than C0, C1, or C3, but the system may experience performance problems until the server returns to full power."
34,"●     C6 Retention: When the CPU is idle, the system reduces power consumption more than with the C3 option. This option consumes slightly more power than the C6 Nonretention option, because the processor is operating at Pn voltage to reduce the package’s C-state exit latency."
34,Energy performance
34,You can specify whether system performance or energy efficiency is more important on this server.
34,The setting can be one of the following:
34,●     Performance: The server provides all server components with full power at all times. This option maintains the highest level of performance and requires the greatest amount of power.
34,●     Balanced Performance: The server provides all server components with enough power to keep a balance between performance and power.
34,●     Balanced Energy: The server provides all server components with enough power to keep a balance between performance and power.
34,●     Energy Efficient: The server provides all server components with less power to reduce power consumption.
34,Note:      Power Technology must be set to Custom or the server ignores the setting for this parameter
34,Autonomous Core C-state
34,"When the operating system requests CPU core C1 state, system hardware automatically changes the request to core C6 state"
34,"This BIOS switch allows 2 options: ""Enabled"" and ""Disabled""."
34,●     Enabled: HALT and C1 request get converted to C6 requests in hardware.
34,●     Disabled: only C0 and C1 are used by the OS. C1 gets enabled automatically when an OS autohalts.
34,"By default, Autonomous Core C-state is disabled."
34,Workload configuration
34,You can configure workload optimization.
34,You can configure the following options:
34,●     Balanced
34,●     I/O Sensitive
34,●     Non-Uniform Memory Access (NUMA)
34,●     Uniform Memory Access (UMA)
34,"By default, I/O Sensitive is enabled."
34,Memory settings
34,You can use several settings to optimize memory performance.
34,"Memory reliability, availability, and serviceability configuration"
34,"Always set the memory reliability, availability, and serviceability (RAS) configuration to Maximum Performance for systems that require the highest performance and do not require memory fault-tolerance options."
34,The following settings are available:
34,●     Maximum Performance: System performance is optimized.
34,●     Mirror Mode 1LM (one-level memory): System reliability is optimized by using half the system memory as backup.
34,Note:      For the optimal balance of performance and system stability it is recommended to use “Platform Default” (ADDDC Sparing enabled) for the Memory RAS configuration. ADDDC Sparing will incur a small performance penalty. If maximum performance is desired independently of system stability the “Maximum-Performance” Memory RAS setting can be used.
34,Non-uniform memory access
34,"Most modern operating systems, particularly virtualization hypervisors, support NUMA because in the latest server designs, a processor is attached to a memory controller: therefore, half the memory belongs to one processor, and half belongs to the other processor. If a core needs to access memory that resides in another processor, a longer latency period is needed to access that part of memory. Operating systems and hypervisors recognize this architecture and are designed to reduce such trips. For hypervisors such as those from VMware and for modern applications designed for NUMA, keep this option enabled."
34,Integrated memory controller interleaving
34,"The Integrated Memory Controller (IMC) BIOS option controls the interleaving between the integrated memory controllers. There are two integrated memory controllers per CPU socket in a x86 server running Intel Xeon scalable processors. If integrated memory controller Interleaving is set to 2-way, addresses will be interleaved between the two-integrated memory controller. If integrated memory controller interleaving is set to 1-way, there will be no interleaving."
34,"Note:      If Sub-NUMA Clustering (SNC) is disabled, integrated memory controller interleaving should be set to Auto. If SNC is enabled, integrated memory controller interleaving should be set to 1-way."
34,The following settings are available:
34,●     1-way Interleave: There is no interleaving.
34,●     2-way Interleave: Addresses are interleaved between the two integrated memory controllers.
34,●     Auto: The CPU determines the integrated memory controller interleaving mode.
34,Sub-NUMA clustering
34,"The SNC BIOS option provides localization benefits similar to the Cluster-on-Die (CoD) option, without some of the disadvantages of CoD. SNC breaks the LLC into two disjointed clusters based on address range, with each cluster bound to a subset of the memory controllers in the system. SNC improves average latency to the LLC and memory. SNC is a replacement for the CoD feature found in previous processor families. For a multisocket system, all SNC clusters are mapped to unique NUMA domains. Integrated memory controller interleaving must be set to the correct value to correspond with the SNC setting."
34,The setting for this BIOS option can be one of the following:
34,●     Disabled: The LLC is treated as one cluster when this option is disabled.
34,●     Enabled: The LLC capacity is used more efficiently and latency is reduced as a result of the core and integrated memory controller proximity. This setting may improve performance on NUMA-aware operating systems.
34,"Note:      If SNC is disabled, integrated memory controller interleaving should be set to Auto. If SNC is enabled, integrated memory controller interleaving should be set to 1-way."
34,Xtended Partition Table prefetch
34,"The XPT prefetcher exists on top of other prefetchers that that can prefetch data in the core DCU, MLC, and LLC. The XPT prefetcher will issue a speculative DRAM read request in parallel to an LLC lookup. This prefetch bypasses the LLC, saving latency. You can specify whether the processor uses the XPT prefetch mechanism to fetch the date into the XPT."
34,The setting can be one of the following:
34,●     Disabled: The processor does not preload any cache data.
34,●     Enabled: The XPT prefetcher preloads the Layer 1 cache with the data it determines to be the most relevant.
34,Intel UltraPath Interconnect prefetch
34,Intel UltraPath Interconnect (UPI) prefetch is a mechanism to get the memory read started early on a Double-Data-Rate (DDR) bus.
34,The setting can be one of the following:
34,●     Disabled: The processor does not preload any cache data.
34,●     Enabled: The UPI prefetcher preloads the Layer 1 cache with the data it determines to be the most relevant.
34,ADDDC Sparing
34,Adaptive Double Device Data Correction (ADDDC) is a memory RAS feature that enables dynamic mapping of failing DRAM by monitoring corrected errors and taking action before uncorrected errors can occur and cause an outage. It is now enabled by default.
34,"After ADDDC sparing remaps a memory region, the system could incur marginal memory latency and bandwidth penalties on memory bandwidth intense workloads that target the impacted region. Cisco recommends scheduling proactive maintenance to replace a failed DIMM after an ADDDC RAS fault is reported."
34,Patrol scrub
34,"You can specify whether the system actively searches for, and corrects, single-bit memory errors even in unused portions of the memory on the server."
34,The setting can be one of the following:
34,●     Disabled: The system checks for memory Error-Correcting Code (ECC) errors only when the CPU reads or writes a memory address.
34,"●     Enabled: The system periodically reads and writes memory searching for ECC errors. If any errors are found, the system attempts to fix them. This option may correct single-bit errors before they become multiple-bit errors, but it may adversely affect performance when the patrol-scrub process is running."
34,Demand scrub
34,Demand scrub occurs when the memory controller reads memory for data or instructions and the demand scrubbing logic detects a correctable error. Correct data is forwarded to the memory controller and written to memory.
34,"With demand scrubbing disabled, the data being read into the memory controller will be corrected by the ECC logic, but no write to main memory occurs. Because the data is not corrected in memory, subsequent read operations to the same data will need to be corrected."
34,Configuring the BIOS for optimized CPU hardware power management
34,"This section summarizes the BIOS settings you can configure to optimize CPU power management. It presents the settings optimized for maximum performance, low latency, and energy efficiency, summarized in Table 4."
34,"Table 4.           BIOS recommendations for maximum performance, low latency, and energy efficiency."
34,BIOS Options
34,BIOS Values (platform-default)
34,Maximum Performance
34,Low-Latency
34,Energy Efficiency
34,Processor Configuration
34,Intel SpeedStep Technology
34,Enabled
34,Platform Default
34,Platform Default
34,Platform Default
34,Intel Hyper-Threading Tech
34,Enabled
34,Platform Default
34,Disabled
34,Platform Default
34,Intel Virtualization Technology (VT)
34,Enabled
34,Disabled
34,Disabled
34,Platform Default
34,Intel VT for Directed I/O
34,Enabled
34,Disabled
34,Disabled
34,Platform Default
34,CPU performance
34,Custom
34,Platform Default
34,Platform Default
34,Platform Default
34,LLC Prefetch
34,Disabled
34,Platform Default
34,Platform Default
34,Platform Default
34,Direct cache access
34,Enabled
34,Platform Default
34,Platform Default
34,Platform Default
34,Advanced Power Management Configuration
34,Power technology
34,Energy-Efficient
34,Custom
34,Custom
34,Custom
34,Intel Turbo Boost
34,Enabled
34,Platform Default
34,Platform Default
34,Disabled
34,P-STATE coordination
34,HW_ALL
34,Platform Default
34,Platform Default
34,Platform Default
34,Energy Performance
34,Balanced Performance
34,Performance
34,Platform Default
34,Platform Default
34,Processor C State
34,Enabled
34,Disabled
34,Disabled
34,Platform Default
34,Processor C1E
34,Enabled
34,Disabled
34,Disabled
34,Platform Default
34,Processor C3
34,Enabled
34,Disabled
34,Disabled
34,Disabled
34,Processor C6
34,Enabled
34,Disabled
34,Disabled
34,Platform Default
34,Processor C7
34,Enabled
34,Disabled
34,Disabled
34,Disabled
34,Package C State limit
34,C0/C1 State
34,Platform Default
34,Platform Default
34,C6 Retention
34,Energy Performance Tuning
34,Platform Default
34,Platform Default
34,Platform Default
34,CPU hardware power mgmt
34,HWPW Native Mode
34,Platform Default
34,Platform Default
34,Platform Default
34,Workload Configuration
34,I/O Sensitive
34,Balanced
34,Balanced
34,Platform Default
34,Autonomous Core C-State
34,Disabled
34,Platform Default
34,Platform Default
34,Enabled
34,Memory & UPI Configuration
34,NUMA Optimized
34,Enabled
34,Platform Default
34,Platform Default
34,Platform Default
34,IMC Interleaving
34,Auto
34,1-way Interleave
34,Platform Default
34,Platform Default
34,XPT Prefetch
34,Auto
34,Platform Default
34,Platform Default
34,Platform Default
34,UPI Prefetch
34,Enabled
34,Platform Default
34,Platform Default
34,Platform Default
34,Sub Numa Clustering
34,Disabled
34,Enabled
34,Platform Default
34,Platform Default
34,Memory RAS configuration
34,ADDDC Sparing
34,Platform Default
34,Platform Default
34,Platform Default
34,ADDDC Sparing
34,Enabled
34,Platform Default
34,Platform Default
34,Platform Default
34,Fan control policy
34,"Fan control policies enable you to control the fan speed to reduce server power consumption and noise levels. Prior to the fan policies, the fan speed increased automatically when the temperature of any server component exceeded the set threshold. To help ensure that the fan speeds were low, the threshold temperatures of components were usually set to high values. Although this behavior suited most server configurations, it did not address the following situations:"
34,"●     Maximum CPU performance: For high performance, certain CPUs must be cooled substantially below the set threshold temperature. This cooling requires very high fan speeds, which results in increased power consumption and noise levels."
34,"●     Low power consumption: To help ensure the lowest power consumption, fans must run very slowly and, in some cases, stop completely on servers that allow this behavior it. But slow fan speeds can cause servers to overheat. To avoid this situation, you need to run fans at a speed that is moderately faster than the lowest possible speed."
34,Following are the fan policies that you can choose:
34,"●     Balanced: This is the default policy. This setting can cool almost any server configuration, but it may not be suitable for servers with PCI Express (PCIe) cards, because these cards overheat easily."
34,●     Low Power: This setting is well suited for minimal-configuration servers that do not contain any PCIe cards.
34,"●     High Power: This setting can be used for server configurations that require fan speeds ranging from 60 to 85 percent. This policy is well suited for servers that contain PCIe cards that easily overheat and have high temperatures. The minimum fan speed set with this policy varies for each server platform, but it is approximately in the range of 60 to 85 percent."
34,"●     Maximum Power: This setting can be used for server configurations that require extremely high fan speeds ranging between 70 and 100 percent. This policy is well suited for servers that contain PCIe cards that easily overheat and have extremely high temperatures. The minimum fan speed set with this policy varies for each server platform, but it is approximately in the range of 70 to 100 percent."
34,Note:      This policy is configurable for standalone Cisco UCS C-Series M5 servers using the Cisco® Integrated Management Controller (IMC) console and the Cisco IMC supervisor. CIMC Web console Ò Compute Ò Power Policies Ò Configured Fan Policy.
34,"For UCS Managed C series servers, it is configurable using Power Control Policies under. Servers Ò Policies Ò root Ò Power control Policies Ò Create Fan Power Control Policy Ò Fan speed Policy."
34,Operating system tuning guidance for best performance
34,You can tune the OS to achieve the best performance.
34,"For Linux, set the following:"
34,●     x86_energy_perf_policy performance
34,"When energy performance tuning is set to OS, the OS controls the Energy Performance Bias (EPB) policy. The EPB features controlled by the policy are Intel Turbo Boost override, memory clock enable (CKE), memory Output Status Register (OSR), Intel QuickPath Interconnect (QPI) L0p, C-state demotion, and I/O bandwidth P-limit. The default OSPM profile is set to Performance, which will not sacrifice performance to save energy."
34,●     cpupower frequency-set -governor performance
34,"The performance governor forces the CPU to use the highest possible clock frequency. This frequency is statically set and will not change. Therefore, this particular governor offers no power-savings benefit. It is suitable only for hours of heavy workload, and even then, only during times in which the CPU is rarely (or never) idle. The default setting is On Demand, which allows the CPU to achieve maximum clock frequency when the system load is high, and the minimum clock frequency when the system is idle. Although this setting allows the system to adjust power consumption according to system load, it does so at the expense of latency from frequency switching."
34,●     Edit /etc/init.d/grub.conf to set intel_pstate=disable
34,Intel_pstate is a part of the CPU performance scaling subsystem in the Linux kernel (CPUFreq). It is a power scaling driver is used automatically on later generations of Intel processors. This driver takes priority over other drivers and is built-in as opposed to being a module. You can force pstate off by appending intel_pstate=disable to the kernel arguments (edit /etc/default/grub)
34,●     tuned-adm profile latency-performance
34,The tuned-adm tool allows users to easily switch among a number of profiles that have been designed to enhance performance for specific use cases.
34,You can apply the tuned-admin server profile for typical latency performance tuning. It disables the tuned and ktune power-saving mechanisms. The CPU speed mode changes to Performance. The I/O elevator is changed to Deadline for each device. The cpu_dma_latency parameter is registered with a value of 0 (the lowest possible latency) for power management QoS to limit latency where possible.
34,Use the following Linux tools to measure maximum turbo frequency and power states:
34,"●     Turbostat: Turbostat is provided in the kernel-tools package. It reports on processor topology, frequency, idle power-state statistics, temperature, and power use on Intel 64 processors. It is useful for identifying servers that are inefficient in terms of power use or idle time. It also helps identify the rate of System Management Interrupts (SMIs) occurring on the system, and it can be used to verify the effects of power management tuning. Use this setting:"
34,turbostat -S
34,"●     Intel PTUmon: The Intel Performance Tuning Utility (PTU) is a cross-platform performance analysis tool set. In addition to such traditional capabilities as tools to identify the hottest modules and functions of the application, track call sequences, and identify performance-critical source code, Intel PTU has new, more powerful data collection, analysis, and visualization capabilities. Intel PTU offers processor hardware event counters for in-depth analysis of the memory system performance, architectural tuning, and other features. Use this setting:"
34,ptumon -l -i 5000
34,Refer the following resources for more information about OS performance tuning:
34,●     Microsoft Windows and Hyper-V tuning is straightforward: set the power policy to High Performance. See:
34,◦    Performance Tuning Guidelines for Microsoft Windows Server 2012 R2
34,◦    Performance Tuning Guidelines for Microsoft Windows Server 2016
34,●     VMware ESXi tuning is straightforward as well: set the power policy to High Performance. See:
34,◦    https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/vmware-perfbest-practices-vsphere6-0-white-paper.pdf
34,"●     To tune Citrix XenServer, set xenpm set-scaling-governor performance. See:"
34,◦    https://support.citrix.com/article/CTX200390
34,"●     To tune Red Hat Enterprise Linux, set CPU power to Performance. See:"
34,◦    https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/pdf/Performance_Tuning_Guide/Red_Hat_Enterprise_Linux-7-Performance_Tuning_Guide-en-US.pdf
34,"●     To tune SUSE Enterprise Linux, set CPU power to Performance. See:"
34,◦    https://www.suse.com/documentation/sles-12/pdfdoc/book_sle_tuning/book_sle_tuning.pdf
34,BIOS recommendations for various workload types
34,This document discusses BIOS settings for the following types of workloads:
34,●     Online transaction processing (OLTP)
34,●     Virtualization
34,●     High-Performance Computing (HPC)
34,●     Java Enterprise Edition (Java EE) application server
34,●     Analytics database Decision-Support System (DSS)
34,Table 5 summarizes the BIOS options and settings available for various workloads.
34,Table 5.           BIOS options for various workloads
34,BIOS Options
34,BIOS Values (platform-default)
34,Online Transaction Processing (OLTP)
34,Virtualization
34,High-Performance Computing (HPC)
34,Java Application Servers
34,Analytic Database Systems (DSS)
34,Processor Configuration
34,Intel SpeedStep Technology
34,Enabled
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Intel Hyper-Threading Tech
34,Enabled
34,Platform Default
34,Platform Default
34,Disabled
34,Platform Default
34,Platform Default
34,Intel Virtualization Technology (VT)
34,Enabled
34,Platform Default
34,Platform Default
34,Disabled
34,Disabled
34,Platform Default
34,Intel VT for Directed I/O
34,Enabled
34,Platform Default
34,Platform Default
34,Disabled
34,Disabled
34,Disabled
34,CPU performance
34,Custom
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,LLC Prefetch
34,Disabled
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Direct cache access
34,Enabled
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Advanced Power Management Configuration
34,Power technology
34,Energy-Efficient
34,Custom
34,Custom
34,Platform Default
34,Custom
34,Custom
34,Intel Turbo Boost
34,Enabled
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,P-STATE coordination
34,HW_ALL
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Energy Performance
34,Balanced Performance
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Processor C State
34,Enabled
34,Disabled
34,Disabled
34,Platform Default
34,Disabled
34,Disabled
34,Processor C1E
34,Enabled
34,Disabled
34,Disabled
34,Platform Default
34,Disabled
34,Disabled
34,Processor C3
34,Enabled
34,Disabled
34,Disabled
34,Platform Default
34,Disabled
34,Disabled
34,Processor C6
34,Enabled
34,Disabled
34,Disabled
34,Platform Default
34,Disabled
34,Disabled
34,Processor C7
34,Enabled
34,Disabled
34,Disabled
34,Platform Default
34,Disabled
34,Disabled
34,Package C State limit
34,C0/C1 State
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Energy Performance Tuning
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,CPU hardware power mgmt
34,HWPW Native Mode
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Workload Configuration
34,I/O Sensitive
34,Platform-default
34,Platform Default
34,Balanced
34,Platform Default
34,Platform-default
34,Autonomous Core C-State
34,Disabled
34,Platform-default
34,Platform-default
34,Platform-default
34,Platform-default
34,Platform-default
34,Memory & UPI Configuration
34,NUMA Optimized
34,Enabled
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,IMC Interleaving
34,Auto
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,XPT Prefetch
34,Auto
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,UPI Prefetch
34,Enabled
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Sub Numa Clustering
34,Disabled
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Memory RAS configuration
34,ADDDC Sparing
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,ADDDC Sparing
34,Enabled
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,Platform Default
34,The following sections describe the BIOS tuning recommendations for all the workloads listed in Table 5.
34,Online transaction processing workloads
34,OLTP systems contain the operational data needed to control and run important transactional business tasks. These systems are characterized by their ability to complete various concurrent database transactions and process real-time data. They are designed to provide optimal data processing speed.
34,OLTP systems are often decentralized to avoid single points of failure. Spreading the work over multiple servers can also support greater transaction processing volume and reduce response time.
34,Processor and memory settings for Cisco UCS managed servers: OLTP
34,Obtaining peak performance requires some system-level tuning. Figure 1 Highlights the BIOS selections that are recommended for optimizing OLTP workloads on Cisco UCS M5 platforms managed by Cisco UCS Manager. Rest of the BIOS settings are configured as “Platform Default”.
34,Figure 1.
34,Processor settings for OLTP workloads
34,"The Intel Turbo Boost and SpeedStep technologies are powerful management features that adjust the CPU voltage and frequency settings to optimize performance and power consumption dynamically. During periods of low CPU consumption, Intel SpeedStep can reduce the CPU frequency by reducing power consumption. Intel Turbo Boost increases the processing speed to accommodate higher demand in situations in which CPU utilization is extremely high. Each core has 20 to 30 percent more processing capability when Intel Turbo Boost is enabled. For example, the Cisco UCS M5 platforms installed with the Intel Xeon Scalable Platinum 8168 CPU operates at a base frequency of 2.7 GHz. If Intel Turbo Boost is enabled, the system can achieve frequencies as high as 3.7 GHz."
34,"When you tune for consistent performance for OLTP applications on a system that does not run at close to 100 percent CPU utilization, you should enable Intel SpeedStep and Turbo Boost and disable C-states. Although this configuration foregoes power savings during idle times, it keeps all CPU cores running at a consistent speed and delivers the most consistent and predictable performance."
34,"Enabling Intel Hyper-Threading Technology helps OLTP systems handle I/O-intensive workloads by allowing the processing of multiple threads per CPU core. OLTP applications typically are multithreaded, with each thread performing a small amount of work that may include I/O operations. A large number of threads results in a considerable amount of context switching, but with Intel Hyper-Threading, the effect of context switching is reduced. When Intel Direct Cache Access is enabled (Figure 2), the I/O controller places data directly into the CPU cache to reduce the cache misses while processing OLTP workloads. This approach results in improved application performance."
34,Figure 2.
34,Intel Directed I/O settings for OLTP workloads
34,"If you are deploying the system in a virtualized environment and the OLTP application uses a directed I/O path, make sure that the VT for Directed IO option is enabled. By default, these options are enabled."
34,Note:      This feature is applicable only if the OLTP system is running in a virtualized environment.
34,Figure 3 shows the recommended settings for optimizing memory for OLTP workloads on servers managed by Cisco UCS Manager.
34,Figure 3.
34,Memory settings for OLTP workloads
34,Processor and memory settings for standalone Cisco UCS C-Series servers: OLTP
34,Figures 4 and 5 show the processor selections that are recommended for OLTP workloads on standalone Cisco UCS C-Series M5 servers.
34,Figure 4.
34,Processor settings for OLTP workloads
34,Figure 5.
34,Power and performance settings for OLTP workloads
34,Figure 6 shows memory settings for OLTP workloads for standalone Cisco UCS C-Series servers.
34,Figure 6.
34,Memory settings for OLTP workloads
34,"OLTP applications have a random memory-access pattern and benefit greatly from larger and faster memory. Therefore, Cisco recommends setting memory RAS features to maximum performance for optimal system performance. In OLTP transactions, if these modes are enabled, I/O operations will be serviced at the highest frequency and will have reduced memory latency."
34,"Note:      If the DIMM pairs in the server have the same type, size, and organization and are populated across the Scalable Memory Interconnect (SMI) channels, you can enable the lockstep mode, an option on the Select Memory RAS menu, to reduce memory-access latency and achieve better performance."
34,Virtualization workloads
34,"Intel Virtualization Technology provides manageability, security, and flexibility in IT environments that use software-based virtualization solutions. With this technology, a single server can be partitioned and can be projected as several independent servers, allowing the server to run different applications on the operating system simultaneously."
34,Processor and memory settings for Cisco UCS managed servers: Virtualization
34,Figure 7 Highlights the BIOS selections that are recommended for virtualized workloads on Cisco UCS M5 platforms managed by Cisco UCS manager. Rest of the BIOS settings are configured as “Platform Default”.
34,Figure 7.
34,Processor settings for virtualized workloads
34,Most of the CPU and memory settings for virtualized workloads are the same as those for OLTP workloads. It is important to enable Intel Virtualization Technology in the BIOS to support virtualization workloads. Make sure that the Intel VT-d option is enabled.
34,"The CPUs that support hardware virtualization allow the processor to run multiple operating systems in the virtual machines. This feature involves some overhead because the performance of a virtual operating system is comparatively slower than that of the native OS. To enhance performance, be sure to enable Intel Turbo Boost and Hyper-Threading for the processors."
34,"The cache prefetching mechanisms (Data-Prefetch-Logic [DPL] prefetch, hardware prefetch, Layer 2 streaming prefetch, and adjacent-cache-line prefetch) usually help increase system performance, especially when memory-access patterns are regular."
34,Intel Directed I/O for virtualized workloads
34,Figure 8 shows the recommended Intel Directed I/O settings for virtualized workloads in Cisco UCS M5 platforms.
34,Figure 8.
34,Intel Directed I/O settings for virtualized workloads
34,"With Cisco Data Center Virtual Machine Fabric Extender VM-FEX technology, virtual machines can now directly write to the virtual network interface cards (vNICs) when the Intel Directed I/O option is enabled at the BIOS level."
34,Memory settings for virtualized workloads
34,Figure 9 shows the recommended memory settings for virtualized workloads in Cisco UCS M5 servers.
34,Figure 9.
34,Memory settings for virtualized workloads
34,"When running applications that access memory randomly, set the Select Memory RAS option to Maximum Performance. This setting helps achieve optimal system performance. In virtualized environments, run the memory at the highest frequency to reduce memory latency."
34,Processor and memory settings for standalone Cisco UCS C-Series servers: Virtualization
34,Figures 10 and 11 show processor and power and performance settings for virtualized workloads in standalone Cisco UCS C-Series M5 servers.
34,Figure 10.
34,Processor settings for virtualized workloads
34,Figure 11.
34,Power and performance settings for virtualized workloads
34,Memory settings for virtualized workloads
34,Figure 12 shows memory settings for virtualized workloads in standalone Cisco UCS C-Series M5 servers.
34,Figure 12.
34,Memory settings for virtualized workloads
34,High-performance computing workloads
34,"HPC refers to cluster-based computing that uses multiple individual nodes that are connected and that work in parallel to reduce the amount of time required to process large data sets that would otherwise take exponentially longer to run on any one system. HPC workloads are computation intensive and typically also network-I/O intensive. HPC workloads require high-quality CPU components and high-speed, low-latency network fabrics for their Message Passing Interface (MPI) connections."
34,"Computing clusters include a head node that provides a single point for administering, deploying, monitoring, and managing the cluster. Clusters also have an internal workload management component, known as the scheduler, that manages all incoming work items (referred to as jobs). Typically, HPC workloads require large numbers of nodes with nonblocking MPI networks so that they can scale. Scalability of nodes is the single most important factor in determining the achieved usable performance of a cluster."
34,Processor and memory settings for Cisco UCS managed servers: HPC
34,Figure 13 Highlights the BIOS selections that are recommended for HPC workloads on Cisco UCS M5 platforms managed by Cisco UCS Manager. Rest of the BIOS settings are configured as “Platform Default”.
34,Figure 13.
34,Processor settings for HPC workloads
34,"You should enable Intel Turbo Boost technology for HPC workloads to increase the computing power. When Intel Turbo Boost is enabled, each core provides higher computing frequency potential, allowing a greater number of parallel requests to be processed efficiently."
34,Intel SpeedStep is enabled because it is required for Intel Turbo Boost to function.
34,"HPC workloads typically do not benefit from Intel Hyper-Threading. Additional threads only serve to create resource contention within the microarchitecture of the CPU. Generally, Intel Hyper-Threading has the greatest impact on workloads in which threads are forced to wait for completion of back-end I/O requests, to reduce thread contention for CPU resources."
34,"Enabling the processor power state C6 helps save power when the CPU is idle. Because HPC is computing intensive, the CPU will likely seldom go into an idle state. However, enabling C-states saves CPU power in the event that there are any inactive requests."
34,"You should set CPU Performance to HPC mode to handle more random, parallel requests by HPC applications. If HPC performs more in-memory processing (for example, for video data), you should enable the prefetcher options so that they can handle multiple parallel requests. This configuration also helps retain some hot data in the Layer 2 cache, and it improves HPC performance (CPU performance)."
34,"HPC requires a high-bandwidth I/O network. When you enable DCA support, network packets go directly into the Layer 3 processor cache instead of the main memory. This approach reduces the number of HPC I/O cycles generated by HPC workloads when certain Ethernet adapters are used, which in turn increases system performance."
34,"You can set the Energy Performance option to Maximum Performance, Balanced Performance, Balanced Power, or Power Saver. Test results demonstrate that most applications run best with the Balanced Performance setting. Applications that are highly I/O sensitive perform best when the Energy Performance option is set to Maximum Performance."
34,Intel Directed I/O for HPC workloads
34,Figure 14 shows the recommended Intel Directed I/O settings for HPC workloads in Cisco UCS M5 platforms.
34,Figure 14.
34,Intel Directed I/O settings for HPC workloads
34,Memory settings for HPC workloads
34,Figure 15 shows the memory settings for HPC workloads on Cisco UCS M5 servers.
34,Figure 15.
34,Memory settings for HPC workloads
34,The NUMA option should be enabled for HPC workloads so that NUMA can determine the memory allocation for each thread run by the HPC applications.
34,"Because HPC workloads perform mostly in-memory processing, you should set DIMMs to run at the highest available frequency to process the data more quickly."
34,Processor and memory settings for standalone Cisco UCS C-Series servers: HPC
34,Figures 16 and 17 show the recommended processor and power and performance settings for HPC workloads in standalone Cisco UCS C-Series M5 servers.
34,Figure 16.
34,Processor settings for HPC workloads
34,Figure 17.
34,Power and performance settings for HPC workloads
34,Figures 18 shows memory settings for HPC workloads for standalone Cisco UCS C-Series M5 servers.
34,Figure 18.
34,Memory settings for HPC workloads
34,Java Enterprise Edition application server workloads
34,"Java EE (previously referred to as the J2EE) defines the core set of APIs and features of Java application servers. Usually, Java EE applications are client-server or server-side applications and require a Java EE application server."
34,Java EE application servers are distinguished by the following characteristics:
34,●     They are fully compliant application servers that implement the full Java EE stack specifications with features such as JBoss Enterprise. Examples of fully compliant application servers are Apache Geronimo and JBoss Application Server.
34,"●     They are web application servers that support only the web tier of Java EE, including the servlet. Examples of fully compliant application servers are Apache Tomcat and Jetty."
34,Processor and memory settings for Cisco UCS managed servers: Java EE
34,Figure 19 Highlights the BIOS selections that are recommended for Java EE application servers on Cisco UCS M5 servers managed by Cisco UCS Manager. Rest of the BIOS settings are configured as “Platform Default”.
34,Figure 19.
34,Processor settings for Java EE application workloads
34,"Intel Turbo Boost Technology enables higher CPU frequency, which helps accelerate processing of application requests. This feature helps reduce end-user response time."
34,"Business scenarios such as batch processes run at a certain time of the day benefit from Intel Turbo Boost. It enables CPU cores to achieve at higher frequency clock speeds, which helps lower batch processing time, thereby helping the business complete and generate business reports more quickly."
34,"You should enable all the C-states. This configuration helps reduce power consumption because only active cores will process requests during nonpeak hours. If the application demands more CPU cores, the inactive cores will become active, which helps increase throughput."
34,"The CPU Performance option should be set to Enterprise. When a web server needs to process a large amount of data in a system, the data-access pattern is predictable (mostly sequential or adjacent lines are accessed). In this situation, it is desirable to enable the prefetchers (MLC and DCU) by setting CPU Performance to Enterprise, to reduce access latency for memory-bound operations."
34,Intel Directed I/O for Java application workloads
34,Figure 20 shows the recommended Intel Directed I/O settings for Java application workloads in Cisco UCS M5 servers.
34,Figure 20.
34,Intel Directed I/O settings for Java application workloads
34,Memory settings for Java EE application server workloads
34,Figure 21 shows the recommended memory settings for Java EE application servers for Cisco UCS M5 servers managed by Cisco UCS Manager.
34,Figure 21.
34,Memory settings for Java EE application workloads
34,"Set the DDR mode to Performance so that the DIMMs work at the highest available frequency for the installed memory and CPU combination. In-memory enterprise applications such as Terracotta Ehcache benefit from the high memory speed. If this mode is enabled in web server workloads, I/O operations will be serviced at the highest frequency, and memory latency will be reduced."
34,Processor and memory settings for standalone Cisco UCS C-Series servers: Java EE
34,Figures 22 and 23 show processor and performance and power settings for Java EE applications in standalone Cisco UCS C-Series M5 servers.
34,Figure 22.
34,Processor settings for Java EE application workloads
34,Figure 23.
34,Power and performance settings for Java EE application workloads
34,Figure 24 shows memory settings for Java EE applications for standalone Cisco UCS C-Series M5 servers.
34,Figure 24.
34,Memory settings for Java EE application workloads
34,Analytics database decision-support system workloads
34,An analytics database is a read-only system that stores historical data for business metrics such as sales performance and inventory levels.
34,"An analytics database is specifically designed to support Business Intelligence (BI) and analytics applications, typically as part of a data warehouse or data mart. This feature differentiates it from operational, transactional, and OLTP databases, which are used for transaction processing: order entry and other “run the business” applications."
34,Processor and memory settings for Cisco UCS managed servers: DSS
34,Figure 25 Highlights the BIOS selections that are recommended for analytics database systems on Cisco UCS M5 servers managed by Cisco UCS Manager. Rest of the BIOS settings are configured as “Platform Default”.
34,Figure 25.
34,Processor settings for analytics database DSS workloads
34,Intel Directed I/O for analytics database DSS workloads
34,Figure 26 shows the recommended Intel Directed I/O settings for analytics database DSS workloads on Cisco UCS M5 servers managed by Cisco UCS Manager.
34,Figure 26.
34,Intel Directed I/O settings for analytics database DSS workloads
34,Memory settings for analytics database DSS workloads
34,Figure 27 show the recommended memory settings for analytics database DSS workloads on Cisco UCS M5 servers managed by Cisco UCS Manager.
34,Figure 27.
34,Memory settings for analytics database DSS workloads
34,Processor and memory settings for standalone Cisco UCS C-Series servers: DSS
34,Figures 28 and 29 show processor and performance and power settings for analytics database DSS workloads on standalone Cisco UCS C-Series M5 servers.
34,Figure 28.
34,Processor settings for analytics database DSS workloads
34,Figure 29.
34,Power and performance settings for analytics database DSS workloads
34,Figure 30 shows memory settings for analytics database DSS workloads in standalone Cisco UCS C-Series M5 servers.
34,Figure 30.
34,Memory settings for analytics database DSS workloads
34,Conclusion
34,"When tuning system BIOS settings for performance, you need to consider a number of processor and memory options. If the best performance is your goal, be sure to choose options that optimize for performance in preference to power savings, and experiment with other options such as CPU prefetchers, CPU power management, and CPU hyperthreading."
34,For more information
34,"For more information about Cisco UCS B-Series and C-Series M5 servers, see:"
34,●     Cisco UCS B200 M5 Blade Server: https://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-b-series-blade-servers/b200m5-specsheet.pdf
34,●     Cisco UCS C220 M5 Rack Server: https://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/c220m5-sff-specsheet.pdf
34,●     Cisco UCS C240 M5 Rack Server: https://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/c240m5-sff-specsheet.pdf
34,●     Cisco UCS B480 M5 Blade Server: https://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-b-series-blade-servers/b480m5-specsheet.pdf
34,●     Cisco UCS C480 M5 Rack Server: https://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-c-series-rack-servers/c480-m5-high-performance-specsheet.pdf
34,Our experts recommend
34,Cisco UCS B200 M6 Blade Server At-a-Glance
34,Cisco UCS M6 Servers with 3rd Gen Intel Xeon CPUs FAQ
34,Learn more
34,Follow Us
34,News & EventsNewsroomEventsBlogsCommunity
34,About Us
34,About Us
34,About Cisco
34,Customer stories
34,Investor relations
34,Social responsibility
34,Environmental sustainability
34,The Trust Center
34,Contact Us
34,Contact Us
34,Contact Cisco
34,Meet our partners
34,Find a reseller
34,Work with Us
34,Careers
34,We Are Cisco
34,Partner with Cisco
34,Cisco Sites
34,Meraki
34,All new Webex
34,Cisco Umbrella
34,Contacts
34,Feedback
34,Help
34,Site Map
34,Terms & Conditions
34,Privacy
34,Privacy Statement
34,Cookies
34,Cookies
34,Trademarks
35,Performance Optimization - PostgreSQL wiki
35,"Want to edit, but don't see an edit button when logged in?"
35,Click here.
35,Performance Optimization
35,From PostgreSQL wiki
35,"Jump to: navigation, search"
35,Contents
35,1 How to Effectively Ask Questions Regarding Performance on Postgres Lists
35,2 General Setup and Optimization
35,3 Critical maintenance for performance
35,4 Database architecture
35,5 Database Hardware Selection and Setup
35,6 Benchmark Workloads
35,How to Effectively Ask Questions Regarding Performance on Postgres Lists
35,Slow_Query_Questions
35,General Setup and Optimization
35,"Tuning Your PostgreSQL Server by Greg Smith, Robert Treat, and Christopher Browne"
35,PostgreSQL Query Profiler in dbForge Studio by Devart
35,Performance Tuning PostgreSQL by Frank Wiles
35,QuickStart Guide to Tuning
35,PostgreSQL by Christopher Browne
35,Performance Tuning by Josh Berkus and Shridhar Daithankar
35,Replacing Slow Loops in PostgreSQL by Joel Burton
35,PostgreSQL Hardware Performance Tuning by Bruce Momjian
35,The effects of data fragmentation in a mixed load database by Dmitry Dvoinikov
35,Understanding Postgres Performance by Craig Kerstiens
35,More on Postgres Performance by Craig Kerstiens
35,Faster PostgreSQL counting by Joe Nelson
35,Row count estimates in Postgres by David Conlin
35,Index-only scans in Postgres by David Conlin
35,Optimize PostgreSQL Server Performance Through Configuration by Tom Swartz
35,Performance courses are available from a number of companies. Check events and trainings for further details.
35,Critical maintenance for performance
35,"Introduction to VACUUM, ANALYZE, EXPLAIN, and COUNT by Jim Nasby."
35,VACUUM FULL and why you should avoid it
35,Planner Statistics
35,Using EXPLAIN
35,Logging Difficult Queries
35,Logging Checkpoints
35,Bulk Loading and Restores
35,Performance Analysis Tools by Craig Ringer
35,Database architecture
35,Limiting and prioritizing user/query/database resource usage by Craig Ringer
35,Prioritizing databases by separating into multiple clusters by Craig Ringer
35,Clustering
35,Shared Storage
35,Database Hardware Selection and Setup
35,Database Hardware
35,Reliable Writes
35,Benchmark Workloads
35,Category:Benchmarking
35,"Retrieved from ""https://wiki.postgresql.org/index.php?title=Performance_Optimization&oldid=35388"""
35,Categories: AdministrationPerformanceBenchmarkingGeneral articles and guides
35,Navigation menu
35,Views
35,Page
35,Discussion
35,View source
35,History
35,Personal tools
35,Log in
35,Navigation
35,Main Page
35,Random page
35,Recent changes
35,Help
35,Search
35,Tools
35,What links here
35,Related changes
35,Special pages
35,Printable version
35,Permanent link
35,Page information
35,"This page was last edited on 29 September 2020, at 13:37."
35,Privacy policy
35,About PostgreSQL wiki
35,Disclaimers
36,WordPress Speed Optimization Guide: Web Hosting | Jelastic
36,MENU
36,For service providers
36,test
36,Cloud Hosting Business
36,WordPress-as-a-Service
36,Kubernetes-as-a-Service
36,Application-as-a-Service
36,Current Hosting Partners
36,Licensing Details
36,Request Demo
36,Products
36,Cloud Options
36,Multi-Cloud
36,Hybrid Cloud
36,Private Cloud
36,Lite Private Cloud
36,Public Cloud
36,PaaS Editions Comparison
36,Services
36,Platform-as-a-Service
36,WordPress-as-a-Service
36,Container-as-a-Service
36,Kubernetes-as-a-Service
36,Application-as-a-Service
36,Marketplace
36,Functionality
36,Runtimes
36,Java Hosting
36,PHP Hosting
36,Node.js Hosting
36,Ruby Hosting
36,Python Hosting
36,Go Hosting
36,Windows Hosting
36,Features
36,Automatic Vertical Scaling
36,Horizontal Scaling
36,Auto-Scalable Clustering
36,Container Types
36,Supported Stacks
36,Automatic Hibernation
36,Two-Factor Authentication
36,Self-Service Portal
36,Load Balancing
36,Traffic Distribution
36,Elastic Storage
36,Private GitLab
36,Cloud Scripting
36,Built-In Billing
36,Pricing
36,test
36,Pay-Per-Use in Public Cloud
36,Revenue Sharing for Partners
36,Private & Multi-Cloud Licensing
36,Support
36,Resources
36,Blog
36,Dev Docs
36,Ops Docs
36,Cloud Scripting Docs
36,Video
36,About Us
36,Company
36,Hosting Partners List
36,Use Cases
36,Whitepapers
36,Events
36,Get in Touch
36,Contact
36,Forum
36,Twitter
36,Facebook
36,LinkedIn
36,Get Started
36,test
36,Sign Up
36,Request Private Cloud
36,Start Cloud Hosting Partnership
36,Request Multi-Cloud PaaS
36,Get Auto-Scalable Clusters
36,Activate Extra Functionality
36,WordPress Speed Optimization Guide: Web Hosting
36,By Tetiana Fydorenchyk |
36,"May 28, 2020"
36,"Having a fast business website is essential for both Google ranking and overall conversion rate. Due to Kissmetrics, 40% of site visitors will abandon a page that takes three or more seconds to load. Earlier, the BBC calculated that they lost an additional 10% of users for every extra second their site took to load. In order to help our readers and customers in gaining higher speed results, we decided to publish a set of articles dedicated to the improvement of website performance using great hints from The Ultimate WordPress Speed Optimization Guide written by Johnny Nguyen. “Faster websites make more money, rank better, and improve overall user experience!” says Johnny.Today we will start with the web hosting optimization part. Each point will be marked with the level of required skills to implement and the impact it will bring.SKILL:BEGINNER – can Google and follow instructions.INTERMEDIATE – working as WordPress contractor.ADVANCED – programmer or server-admin.IMPACT:LOW – maybe 100-200ms difference. Possibly unnoticeable.MEDIUM – around 500ms difference.HIGH – 1 second difference or more.Your webhosting speed determines how fast it can process code, and how many visitors it can handle. Compare your website to a car. To make a car go faster, you either A) get a stronger engine and/or B) lighten the weight. For websites, the web-server is the “engine” and the code is the”weight”.The goal is to improve our web-server “engine” while decreasing code “weight”, ok?Changing your webhosting is one of the easiest ways to improve speed. Those of you on cheap $5/month shared webhosting will benefit the most from moving to a managed hosting service or even your own VPS. The difference will be night and day without any site changes. Moving from managed hosting to an optimized VPS or dedicated “bare metal” server will be another night-and-day jump. The difference isn’t only speed but also a matter of cost (savings). A fast server can handle more visitors than a slow one. If your server can handle double the traffic, theoretically the bill can be twice as cheap. Not a big deal for a small site but what about a huge ecommerce site with a $1k/month server bill? 50% cost reduction sounds mighty attractive!1. Choose nearby datacenter location (BEG, LOW-MED)Obviously, you should pick a server location that’s closest to your visitors. Ideally, you don’t want your DNS ping time more than 100ms from the server to your visitor’s computer. There are many implications depending on your needs.Local businesses should get a server as close to their visitors as possible. Keep it within 100ms or less, within 50ms is better. Check ping times with WonderNetwork.The USA is about 80ms from coast to coast. Canada and Mexico are close enough as well.All of Western Europe is only 40-50ms, very close.Asia is within 80ms between most countries.India/Pakistan, Australia/NZ, Africa are somewhat isolated. Local businesses there need a local datacenter. Even Singapore to Australia is borderline “far” by DNS standards (~150ms).South America can be unreliable infrastructure. For that reason, many companies in Central/South America still use US-based datacenters like in California, Texas, or Florida (Miami).If you have worldwide traffic (including Asia/Pacific) and no particular core region, I like USA west coast as perfect location for fast traffic to Europe and Asia.If you have only USA & Europe traffic and no particular core region, I like USA east coast for fast traffic to Europe.It’s also good to have a webhosting company on the same timezone as your core audience. That way they can (quickly) support or troubleshoot issues when most of your visitors are awake.Those of you thinking a CDN can make up for far server location (that’s not necessarily true!)Those of you hunting for dedicated nodes…the best is TIER-4 datacenter with four 9’s (99.9999% uptime guarantee). But good luck getting those guaranteed!Uptime calculator (99.9% uptime means 43min downtime per month)Nearest.host – cool site showing nearby server companies.2. Choose the right website hosting service (BEG, HIGH)Shared hosting ($5-30/month) – fine for small sites and low traffic up to 100k hits/month. No access to server configurations.VPS/cloud hosting ($30-300/month) – great for medium sites and traffic up to 30 million hits/month.Dedicated (bare metal) server ($200/month & up) – great for large sites with TONS of traffic.Buy the best that you can comfortably afford. A small website doesn’t need much power but it’s still noticeable when you get a better server and appreciated more than you think. Think of a new phone that opens apps just a fraction of a second quicker. You really can feel the difference and it improves user experience tremendously.Shared webhosting is usually slow because they stuff hundreds of customers/websites onto the same server (maximize profits). This increases slowdowns, unexpected crashes or server restarts, security attacks, and your email IP getting marked as spam.Shared hosting environments are also slow because they load many scripts/modules to maximize compatibility for as many users as possible. And without dedicated resources, your visitors end up waiting in line while the server is busy handling other websites first.VPS/Dedicated servers are faster because there’s more resources available per account and your resources are serving only your websites. You have more control over your environment, can configure it for your needs. VPS/dedicated can be costly or difficult to manage for regular users. There are cloud-panel services to help manage it and also fully-managed services where they take care of everything for you.Those unable to handle technical responsibilities of VPS can go for “premium shared hosting”. They don’t crowd the server as much but the performance (while better than regular shared hosting) will still be far behind a VPS.3. Choose a high performance web server (INT-ADV, HIGH)Use any web server software but Apache. The best is NGINX or LiteSpeed, or highly-optimized Apache (rare to find). The higher your traffic, the more noticeable the difference.NGINX shines at simple sites. Just set it and go. Not much settings to optimize. But once you have a complicated site, NGINX is a mixed bag. Some NGINX features aren’t easy to configure. If you have a server-admin to fine-tune, it’s great but many people don’t.LiteSpeed has more easy-accessible features than NGINX. Like when you need some things cached but not others, or dealing with server-level redirects via htaccess. LiteSpeed also has a WordPress cache plugin which NGINX doesn’t. That’s a HUGE advantage. (I personally prefer LiteSpeed.)OpenLiteSpeed is the free community version of LiteSpeed. It’s a great alternative for those wanting the free price of NGINX but the powerful LiteSpeed cache plugin.Some webhosts have the Apache+NGINX hybrid stack. I feel those are outdated now and makes for unnecessarily slower/heavier stack.If using Apache, MPM events are best (compared to worker or prefork).Keep your webserver updated. Later versions can speed up certain protocols and processes noticeably.4. Web server configuration (ADV, MED-HIGH)Most web servers come with safe/functional configurations right off the bat. Adequate for the average small site with little traffic. It’s when you get more traffic and more security attacks, or have more demanding apps that fine-tuning the configurations makes a big difference.Timeout – 30 to 60 seconds is a safe start. You can increase up to 600 or beyond if needed for long processes (import, export, backups). Keep in mind that allows poorly-coded processes or hack exploits to run out your server resources.# of child processes allowed – depends on the server environment. Default should be fine.Concurrent connections allowed – anywhere from 1-20k. Higher is not necessarily better!Keep alive – on, off, or LiteSpeed’s “smart keep-alive”. I think “on” is safer. If you have LiteSpeed, the smart keep-alive is awesome!Keep alive timeout – 3-5 seconds is a safe start. Increase if needed.How many threads, body/buffer size, workers, clients, etc….all that you can look up online. It depends on your server size and use scenario. Jump on forums and ask around or have a sys-admin configure for you. Keep in mind different admins have their own ways of configuring.The most important distinction for me is to decide whether this server should be set aggressive or conservative:AGGRESSIVE configuration – gives every site as much resources as possible. Good for low-tenant or dedicated servers.CONSERVATIVE configuration – gives every site as little resources as possible. Good for high-tenant or shared servers.5. Disable unused services (INT, HIGH)Many servers are automatically set up with all features running to make things easy for you. But they’re just like brand new computers with pre-installed software. Get rid of the ones you don’t use. Even if they don’t use much memory, they can still be bombarded by hackers and that eats resources.DNS – disable if you’re using external DNS service. (Cloudflare, DNSME, etc.)Email – disable if you’re using 3rd-party email. (G-Suite, MXroute, etc.)FTP/SFTP – disable if not using.Memcache/Redis – disable if you don’t use it.Other services – Varnish, Elastipress, etc.If you want to be OCD, scan your system for all listening ports and services.6. Remove unused server modules (ADV, LOW)Want to be even more OCD? Disable every single module not used by the server. Some of them are junk unused server stuff; others are unused Linux distro stuff. Old school Apache-compatible stacks or unoptimized control panels tend to have many unused modules enabled by default (while also not enabling ones you might need).Read documentation and check online before blindly removing or replacing them. The danger is you disable things you need (or worse, one that improves performance). You should make a list of disabled services/modules to reference later or give to a contractor when troubleshooting.7. Use the latest PHP version (INT-ADV, HIGH)The PHP version alone makes a HUGE difference.Use the latest PHP version possible! (Easily-configured from your webhosting control panel)For example, PHP 7.0 is 3 times faster than PHP 5.6.Even PHP 7.3 is 10% faster than PHP 7.2.At the time of this writing, PHP 7.4 is available.Be wary of any webhosts still using old PHP!Keeping your website PHP version updated is not only for speed but also security. The only issue is some themes or plugins may not be compatible with the latest PHP version. You’ll know because your site doesn’t work right, or looks weird. So test carefully and keep themes/plugins updated, which helps them stay compatible with the latest PHP.8. Recommended php.ini configurations (INT, MED)Most of you (on shared hosting) won’t even have access to these settings or know how to set them. But nonetheless, here are my recommendations.max_execution_time – lower (30-60 sec) is better to prevent resources hogs from lagging out the server. But you may need higher execution times for long processes like imports, exports, backups.max_input_time – lower (60 sec) is better. Increase only if you’re trying to import something that takes forever.max_input_vars – set to “1000”, unless some plugins need higher.memory_limit – try “256M” to be safe. Raise if you have heavy plugins. I like to set lower so I’m notified immediately when there are memory hogs. The “error_log” will tell you if you need more.zlib.output_compression – may or may not help. I leave it off.9. Use an updated MySQL fork version (INT-ADV, LOW)Most people only know of MySQL, which is now acquired/owned by Oracle. There’s also MariaDB (a fork of MySQL by its original creators) and Percona, and also others.MySQL 8 is much better than MySQL 5.7.But it’s better if you can use MariaDB over MySQL. Community-friendly and better performance than vanilla MySQL 5.7.Use the latest MariaDB version that you can.Whatever you do, just don’t use MySQL 5.7.What about Percona? What about the other 3rd-party MySQL-compatible forks? For most sites, it makes little difference if any. Don’t forget to backup your database before changing or upgrading MySQL.MySQL vs MariaDB vs Percona10. Convert MySQL tables from MyISAM to InnoDB (BEG, LOW)Make sure your tables are set to InnoDB instead of MyISAM.InnoDB is newer and regarded as being better overall (faster, safer).MyISAM can be faster in some scenarios (when mostly read-only).You can convert manually in phpMyAdmin or use a plugin (Servebolt Optimizer or LiteSpeed Cache). Can delete the plugin afterwards if you don’t need it.11. Tuning MySQL configurations (ADV, LOW)Usually not required (or noticeably-beneficial) for the average site but can help tremendously for large sites with high traffic and varying query lengths.You can run MySQLTuner for general recommendations or ask around the sys-admin community to see what everyone else uses.Buffer size, packet size, cache, connections, cache, stack, etc…are all among the general things to tune.Simple Linode guide.When trying out random configurations online or copying somebody else’s, please make sure their environment is similar to yours. The main distinctions are:server size, resources availablehow many clients/sites on serverhow many end users on serverhow much traffichow big are the siteswhat kind of read/write behaviorIt’s important to know whether their settings are for efficiency (high-tenant webhost) or aggressive performance (low-tenant webserver).12. Server full-page caching (ADV, HIGH)Full-page caching can help speed-up any website. But caching directly from the server is much more powerful and resource-efficient than PHP/application-level caching done through a plugin.Some Apache or NGINX servers use Varnish – ugghhh, outdated. Don’t use Varnish proxy. Just upgrade to pure-LiteSpeed or pure-NGINX stack.LiteSpeed servers can use LiteSpeed cache – powerful, many features, and comes with a handy WordPress cache plugin (called “LiteSpeed Cache”).NGINX servers can use FastCGI – great, super fast! While there’s no official NGINX cache plugin for WordPress, there are various “NGINX helper” plugins to facilitate basic cache functions (like purging).To be safe, you should disable caching on pages with forms, carts, or checkouts. Private pages (for logged-in users) CAN be cached but don’t mess with that unless you have that much private traffic and ready to spend hours configuring private cache.You can only enable server-level caching if you own or have access to the server. Otherwise, your webhost decides what caching options you have.Shared hosting usually allows all caching plugins.Managed hosting usually limits you to only their proprietary one.13. Memory object-caching (ADV, LOW-HIGH)Object-caching caches only the database queries instead of the entire page html. This technically makes it “slower” than full-page caching (since you’re not caching the entire page) but useful for speeding up dynamic pages or private pages (logged-in users, admin backend) that can’t be static-cached.Any site with lots of constantly-refreshed data on frontend, or lots of numbers and reports on backend…would stand to benefit from object caching. Mostly-static sites or low-traffic sites would not benefit from object-caching at all; don’t use it on them…it can make them slower!Redis is the gold standard in object caching now. It’s superior to the older memcache in almost every way.Memcache is only used in rare situations where Redis doesn’t work or is slower.If your data doesn’t change much, you can set longer object caching times (e.g. 60 mins and up). Longer times means fewer database queries.Otherwise, stick with the default 5-10 mins to be safe…unless you don’t mind users seeing stale data.Object caching can be managed by WordPress plugins. Most ideal if you have one cache plugin to manage both full-page caching and object caching.You can get ~25% faster object caching by using a Unix SocketUNIX sockets are run from a lower-level layer on the OSI networking model and therefore faster than standard TCP sockets.Redis and Memcache UNIX socket configuration guides for CentOS.Redis and Memcache UNIX socket configuration guides for Ubuntu.Note: with UNIX socket enabled, only one server user account (and presumably all sites by that user) can use object caching. So you can’t use this if you plan to have multiple server users deploying object cache.Some background on memory-caching…Memory-caching is the gold standard in caching, because cache runs faster from memory than than from disk. The issue is we have limited amounts of memory (most of it already used for applications) so we can’t store the entire site cache in there. It matters less nowadays anyway since server disks are so much faster now (thanks to SSD technology).Sure memory is more abundant now too but then again, applications are bigger. You may have heard of others loading their entire site into memory…some using the cache route, others by mounting their WordPress directory into memory. It works great if your site is small enough but for most people: your memory is only big enough to store database queries, anything else you want to cache will be stored on your disk.14. Use the latest HTTP protocol (BEG, HIGH)HTTP/2 loads browser requests so much faster than HTTP/1 (thanks to parallelization). It feels like 3 times faster to me.You should be using HTTP/2 or even HTTP/3 (recently released).Avoid older web servers still on the archaic HTTP/1.Check your site for HTTP/2 and HTTP/3.Using HTTP/2 requires HTTPS/SSL. If your site isn’t in HTTPS, do it now!15. Content encoding (INT, HIGH)GZIP is so 2016. Every web-server should have BROTLI compression nowadays. It’s superior to GZIP (produces smaller files AND in less time). But shockingly, BROTLI still isn’t available on all web-servers.If using BROTLI – set static compression to 4.If using GZIP – set dynamic compression to 1, static compression to 6.You can push static compression levels higher if your CPU is strong (or low-usage server) and/or your static content is cached for a long time (long expiry times). If you’re using a CDN or Cloudflare, make sure you enable BROTLI compression there as well.16. Control-panel (INT, HIGH)This issue matters only for VPS users. Control panels used to be critiqued for the initial weight they put on the server. That’s because control panels were originally designed for large dedicated servers, but have since been optimized for smaller VPS. While it’s true that having no control panel is lighter than having one, it makes day-to-day tasks harder. Their footprint is now negligible considering how useful panels can be.The best performing control panel is one that fits your needs.Allows you to pick the web server of your choice – NGINX or LiteSpeed.Easily configure redirects at server level – instead of slower PHP-level redirection plugin.Easily configure granular caching rules – choosing what and what not to cache.Easy to manage – for you or your sys-admin.Can cage users – preventing resource-hogs on high-tenant servers.Secure against hacks – as hacking attempts eat many resources.Easy to use – for you or your clients.17. Use external DNS service (INT, LOW)Lower DNS latency (small benefit)Easy to update DNS (convenience)Will using an external DNS like Cloudflare or DNS Made Easy make a world of difference in terms of webhosting speed? I think it improves lookup time but not so noticeable unless your previous DNS server was a piece of junk by cheap webhost.The main benefit for me is how quickly I can redirect things. Suppose you get hacked and need to redirect through a security proxy. Or maybe you’re switching certain aspects of your site to another server. In moments like this, having a DNS service is so convenient. You can switch things over with very little downtime, and even switch them back quickly if there’s an issue.DNS services may seem like an extra hassle to setup, but once in place they allow you to integrate new services and mitigate performance issues so much faster.18. Run WP-cron from your server (BEG, MED)Many WordPress tasks need a trigger to function. Such as sending out system emails, run backups, release scheduled posts. By default, WordPress uses a function called WP-Cron (conveniently located at yourdomain.com/wp-cron.php). It works by checking (and executing) for any pending tasks any time someone visits the website. It’s great for small sites, but terrible if you have tons of traffic (triggering many unnecessary cron-checks). Also an obvious DDOS vulnerability.The sensible thing is to disable WP-cron and use a real cron job whether from your server or an external cron service. Some cron jobs visit the website directly. Others go through the Linux directory. Use whichever one works. I think a 5-minute interval is good.19. Caging users & resource limits (ADV, MED-HIGH)Do you have many sites or clients on the same machine? Are you unable to police them all effectively? If you’re losing control of your tenants, it’s probably time to limit their resources. I’ll start you off with a few tactics below. You look up the rest.Cloudlinux & CageFS – limit CPU and memory usageServer-wide bans on cache-prebuilding, broken-link checkers, and other resource hogs.Decrease php execution time.Global configs blocking the usual offenders, bots, etc.Worst-case scenario, just split off some clients onto another server. Split them up by size, space usage, traffic amount, whatever you want.19. Caging users & resource limits (ADV, MED-HIGH)Do you have many sites or clients on the same machine? Are you unable to police them all effectively? If you’re losing control of your tenants, it’s probably time to limit their resources. I’ll start you off with a few tactics below. You look up the rest.Cloudlinux & CageFS – limit CPU and memory usageServer-wide bans on cache-prebuilding, broken-link checkers, and other resource hogs.Decrease php execution time.Global configs blocking the usual offenders, bots, etc.Worst-case scenario, just split off some clients onto another server. Split them up by size, space usage, traffic amount, whatever you want.20. Tracking down resource hogs (ADV, MED-HIGH)We often run into slow server issues with no obvious clue of where to look. Today, it’s this client. Tomorrow, it’s that client. It seems any site can be the culprit on any given day. When you have so many clients, and none of them can afford switching plugins on and off, it is really really difficult to track down the problem.Here are some ideas:Check server logs – are you being hacked? Are there excessive requests?Check server monitors – which users are hogging the CPU, memory, and bandwidth?Once you know which site it is…check WordPress error log. Run Query Monitor.Of course, it might not even happen all the time. You have to track down what users or processes were doing when the slowdown happened.Sometimes you’ll need more of a developer mentality. What plugin was updated last? Any new themes or plugins that were custom-coded? (Check for memory-exhaustive commands.) Yes, you can use applications monitors like New Relic but for me, it’s overkill. The trickiest problems are when it seems like every site is the problem. Or also when the server load is low but the sites are still slow. Good luck!In the next part of this blog series we’ll cover caching optimization for your websites, so stay tuned. For a full version of the article go to The Ultimate WordPress Speed Optimization Guide.Need an optimized WordPress hosting that meets the needs of your project? Get in touch with Jelastic for choosing the best option and receiving technical assistance while migration.Related ArticlesEnterprise WordPress Hosting: Automatic Scaling and High AvailabilityWordPress Hosting in Elastic Standalone ContainerHow to Migrate a WordPress Site to Jelastic PaaSSetup WordPress Multisite Network with Domain Mapping and CDNWebinar Summary: WordPress Cluster for Enterprise High Availability and On-Demand ScalingSubscribe to get the latest updates"
36,GET STARTED FOR FREE
36,E-mail
36,Search
36,CategoriesDevelopmentHosting BusinessMulti-CloudWordPressDatabasesJavaContainersKubernetesUse CasesNews and ReleasesSolution
36,Multi-Cloud
36,Private Cloud
36,Public Cloud
36,Cloud Business for Service Providers
36,WordPress Hosting
36,Kubernetes HostingResources
36,Blog
36,Documentation
36,Pricing Overview
36,Video
36,Whitepapers
36,Use CasesGet started
36,Register to Public Cloud
36,Get Private Cloud License
36,Start Hosting Partnership
36,Request Automated Clustering
36,Request Specific Functionality
36,Contact Recent PublicationsJelastic PaaS Q1 2021 Resulted with Five New Cloud Hosting Partners Publicly LaunchedWWF Belgium Migrates to Jelastic PaaS on Infomaniak DatacenterCloud Lessons Learned from OVH Datacenter Fire
36,twitter
36,facebook
36,linkedin
36,youtube
36,github
36,"stackoverflow© 2020 Jelastic. All Rights Reserved, Jelastic, Inc. 228 Hamilton Avenue, 3rd Floor, Palo Alto, CA 94301Terms of UsePrivacy PolicyManage Personal Data"
36,×Thanks for Request!Jelastic team will contact you within 24 hours.
36,Private Cloud Company *Project Details *agreeI read and agree to Jelastic Terms of Use and Privacy Policy*
36,Multi Cloud Company *Project Details *agreeI read and agree to Jelastic Terms of Use and Privacy Policy*
37,Performance Tuning in Athena - Amazon Athena
37,Performance Tuning in Athena - Amazon Athena
37,AWSDocumentationAmazon AthenaUser Guide
37,Physical LimitsQuery Optimization
37,TechniquesAdditional Resources
37,Performance Tuning in Athena
37,This topic provides general information and specific suggestions for improving the
37,performance of Athena when you have large amounts of data and experience memory usage
37,performance issues.
37,Physical Limits
37,"In general, Athena limits the runtime of each query to 30 minutes. Queries that run"
37,beyond this limit are automatically cancelled without charge. If a query runs out
37,"memory or a node crashes during processing, errors like the following can occur:"
37,INTERNAL_ERROR_QUERY_ENGINE
37,EXCEEDED_MEMORY_LIMIT: Query exceeded local memory limit
37,Query exhausted resources at this scale factor
37,Encountered too many errors talking to a worker node. The node may have crashed or be under too much load.
37,Query Optimization
37,Techniques
37,"For queries that require resources beyond existing limits, you can either optimize"
37,the
37,"query or restructure the data being queried. To optimize your queries, consider the"
37,suggestions in this section.
37,Data Size
37,File Formats
37,"Joins, Grouping, and"
37,Unions
37,Partitioning
37,Window Functions
37,Use More Efficient
37,Functions
37,Data Size
37,Avoid single large files – Single files are
37,"loaded into a single node for processing. If your file size is extremely large, try"
37,to break up the file into smaller files and use partitions to organize them.
37,Read a smaller amount of data at once –
37,Scanning a large amount of data at one time can slow down the query and increase
37,cost. Use partitions or filters to limit the files to be scanned.
37,Avoid having too many columns – The message
37,GENERIC_INTERNAL_ERROR:
37,io.airlift.bytecode.CompilationException can occur when Athena fails
37,to compile the query to bytecode. This exception is usually caused by having too
37,many columns in the query. Reduce the number of the columns in the query or create
37,subqueries and use a JOIN that retrieves a smaller amount of
37,data.
37,Avoid large query outputs – Because query
37,"results are written to Amazon S3 by a single Athena node, a large amount of output"
37,data
37,"can slow performance. To work around this, try using CTAS to create a new table with the"
37,result of the query or INSERT INTO to
37,append new results into an existing table.
37,Avoid CTAS queries with a large output –
37,"Because output data is written by a single node, CTAS queries can also use a large"
37,"amount of memory. If you are outputting large amount of data, try separating the"
37,task into smaller queries.
37,"If possible, avoid having a large number of small"
37,files – Amazon S3 has a limit of 5500
37,requests per second. Athena queries share the same limit. If you need to scan
37,"millions of small objects in a single query, your query can be easily throttled by"
37,"Amazon S3. To avoid excessive scanning, use AWS Glue ETL to periodically compact your"
37,files
37,"or partition the table and add partition key filters. For more information, see"
37,Reading Input Files in Larger Groups in the AWS Glue Developer Guide or
37,How can I
37,configure an AWS Glue ETL job to output larger files? in the AWS
37,Knowledge Center.
37,Avoid scanning an entire table – Use the
37,following techniques to avoid scanning entire tables:
37,"Limit the use of ""*"". Try not to select all columns unless"
37,necessary.
37,Avoid scanning the same table multiple times in the same query
37,Use filters to reduce the amount of data to be scanned.
37,"Whenever possible, add a LIMIT clause."
37,Avoid referring to many views and tables in a single
37,query – Because queries with many views and/or tables must load
37,"a large amount of data to a single node, out of memory errors can occur. If"
37,"possible, avoid referring to an excessive number of views or tables in a single"
37,query.
37,Avoid large JSON strings – If data is stored
37,"in a single JSON string and the size of the JSON data is large, out of memory errors"
37,can occur when the JSON data is processed.
37,File Formats
37,Use an efficient file format such as Parquet or ORC
37,"– To dramatically reduce query running time and costs, use compressed Parquet"
37,or ORC files to store your data. To convert your existing dataset to those formats
37,"in Athena, you can use CTAS. For more information, see Using CTAS and INSERT INTO for ETL and Data"
37,Analysis.
37,Switch between ORC and Parquet formats –
37,Experience shows that the same set of data can have significant differences in
37,processing time depending on whether it is stored in ORC or Parquet format. If you
37,"are experiencing performance issues, try a different format."
37,Hudi queries – Because Hudi queries bypass
37,"the native reader and split generator for files in parquet format, they can be slow."
37,Keep this in mind when querying Hudi datasets.
37,"Joins, Grouping, and"
37,Unions
37,Reduce the usage of memory intensive operations
37,"– Operations like JOIN, GROUP BY, ORDER"
37,"BY, and UNION all require loading large amount of data into"
37,"memory. To speed up your query, find other ways to achieve the same results, or add"
37,a clause like LIMIT to the outer query whenever possible.
37,Consider using UNION ALL – To eliminate
37,"duplicates, UNION builds a hash table, which consumes memory. If your"
37,"query does not require the elimination of duplicates, consider using UNION"
37,ALL for better performance.
37,Use CTAS as an intermediary step to speed up JOIN
37,operations – Instead of loading and processing intermediary data
37,"with every query, use CTAS to persist the intermediary data into Amazon S3. This can"
37,help
37,speed up the performance of operations like JOIN.
37,Partitioning
37,Limit the number of partitions in a table –
37,"When a table has more than 100,000 partitions, queries can be slow because of the"
37,large number of requests sent to AWS Glue to retrieve partition information. To resolve
37,"this issue, try one of the following options:"
37,Use ALTER TABLE DROP PARTITION to remove stale
37,partitions.
37,"If your partition pattern is predictable, use partition projection."
37,Remove old partitions even if they are empty
37,"– Even if a partition is empty, the metadata of the partition is still stored"
37,in AWS Glue. Loading these unneeded partitions can increase query runtimes. To remove
37,"the unneeded partitions, use ALTER TABLE DROP PARTITION."
37,Look up a single partition – When looking up
37,"a single partition, try to provide all partition values so that Athena can locate"
37,the
37,"partition with a single call to AWS Glue. Otherwise, Athena must retrieve all partitions"
37,and filter them. This can be costly and greatly increase the planning time for your
37,"query. If you have a predictable partition pattern, you can use partition"
37,projection to avoid the partition look up calls to AWS Glue.
37,Set reasonable partition projection properties
37,"– When using partition projection, Athena tries to"
37,"create a partition object for every partition name. Because of this, make sure that"
37,the table properties that you define do not create a near infinite amount of
37,possible partitions.
37,"To add new partitions frequently, use ALTER TABLE ADD"
37,PARTITION – If you use MSCK REPAIR TABLE
37,"to add new partitions frequently (for example, on a daily basis) and are"
37,"experiencing query timeouts, consider using ALTER TABLE ADD PARTITION."
37,MSCK REPAIR TABLE is best used when creating a table for the first
37,time or when there is uncertainty about parity between data and partition
37,metadata.
37,Avoid using coalesce()in a WHERE clause with partitioned
37,"columns – Under some circumstances, using the coalesce() or other functions in a WHERE clause against"
37,"partitioned columns might result in reduced performance. If this occurs, try"
37,rewriting your query to provide the same functionality without using
37,coalesce().
37,Window Functions
37,Minimize the use of window functions –
37,Window
37,functions such as rank()
37,"are memory intensive. In general, window functions require an entire dataset to be"
37,"loaded into a single Athena node for processing. With an extremely large dataset,"
37,"this can risk crashing the node. To avoid this, try the following options:"
37,Filter the data and run window functions on a subset of the data.
37,Use the PARTITION BY clause with the window function whenever
37,possible.
37,Find an alternative way to construct the query.
37,Use More Efficient
37,Functions
37,Replace row_number() OVER (...) as rnk ... WHERE rnk =
37,"1 – To speed up a query with a row_number() clause like this, replace this syntax with a combination"
37,"of GROUP BY, ORDER BY, and LIMIT 1."
37,Use regular expressions instead of LIKE on
37,large strings – Queries that include clauses such as LIKE
37,'%string%' on large strings can be very
37,costly. Consider using the regexp_like() function and a regular expression instead.
37,"Use max() instead of element_at(array_sort(), 1)"
37,"– For increased speed, replace the nested functions"
37,"element_at(array_sort(), 1) with max()."
37,Additional Resources
37,"For additional information on performance tuning in Athena, consider the following"
37,resources:
37,Read the AWS Big Data blog post Top 10
37,Performance Tuning Tips for Amazon Athena
37,Read other Athena
37,posts in the AWS Big Data Blog
37,Visit the Amazon Athena Forum
37,Consult the Athena
37,topics in the AWS Knowledge Center
37,"Contact AWS Support (in the AWS console, click Support,"
37,Support Center)
37,Javascript is disabled or is unavailable in your
37,browser.
37,"To use the AWS Documentation, Javascript must be"
37,enabled. Please refer to your browser's Help pages for instructions.
37,Document Conventions
37,Troubleshooting
37,"Code Samples, Service Quotas, and Previous JDBC Driver"
37,Did this page help you? - Yes
37,Thanks for letting us know we're doing a good
37,job!
37,"If you've got a moment, please tell us what we did right"
37,so we can do more of it.
37,Did this page help you? - No
37,Thanks for letting us know this page needs work. We're
37,sorry we let you down.
37,"If you've got a moment, please tell us how we can make"
37,the documentation better.
39,MySQL Performance Tuning Part 1 - Architecture - Distributed Systems Authority
39,Skip to content
39,Distributed Systems Authority
39,"Ideas behind Reliable, Scalable, and Maintenable Systems"
39,Blog
39,Videos
39,Distributed Systems Authority
39,"Ideas behind Reliable, Scalable, and Maintenable Systems"
39,Toggle Navigation
39,Toggle Navigation
39,Blog
39,Videos
39,Search for...
39,"MySQL Performance Tuning Part 1 – Architectureby Lucian OpreaJanuary 1, 2021January 10, 2021"
39,Table Of ContentsKey Takeaways MySQL’s Logical ArchitectureQuery Execution BasicsUtility LayerThe MySQL Cient/Server ProtocolThe Query CacheSQL LayerThe parser and the preprocessorThe query optimizerThe query execution engineReturning results to the clientStorage Engine LayerThe InnoDB EngineSources of InformationConsider the Whole StackQuery Tuning MethodologyConclusionsBibliography
39,Key Takeaways
39,"We need to understand MySQL design so that we can work with it, and not against it.Everything in InnoDB is an index. The primary key is used for the clustered index, so we must choose it with care.Monitoring is an absolute must in performance tuning."
39,MySQL’s Logical Architecture
39,"MySQL is very different from other database servers, and its architectural characteristics make it useful for a wide range of purposes. MySQL is not perfect, but it is flexible enough to work well in very demanding environments, such as web applications, data warehouses, content indexing, highly available redundant systems, online transaction processing (OLTP), and much more."
39,"To get the most from MySQL, we need to understand its design so that we can work with it, and not against it."
39,"MySQL’s most unusual and important feature is its storage-engine architecture, whose design separates query processing and other server tasks from data storage and retrieval."
39,Query Execution Basics
39,"If you need to get high performance from your MySQL server, one of the best ways to invest your time is in learning how MySQL optimizes and executes queries. Once you understand this, much of query optimization is a matter of reasoning from principles, and query optimization becomes a very logical process."
39,Here is the process MySQL follows when we send a query to the server.
39,"The client sends the SQL statement to the server.The server checks the query cache. If there’s a hit, it returns the stored result from the cache; otherwise, it passes the SQL statement to the next step.The server parses, preprocesses, and optimizes the SQL into a query execution plan.The query execution engine executes the plan by making calls to the storage engine API.The server sends the result to the client."
39,"Each step takes time and may itself be a complex operation consisting of several subparts, which we discuss in the following sections."
39,MySQL Architecture
39,MySQL Server can be divided into 3 layers.
39,Utility Layer
39,"The first layer contains the services that aren’t unique to MySQL. Client-server architecture and most of the network-based tools need these services. Connection handling, authentication, security, and so forth."
39,The MySQL Cient/Server Protocol
39,"This CLIENT/SERVER protocol, makes MySQL communication simple and fast, but it limits it in some ways too. For one thing, it means there’s no flow control; once one side sends a message, the other side must fetch the entire message before responding. When the server responds, the client has to receive the entire result set. This is why LIMIT clauses are so important."
39,"Now, if we are using libraries to connect to MySQL, then we should be aware that the default behaviour for most libraries is to fetch the whole result and buffer it in memory. This is important because until all the rows have been fetched, the MySQL server will not release the locks and other resources required by the query. The query will be in the “Sending data” state. When the client library fetches the results all at once, it reduces the amount of work the server needs to do: the server can finish and clean up the query as quickly as possible."
39,"Most client libraries let you treat the result set as though you’re fetching it from the server, although in fact, you’re just fetching it from the buffer in the library’s memory."
39,"This works fine most of the time, but it’s not a good idea for huge result sets that might take a long time to fetch and use a lot of memory. You can use less memory, and start working on the result sooner, if you instruct the library not to buffer the result. The downside is that the locks and other resources on the server will remain open while your application is interacting with the library."
39,The Query Cache
39,"Before even parsing the query, though, the server consults the query cache, which can store only SELECT statements, along with their result sets. If anyone issues a query that’s identical to one already in the cache, the server doesn’t need to parse, optimize, or execute the query at all—it can simply pass back the stored result set."
39,SQL Layer
39,"The second layer is where things get interesting. Much of MySQL’s brains are here, including the code for query analysis, optimization, caching, and all the built-in functions (e.g., dates, times, math, and encryption). Any functionality provided across storage engines lives at this level: stored procedures, triggers, and views, for example."
39,"The next step in the query lifecycle turns a SQL query into an execution plan for the query execution engine. It has several substeps: parsing, preprocessing, and optimization."
39,"The goal of this section is not trying to document the MySQL internals, but simply to understand how MySQL executes queries so that we can write better ones."
39,The parser and the preprocessor
39,"To begin, MySQL’s parser breaks the query into tokens and builds a “parse tree” from them. The parser uses MySQL’s SQL grammar to interpret and validate the query. For example, for mistakes such as quoted strings that aren’t terminated, or checking that tables and columns exist."
39,"Next, the preprocessor checks privileges. This is normally very fast unless your server has large numbers of privileges."
39,The query optimizer
39,The parse tree is now valid and ready for the optimizer to turn it into a query execution plan. A query can often be executed many different ways and produce the same result. The optimizer’s job is to find the best option.
39,"MySQL uses a cost-based optimizer, which means it tries to predict the cost of various execution plans and choose the least expensive."
39,"It bases the estimate on statistics: the number of pages per table or index, the cardinality (number of distinct values) of the indexes, the length of the rows and keys, and the key distribution. The optimizer does not include the effects of any type of caching in its estimates—it assumes every read will result in a disk I/O operation."
39,"The optimizer might not always choose the best plan, for many reasons:"
39,"The statistics could be wrong. The server relies on storage engines to provide statistics, and they can range from exactly correct to wildly inaccurate. For example, the InnoDB storage engine doesn’t maintain accurate statistics about the number of rows in a table because of its MVCC architecture.The cost metric is not exactly equivalent to the true cost of running the query. It might be more or less expensive than MySQL’s approximation. For example when MySQL doesn’t understand which pages are in memory and which pages are on disk, so it doesn’t really know how much I/O the query will cause.You probably want the fastest execution time, but MySQL doesn’t really try to make queries fast; it tries to minimize their cost.The optimizer can’t always estimate every possible execution plan, so it might miss an optimal plan."
39,"MySQL’s query optimizer can apply Static optimizations such as those independent of values, For example, constant values in a WHERE clause. They can be performed once and will always be valid, even when the query is re-executed."
39,"In contrast, dynamic optimizations are based on context and can depend on many factors, such as which value is in a WHERE clause or how many rows are in an index. They must be reevaluated each time the query is executed. You can think of these as “runtime optimizations.”"
39,"MySQL knows how to do a lot of optimizations on its own such as Reordering joins, converting joins, Applying algebraic equivalence rules, COUNT(), MIN(), and MAX() optimizations, Subquery optimization, and so on."
39,"Of course, as smart as the optimizer is, there are times when it doesn’t give the best result. Sometimes you might know something about the data that the optimizer doesn’t."
39,"If you know the optimizer isn’t giving a good result, and you know why, you can help it. Some of the options are to add a hint to the query, rewrite the query, redesign your schema, or add indexes as we’ll see in future sections."
39,The query execution engine
39,"The parsing and optimizing stage outputs a query execution plan, which MySQL’s query execution engine uses to process the query."
39,"In contrast to the optimization stage, the execution stage is usually not all that complex: MySQL simply follows the instructions given in the query execution plan. Many of the operations in the plan invoke methods implemented by the storage engine interface, also known as the handler API."
39,Returning results to the client
39,"The final step in executing a query is to reply to the client. Even queries that don’t return a result set still reply to the client connection with information about the query, such as how many rows it affected."
39,"If the query is cacheable, MySQL will also place the results into the query cache at this stage."
39,Storage Engine Layer
39,The third layer contains the storage engines. They are responsible for storing and retrieving all data stored “in” MySQL. They are implemented as plugins which makes it relatively easy to implement different ways to handle data.
39,The main storage engine – and the only one that will be considered in this guide – is InnoDB which is fully transactional and has very good support for high-concurrency workloads.
39,An example of another storage engine is NDBCluster which is also transactional and is used as part of MySQL NDB Cluster.
39,"The optimizer does not really care what storage engine a particular table uses, but the storage engine does affect how the server optimizes the query. The optimizer asks the storage engine about some of its capabilities and the cost of certain operations, and for statistics on the table data."
39,"The storage engine may itself be complex. For InnoDB, it includes a buffer pool used to cache data and indexes, redo and undo logs, other buffers, as well as tablespace files. If the query returns rows, these are sent back from the storage engine through the SQL layer to the application."
39,"Before even parsing the query, though, the server consults the query cache, which can store only SELECT statements, along with their result sets. If anyone issues a query that’s identical to one already in the cache, the server doesn’t need to parse, optimize, or execute the query at all—it can simply pass back the stored result set."
39,"In query tuning, the most important steps are the optimizer and execution steps including the storage engine. Most of the information in this guide relates to these three parts either directly or indirectly."
39,The InnoDB Engine
39,"InnoDB is a general-purpose storage engine that balances high reliability and high performance and it was designed for processing many short-lived transactions. You should use InnoDB for your tables unless you have a compelling need to use a different engine. If you want to study storage engines, it is also well worth your time to study InnoDB in depth to learn as much as you can about it, rather than studying all storage engines equally."
39,InnoDB tables are built on a clustered index and is the term used for how InnoDB organizes the data. The name comes from the fact that index values are clustered together. Everything in InnoDB is an index. The row data is in the leaf pages of a B-tree index.
39,"The primary key is used for the clustered index. If you do not specify an explicit primary key, InnoDB will look for a unique index that does not allow NULL values. If that does not exist either, InnoDB will add a hidden 6-byte integer column using a global (for all InnoDB tables) auto-increment value to generate a unique value."
39,"As a result, it provides very fast primary key lookups. However, secondary indexes (indexes that aren’t the primary key) contain the primary key columns, so if your primary key is large, other indexes will also be large. You should strive for a small primary key if you’ll have many indexes on a table."
39,The choice of primary key also has performance implications. These will be discussed in the section “Index Strategies” later in another section.
39,Sources of Information
39,"If you take just one thing with you from following this section, then let it be that monitoring is critical to maintaining a healthy system. Everything about high performance should revolve around monitoring."
39,Your monitoring should use several sources of information. These include but are not limited to
39,"The Performance Schema – which includes information ranging from low-level mutexes to query and transaction metrics. This is the single most important source of information for query performance tuning. The sys schema provides a range of ready-made reports based on the Performance Schema, but they include filters, sorting, and formatting that make the reports easy to use."
39,USE performance_schema;
39,SHOW TABLES;
39,"The Information Schema – which includes schema information, InnoDB statistics, and more."
39,"SHOW statements – which, for example, include information from InnoDB with detailed engine statistics."
39,SHOW TABLE STATUS LIKE 'city' \G;
39,"The slow query log – which can record queries matching certain criteria such as taking longer than a predefined threshold, even after instance restarts.The EXPLAIN statement to return the query execution plan. This is an invaluable tool to investigate why a query is not performing well due to missing indexes, the query being written in a suboptimal way, or MySQL choosing a suboptimal way to execute the query. The EXPLAIN statement is mostly used in an ad hoc fashion when investigating a specific query.Operating system metrics such as disk utilization, memory usage, and network usage. Do not forget simple metrics such as the amount of free storage as running out of storage will cause an outage."
39,Consider the Whole Stack
39,"When an application needs the result of the query or needs to store data in MySQL, it sends the request over the network to MySQL, and in order to execute the request, MySQL interacts with the operating system and uses host resources such as memory and disk. Once the result of the request is ready, it is communicated back to the application through the network."
39,MySQL Stack
39,Query Tuning Methodology
39,Performance tuning can be seen as a continuous process where an iterative approach is used to gradually improve performance over time.
39,MySQL Query Tuning Methodology
39,"First, we should have a clear problem description. It is not enough to say “MySQL is slow”. A specific problem may be that “The query used to display user’s transactions takes five seconds” or that “MySQL can only sustain 5000 transactions per second.” The more specific we are, the better chance we have solving the problem."
39,"The preparation work should also include collecting a baseline from our monitoring or running a data collection that illustrates the problem. Without the baseline, we may not be able to prove that we have solved the issue at the end of the troubleshooting."
39,"Then we can work on determining the cause of the poor performance is. Here,"
39,"we can be open-minded and consider the whole stack, so we don’t end up staring ourselves blind on one aspect that turns out not to have anything to do with the problem."
39,"Finding the cause is often the hardest part of an investigation, but we’ll cover some good starting points and common causes in a later section."
39,"And once the cause is clear, we can determine what we want to do to solve the problem."
39,"The first step is to brainstorm possible solutions; second, you must choose which one to implement. It can happen that the solution we first chose does not work or have unacceptable side effects."
39,MySQL Query Tuning Methodology
39,"When we apply a solution we picked it’s important to create an action plan for it. Here it is important to be very specific, so we can ensure that the action plan we test is also the one we’ll end up applying to our production system."
39,"We then need to test the action plan on a test system. It is important that it reflects production as closely as possible. The data we have on the test system must be representative of our production data. One way to achieve this is to copy the production data, optionally using data masking to avoid copying sensitive information such as personal details and credit card information out of our production system."
39,"It is possible that for one reason or another, the solution does not completely work as expected on production. One possibility is that the index statistics that are random in nature were different, so an ANALYZE TABLE statement to update the index statistics was necessary when applying the solution to the production system."
39,"If the solution works, you should collect a new baseline that you can use for future monitoring and optimizations."
39,"We are then ready to start all over, either doing a second iteration to improve the performance further for the problem we have just been looking at, or we may need to work on a second problem."
39,Conclusions
39,"MySQL has a layered architecture, with server-wide services and query execution on top and storage engines underneath."
39,"The key takeaways are that you need to consider the whole stack from the end user to the low-level details of the host and operating system and monitoring is an absolute must in performance tuning. Executing a query includes several steps, of which the optimizer and execution steps are the most important and are ones that you will learn the most about in later sections."
39,Bibliography
39,"MySQL 8 Query Performance Tuning, Jesper Wisborg Krogh, Apress, March 2020High Performance MySQL, 3rd Edition, Baron Schwartz, Peter Zaitsev, Vadim Tkachenko, O’Reilly Media, Inc., March 2012"
39,Leave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment Name *
39,Email *
39,Website
39,"Save my name, email, and website in this browser for the next time I comment."
39,PostgreSQL High Performance Tuning Guide Course
39,Use the following coupon
39,to get 66% percent off.
39,"""LEARNING202104"""
39,Check the Full Course
39,Search for:
39,Recent Posts
39,MySQL High Performance Part 3 – EXPLAIN Queries
39,MySQL High Performance Part 2 – Finding Candidates for Query Optimizations
39,MySQL Performance Tuning Part 1 – Architecture
39,DCA – Describe and demonstrate how to deploy a service on a Docker overlay network
39,Solr 8 Facet Query – Hands-On Example
39,Categories
39,Apache Solr
39,Docker Certified Associate
39,MySQL
39,PostgreSQL
39,Reactive Systems
39,Recent CommentsBriannoumb on Server Tuning Guideline – PostgreSQL 12 High Performance Guide (Part 10/12)Briannoumb on Server Tuning Guideline – PostgreSQL 12 High Performance Guide (Part 10/12)
40,Range join optimization | Databricks on AWS
40,Support
40,Feedback
40,Try Databricks
40,Help Center
40,Documentation
40,Knowledge Base
40,Amazon Web Services
40,Microsoft Azure
40,Google Cloud Platform
40,Documentation for Databricks on AWS
40,Getting started with Databricks
40,Databricks SQL Analytics guide
40,Databricks Workspace guide
40,Get started with Databricks Workspace
40,Language roadmaps
40,User guide
40,Data guide
40,Delta Lake and Delta Engine guide
40,Introduction
40,Delta Lake quickstart
40,Introductory notebooks
40,Ingest data into Delta Lake
40,Table batch reads and writes
40,Table streaming reads and writes
40,"Table deletes, updates, and merges"
40,Table utility commands
40,Constraints
40,Table versioning
40,Delta Lake API reference
40,Concurrency control
40,Integrations
40,Migration guide
40,Best practices
40,Frequently asked questions (FAQ)
40,Delta Lake resources
40,Delta Engine
40,Optimize performance with file management
40,Auto Optimize
40,Optimize performance with caching
40,Dynamic file pruning
40,Isolation levels
40,Bloom filter indexes
40,Optimize join performance
40,Range join optimization
40,Skew join optimization
40,Optimized data transformation
40,Machine learning and deep learning guide
40,MLflow guide
40,Genomics guide
40,Administration guide
40,API reference
40,Release notes
40,Resources
40,"Updated Apr 13, 2021"
40,Send us feedback
40,Documentation
40,Databricks Workspace guide
40,Delta Lake and Delta Engine guide
40,Delta Engine
40,Optimize join performance
40,Range join optimization
40,Range join optimization
40,A range join occurs when two relations are joined using a point in interval or interval overlap condition.
40,"The range join optimization support in Databricks Runtime can bring orders of magnitude improvement in query performance, but requires careful manual tuning."
40,Point in interval range join
40,A point in interval range join is a join in which the condition contains predicates specifying that a value from one relation is between two values from the other relation. For example:
40,-- using BETWEEN expressions
40,SELECT *
40,FROM points JOIN ranges ON points.p BETWEEN ranges.start and ranges.end;
40,-- using inequality expressions
40,SELECT *
40,FROM points JOIN ranges ON points.p >= ranges.start AND points.p < ranges.end;
40,-- with fixed length interval
40,SELECT *
40,FROM points JOIN ranges ON points.p >= ranges.start AND points.p < ranges.start + 100;
40,-- join two sets of point values within a fixed distance from each other
40,SELECT *
40,FROM points1 p1 JOIN points2 p2 ON p1.p >= p2.p - 10 AND p1.p <= p2.p + 10;
40,-- a range condition together with other join conditions
40,SELECT *
40,"FROM points, ranges"
40,WHERE points.symbol = ranges.symbol
40,AND points.p >= ranges.start
40,AND points.p < ranges.end;
40,Interval overlap range join
40,An interval overlap range join is a join in which the condition contains predicates specifying an overlap of intervals between two values from each relation. For example:
40,"-- overlap of [r1.start, r1.end] with [r2.start, r2.end]"
40,SELECT *
40,FROM r1 JOIN r2 ON r1.start < r2.end AND r2.start < r1.end;
40,-- overlap of fixed length intervals
40,SELECT *
40,FROM r1 JOIN r2 ON r1.start < r2.start + 100 AND r2.start < r1.start + 100;
40,-- a range condition together with other join conditions
40,SELECT *
40,FROM r1 JOIN r2 ON r1.symbol = r2.symbol
40,AND r1.start <= r2.end
40,AND r1.end >= r2.start;
40,Range join optimization
40,The range join optimization is performed for joins that:
40,Have a condition that can be interpreted as a point in interval or interval overlap range join.
40,"All values involved in the range join condition are of a numeric type (integral, floating point, decimal), DATE, or TIMESTAMP."
40,"All values involved in the range join condition are of the same type. In the case of the decimal type, the values also need to be of the same scale and precision."
40,"It is an INNER JOIN, or in case of point in interval range join, a LEFT OUTER JOIN with point value on the left side, or RIGHT OUTER JOIN with point value on the right side."
40,Have a bin size tuning parameter.
40,Bin size
40,"The bin size is a numeric tuning parameter that splits the values domain of the range condition into multiple bins of equal size. For example, with a bin size of 10, the optimization splits the domain into bins that are intervals of length 10."
40,"If you have a point in range condition of p BETWEEN start AND end, and start is 8 and end is 22, this value interval overlaps with three bins of length 10 – the first bin from 0 to 10, the second bin from 10 to 20, and the third bin from 20 to 30. Only the points that fall within the same three bins need to be considered as possible join matches for that interval. For example, if p is 32, it can be ruled out as falling between start of 8 and end of 22, because it falls in the bin from 30 to 40."
40,Note
40,"For DATE values, the value of the bin size is interpreted as days. For example, a bin size value of 7 represents a week."
40,"For TIMESTAMP values, the value of the bin size is interpreted as seconds. If a sub-second value is required, fractional values can be used. For example, a bin size value of 60 represents a minute, and a bin size value of 0.1 represents 100 milliseconds."
40,You can specify the bin size either by using a range join hint in the query or by setting a session configuration parameter.
40,The range join optimization is applied only if you manually specify the bin size. Section Choose the bin size describes how to choose an optimal bin size.
40,Enable range join using a range join hint
40,"To enable the range join optimization in a SQL query, you can use a range join hint to specify the bin size."
40,The hint must contain the relation name of one of the joined relations and the numeric bin size parameter.
40,"The relation name can be a table, a view, or a subquery."
40,"SELECT /*+ RANGE_JOIN(points, 10) */ *"
40,FROM points JOIN ranges ON points.p >= ranges.start AND points.p < ranges.end;
40,"SELECT /*+ RANGE_JOIN(r1, 0.1) */ *"
40,"FROM (SELECT * FROM ranges WHERE ranges.amount < 100) r1, ranges r2"
40,WHERE r1.start < r2.start + 100 AND r2.start < r1.start + 100;
40,"SELECT /*+ RANGE_JOIN(c, 500) */ *"
40,FROM a
40,JOIN b ON (a.b_key = b.id)
40,JOIN c ON (a.ts BETWEEN c.start_time AND c.end_time)
40,Note
40,"In the third example, you must place the hint on c."
40,"This is because joins are left associative, so the query is interpreted as (a JOIN b) JOIN c,"
40,and the hint on a applies to the join of a with b and not the join with c.
40,"You can also place a range join hint on one of the joined DataFrames. In that case, the hint contains just the numeric bin size parameter."
40,"val df1 = spark.table(""ranges"").as(""left"")"
40,"val df2 = spark.table(""ranges"").as(""right"")"
40,"val joined = df1.hint(""range_join"", 10)"
40,".join(df2, $""left.type"" === $""right.type"" &&"
40,"$""left.end"" > $""right.start"" &&"
40,"$""left.start"" < $""right.end"")"
40,val joined2 = df1
40,".join(df2.hint(""range_join"", 0.5), $""left.type"" === $""right.type"" &&"
40,"$""left.end"" > $""right.start"" &&"
40,"$""left.start"" < $""right.end"")"
40,Enable range join using session configuration
40,"If you don’t want to modify the query, you can specify the bin size as a configuration parameter."
40,SET spark.databricks.optimizer.rangeJoin.binSize=5
40,"This configuration parameter applies to any join with a range condition. However, a different bin size set through a range join hint always overrides the one set through the parameter."
40,Choose the bin size
40,The effectiveness of the range join optimization depends on choosing the appropriate bin size.
40,"A small bin size results in a larger number of bins, which helps in filtering the potential matches."
40,"However, it becomes inefficient if the bin size is significantly smaller than the encountered value intervals, and the value intervals overlap multiple bin intervals. For example, with a condition p BETWEEN start AND end, where start is 1,000,000 and end is 1,999,999, and a bin size of 10, the value interval overlaps with 100,000 bins."
40,"If the length of the interval is fairly uniform and known, we recommend that you set the bin size to the typical expected length of the value interval. However, if the length of the interval is varying and skewed, a balance must be found to set a bin size that filters the short intervals efficiently, while preventing the long intervals from overlapping too many bins. Assuming a table ranges, with intervals that are between columns start and end, you can determine different percentiles of the skewed interval length value with the following query:"
40,"SELECT APPROX_PERCENTILE(CAST(end - start AS DOUBLE), ARRAY(0.5, 0.9, 0.99, 0.999, 0.9999)) FROM ranges"
40,"A recommended setting of bin size would be the maximum of the value at the 90th percentile, or the value at the 99th percentile divided by 10, or the value at the 99.9th percentile divided by 100 and so on. The rationale is:"
40,"If the value at the 90th percentile is the bin size, only 10% of the value interval lengths are longer than the bin interval, so span more than 2 adjacent bin intervals."
40,"If the value at the 99th percentile is the bin size, only 1% of the value interval lengths span more than 11 adjacent bin intervals."
40,"If the value at the 99.9th percentile is the bin size, only 0.1% of the value interval lengths span more than 101 adjacent bin intervals."
40,"The same can be repeated for the values at the 99.99th, the 99.999th percentile, and so on if needed."
40,The described method limits the amount of skewed long value intervals that overlap multiple bin intervals.
40,The bin size value obtained this way is only a starting point for fine tuning; actual results may depend on the specific workload.
40,"© Databricks 2021. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation."
40,Send us feedback
40,| Privacy Policy | Terms of Use
42,"Developer Tips, Tricks & Resources – Stackify"
42,"Developer Tips, Tricks & Resources Archives - Stackify"
42,Product
42,Features OverviewApplication Performance ManagementCentralized LoggingFull Transaction Tracing
42,Menu OverviewApplication Performance ManagementCentralized LoggingFull Transaction Tracing Error TrackingApplication & Server MonitoringReal User MonitoringDeployment Tracking
42,Menu Error TrackingApplication & Server MonitoringReal User MonitoringDeployment Tracking
42,"Try Our Free Code ProfilerWant to write better code? Check out our free transaction tracing tool, Prefix!"
42,Features OverviewApplication Performance ManagementCentralized LoggingFull Transaction TracingError TrackingApplication & Server MonitoringReal User MonitoringDeployment Tracking
42,"Menu OverviewApplication Performance ManagementCentralized LoggingFull Transaction TracingError TrackingApplication & Server MonitoringReal User MonitoringDeployment TrackingTry Our Code ProfilerWant to write better code? Check out our free transaction tracing tool, Prefix! PricingSolutions By Role For DevelopersFor DevOps"
42,Menu For DevelopersFor DevOpsBy Technology Azure MonitoringAWS MonitoringCompatibility
42,Menu Azure MonitoringAWS MonitoringCompatibilityBy Language .NETJavaPHPNode.jsRubyPython
42,Menu .NETJavaPHPNode.jsRubyPython
42,By Role For DevelopersFor DevOps
42,Menu For DevelopersFor DevOpsBy Technology Azure MonitoringAWS MonitoringCompatibility
42,Menu Azure MonitoringAWS MonitoringCompatibilityBy Language .NETJavaPHPNode.jsRubyPython
42,Menu .NETJavaPHPNode.jsRubyPython Learn Resources BlogeBooksCase StudiesWebinarsROI CalculatorNews
42,Menu BlogeBooksCase StudiesWebinarsROI CalculatorNewsTechnical DocumentationSupportIdeas Portal
42,Menu DocumentationSupportIdeas Portal
42,Resources BlogeBooksCase StudiesWebinarsROI CalculatorNews
42,Menu BlogeBooksCase StudiesWebinarsROI CalculatorNewsTechnical DocumentationSupportIdeas Portal
42,Menu DocumentationSupportIdeas Portal Login
42,Start Free Trial
42,Product
42,Features OverviewApplication Performance ManagementCentralized LoggingFull Transaction Tracing
42,Menu OverviewApplication Performance ManagementCentralized LoggingFull Transaction Tracing Error TrackingApplication & Server MonitoringReal User MonitoringDeployment Tracking
42,Menu Error TrackingApplication & Server MonitoringReal User MonitoringDeployment Tracking
42,"Try Our Free Code ProfilerWant to write better code? Check out our free transaction tracing tool, Prefix!"
42,Features OverviewApplication Performance ManagementCentralized LoggingFull Transaction TracingError TrackingApplication & Server MonitoringReal User MonitoringDeployment Tracking
42,"Menu OverviewApplication Performance ManagementCentralized LoggingFull Transaction TracingError TrackingApplication & Server MonitoringReal User MonitoringDeployment TrackingTry Our Code ProfilerWant to write better code? Check out our free transaction tracing tool, Prefix! PricingSolutions By Role For DevelopersFor DevOps"
42,Menu For DevelopersFor DevOpsBy Technology Azure MonitoringAWS MonitoringCompatibility
42,Menu Azure MonitoringAWS MonitoringCompatibilityBy Language .NETJavaPHPNode.jsRubyPython
42,Menu .NETJavaPHPNode.jsRubyPython
42,By Role For DevelopersFor DevOps
42,Menu For DevelopersFor DevOpsBy Technology Azure MonitoringAWS MonitoringCompatibility
42,Menu Azure MonitoringAWS MonitoringCompatibilityBy Language .NETJavaPHPNode.jsRubyPython
42,Menu .NETJavaPHPNode.jsRubyPython Learn Resources BlogeBooksCase StudiesWebinarsROI CalculatorNews
42,Menu BlogeBooksCase StudiesWebinarsROI CalculatorNewsTechnical DocumentationSupportIdeas Portal
42,Menu DocumentationSupportIdeas Portal
42,Resources BlogeBooksCase StudiesWebinarsROI CalculatorNews
42,Menu BlogeBooksCase StudiesWebinarsROI CalculatorNewsTechnical DocumentationSupportIdeas Portal
42,Menu DocumentationSupportIdeas Portal Login
42,"Why Python cProfile is the Recommended Profiling Interface Nirav Parmar April 12, 2021 Developer Tips, Tricks & ResourcesPerformance optimization is a basic need for software development. When it comes to optimizing app performance,  tracking frequency, maintaining production, or perpetuation method calls, profilers play a vital role. Learn why Python cProfile is a recommended profiling interface and how it enhances your software performance. Python profiling  has three critical parts including: Definition & explanation; Tools  for a generic app …Read More"
42,DeveloperPrefixprofilingPython
42,"How to Choose the Best Performance Profiling Tools Charris Herrera April 9, 2021 Developer Tips, Tricks & ResourcesYou finish writing your code and launch your application. Then, you begin experiencing performance issues. How can you fix this? It doesn’t matter how talented your development team is, every code should always be analyzed, debugged, and reviewed to make it run faster. What you need is a performance profiling tool. In this article, you will learn about performance profiling …Read More"
42,DeveloperPrefixprofilingretrace
42,"Node.js Server Monitoring: A How to Guide Christoph Leitner April 5, 2021 Developer Tips, Tricks & ResourcesNode.js is one of the most popular Javascript frameworks in 2021. With the increasing demand for Node.js comes the crucial next step of Node.js server monitoring.  The best way to monitor your Node.js server is with an Application Performance Monitoring (APM) tool.  Keep in mind, Node.js server monitoring is a bit of a tricky task, and there are particular challenges …Read More"
42,Developermonitoringnode.jsretrace
42,"How to Optimize Server Performance Evangeline Reece March 29, 2021 Developer Tips, Tricks & ResourcesOptimizing server performance is important in supporting end-user requirements.  Using server optimization, actively monitor:  Availability Server operations Performance Security And other procedures Web server monitoring and optimization helps you to troubleshoot bottlenecks as they emerge and optimize server performance.  In this post, we will discuss how to optimize performance and why it is important. What is server optimization? Web server …Read More"
42,Developermonitoringperformanceretrace
42,"What is Application Discovery and Dependency Mapping Charris Herrera March 26, 2021 Developer Tips, Tricks & ResourcesModern applications are interdependent on different devices and servers. If you are not familiar with your software structure, you will encounter problems when you make changes within your application. To prevent this, you need to observe application discovery and dependency mapping (ADDM). In this article, you will find out: What is application discovery and dependency mapping? Types of application discovery …Read More"
42,application monitoringapplication performanceDeveloperretrace
42,"Tips for Javascript Perfomance Testing Andreea Zorz March 22, 2021 Developer Tips, Tricks & ResourcesNo user likes to spend time waiting for a web page to load. No web developer likes a website or app to fail either. It’s important to focus on creating quick loading pages with error-free code.  JavaScript is used to build the majority of the dynamic, real-time applications. JavaScript’s strength is in the context of apps that update content dynamically with …Read More"
42,DeveloperjavascriptretraceRUM
42,"Top Java Software Errors: 50 Common Java Errors and How to Avoid Them Stackify March 18, 2021 Developer Tips, Tricks & Resources,"
42,"Insights for Dev ManagersImagine, you are developing Java software and suddenly you encounter an error? Where could you have possibly gone wrong? There are many types of errors that you will encounter while developing Java software, but most are avoidable. Some errors are minor lapses when writing codes but that is very much mendable. If you have an error monitoring tool such as …Read More"
42,DeveloperexceptionsJavaretrace
42,"Python Optimization: 3 Easy Steps George Newton March 15, 2021 Developer Tips, Tricks & ResourcesPython is one of the best programming resources available for designing machine learning systems. With a variety of technical abilities and potentially time-saving loops and processes, it can be an invaluable tool. However, it’s these capabilities that also make Python difficult to use. In many cases, Python may seem sluggish as it tries to navigate intricate, complicated strings of code. …Read More"
42,apmPrefixprofilingPythonretrace
42,"How to monitor website availability Charris Herrera March 12, 2021 Developer Tips, Tricks & Resources“100% website availability.” Which webmaster would not want to see this availability report? Every website owner would like their website available for users to be 99.9% all of the time. Without a website that is accessible and running smoothly at any time of day, all web-related investments will go to waste. That is why website availability monitoring is so important. …Read More"
42,Developermonitoringretrace
42,"Web Applications Monitoring Guide Gert Svaiko March 8, 2021 Developer Tips, Tricks & ResourcesCompanies are juggling multiple servers, applications, transactions, and web services in the modern digital world. This brings out the difficulty of keeping track of it all. Here’s where web applications monitoring steps in and gives an in-depth overview of all applications’ performance. Let’s take a step back and dig into the basics of web application monitoring. What is web applications …Read More"
42,monitoringretrace
42,"What Is NullReferenceException? Object reference not set to an instance of an object Matt Watson March 5, 2021 Developer Tips, Tricks & Resources“Object Reference Not Set to an instance of an object.” Cast the first stone those who never struggled with this error message when they were a beginner C#/.NET programmer. This infamous and dreaded error message happens when you get a NullReferenceException. This exception is thrown when you try to access a member—for instance, a method or a property—on a variable …Read More"
42,.NETasp.netDevelopererror handlingerror trackingPrefix
42,"Why you should use Central Error Logging Services Charris Herrera February 26, 2021 Developer Tips, Tricks & ResourcesLogs are vital for every application that runs in a server environment. Logs provide essential information which points to whether the current system is operating properly. Looking through logs, you will gather data on system issues, errors, and trends. However, it is not feasible to manually look up errors on various servers across thousands of log files. The solution? Central …Read More"
42,Developerloggingretrace
42,"ASP.NET Performance: 9 Types of Tools You Need to Know! Matt Watson February 19, 2021 Developer Tips, Tricks & Resources,"
42,"Insights for Dev Managers,"
42,"PopularOne of the best things about being a .NET developer is all the amazing ASP.NET performance tools that can make your life easier. This blog post is a list of the various types of ASP.NET performance tools at your disposal for finding and optimizing ASP.NET performance problems. Depending on the task, some of these tools will be much better than …Read More"
42,app monitoringapp performance tipsPrefixretrace
42,"Performance Tuning in MySQL Alex Williams February 15, 2021 Developer Tips, Tricks & ResourcesMySQL is an open source database application that creates meaningful structure and accessibility for large amounts of data.. But, with large data comes performance issues. This article will give you performance tuning in MySQL tips in order to boost its performance. Make Sure You Are Using The Latest MySQL Version  If you are a legacy or older database, this may …Read More"
42,Developerretrace
42,"How to Find Memory Leaks Charris Herrera February 12, 2021 Developer Tips, Tricks & ResourcesOne of the most common problems developers face regarding memory is finding memory leaks. How do you know if your application’s memory is leaking? How do you find memory leaks? In this article, you will learn:  what is a memory leak, how memory leaks can affect your application, what causes memory leaks, finding memory leaks. What is a Memory Leak? …Read More"
42,Developerretrace
42,"What is Load Testing? How It Works, Tools, Tutorials, and More Alexandra Altvater February 5, 2021 Developer Tips, Tricks & ResourcesLoad testing is a type of performance testing that simulates a real-world load on any software, application, or website. Without it, your application could fail miserably in real-world conditions. That’s why we build tools like Retrace to help you monitor application performance and fix bugs before your code ever gets to production. Load testing examines how the system behaves during …Read More"
42,deploymentload testingload testing toolsretrace
42,"Python Load Testing Best Practices Aleksandra Bondrenko February 1, 2021 Developer Tips, Tricks & ResourcesPython is one of the leading programming languages in the world. Many of the world’s most popular websites run, at least in part, on Python, including Instagram, Google, Instacart, Uber, Netflix, and Spotify. When the language is used properly, it can be very efficient and requires low processing power in most cases. It’s important for sites to rely on python …Read More"
42,Developerretracesoftware testingtesting
42,"Server Performance Monitoring 101 Kris Flores January 29, 2021 Developer Tips, Tricks & ResourcesServer performance monitoring is essential in maintaining the health, safety, and integrity of your business’s servers. For modern businesses, no matter the industry, servers play an all-important role. Whether you store records and sensitive customer data in the cloud, employ a software environment that drives all your company’s business activities, or are in an industry that relies on sensors to …Read More"
42,Developermonitoringretrace
42,"How To Report On Software Testing Erika Rykun January 25, 2021 Developer Tips, Tricks & ResourcesBeing able to write concise, easily comprehensible software testing reports is an important skill for software development team members to possess, particularly those in quality assurance, development, and support. Poorly written software testing reports can make the development process more difficult and less productive. Imagine a client asks if their app is ready for launch and based on your assessment, …Read More"
42,Developerretracetesting
42,"Best Website Performance Testing Tools Charris Herrera January 22, 2021 Developer Tips, Tricks & ResourcesWhat is the usual criteria in choosing an online store? It should have reasonable prices, sell quality products, and most of all, it should have a fast loading time. A website’s performance is essential. A two-second delay can make a big difference to your website and revenue as well. In fact, Neil Patel reported that a mere second delay may …Read More"
42,apmDevelopermonitoringretrace
42,Page 1 of 33123...33→Search Stackify Search Topics/KeywordsASP.NETProduct Updates.NET CoreApp MonitoringJavaApp Performance TipsAzureError HandlingAWSLogging TipsCloudDevOpsPopular Posts ASP.NET Performance: 9 Types of Tools You Need to Know! How to Troubleshoot IIS Worker Process (w3wp) High CPU Usage How to Monitor IIS Performance: From the Basics to Advanced IIS Performance Monitoring SQL Performance Tuning: 7 Practical Tips for Developers Looking for New Relic Alternatives & Competitors? Learn Why Developers Pick RetraceRecent Posts Why Python cProfile is the Recommended Profiling Interface How to Choose the Best Performance Profiling Tools Node.js Server Monitoring: A How to Guide How to Optimize Server Performance What is Application Discovery and Dependency Mapping
42,Get In Touch
42,Contact UsRequest a DemoStart Free Trial
42,"Menu Contact UsRequest a DemoStart Free TrialPO Box 2159Mission, KS 66201816-888-5055"
42,Facebook
42,Twitter
42,Youtube
42,Linkedin
42,Products
42,RetracePrefixStackify Hidden Menu Item.NET MonitoringJava MonitoringPHP MonitoringNode.js MonitoringRuby MonitoringPython MonitoringRetrace vs New RelicRetrace vs Application Insights
42,Menu RetracePrefixStackify Hidden Menu Item.NET MonitoringJava MonitoringPHP MonitoringNode.js MonitoringRuby MonitoringPython MonitoringRetrace vs New RelicRetrace vs Application InsightsSolutions
42,Application Performance ManagementCentralized LoggingCode ProfilingError TrackingApplication & Server MonitoringReal User MonitoringStackify Hidden Menu ItemFor DevelopersFor DevOps
42,Menu Application Performance ManagementCentralized LoggingCode ProfilingError TrackingApplication & Server MonitoringReal User MonitoringStackify Hidden Menu ItemFor DevelopersFor DevOpsResources
42,What is APM?PricingCase StudiesBlogDocumentationFree eBooksFree WebinarsVideosIdeas PortalROI CalculatorSupportNews
42,Menu What is APM?PricingCase StudiesBlogDocumentationFree eBooksFree WebinarsVideosIdeas PortalROI CalculatorSupportNewsCompany
42,About UsNewsCareersGDPRSecurity InformationTerms & ConditionsPrivacy PolicySitemap
42,"Menu About UsNewsCareersGDPRSecurity InformationTerms & ConditionsPrivacy PolicySitemapPO Box 2159Mission, KS 66201816-888-5055"
42,Facebook
42,Twitter
42,Youtube
42,Linkedin
42,© 2020 Stackify
42,Start Your Free Trial NowNo credit card required. 14 days free.
43,FAQ: IDM performance and tuning - Knowledge - BackStage
43,"KnowledgelibrariesKnowledge Basearticlesa32504603 Search FAQForgeRock Identity PlatformDoes not apply to Identity Cloud FAQ: IDM performance and tuningLast updated Apr 8, 2021"
43,The purpose of this FAQ is to provide answers to commonly asked questions regarding performance and tuning for IDM.
43,Frequently asked questions
43,Q. Are there any recommendations for sizing IDM servers?
43,Q. Is there any best practice advice for benchmark testing?
43,Q. What is the recommended Java™ Virtual Machines (JVM) heap size for IDM?
43,Q. How can I troubleshoot performance issues?
43,Q. How can I improve reconciliation performance?
43,Q. Are there any recommendations for sizing the database needed for the IDM repository?
43,Q. How can I manage the QueuedThreadPool in Jetty®?
43,Q. Are there any recommendations for sizing IDM servers?
43,"A. No, the performance of IDM depends entirely on your specific environment and the exact nature of scripted customizations. To establish appropriate sizing, you will need to perform adequate benchmark testing."
43,See Release Notes › Before You Install for further information.
43,Note
43,"Sizing and/or tuning recommendations are outside the scope of ForgeRock support; if you want more tailored advice, consider engaging Deployment Support Services."
43,Q. Is there any best practice advice for benchmark testing?
43,A. ForgeRock recommends the following:
43,Maintain a staging environment that matches production where you can simulate the load you will have in production. This allows you to resolve any issues you identify in a non-production environment.
43,Establish a simple benchmark prior to adding external resources; you can then incrementally add resources and processes to establish what impact each one has.
43,Q. What is the recommended Java Virtual Machines (JVM) heap size for IDM?
43,"A. There are no definitive rules for the size of JVM heap size required as it will vary across individual environments and applications, but you should refer to Best practice for JVM Tuning with G1 GC or Best practice for JVM Tuning with CMS GC for best practice advice. Additionally, ensure you configure JVM garbage collection appropriately as GC times can increase with large heap sizes causing significant impacts to application performance and CPU utilization. See How do I change the JVM heap size for IDM (All versions)? for further information."
43,Note
43,"For a 32-bit JVM or a 32-bit operating system, the limit for the process size is 4GB, that is, 2^32; this cannot be exceeded regardless of the amount of heap space allocated."
43,Q. How can I troubleshoot performance issues?
43,"A. If you have no benchmark tests for comparison and encounter performance issues, the recommendation is to reduce your system to the bare minimum and then incrementally add back resources and processes in order to identify which one is causing the bottleneck."
43,The following tips should also help:
43,Bypass your load balancer and perform tests directly on the IDM server to determine if IDM is causing the bottleneck or if it is something environmental.
43,Disable implicit sync in the mappings file to help you understand the performance of a connector without downstream syncs.
43,"If you suspect a connector is causing an issue, you can disable your connectors by setting all their situations to ASYNC: Synchronization Guide › Synchronization Situations and Actions. You can then perform a benchmark test and reintroduce your connectors one at a time (repeating your tests in between to identify the culprit)."
43,"If you suspect reconciliation is the issue, you should troubleshoot per the advice in Q. How can I improve reconciliation performance?"
43,Collect stack traces and heap dumps at the point the issue is occurring to aid further troubleshooting: How do I collect JVM data for troubleshooting IDM (All versions)?
43,Identify threads causing high CPU: How do I find which thread is consuming CPU in a Java process in IDM (All versions)?
43,Add thread IDs to log statements to make it possible to trace transactions through standard audit and openidm0.log.0 logs: Release Notes › Adding Thread IDs to Log Messages (IDM 6.5 and later) or How do I add Thread IDs to log statements in IDM 5.x and 6?
43,Q. How can I improve reconciliation performance?
43,"A. Firstly, you should identify reconciliation performance issues per the advice in How do I identify reconciliation performance issues in IDM (All versions)?; this article also offers tuning advice. In addition to this, you can:"
43,Configure pooled connections and increase the number of instances available as detailed in How do I configure pooled connections for a connector in IDM (All versions)?
43,Review the suggestions in Synchronization Guide › Tuning Reconciliation Performance to ensure the defaults are suitable for your environment.
43,Q. Are there any recommendations for sizing the database needed for the IDM repository?
43,"A. Database sizing for the IDM repository depends on various factors such as the number of managed objects (users, groups, roles etc), relationships, links, audit logs, configurations, workflow, reconciliations etc. Since all these factors vary from one deployment to another, it is not possible to give specific recommendations. However, the following guidelines should help when estimating the size of database required:"
43,Managed objects
43,You can calculate the size of your IDM deployment by first calculating the size of your database for sample data (such as 20 users) and then multiplying that by the expected number of users. The following tables typically grow as more managed objects are added:
43,managedobjects - one entry per managed object.
43,"managedobjectproperties - N entries per managed object, where N is the number of indexed or searchable properties: Object Modeling Guide › Create and Modify Object Types."
43,"links - the total number of links is less than or equal to the number of managed objects multiplied by the number of unique mappings. For example, if you have 20 managed users and 3 mappings (systemLdapAccounts_managedUser, managedUser_systemLdapAccounts and systemADAccount_managedUser), the total number of links would be less than or equal to 40 (20 * 2). The systemLdapAccounts_managedUser and managedUser_systemLdapAccounts mappings are bidirectional syncs and may use the same links: Synchronization Guide › Mapping Data Between Resources."
43,"relationships, relationshipproperties - relationships are especially sensitive to growth when searchablebydefault is set to true (the various repo.jdbc.json files provided in the /path/to/idm/db directory have different defaults so you may end up generating more of this data that you need or expect). Roles also utilize the relationships table."
43,Flowable (IDM 7 and later) - these tables can grow over time as running workflows result in persisting data in these tables.
43,Activiti (Pre-IDM 7) - these tables can grow over time as running workflows result in persisting data in these tables.
43,"Most of the other tables such as configobjects, internaluser etc are static once you have a working IDM configuration and should not have much impact on database sizing."
43,Audit logs
43,"Audit logs can have a huge impact on database sizing and disk space. By default, IDM stores audit logs in both CSV files and in the database. The size of audit logs depends on IDM usage. There are different types of audit logs and their corresponding events: Audit Guide › Audit Event Topics. The following tables in particular grow with each reconciliation:"
43,"recon/audit - this table reports the status for each user and grows by one entry for each source record. Additionally, a further entry is created for each target record that is not linked or correlated (CONFIRMED, FOUND). The size of a single reconciliation report depends on the data size of source and target; the overall data requirement depends on how many reports you keep. You can reduce the size of the recon/audit table for actions you're not interested in by setting the action to NOREPORT."
43,"recon/activity - this table reports all activity and grows by one entry for each modifying action regardless of the source of the action, for example, reconciliation, synchronization etc. The overall size depends on how many changes are processed and how long these records are kept."
43,The following best practices should be heeded:
43,Use a different database for audit logging instead of the IDM repository. This is demonstrated in the audit-jdbc sample: Samples Guide › Direct Audit Information To MySQL.
43,Automatically purge obsolete audit logs using schedulers: Audit Guide › Purge Obsolete Audit Information.
43,Filter audit log events to reduce audit data: Audit Guide › Filter Audit Data.
43,Archive and back up audit log files.
43,Q. How can I manage the QueuedThreadPool in Jetty?
43,A. You can change the Jetty thread pool settings by updating the config.properties file (located in the /path/to/idm/conf directory) or by setting the OPENIDM_OPTS environment variable when you start IDM. See Installation Guide › Adjust Jetty Thread Settings for further information.
43,See Also
43,How do I enable Garbage Collector (GC) Logging for IDM (All versions)?
43,How do I collect data for troubleshooting high CPU utilization on IDM (All versions) servers?
43,Performance tuning and monitoring ForgeRock products
43,Monitoring Guide
43,Synchronization Guide › Tuning Reconciliation Performance
43,Related Training
43,N/A
43,"Send Feedback Copyright and TrademarksCopyright © 2021 ForgeRock, all rights reserved."
43,Loading...
43,Something went wrong... You can report this issue at backstage.forgerock.com
44,[PDF] ORACLE DATABASE EXPRESS EDITION 11G RELEASE 2 FOR WINDOWS X64 PDF | www.blinkprods.com
44,ORACLE DATABASE EXPRESS EDITION 11G RELEASE 2 FOR WINDOWS X64 PDF
44,"File Name: oracle database express edition 11g release 2 for windows x64 pdf.pdfSize: 2919 KBType: PDF, ePub, eBook"
44,"Category: BookUploaded: 10 May 2019, 13:14 PMRating: 4.6/5 from 736 votes."
44,Status: AVAILABLE
44,Last checked: 17 Minutes ago!
44,"In order to read or download oracle database express edition 11g release 2 for windows x64 pdf ebook, you need to create a FREE account."
44,Download Now!
44,"eBook includes PDF, ePub and Kindle version"
44,In order to read or download
44,Disegnare Con La Parte Destra Del Cervello Book Mediafile Free File Sharing
44,"ebook, you need to create a FREE account."
44,Download Now!
44,"eBook includes PDF, ePub and Kindle version"
44,Register a free 1 month Trial Account.
44,Download as many books as you like (Personal use)
44,Cancel the membership at any time if not satisfied.
44,Join Over 80000 Happy Readers
44,Book Descriptions:
44,"We have made it easy for you to find a PDF Ebooks without any digging. And by having access to our ebooks online or by storing it on your computer, you have convenient answers with Oracle Database Express Edition 11g Release 2 For Windows X64 Pdf. To get started finding Oracle Database Express Edition 11g Release 2 For Windows X64 Pdf, you are right to find our website which has a comprehensive collection of manuals listed. Our library is the biggest of these that have literally hundreds of thousands of different products represented."
44,Home | Contact | DMCA
44,29 Comments
44,Comment
44,Jenny Martins
44,"Finally I get this ebook, thanks for all these Oracle Database Express Edition 11g Release 2 For Windows X64 Pdf I can get now!"
44,Reply
44,1 Like Follow
44,1 hour ago
44,Lisa Doran
44,cooool I am so happy xD
44,Reply
44,12 Like Follow
44,1 hour ago
44,Markus Jensen
44,"I did not think that this would work, my best friend showed me this website, and it does! I get my most wanted eBook"
44,Reply
44,2 Like Follow
44,1 hour ago
44,Michael Strebensen
44,wtf this great ebook for free?!
44,Reply
44,2 Like Follow
44,minutes ago
44,Hun Tsu
44,My friends are so mad that they do not know how I have all the high quality ebook which they do not!
44,Reply
44,2 Like Follow
44,3 hour ago
44,Tina Milan
44,It's very easy to get quality ebooks ;)
44,Reply
44,2 Like Follow
44,3 hour ago
44,Jim Letland
44,hahahahaha
44,Reply
44,2 Like Follow
44,5 hour ago
44,Lukasz Czaru
44,so many fake sites. this is the first one which worked! Many thanks
44,Reply
44,5 Like Follow
44,6 hour ago
44,Georgina Kalafikis
44,wtffff i do not understand this!
44,Reply
44,1 Like Follow
44,8 hour ago
44,Martin Borton
44,"Just select your click then download button, and complete an offer to start downloading the ebook. If there is a survey it only takes 5 minutes, try any survey which works for you."
44,Reply
44,17 Like Follow
44,8 hour ago
44,Ida Kelvin
44,lol it did not even take me 5 minutes at all! XD
44,Reply
44,13 Like Follow
44,8 hour ago
44,Oracle Database Express Edition 11g Release 2 For Windows X64 Pdf Download PDF
47,Best Performance Practices for Hibernate 5 and Spring Boot 2 (Part 1) - DZone Java
47,Java Zone
47,"Thanks for visiting DZone today,"
47,Edit Profile
47,Manage Email Subscriptions
47,How to Post to DZone
47,Sign Out
47,View Profile
47,Post
47,Over 2 million developers have joined DZone.
47,Log In
47,Join
47,Refcardz
47,Research
47,Webinars
47,Zones
47,Agile
47,Big Data
47,Cloud
47,Database
47,DevOps
47,Integration
47,IoT
47,Java
47,Microservices
47,Open Source
47,Performance
47,Security
47,Web Dev
47,DZone
47,Java Zone
47,Best Performance Practices for Hibernate 5 and Spring Boot 2 (Part 1)
47,Best Performance Practices for Hibernate 5 and Spring Boot 2 (Part 1)
47,Make sure you are practicing the best performance practices in your Spring Boot and Hibernate projects.
47,Anghel Leonard
47,CORE
47,"May. 11, 20"
47,Java Zone
47,Tutorial
47,Like
47,(114)
47,Comment
47,Save
47,Tweet
47,285.81K
47,Views
47,Join the DZone community and get the full member experience.
47,Join For Free
47,"In this series of articles, we will tackle some persistence layer performance issues via Spring Boot applications. Moreover, for a detailed explanation of 150+ performance items check out my book, Spring Boot Persistence Best Practices. You'll simply love it :)"
47,This book helps every Spring Boot developer to squeeze the performances of the persistence layer.
47,Item 1: Attribute Lazy Loading Via Bytecode Enhancement
47,"By default, the basic attributes of an entity are loaded eagerly (all at once). Are you sure that you always want that?"
47,"Description: If not, then it is important to know that basic attributes can be loaded lazily as well via the Hibernate Bytecode Enhancement mechanism. This is useful for lazy loading the column types that store large amounts of data: CLOB, BLOB, VARBINARY, etc. or columns that should be loaded only on demand."
47,Key points:
47,"For Maven, in pom.xml, activate Hibernate BytecodeEnhancement (e.g. use Maven bytecode enhancement plugin as follows)"
47,Mark the columns that should be loaded lazily with @Basic(fetch = FetchType.LAZY)
47,"Inapplication.properties, disable Open Session in View"
47,Source code can be found here.
47,You should read as well:
47,Default Values For Lazy Loaded Attributes
47,Attribute Lazy Loading And Jackson Serialization
47,If this approach is not proper for you then the same result can be obtained via subentities. Consider reading  Attributes Lazy Loading Via Subentities.
47,Item 2: View Binding Parameter Values Via Log4J 2
47,"Without seeing and inspecting the SQL fired behind the scenes and the corresponding binding parameters, we are prone to introduce performance penalties that may remain there for a long time (e.g. N+1)."
47,"Update (please read): The solution described below is useful if you already have Log4J 2 in your project. If not, it is better to rely onTRACE(thank you Peter Wippermann for your suggestion) orlog4jdbc(thank you, Sergei Poznanski, for your suggestion and SO answer). Both approaches don't require the exclusion of Spring Boot's Default Logging. An example ofTRACEcan be found here, and an example oflog4jdbchere."
47,"Description based on Log4J 2: While the application is under development, maintenance is useful to view and inspect the prepared statement binding parameter values instead of assuming them. One way to do this is via Log4J 2 logger setting. Key points:"
47,"For Maven, inpom.xml, exclude Spring Boot's Default Logging (read update above)"
47,"For Maven, inpom.xml, add the Log4j 2 dependency"
47,"In log4j2.xml, add the following:"
47,"<Logger name=""org.hibernate.type.descriptor.sql"" level=""trace""/>"
47,Output sample:
47,Source code can be found here.
47,Item 3: How To View Query Details Via DataSource-Proxy
47,"Without ensuring that batching is actually working, we are prone to serious performance penalties. There are different cases when batching is disabled, even if we have it set up and think that it is working behind the scene. For checking, we can usehibernate.generate_statisticsto display details (including batching details), but we can go with the DataSource-Proxy library, as well. But, don't conclude that DataSource-Proxy should be used only in the presence of batching. Its general purpose is to provide details about the triggered queries."
47,"Description: View the query details (query type, binding parameters, batch size, etc.) via DataSource-Proxy."
47,Key points:
47,"For Maven, add in the pom.xml the DataSource-Proxy dependency"
47,Create a bean post-processor to intercept the DataSource bean
47,Wrap the DataSource bean via the ProxyFactory and implementation of the MethodInterceptor
47,Output sample:
47,Source code can be found here.
47,Item 4: Batch Inserts Via saveAll(Iterable<S> entities)
47,"By default, 100 inserts will result in 100 SQLINSERTstatements and this is bad since it results in 100 database round trips."
47,"Description: Batching is a mechanism capable of groupingINSERTs,UPDATEs,andDELETEs,and as a consequence, it significantly reduces the number of database round trips. One way to achieve batch inserts consists of using theSimpleJpaRepository#saveAll(Iterable< S> entities)method. Here, we do this with MySQL. The recommended batch size is between 5 and 30."
47,Key points:
47,"Inapplication.properties,set spring.jpa.properties.hibernate.jdbc.batch_size"
47,"Inapplication.properties,set spring.jpa.properties.hibernate.generate_statistics (just to check that batching is working)"
47,"In application.properties,set JDBC URL with rewriteBatchedStatements=true (optimization specific for MySQL)"
47,"Inapplication.propertiesset JDBC URL withcachePrepStmts=true(enable caching and is useful if you decide to set prepStmtCacheSize, prepStmtCacheSqlLimit, etc as well; without this setting the cache is disabled)"
47,Inapplication.propertiesset JDBC URL withuseServerPrepStmts=true(this way you switch to server-side prepared statements (may lead to significant performance boost))
47,"In the entity, use the assigned generator since MySQL IDENTITYwill cause insert batching to be disabled"
47,"In the entity, add a property annotated with @Versionto avoid the extra-SELECT fired before batching (also prevent lost updates in multi-request transactions). Extra-SELECTsare the effect of using merge()instead ofpersist(). Behind the scenes, saveAll()uses save(),which in case of non-new entities (entities having IDs), will callmerge(), which instructs Hibernate to fire to a SELECT statement to ensure that there is no record in the database having the same identifier."
47,"Pay attention to the number of inserts passed tosaveAll()to not ""overwhelm"" the Persistence Context. Normally, the EntityManagershould be flushed and cleared from time to time, but during the saveAll()execution, you simply cannot do that, so if in saveAll()there is a list with a high amount of data, all that data will hit the Persistence Context (1st level cache) and will be in-memory until flush time. Using a relatively small amount of data should be OK."
47,The saveAll()   method return a List<S>  containing the persisted entities; each persisted entity is added into this list; if you just don't need this List  then it is created for nothing
47,"For a large amount of data, call saveAll()   per batch and set batch size between 5 and 30. Moreover, please check the next item as well (item 5)."
47,Output sample:
47,Source code can be found here.
47,For a detailed explanation of this item and 150+ items check out my book Spring Boot Persistence Best Practices. This book helps every Spring Boot developer to squeeze the performances of the persistence layer.
47,Item 5: How To Optimize Batch Inserts of Parent-Child Relationships And Batch Per Transaction
47,"Description: Let's suppose that we have a one-to-many relationship between Authorand Book   entities. When we save an author, we save his books as well thanks to cascading all/persist. We want to create a bunch of authors with books and save them in the database (e.g., a MySQL database) using the batching technique. By default, this will result in batching each author and the books per author (one batch for the author and one batch for the books, another batch for the author and another batch for the books, and so on). In order to batch authors and books, we need to order inserts as in this application."
47,"Moreover, this example commits the database transaction after each batch execution. This way we avoid long-running transactions and, in case of a failure, we rollback only the failed batch and don't lose the previous batches. For each batch, the Persistent Context is flushed and cleared, therefore we maintain a thin Persistent Context. This way the code is not prone to memory errors and performance penalties caused by slow flushes."
47,Key points:
47,"Besides all settings specific to batching inserts in MySQL (see Item 4), we need to set up in application.propertiesthe following property:  spring.jpa.properties.hibernate.order_inserts=true"
47,Output sample without ordering inserts (80 batches):
47,Output sample with ordering inserts (17 batches):
47,How much it counts? Check this out for 5 authors with 5 books each to 500 authors with 5 books each:
47,Source code can be found here.
47,You may also like the following:
47,"""Item 6: Batch Inserts Via EntityManager With Batch Per Transaction"""
47,"""Item 7: Batch Inserts In Spring Boot Style Via CompletableFuture"""
47,For a detailed explanation of these items and 150+ items check out my book Spring Boot Persistence Best Practices. This book helps every Spring Boot developer to squeeze the performances of the persistence layer.
47,Item 8: Direct Fetching Via Spring Data / EntityManager / Session
47,"The way, we fetch data from the database determines how an application will perform. In order to build the optimal fetching plan, we need to be aware of each fetching type. Direct fetching is the simplest (since we don't write any explicit query) and very useful when we know the entity Primary Key."
47,"Description: Direct fetching via Spring Data, EntityManager, and Hibernate Sessionexamples."
47,Key points:
47,"Direct fetching via Spring Data, findById()"
47,Direct fetching via EntityManager#find()
47,Direct fetching via Hibernate Session#get()
47,"Source code can be found here. Direct fetching multiple entities by id can be done via Spring Data, findAllById() and the great Hibernate MultiIdentifierLoadAccess interface."
47,Item 9: DTOs Via Spring Data Projections
47,Fetching more data than needed is one of the most common issues causing performance penalties. Fetching entities without the intention of modifying them is also a bad idea.
47,Description: Fetch only the needed data from the database via Spring Data Projections (DTOs). See also items 25-32.Key points:
47,Write an interface (projection) containing getters only for the columns that should be fetched from the database
47,Write the proper query returning a List<projection>
47,"If possible, limit the number of returned rows (e.g., via LIMIT). Here, we can use the Query Builder mechanism built into the Spring Data repository infrastructure."
47,"Output example (select first 2 rows; select only ""name"" and ""age""):"
47,Source code can be found here.
47,"Note: Using projections is not limited to use the Query Builder mechanism built into Spring Data repository infrastructure. We can fetch projections via JPQL or native queries as well. For example, in this application, we use a JPQL. Moreover, Spring projection can be nested. Consider this application and this application. Pay extra attention on using nested projection and the involved performance penalties. For a detailed explanation of this aspect and 150+ items check out my book Spring Boot Persistence Best Practices. This book helps every Spring Boot developer to squeeze the performances of the persistence layer."
47,Item 10: How To Store UTC Timezone In MySQL
47,Storing date-time and timestamps in the database in different/specific formats can cause real issues when dealing with conversions.
47,"Description: This recipe shows you how to store date-time, and timestamps in UTC time zone in MySQL. For other RDBMSs (e.g. PostgreSQL), just remove ""useLegacyDatetimeCode=false"" and adapt the JDBC URL.Key points:"
47,spring.jpa.properties.hibernate.jdbc.time_zone=UTC
47,spring.datasource.url=jdbc:mysql://localhost:3306/db_screenshot?useLegacyDatetimeCode=false
47,Source code can be found here.
47,Item 11: Populating a Child-Side Parent Association Via Proxy
47,"Executing more SQL statements than needed is always a performance penalty. It is important to strive to reduce their number as much as possible, and relying on references is one of the easy to use optimization."
47,"Description: A Hibernate proxy can be useful when a child entity can be persisted with a reference to its parent ( @ManyToOne  or @OneToOne   lazy association). In such cases, fetching the parent entity from the database (execute theSELECTstatement) is a performance penalty and a pointless action. Hibernate can set the underlying foreign key value for an uninitialized proxy.Key points:"
47,Rely on EntityManager#getReference()
47,"In Spring, use JpaRepository#getOne()"
47,"Used in this example, in Hibernate, use load()"
47,"Assume two entities, Author   and Book , involved in a unidirectional @ManyToOne   association ( Author is the parent-side)"
47,"We fetch the author via a proxy (this will not trigger a SELECT ), we create a new book, we set the proxy as the author for this book and we save the book (this will trigger an INSERT  in the book  table)"
47,Output sample:
47,"The console output will reveal that only an INSERT is triggered, and no SELECT"
47,Source code can be found here.
47,Item 12: Reproducing N+1 Performance Issue
47,"N+1 is another issue that may cause serious performance penalties. In order to eliminate it, you have to find/recognize it. It is not always easy, but here is one of the most common scenarios that lead to N+1."
47,"Description: N+1 is an issue of lazy fetching (but, eager is not exempt). Just in case you didn't have the chance to see it in action, this application reproduces the N+1 behavior. In order to avoid N+1 is better to rely on JOIN+DTO (there are examples of JOIN+DTOs in items 36-42).Key points:"
47,"Define two entities, Author  and Book  in a lazy bidirectional @OneToMany   association"
47,"Fetch all Book   lazy, so without Author   (results in 1 query)"
47,Loop the fetched Book   collection and for each entry fetch the corresponding Author   (results N queries)
47,"Or, fetch all  Author  lazy, so without Book   (results in 1 query)"
47,Loop the fetched Author   collection and for each entry fetch the corresponding Book   (results N queries)
47,Output sample:
47,Source code can be found here.
47,Item 13: Optimize Distinct SELECTs Via HINT_PASS_DISTINCT_THROUGH Hint
47,"Description: Starting with Hibernate 5.2.2, we can optimize JPQL (HQL) query entities of type SELECT DISTINCT via HINT_PASS_DISTINCT_THROUGH hint. Keep in mind that this hint is useful only for JPQL (HQL)  JOIN FETCH -ing queries. It is not useful for scalar queries (e.g.,  List ), DTO or HHH-13280. In such cases, the DISTINCT  JPQL keyword is needed to be passed to the underlying SQL query. This will instruct the database to remove duplicates from the result set."
47,Key points:
47,"Use @QueryHints(value = @QueryHint(name = HINT_PASS_DISTINCT_THROUGH, value = ""false""))"
47,Output sample:
47,Source code can be found here.
47,For a detailed explanation of this item and 150+ items check out my book Spring Boot Persistence Best Practices. This book helps every Spring Boot developer to squeeze the performances of the persistence layer.
47,Item 14: Enable Dirty Tracking
47,"Java Reflection is considered slow and, therefore, a performance penalty."
47,"Description: Prior to Hibernate version 5, the dirty checking mechanism relies on the Java Reflection API. Starting with Hibernate version 5, the dirty checking mechanism relies on Bytecode Enhancement. This approach sustains better performance, especially when you have a relatively large number of entities.Key points:"
47,Add the corresponding plugin in pom.xml(e.g. use Maven Bytecode Enhancement plugin)
47,Output sample:
47,"The bytecode enhancement effect can be seen on Author.class, here."
47,Source code can be found here.
47,Item 15: Use Java 8 Optional in Entities and Queries
47,"Treating Java 8Optionalas a ""silver bullet"" for dealing with nulls can cause more harm than good. Using things for what they were designed is the best approach. A detailed chapter of good practices for Optional  API is available in my book, Java Coding Problems."
47,Description: This application is a proof of concept of how is correct to use the Java 8 Optionalin entities and queries.Key points:
47,Use the Spring Data built-in query-methods that return Optional(e.g.findById())
47,Write your own queries that returnOptional
47,UseOptionalin entities getters
47,"In order to run different scenarios check the file,data-mysql.sql"
47,Source code can be found here.
47,Item 16: How to Correctly Shape an @OneToMany Bidirectional Relationship
47,"There are a few ways to screw up your@OneToManybidirectional relationship implementation. And, I am sure that this is a thing that you want to do it correctly right from the start."
47,Description: This application is a proof of concept of how is correct to implement the bidirectional @OneToManyassociation.Key points:
47,Always cascade from parent to child
47,UsemappedByon the parent
47,UseorphanRemovalon the parent in order to remove children without references
47,Use helper methods on the parent to keep both sides of the association in sync
47,Always use lazy fetch
47,"As entities identifiers, use assigned identifiers (business key, natural key ( @NaturalId )) and/or database-generated identifiers and override (on child-side) properly the equals()   and hashCode()   methods as here"
47,"If toString()   needs to be overridden, then pay attention to involve only the basic attributes fetched when the entity is loaded from the database"
47,"Note: Pay attention to remove operations, especially to removing child entities. The CascadeType.REMOVE   and orphanRemoval=true   may produce too many queries. In such scenarios, relying on bulk operations is most of the time the best way to go for deletions. Check this out."
47,SlideShare presentation of this item can be found here.
47,Source code can be found here.
47,Item 17: JPQL Query Fetching
47,"When direct fetching is not an option, we can think of JPQL query fetching."
47,"Description: This application is a proof of concept of how to write a query viaJpaRepository,EntityManagerandSession.Key points:"
47,"ForJpaRepository, use@Queryor Spring Data Query Creation"
47,"ForEntityManager andSession, use thecreateQuery()method"
47,Source code can be found here.
47,Item 18: MySQL and Hibernate 5 Avoid AUTO Generator Type
47,"In MySQL, theTABLEgenerator is something that you will always want to avoid. Never use it!"
47,"Description: In MySQL and Hibernate 5, theGenerationType.AUTO generator type will result in using theTABLEgenerator. This adds a significant performance penalty. Turning this behavior toIDENTITY generator can be obtained by usingGenerationType.IDENTITYor the native generator.Key points:- UseGenerationType.IDENTITYinstead ofGenerationType.AUTO- Use the native generator exemplified in this source code"
47,Output sample:
47,Source code can be found here.
47,Item 19: Redundant save() Call
47,"We love to call this method, don't we? But, calling it for managed entities is a bad idea since Hibernate uses Hibernate dirty checking mechanism to help us to avoid such redundant calls."
47,Description: This application is an example when callingsave()for a managed entity is redundant.Key points:
47,Hibernate triggersUPDATEstatements for managed entities without the need to explicitly call thesave()method
47,"Behind the scenes, this redundancy implies a performance penalty as well (see here)"
47,Source code can be found here.
47,Item 20: PostgreSQL (BIG)SERIAL and Batching Inserts
47,"In PostgreSQL, usingGenerationType.IDENTITYwill disable insert batching."
47,"Description: The (BIG)SERIALis acting ""almost"" like MySQL, AUTO_INCREMENT. In this example, we use theGenerationType.SEQUENCE, which enables insert batching, and we optimize it via thehi/lo optimization algorithm.Key points:"
47,UseGenerationType.SEQUENCEinstead ofGenerationType.IDENTITY
47,Rely on the hi/lo   algorithm to fetch a hi value in a database roundtrip (the hi value is useful for generating a certain/given number of identifiers in-memory; until you haven't exhausted all in-memory identifiers there is no need to fetch another hi).
47,You can go even further and use the Hibernate pooled   and pooled-lo   identifier generators (these are optimizations of hi/lo   that allows external services to use the database without causing duplication keys errors). Check this out! And this!
47,Output sample:
47,Source code can be found here.
47,Item 21: JPA Inheritance — Single Table
47,"JPA supportsSINGLE_TABLE,JOINED,TABLE_PER_CLASSinheritance strategies. Each of them has its pros and cons. For example, in the case ofSINGLE_TABLE, reads and writes are fast, but as the main drawback, NOT NULL constraints are not allowed for columns from subclasses."
47,Description: This application is a sample of JPA Single Table inheritance strategy (SINGLE_TABLE)Key points:
47,This is the default inheritance strategy (@Inheritance(strategy=InheritanceType.SINGLE_TABLE))
47,All the classes in a hierarchy are mapped to a single table in the database
47,Subclasses attributes non-nullability is ensured via @NotNull   and MySQL triggers
47,The default discriminator column memory footprint was optimized by declaring it of type TINYINT
47,Output example:
47,Source code can be found here.
47,You may also like:
47,JPA Inheritance - JOINED
47,JPA Inheritance - TABLE_PER_CLASS
47,JPA Inheritance - @MappedSuperclass
47,Item 22: How to Count and Assert SQL Statements
47,"Without counting and asserting SQL statements, it is very easy to lose control of the SQL executed behind the scene and, therefore, introduce performance penalties."
47,"Description: This application is a sample of counting and asserting SQL statements triggered ""behind the scenes."" It is very useful to count the SQL statements in order to ensure that your code is not generating more SQLs that you may think (e.g., N+1 can be easily detected by asserting the number of expected statements).Key points:"
47,"For Maven, inpom.xml, add dependencies for DataSource-Proxyand Vlad Mihalcea's db-util"
47,Create theProxyDataSourceBuilderwithcountQuery()
47,Reset the counter viaSQLStatementCountValidator.reset()
47,"Assert INSERT, UPDATE, DELETE,and SELECTvia assertInsert{Update/Delete/Select}Count(long expectedNumberOfSql"
47,Output example (when the number of expected SQLs is not equal with the reality an exception is thrown):
47,Source code can be found here.
47,Item 23: How To Use JPA Callbacks
47,Don't reinvent the wheel when you need to tie up specific actions to a particular entity lifecycle event. Simply rely on built-in JPA callbacks.
47,"Description: This application is a sample of enabling the JPA callbacks (Pre/PostPersist, Pre/ PostUpdate, Pre/ PostRemove, and PostLoad).Key points:"
47,"In the entity, write callback methods and use the proper annotations"
47,Callback methods annotated on the bean class must returnvoid and take no arguments
47,Output sample:
47,Source code can be found here.
47,Item 24: @OneToOne and @MapsId
47,Description: Instead of regular unidirectional/bidirectional @OneToOne   better rely on a unidirectional @OneToOne   and  @MapsId . This application is a proof of concept.
47,Key points:
47,Use@MapsIdon the child side
47,Use @JoinColumn   to customize the name of the primary key column
47,"Mainly, for  @OneToOne  associations (unidirectional and bidirectional), @MapsId   will share the primary key with the parent table (id property acts as both primary key and foreign key)"
47,Note:  @MapsId  can be used for @ManyToOne   as well.
47,"Source code can be found here. A detailed dissertation is available in my book, Spring Boot Persistent Best Practices."
47,Item 25: DTOs Via SqlResultSetMapping
47,"Fetching more data than needed is bad. Moreover, fetching entities (add them in the Persistence Context) when you don't plan to modify them is one of the most common mistakes that draw implicitly performance penalties. Items 25-32 show different ways of extracting DTOs."
47,"Description: Using DTOs allows us to extract only the needed data. In this application, we rely on SqlResultSetMappingandEntityManager.Key points:"
47,UseSqlResultSetMapping andEntityManager
47,"For using Spring Data Projections, check issue number 9 above."
47,Source code can be found here.
47,Stay tuned for our next installment where we explore the remaining top 25 best performance practices for Spring Boot 2 and Hibernate 5!
47,For a detailed explanation of 150+ performance items check out my book Spring Boot Persistence Best Practices. This book helps every Spring Boot developer to squeeze the performances of the persistence layer.
47,See you in part 2!
47,Topics:
47,"hibernate 5,"
47,"persistence,"
47,"java,"
47,"spring data,"
47,"spring boot,"
47,"tutorial,"
47,"performance,"
47,spring boot 2
47,Opinions expressed by DZone contributors are their own.
47,Popular on DZone
47,A Programmer Learning List (for Beginners)
47,ReactJS Vs. AngularJS
47,Nginx: Reverse Proxy and Load Balancing
47,The Engineer’s Complete Guide to Code Quality
47,Comments
47,Java Partner Resources
47,ABOUT US
47,About DZone
47,Send feedback
47,Careers
47,ADVERTISE
47,Developer
47,Marketing Blog
47,Advertise with DZone
47,+1 (919) 238-7100
47,CONTRIBUTE ON DZONE
47,MVB Program
47,Become a Contributor
47,Visit the Writers' Zone
47,LEGAL
47,Terms of Service
47,Privacy Policy
47,CONTACT US
47,600 Park Offices Drive
47,Suite 150
47,"Research Triangle Park, NC 27709"
47,support@dzone.com
47,+1 (919) 678-0300
47,Let's be friends:
47,DZone.com is powered by
48,MySQL Performance Monitoring and Tuning | SolarWinds
48,Government
48,Customer Portal
48,Partners
48,Events
48,Contact Us
48,English
48,Deutsch
48,Español
48,Français
48,日本語
48,한국어
48,Português
48,Toggle navigation
48,English
48,Deutsch
48,Español
48,Français
48,日本語
48,한국어
48,Português
48,Products
48,Network Management
48,Systems Management
48,Database Management
48,IT Security
48,IT Service Management
48,Application Management
48,Managed Service Providers
48,All Products
48,Products
48,Network Performance Monitor
48,NetFlow Traffic Analyzer
48,Network Configuration Manager
48,IP Address Manager
48,User Device Tracker
48,VoIP & Network Quality Manager
48,Network Automation Manager
48,Log Analyzer
48,Network Topology Mapper
48,Engineer's Toolset
48,ipMonitor
48,Kiwi CatTools
48,Kiwi Syslog Server
48,Bundles
48,Network Bandwidth Analyzer Pack
48,Log and Network Performance Pack
48,IP Control Bundle
48,View All Network Management Products
48,Unify log management and infrastructure performance with SolarWinds Log Analyzer.
48,Aggregate. Search. Chart.Learn More
48,Products
48,Server & Application Monitor
48,Virtualization Manager
48,Storage Resource Monitor
48,ipMonitor
48,Serv-U Managed File Transfer
48,Serv-U Secured FTP
48,Server Configuration Monitor
48,Log Analyzer
48,Access Rights Manager
48,Application Performance Monitoring
48,Web Performance Monitor
48,SolarWinds Backup
48,Bundles
48,Systems Management Bundle
48,Server Performance & Configuration Bundle
48,Log and Systems Performance Pack
48,Application Performance Optimization Pack
48,IT Operations Manager
48,View All Systems Management Products
48,Easy-to-use system and application change monitoring with Server Configuration Monitor.
48,Learn More
48,Products
48,Database Performance Analyzer
48,Oracle
48,SQL Server
48,Azure SQL Database
48,Database Performance Monitor
48,PostgreSQL
48,MongoDB
48,MySQL
48,Redis
48,View All Database Management Products
48,Monitor your cloud-native Azure SQL databases with a cloud-native monitoring solution.
48,Azure SQL performance monitoring simplifed.
48,Learn More
48,Products
48,Access Rights Manager
48,Security Event Manager
48,(formerly Log & Event Manager)
48,Server Configuration Monitor
48,Patch Manager
48,Identity Monitor
48,Serv-U Managed File Transfer
48,Serv-U Secured FTP
48,Serv-U Gateway
48,View All IT Security Products
48,Help Reduce Insider Threat Risks with SolarWinds® Access Rights Manager
48,Manage and Audit Access Rights across your Infrastructure.
48,Learn More
48,Products
48,Service Desk
48,Web Help Desk
48,ipMonitor
48,Desktop Management
48,Dameware Remote Everywhere
48,Dameware Remote Support
48,Dameware Mini Remote Control
48,Help Desk Essentials
48,View All IT Service Management Products
48,SolarWinds Service Desk is a 2020 TrustRadius Winner
48,Service Desk is a winner in two categories: IT Asset Management and IT Service Management (ITSM)
48,Products
48,AppOptics
48,"SaaS-based infrastructure and application performance monitoring, tracing, and custom metrics for hybrid and cloud-custom applications."
48,Server & Application Monitor
48,Infrastructure and application performance monitoring for commercial off-the-shelf and SaaS applications; built on the SolarWinds® Orion® platform.
48,Loggly
48,"Fast and powerful hosted aggregation, analytics and visualization of terabytes of machine data across hybrid applications, cloud applications, and infrastructure."
48,Log Analyzer
48,"Monitoring and visualization of machine data from applications and infrastructure inside the firewall, extending the SolarWinds® Orion® platform."
48,Papertrail
48,"Real-time live tailing, searching, and troubleshooting for cloud applications and environments."
48,Pingdom
48,"Real user, and synthetic monitoring of web applications from outside the firewall."
48,Web Performance Monitor
48,Web application performance monitoring from inside the firewall.
48,View All Application Management Products
48,AppOptics: Next-gen SaaS-based application performance & infrastructure monitoring.
48,Accelerates the identification and getting to the root cause of application performance issues.
48,Learn More
48,Products
48,N-able N-central
48,Automate what you need. Tackle complex networks. Built to help maximize efficiency and scale.
48,N-able Remote Monitoring & Management
48,"Get a comprehensive set of RMM tools to efficiently secure, maintain, and improve your clients’ IT systems."
48,N-able Backup & Recovery
48,"Manage backup for servers, workstations, applications, and business documents from one cloud-based dashboard."
48,N-able Mail Protection & Archiving
48,Protect users from email threats and downtime.
48,N-able Password Management
48,Easily adopt and demonstrate best practice password and documentation management workflows.
48,N-able PSA & Ticketing
48,"Manage ticketing, reporting, and billing to increase helpdesk efficiency."
48,N-able Remote Support
48,Help support customers and their devices with remote support tools designed to be fast and powerful.
48,View All Managed Service Provider Products
48,Microsoft 365 + N-able. Manage more devices from one dashboard.
48,Bringing together N-able RMM and Microsoft Intune management capabilities.
48,Learn More
48,Solutions
48,CHALLENGE
48,Role
48,Technology
48,Remote Monitoring
48,Solutions
48,Network Solutions
48,IT Security Solutions
48,Enterprise Solutions
48,MSP Solutions
48,Infrastructure Management Solutions
48,IT Asset Management
48,SolarWinds Orion Platform
48,Database Management
48,Application Performance Management
48,Compliance Solutions
48,Scalability Solutions
48,Software Defined Solutions
48,Customer Success
48,IT Automation Software
48,IT Cost Optimization
48,IT Agility
48,Education IT Solutions
48,Global System Integrators
48,Small Business Solutions
48,Solutions
48,IT Operations Solutions
48,IT Help Desk Solutions
48,IT Service Desk Solutions
48,ITSM Solutions
48,ITIL Solutions
48,Enterprise Help Desk Solutions
48,Global System Integrators
48,Solutions
48,Office 365 Solutions
48,Active Directory
48,Azure Cloud Solutions
48,Cisco Solutions
48,MySQL Solutions
48,Postgres Solutions
48,DigitalOcean Application Solutions
48,Azure SQL Solutions
48,Solutions
48,Remote Monitoring Solutions
48,Remote Infrastructure Management Solutions
48,Secure Remote Access Solutions
48,Support Remote Workforce Solutions
48,Business Continuity Solutions
48,User Experience Monitoring Solutions
48,Support
48,Renew Maintenance
48,"Renew to download the latest product features, get 24/7 tech support, and access to instructor-led training."
48,Renew Maintenance
48,Learn about Auto-Renewal
48,Access the Success Center
48,"Find product guides, documentation, training, onboarding information, and support articles."
48,Access the Success Center
48,Orion Assistance Program
48,SmartStart Onboarding
48,Support Offerings
48,Technical Support
48,"Submit a ticket for technical and product assistance, or get customer service help."
48,Americas: +1-512-682-9300
48,EMEA: +353 21 5002900
48,APAC: +61 2 8412 4900
48,Submit a Ticket
48,Training & Certification
48,"Learn through self-study, instructor-led, and on-demand classes with the SolarWinds Academy."
48,SolarWinds Academy
48,SolarWinds Certified Professional
48,Customer Portal
48,Download the latest product versions and hotfixes. Manage your portal account and all your products.
48,Access the Customer Portal
48,Community
48,THWACK
48,"Connect with more than 150,000+ community members. Get help, be heard by us and do your job better using our products."
48,View THWACK
48,Orange Matter
48,Get practical advice on managing IT infrastructure from up-and-coming industry voices and well-known tech leaders.
48,View Orange Matter
48,LogicalRead Blog
48,"Into databases? Find articles, code and a community of database experts."
48,View LogicalRead Blog
48,Secure by Design Resource Center
48,"Get the latest SolarWinds investigation updates, advice from leading cybersecurity experts we’re working with, and learn about our Secure by Design journey."
48,View Resources
48,FREE TRIALS
48,Contact Sales
48,Online Quote
48,Products
48,Network Management
48,Network Performance Monitor
48,NetFlow Traffic Analyzer
48,Network Configuration Manager
48,IP Address Manager
48,User Device Tracker
48,VoIP & Network Quality Manager
48,Network Automation Manager
48,Log Analyzer
48,Network Topology Mapper
48,Engineer's Toolset
48,ipMonitor
48,Kiwi CatTools
48,Kiwi Syslog Server
48,Network Bandwidth Analyzer Pack
48,Log and Network Performance Pack
48,IP Control Bundle
48,Systems Management
48,Server & Application Monitor
48,Virtualization Manager
48,Storage Resource Monitor
48,ipMonitor
48,Serv-U Managed File Transfer
48,Serv-U Secured FTP
48,Server Configuration Monitor
48,Log Analyzer
48,Access Rights Manager
48,Application Performance Monitoring
48,Web Performance Monitor
48,SolarWinds Backup
48,Systems Management Bundle
48,Server Performance & Configuration Bundle
48,Log and Systems Performance Pack
48,Application Performance Optimization Pack
48,IT Operations Manager
48,Database Management
48,Database Performance Analyzer
48,Oracle
48,SQL Server
48,Azure SQL Database
48,MySQL
48,SAP ASE
48,MariaDB
48,DB2
48,Amazon Aurora
48,Database Performance Monitor
48,PostgreSQL
48,MongoDB
48,MySQL
48,Redis
48,IT Security
48,Access Rights Manager
48,Security Event Manager
48,Server Configuration Monitor
48,Patch Manager
48,Identity Monitor
48,Serv-U Managed File Transfer
48,Serv-U Secured FTP
48,Serv-U Gateway
48,IT Service Management
48,Service Desk
48,Web Help Desk
48,ipMonitor
48,Desktop Management
48,Dameware Remote Everywhere
48,Dameware Remote Support
48,Dameware Mini Remote Control
48,Help Desk Essentials
48,Application Management
48,AppOptics
48,Server & Application Monitor
48,Loggly
48,Log Analyzer
48,Papertrail
48,Pingdom
48,Web Performance Monitor
48,Managed Service Providers
48,N-able N-central
48,N-able Remote Monitoring & Management
48,N-able Backup & Recovery
48,N-able Mail Protection & Archiving
48,N-able Password Management
48,N-able PSA & Ticketing
48,N-able Remote Support
48,Solutions
48,CHALLENGE
48,Network Solutions
48,IT Security Solutions
48,Enterprise Solutions
48,MSP Solutions
48,Infrastructure Management Solutions
48,IT Asset Management
48,SolarWinds Orion Platform
48,Database Management
48,Application Performance Management
48,Compliance Solutions
48,Scalability Solutions
48,Software Defined Solutions
48,Customer Success
48,IT Automation Software
48,IT Cost Optimization
48,IT Agility
48,Education IT Solutions
48,Global System Integrators
48,Small Business Solutions
48,Role
48,IT Operations Solutions
48,IT Help Desk Solutions
48,IT Service Desk Solutions
48,ITSM Solutions
48,ITIL Solutions
48,Enterprise Help Desk Solutions
48,Global System Integrators
48,Technology
48,Office 365 Solutions
48,Active Directory
48,Azure Cloud Solutions
48,Cisco Solutions
48,MySQL Solutions
48,Postgres Solutions
48,DigitalOcean Application Solutions
48,Azure SQL Solutions
48,Remote Monitoring
48,Remote Monitoring Solutions
48,Remote Infrastructure Management Solutions
48,Secure Remote Access Solutions
48,Support Remote Workforce Solutions
48,Business Continuity Solutions
48,User Experience Monitoring Solutions
48,Support
48,Renew Maintenance
48,Renew Maintenance
48,Learn about Auto-Renewal
48,Access the Success Center
48,Access the Success Center
48,Orion Assistance Program
48,SmartStart Onboarding
48,Support Offerings
48,Technical Support
48,Americas: +1-512-682-9300
48,EMEA: +353 21 5002900
48,APAC: +61 2 8412 4900
48,Submit a Ticket
48,Training & Certification
48,SolarWinds Academy
48,SolarWinds Certified Professional
48,Customer Portal
48,Access the Customer Portal
48,Community
48,THWACK
48,View THWACK
48,Orange Matter
48,View Orange Matter
48,LogicalRead Blog
48,View LogicalRead Blog
48,Secure by Design Resource Center
48,View Resources
48,FREE TRIALS
48,Contact Sales
48,Online Quote
48,View All Products
48,View Free Tools
48,Database Performance Analyzer
48,Features
48,Features
48,Azure SQL Server Performance
48,Database Anomaly Detection
48,SQL Query Analyzer
48,SQL Database Monitor
48,SQL Server Audit Log Tool
48,SQL Server Performance Tuning Tool
48,SQL Query Optimization
48,SQL Server Activity Monitoring
48,Amazon RDS Monitoring
48,VM Option
48,Oracle Exadata Performance Monitoring
48,Database Performance Analyzer Oracle SE
48,Database Optimization
48,MySQL Performance Tuning
48,MySQL Query Analyzer
48,MySQL Slow Query Log
48,Database Monitoring Tools
48,Postgres Performance Tuning
48,SolarWinds is a 2020 TrustRadius Winner in SQL Server Performance Monitoring and Database Performance Monitoring
48,View All Features
48,Databases
48,Databases
48,Oracle
48,SQL server
48,Azure SQL database
48,Aurora
48,PostgreSQL
48,Db2
48,SAP ASE
48,MySQL
48,MariaDB
48,View All Databases
48,Pricing
48,Pricing
48,Get a Quote
48,Licensing Options
48,Resources
48,Technical Resources
48,Datasheet
48,Admin Guide
48,Install Guide
48,Getting Started Guide
48,Documentation
48,Educational Resources
48,Demo
48,Videos
48,SolarWinds Academy
48,Webcasts
48,Whitepapers
48,SolarWinds DPA vs. Redgate SQL Monitor
48,SolarWinds DPA vs. Quest Foglight
48,Connect with Us
48,SolarWinds Events
48,DPA Product Forum
48,Customer Service & Support
48,View All Resources
48,DOWNLOAD FREE TRIAL
48,EMAIL LINK TO FREE TRIAL
48,Database Performance Analyzer
48,Toggle navigation
48,Menu
48,Features
48,Azure SQL Server Performance
48,Database Anomaly Detection
48,SQL Query Analyzer
48,SQL Database Monitor
48,SQL Server Audit Log Tool
48,SQL Server Performance Tuning Tool
48,SQL Query Optimization
48,SQL Server Activity Monitoring
48,Amazon RDS Monitoring
48,VM Option
48,Oracle Exadata Performance Monitoring
48,Database Performance Analyzer Oracle SE
48,Database Optimization
48,MySQL Performance Tuning
48,MySQL Query Analyzer
48,MySQL Slow Query Log
48,Database Monitoring Tools
48,Postgres Performance Tuning
48,View All Features
48,Databases
48,Oracle
48,SQL server
48,Azure SQL database
48,Aurora
48,PostgreSQL
48,Db2
48,SAP ASE
48,MySQL
48,MariaDB
48,View All Databases
48,Pricing
48,Get a Quote
48,Licensing Options
48,Resources
48,Datasheet
48,Admin Guide
48,Install Guide
48,Getting Started Guide
48,Documentation
48,Demo
48,Videos
48,SolarWinds Academy
48,Webcasts
48,Whitepapers
48,SolarWinds DPA vs. Redgate SQL Monitor
48,SolarWinds DPA vs. Quest Foglight
48,SolarWinds Events
48,DPA Product Forum
48,Customer Service & Support
48,View All Resources
48,DOWNLOAD FREE TRIAL
48,EMAIL LINK TO FREE TRIAL
48,MySQL Database Performance Monitoring Tools
48,Quickly identify causes of MySQL problems and get tuning advice
48,DOWNLOAD FREE TRIAL
48,Fully functional for 14 days
48,EMAIL LINK TO FREE TRIAL
48,Fully functional for 14 days
48,INTERACTIVE DEMO
48,INTERACTIVE DEMO
48,Get a Complete Picture
48,View Response Time Data
48,Team Collaboration
48,View Historical Analysis
48,Monitor Assorted Metrics
48,Get a complete picture of MySQL performance with multi-dimensional database performance analysis
48,Get a complete picture of MySQL performance with multi-dimensional database performance analysis
48,"SolarWinds® Database Performance Analyzer (DPA) offers multi-dimensional performance analysis, which is built to correlate response time, server resources, SQL statements, wait events/types, and relevant context. This helps give you a complete picture of MySQL performance, allowing you to quickly identify the root cause of complex performance problems. DPA is designed to constantly look at thousands of metrics possibly contributing to application performance issues, so you can pinpoint what happened at any given time."
48,DOWNLOAD FREE TRIAL
48,Fully functional for 14 days
48,EMAIL LINK TO FREE TRIAL
48,Fully functional for 14 days
48,Learn more
48,Learn more
48,View wait-based response time analytics to help identify the cause of MySQL slowdowns
48,View wait-based response time analytics to help identify the cause of MySQL slowdowns
48,"Traditional database monitoring tools have dashboards showing hundreds of meaningless numbers requiring extensive analysis. Current application performance management tools provide hints, but they don’t help you find root causes."
48,"SolarWinds DPA is a powerful tool built to address these gaps. DPA also incorporates wait time analysis, so it focuses on the speed at which the database responds to application requests instead of health. This can offer users a pragmatic approach to determining what causes MySQL slowdowns."
48,DOWNLOAD FREE TRIAL
48,Fully functional for 14 days
48,EMAIL LINK TO FREE TRIAL
48,Fully functional for 14 days
48,Learn more
48,Learn more
48,Easily collaborate across teams with a focus on MySQL performance
48,Easily collaborate across teams with a focus on MySQL performance
48,"With DPA, all teams can see a complete and accurate performance view of the database, applications, host OS, server, network, and storage systems from a dashboard capable of covering multiple database technologies, whether they’re on-premises or in the cloud."
48,"This shared view is designed to eliminate guessing and finger-pointing, since developers and operations teams can see the effects of any changes they make from a single source."
48,DOWNLOAD FREE TRIAL
48,Fully functional for 14 days
48,EMAIL LINK TO FREE TRIAL
48,Fully functional for 14 days
48,Learn more
48,Learn more
48,View detailed historical analysis and dynamic baselines for greater context around your MySQL server
48,View detailed historical analysis and dynamic baselines for greater context around your MySQL server
48,"DPA doesn’t just work in real time; it also includes historical data analysis. SolarWinds DPA can collate data points from two years ago or from a few seconds ago with one-second resolution, and it presents the results in easy-to-understand charts capable of leading to more detailed data surrounding an issue."
48,"This historical data analysis can help you more easily identify opportunities for improvement so your MySQL optimization doesn’t just revolve around reactively fixing problems. Instead, historical data analysis can help you make proactive changes."
48,DOWNLOAD FREE TRIAL
48,Fully functional for 14 days
48,EMAIL LINK TO FREE TRIAL
48,Fully functional for 14 days
48,Learn more
48,Learn more
48,Monitor a range of MySQL performance metrics with DPA
48,Monitor a range of MySQL performance metrics with DPA
48,"MySQL database performance monitoring is essential for optimization. DPA is designed to track a range of MySQL database performance metrics, allowing for multi-dimensional analysis depending on the issue and your organization’s needs."
48,DPA is built to monitor metrics like the following:
48,Query throughput (questions and writes)
48,"Buffer pool usage (InnoDB data, including buffer pool utilization and reads)"
48,"Query execution performance (query run time, query errors, slow queries)"
48,Connections (threads connected and aborted connects)
48,DOWNLOAD FREE TRIAL
48,Fully functional for 14 days
48,EMAIL LINK TO FREE TRIAL
48,Fully functional for 14 days
48,Learn more
48,Learn more
48,Get More on MySQL Performance Monitoring
48,Do you find yourself asking…
48,What is MySQL performance monitoring?
48,How does MySQL performance monitoring work?
48,Why is monitoring MySQL performance important?
48,What does a MySQL performance monitoring tool do?
48,How does MySQL performance monitoring work in DPA?
48,What is MySQL performance monitoring?
48,MySQL database performance monitoring is the process of tracking metrics related to how MySQL is functioning. This process can help catch issues and ensure the database continues to support the function of other applications and systems.
48,"MySQL is an open-source relational database management system (RDBMS) based on Structured Query Language (SQL). Because it’s based on SQL, MySQL users don’t need to learn new commands to use the system. This ease of use is just one of the reasons MySQL is the world’s most popular open-source relational database server. However, admins still need to understand how to optimize its performance, which requires MySQL performance monitoring and tuning."
48,"The basic work of a database is to run queries. When MySQL performance isn’t at the level you expect, a variety of factors may be causing the issue. It’s possible the issue could relate to hardware and a lack of memory; it could also be a problem with the way queries are being written or indexed. There might even be an issue in the cache. The only way to determine what’s causing poor performance is to monitor all the elements in your system capable of impacting MySQL performance. This involves monitoring metrics related to the database server and the speed at which it executes queries, and it involves monitoring all other system metrics with a potential impact on MySQL performance."
48,How does MySQL performance monitoring work?
48,"To monitor MySQL performance, the database administrator collects and interprets metrics related to MySQL server performance and identifies when and why performance issues are arising."
48,"The MySQL performance monitor then uses this information to determine and tackle the root cause of the problem and return performance to normal. Ideally, admins can use the insights gained from monitoring to proactively improve MySQL performance before issues arise."
48,"There are hundreds of metrics capable of providing useful information about what could be causing or contributing to a performance failure. Some of these metrics—like CPU usage, load, swap usage, memory usage, disk usage, and network bandwidth—relate to your system."
48,"Given that MySQL engages with the entire system, sometimes issues with MySQL can be caused by problems outside the server itself. Effective monitoring involves keeping tabs on these system metrics in addition to the plethora of performance metrics directly related to the MySQL server, like query run time, query errors, slow queries, questions, read/writes, and more."
48,"If you want to optimize MySQL, monitor the performance of your whole system and not just the database server. By monitoring metrics from across the system, you can gain insights to help you maintain or improve query execution times."
48,Why is monitoring MySQL performance important?
48,"Monitoring MySQL performance is important because it helps you make sure the database server is executing queries as expected. This, in turn, helps ensure end users don’t experience application slowdowns, and it can help keep business productivity high."
48,"When performance problems occur, monitoring MySQL performance reports and the metrics they contain can help you quickly get to the bottom of the issue, so you can begin troubleshooting solutions."
48,"Effective monitoring lets you do more than just be reactive when performance issues arise—it also gives you the power to proactively improve MySQL performance. With a quality MySQL performance monitoring tool, you can identify opportunities to improve your MySQL performance even if the monitoring systems are showing all green."
48,"Beyond simply improving performance, MySQL performance monitoring can save you money. Any database application or server will hit hardware limits at a certain point. Often, the easiest fix seems to be buying more hardware. However, if there’s an underlying problem causing resources to be wasted, you might find yourself continuously spending money on more hardware and not solving the problem. By finding and addressing the underlying issues making you hit hardware limits, you can reduce your costs."
48,What does a MySQL performance monitoring tool do?
48,A MySQL performance monitoring tool is built to consistently measure the hundreds of metrics tied to the database server’s performance and alert you when an issue requires your attention.
48,"Don’t manually search through metrics—a monitoring tool can automatically provide you with the information you need to understand performance problems. Monitoring tools are built to pull out and correlate the relevant metrics to give you a complete picture of your MySQL performance. From system metrics like CPU and memory to specific metrics about queries, questions, logs, errors, and more, a MySQL performance monitor tool is designed to track everything and alert you when the system crosses problematic thresholds."
48,"Beyond sending alerts, a quality performance monitoring tool can help you identify the root cause of performance issues. By directing you to the proper metrics, it can help you better focus your efforts."
48,"Unlike regular monitoring, which only lets you see current performance metrics, a good MySQL performance monitoring tool provides historical trend analysis, so you can see whether the problems you’re experiencing have a historical precedent. Without this analysis, it’s nearly impossible to identify how performance has changed, what’s normal, and what has caused problems in the past."
48,How does MySQL performance monitoring work in DPA?
48,"SolarWinds® Database Performance Analyzer for MySQL is designed to take the heavy lifting out of MySQL performance monitoring. The tool is built to monitor metrics specific to MySQL and those related to your broader system, collecting and correlating the ones most important to optimizing MySQL performance and alerting you when a metric demands your attention."
48,"With DPA, you can easily isolate and target the root cause of performance issues. With the tool’s historical trend analysis, you can see how performance has changed and whether the problems you’re experiencing now are related to past problems. If you just want to focus on what’s happening right now, this is possible thanks to the “current view” in DPA. With easy-to-understand bar charts, the DPA monitoring dashboard is built to offer you a simplified visual understanding of your data."
48,"With DPA, you don’t have to wait for something to go wrong before you begin improving MySQL performance. DPA allows you to more easily gain insights to help you make improvements and optimize performance, even when your entire dashboard is green."
48,"With DPA, you can rest easy knowing the thousands of metrics related to MySQL performance are constantly being monitored and you’ll be alerted as soon as any of these metrics demand your attention."
48,What is MySQL performance monitoring?
48,MySQL database performance monitoring is the process of tracking metrics related to how MySQL is functioning. This process can help catch issues and ensure the database continues to support the function of other applications and systems.
48,"MySQL is an open-source relational database management system (RDBMS) based on Structured Query Language (SQL). Because it’s based on SQL, MySQL users don’t need to learn new commands to use the system. This ease of use is just one of the reasons MySQL is the world’s most popular open-source relational database server. However, admins still need to understand how to optimize its performance, which requires MySQL performance monitoring and tuning."
48,"The basic work of a database is to run queries. When MySQL performance isn’t at the level you expect, a variety of factors may be causing the issue. It’s possible the issue could relate to hardware and a lack of memory; it could also be a problem with the way queries are being written or indexed. There might even be an issue in the cache. The only way to determine what’s causing poor performance is to monitor all the elements in your system capable of impacting MySQL performance. This involves monitoring metrics related to the database server and the speed at which it executes queries, and it involves monitoring all other system metrics with a potential impact on MySQL performance."
48,Close
48,How does MySQL performance monitoring work?
48,"To monitor MySQL performance, the database administrator collects and interprets metrics related to MySQL server performance and identifies when and why performance issues are arising."
48,"The MySQL performance monitor then uses this information to determine and tackle the root cause of the problem and return performance to normal. Ideally, admins can use the insights gained from monitoring to proactively improve MySQL performance before issues arise."
48,"There are hundreds of metrics capable of providing useful information about what could be causing or contributing to a performance failure. Some of these metrics—like CPU usage, load, swap usage, memory usage, disk usage, and network bandwidth—relate to your system."
48,"Given that MySQL engages with the entire system, sometimes issues with MySQL can be caused by problems outside the server itself. Effective monitoring involves keeping tabs on these system metrics in addition to the plethora of performance metrics directly related to the MySQL server, like query run time, query errors, slow queries, questions, read/writes, and more."
48,"If you want to optimize MySQL, monitor the performance of your whole system and not just the database server. By monitoring metrics from across the system, you can gain insights to help you maintain or improve query execution times."
48,Close
48,Why is monitoring MySQL performance important?
48,"Monitoring MySQL performance is important because it helps you make sure the database server is executing queries as expected. This, in turn, helps ensure end users don’t experience application slowdowns, and it can help keep business productivity high."
48,"When performance problems occur, monitoring MySQL performance reports and the metrics they contain can help you quickly get to the bottom of the issue, so you can begin troubleshooting solutions."
48,"Effective monitoring lets you do more than just be reactive when performance issues arise—it also gives you the power to proactively improve MySQL performance. With a quality MySQL performance monitoring tool, you can identify opportunities to improve your MySQL performance even if the monitoring systems are showing all green."
48,"Beyond simply improving performance, MySQL performance monitoring can save you money. Any database application or server will hit hardware limits at a certain point. Often, the easiest fix seems to be buying more hardware. However, if there’s an underlying problem causing resources to be wasted, you might find yourself continuously spending money on more hardware and not solving the problem. By finding and addressing the underlying issues making you hit hardware limits, you can reduce your costs."
48,Close
48,What does a MySQL performance monitoring tool do?
48,A MySQL performance monitoring tool is built to consistently measure the hundreds of metrics tied to the database server’s performance and alert you when an issue requires your attention.
48,"Don’t manually search through metrics—a monitoring tool can automatically provide you with the information you need to understand performance problems. Monitoring tools are built to pull out and correlate the relevant metrics to give you a complete picture of your MySQL performance. From system metrics like CPU and memory to specific metrics about queries, questions, logs, errors, and more, a MySQL performance monitor tool is designed to track everything and alert you when the system crosses problematic thresholds."
48,"Beyond sending alerts, a quality performance monitoring tool can help you identify the root cause of performance issues. By directing you to the proper metrics, it can help you better focus your efforts."
48,"Unlike regular monitoring, which only lets you see current performance metrics, a good MySQL performance monitoring tool provides historical trend analysis, so you can see whether the problems you’re experiencing have a historical precedent. Without this analysis, it’s nearly impossible to identify how performance has changed, what’s normal, and what has caused problems in the past."
48,Close
48,How does MySQL performance monitoring work in DPA?
48,"SolarWinds® Database Performance Analyzer for MySQL is designed to take the heavy lifting out of MySQL performance monitoring. The tool is built to monitor metrics specific to MySQL and those related to your broader system, collecting and correlating the ones most important to optimizing MySQL performance and alerting you when a metric demands your attention."
48,"With DPA, you can easily isolate and target the root cause of performance issues. With the tool’s historical trend analysis, you can see how performance has changed and whether the problems you’re experiencing now are related to past problems. If you just want to focus on what’s happening right now, this is possible thanks to the “current view” in DPA. With easy-to-understand bar charts, the DPA monitoring dashboard is built to offer you a simplified visual understanding of your data."
48,"With DPA, you don’t have to wait for something to go wrong before you begin improving MySQL performance. DPA allows you to more easily gain insights to help you make improvements and optimize performance, even when your entire dashboard is green."
48,"With DPA, you can rest easy knowing the thousands of metrics related to MySQL performance are constantly being monitored and you’ll be alerted as soon as any of these metrics demand your attention."
48,Close
48,"""This product is invaluable to my team for problem resolution. We could not do our jobs as efficiently, nor could we solve problems as quickly, without this product."""
48,Administrator
48,S&P 500 Company
48,Optimize your database with MySQL performance monitoring software
48,Database Performance Analyzer for MySQL
48,Use historical trends to put performance issues in context.
48,Identify the root cause behind problems to more quickly resolve issues.
48,Proactively optimize MySQL performance before problems arise.
48,DOWNLOAD FREE TRIAL
48,Fully functional for 14 days
48,EMAIL LINK TO FREE TRIAL
48,Fully functional for 14 days
48,Learn More
48,Learn More
48,Let’s talk it over.
48,Contact our team. Anytime.
48,+1-866-530-8100
48,sales@solarwinds.com
48,Learn More About Our Popular Products
48,Automated User Provisioning
48,Automated User Provisioning
48,NTFS Permissions Report Tool
48,NTFS Permissions Report Tool
48,Active Directory Auditing Tool
48,Active Directory Auditing Tool
48,Network Audit
48,Network Audit
48,IP Address Scanner
48,IP Address Scanner
48,Network Troubleshooting
48,Network Troubleshooting
48,Server Monitoring Software
48,Server Monitoring Software
48,Virtualization Manager
48,Virtualization Manager
48,Database Performance Monitoring
48,Database Performance Monitoring
48,Service Desk
48,Service Desk
48,IT Help Desk
48,IT Help Desk
48,IT Asset Management
48,IT Asset Management
48,We’re Geekbuilt.®
48,"Developed by network and systems engineers who know what it takes to manage today's dynamic IT environments, SolarWinds has a deep connection to the IT community."
48,"The result? IT management products that are effective, accessible, and easy to use."
48,Company
48,Investors
48,EVENTS
48,Career Center
48,Security Advisory
48,Resource Center
48,Preference Center
48,For Customers
48,For Government
48,GDPR Resource Center
48,Legal Documents
48,Privacy
48,California Privacy Rights
48,Security Information
48,Documentation & Uninstall Information
48,Trust Center
48,"© 2021 SolarWinds Worldwide, LLC. All rights reserved."
48,Close
48,{{STATIC CONTENT}}
48,{{CAPTION_TITLE}}
48,{{CAPTION_CONTENT}}
48,{{TITLE}}
49,Optimizer Hints in Impala | 6.3.x | Cloudera Documentation
49,Documentation
49,Products
49,Services & Support
49,Solutions
49,Cloudera Enterprise 6.3.x | Other versions
49,CDH
49,Component GuidesImpalaSQL ReferenceSQL
49,Statements
49,View All Categories
49,Getting Started
49,Cloudera Personas
49,Planning a New Cloudera Enterprise Deployment
49,CDH
49,Hive
49,Impala
49,Kudu
49,Sentry
49,Spark
49,External Documentation
49,Cloudera Manager
49,Software Management
49,Parcels
49,Navigator
49,Getting Started
49,FAQ
49,Navigator Encryption
49,Navigator Key Trustee Server
49,Navigator Key HSM
49,Navigator HSM KMS
49,Navigator Encrypt
49,Proof-of-Concept Installation Guide
49,Before You Begin
49,Installing a Proof-of-Concept Cluster
49,Step 1: Run the Cloudera Manager Installer
49,Step 2: Install CDH Using the Wizard
49,Step 3: Set Up a Cluster
49,Managing the Embedded Database
49,Migrating Embedded PostgreSQL Database to External PostgreSQL Database
49,Getting Support
49,FAQ
49,Release Notes
49,Requirements and Supported Versions
49,Installation
49,Before You Install
49,Storage Space Planning for Cloudera Manager
49,Configure Network Names
49,Disabling the Firewall
49,Setting SELinux mode
49,Enable an NTP Service
49,Install Python 2.7 on Hue Hosts
49,Impala Requirements
49,Required Privileges
49,Ports
49,Cloudera Manager and Navigator
49,Navigator Encryption
49,CDH Components
49,DistCp
49,Third-Party Components
49,Recommended Role Distribution
49,Custom Installation Solutions
49,Configuring a Local Parcel Repository
49,Configuring a Local Package Repository
49,Manually Install Cloudera Software Packages
49,Creating Virtual Images of Cluster Hosts
49,Configuring a Custom Java Home Location
49,Creating a CDH Cluster Using a Cloudera Manager Template
49,Service Dependencies in Cloudera Manager
49,Installing Cloudera Manager and CDH
49,Step 1: Configure a Repository
49,Step 2: Install JDK
49,Step 3: Install Cloudera Manager Server
49,Step 4: Install Databases
49,Install and Configure MariaDB
49,Install and Configure MySQL
49,Install and Configure PostgreSQL
49,Install and Configure Oracle Database
49,Step 5: Set up the Cloudera Manager Database
49,Step 6: Install CDH and Other Software
49,Step 7: Set Up a Cluster
49,Installing Navigator Data Management
49,Installing Navigator Encryption
49,Installing Cloudera Navigator Key Trustee Server
49,Installing Cloudera Navigator Key HSM
49,Installing Key Trustee KMS
49,Installing Navigator HSM KMS Backed by Thales HSM
49,Installing Navigator HSM KMS Backed by Luna HSM
49,Installing Cloudera Navigator Encrypt
49,After Installation
49,Deploying Clients
49,Testing the Installation
49,Installing the GPL Extras Parcel
49,Migrating from Packages to Parcels
49,Migrating from Parcels to Packages
49,Secure Your Cluster
49,Troubleshooting Installation Problems
49,Uninstalling Cloudera Software
49,Uninstalling a CDH Component From a Single Host
49,Upgrade Guide
49,Cluster Management
49,Cloudera Manager
49,Cloudera Manager Admin Console
49,Home Page
49,Documentation
49,Automatic Logout
49,FAQ
49,Cloudera Manager API
49,Cluster Automation
49,Cloudera Manager Administration
49,"Starting, Stopping, and Restarting the Cloudera Manager Server"
49,Configuring Cloudera Manager Server Ports
49,Moving the Cloudera Manager Server to a New Host
49,Migrating Embedded PostgreSQL Database to External PostgreSQL Database
49,Migrating from PostgreSQL Database Server to MySQL/Oracle Database Server
49,Managing the Cloudera Manager Server Log
49,Cloudera Manager Agents
49,"Starting, Stopping, and Restarting Cloudera Manager Agents"
49,Configuring Cloudera Manager Agents
49,Managing Cloudera Manager Agent Logs
49,Configuring Network Settings
49,Managing Licenses
49,Sending Usage and Diagnostic Data to Cloudera
49,Exporting and Importing Cloudera Manager Configuration
49,Backing Up Cloudera Manager
49,Other Tasks and Settings
49,Cloudera Management Service
49,Extending Cloudera Manager
49,Cluster Configuration Overview
49,Modifying Configuration Properties Using Cloudera Manager
49,Autoconfiguration
49,Custom Configuration
49,Stale Configurations
49,Client Configuration Files
49,Viewing and Reverting Configuration Changes
49,Exporting and Importing Cloudera Manager Configuration
49,Cloudera Manager Configuration Properties Reference
49,Managing Clusters
49,Adding and Deleting Clusters
49,"Starting, Stopping, Refreshing, and Restarting a Cluster"
49,Pausing a Cluster in AWS
49,Renaming a Cluster
49,Cluster-Wide Configuration
49,Virtual Private Clusters and Cloudera SDX
49,Compatibility Considerations for Virtual Private Clusters
49,"Tutorial: Using Impala, Hive and Hue with Virtual Private Clusters"
49,Networking Considerations for Virtual Private Clusters
49,Managing Services
49,HBase
49,HDFS
49,Data Durability
49,Enabling Erasure Coding
49,NameNodes
49,Backing Up and Restoring HDFS Metadata
49,Moving NameNode Roles
49,Sizing NameNode Heap Memory
49,Backing Up and Restoring NameNode Metadata
49,DataNodes
49,Configuring Storage Directories for DataNodes
49,Configuring Storage Balancing for DataNodes
49,Performing Disk Hot Swap for DataNodes
49,JournalNodes
49,Configuring Short-Circuit Reads
49,Configuring HDFS Trash
49,Preventing Inadvertent Deletion of Directories
49,HDFS Balancers
49,Enabling WebHDFS
49,Adding HttpFS
49,Adding and Configuring an NFS Gateway
49,Setting HDFS Quotas
49,Configuring Mountable HDFS
49,Configuring Centralized Cache Management in HDFS
49,Configuring Proxy Users to Access HDFS
49,Using CDH with Isilon Storage
49,Configuring Heterogeneous Storage in HDFS
49,Hive
49,Hue
49,Adding a Hue Service and Role Instance
49,Managing Hue Analytics Data Collection
49,Enabling Hue Applications Using Cloudera Manager
49,Impala
49,The Impala Service
49,Modifying Impala Startup Options
49,Post-Installation Configuration for Impala
49,Configuring Impala to Work with ODBC
49,Configuring Impala to Work with JDBC
49,Key-Value Store Indexer
49,Kudu
49,Solr
49,Spark
49,Managing Spark Using Cloudera Manager
49,Managing the Spark History Server
49,Sqoop 1 Client
49,YARN (MRv2) and MapReduce (MRv1)
49,Managing YARN
49,Managing YARN ACLs
49,Managing MapReduce
49,Managing ZooKeeper
49,Configuring Services to Use the GPL Extras Parcel
49,Managing Hosts
49,Viewing Host Details
49,Using the Host Inspector
49,Adding a Host to the Cluster
49,Specifying Racks for Hosts
49,Host Templates
49,Performing Maintenance on a Cluster Host
49,Tuning and Troubleshooting Host Decommissioning
49,Maintenance Mode
49,Changing Hostnames
49,Deleting Hosts
49,Moving a Host Between Clusters
49,Managing Services
49,Adding a Service
49,Comparing Configurations for a Service Between Clusters
49,Add-on Services
49,"Starting, Stopping, and Restarting Services"
49,Rolling Restart
49,Aborting a Pending Command
49,Deleting Services
49,Renaming a Service
49,Configuring Maximum File Descriptors
49,Exposing Hadoop Metrics to Graphite
49,Exposing Hadoop Metrics to Ganglia
49,Managing Roles
49,Role Instances
49,Role Groups
49,Monitoring and Diagnostics
49,Introduction to Cloudera Manager Monitoring
49,Time Line
49,Health Tests
49,Home Page
49,"Viewing Charts for Cluster, Service, Role, and Host Instances"
49,Configuring Monitoring Settings
49,Monitoring Clusters
49,Inspecting Network Performance
49,Monitoring Services
49,Monitoring Service Status
49,Viewing Service Status
49,Viewing Service Instance Details
49,Viewing Role Instance Status
49,The Processes Tab
49,Running Diagnostic Commands for Roles
49,Periodic Stacks Collection
49,Viewing Running and Recent Commands
49,Monitoring Resource Management
49,Monitoring Hosts
49,Host Details
49,Host Inspector
49,Monitoring Activities
49,Monitoring MapReduce Jobs
49,Viewing and Filtering MapReduce Activities
49,"Viewing the Jobs in a Pig, Oozie, or Hive Activity"
49,Task Attempts
49,Viewing Activity Details in a Report Format
49,Comparing Similar Activities
49,Viewing the Distribution of Task Attempts
49,Monitoring Impala Queries
49,Query Details
49,Monitoring YARN Applications
49,Monitoring Spark Applications
49,Events
49,Alerts
49,Managing Alerts
49,Configuring Alert Email Delivery
49,Configuring Alert SNMP Delivery
49,Configuring Custom Alert Scripts
49,Triggers
49,Cloudera Manager Trigger Use Cases
49,Lifecycle and Security Auditing
49,Charting Time-Series Data
49,Dashboards
49,tsquery Language
49,Metric Aggregation
49,Logs
49,Viewing the Cloudera Manager Server Log
49,Viewing the Cloudera Manager Agent Logs
49,Managing Disk Space for Log Files
49,Reports
49,Directory Usage Report
49,Disk Usage Reports
49,"Activity, Application, and Query Reports"
49,The File Browser
49,Downloading HDFS Directory Access Permission Reports
49,Troubleshooting Cluster Configuration and Operation
49,Monitoring Reference
49,Cloudera Manager Entity Types
49,Cloudera Manager Entity Type Attributes
49,Cloudera Manager Events
49,HEALTH_CHECK Category
49,SYSTEM Category
49,AUDIT_EVENT Category
49,HBASE Category
49,LOG_MESSAGE Category
49,ACTIVITY_EVENT Category
49,Cloudera Manager Health Tests
49,Active Database Health Tests
49,Active Key Trustee Server Health Tests
49,Activity Monitor Health Tests
49,Alert Publisher Health Tests
49,Authentication Server Health Tests
49,Authentication Server Load Balancer Health Tests
49,Authentication Service Health Tests
49,Cloudera Management Service Health Tests
49,DataNode Health Tests
49,Event Server Health Tests
49,Failover Controller Health Tests
49,Flume Health Tests
49,Flume Agent Health Tests
49,Garbage Collector Health Tests
49,HBase Health Tests
49,HBase REST Server Health Tests
49,HBase Thrift Server Health Tests
49,HDFS Health Tests
49,History Server Health Tests
49,Hive Health Tests
49,Hive Execution Health Tests
49,Hive Metastore Server Health Tests
49,HiveServer2 Health Tests
49,Host Health Tests
49,Host Monitor Health Tests
49,HttpFS Health Tests
49,Hue Health Tests
49,Hue Server Health Tests
49,Impala Health Tests
49,Impala Catalog Server Health Tests
49,Impala Daemon Health Tests
49,Impala Llama ApplicationMaster Health Tests
49,Impala StateStore Health Tests
49,JobHistory Server Health Tests
49,JobTracker Health Tests
49,JournalNode Health Tests
49,Kafka Health Tests
49,Kafka Broker Health Tests
49,Kafka MirrorMaker Health Tests
49,Kerberos Ticket Renewer Health Tests
49,Key Management Server Health Tests
49,Key Management Server Proxy Health Tests
49,Key-Value Store Indexer Health Tests
49,Kudu Health Tests
49,Lily HBase Indexer Health Tests
49,Load Balancer Health Tests
49,MapReduce Health Tests
49,Master Health Tests
49,Monitor Health Tests
49,NFS Gateway Health Tests
49,NameNode Health Tests
49,Navigator Audit Server Health Tests
49,Navigator Luna KMS Metastore Health Tests
49,Navigator Luna KMS Proxy Health Tests
49,Navigator Metadata Server Health Tests
49,Navigator Thales KMS Metastore Health Tests
49,Navigator Thales KMS Proxy Health Tests
49,NodeManager Health Tests
49,Oozie Health Tests
49,Oozie Server Health Tests
49,Passive Database Health Tests
49,Passive Key Trustee Server Health Tests
49,RegionServer Health Tests
49,Reports Manager Health Tests
49,ResourceManager Health Tests
49,SecondaryNameNode Health Tests
49,Sentry Health Tests
49,Sentry Server Health Tests
49,Service Monitor Health Tests
49,Solr Health Tests
49,Solr Server Health Tests
49,Spark Health Tests
49,Spark (Standalone) Health Tests
49,Tablet Server Health Tests
49,TaskTracker Health Tests
49,Telemetry Publisher Health Tests
49,Tracer Health Tests
49,WebHCat Server Health Tests
49,Worker Health Tests
49,YARN (MR2 Included) Health Tests
49,ZooKeeper Health Tests
49,ZooKeeper Server Health Tests
49,Cloudera Manager Metrics
49,Accumulo Metrics
49,Active Database Metrics
49,Active Key Trustee Server Metrics
49,Activity Metrics
49,Activity Monitor Metrics
49,Agent Metrics
49,Alert Publisher Metrics
49,Attempt Metrics
49,Authentication Server Metrics
49,Authentication Server Load Balancer Metrics
49,Authentication Service Metrics
49,Cloudera Management Service Metrics
49,Cloudera Manager Server Metrics
49,Cluster Metrics
49,DSSD DataNode Metrics
49,DataNode Metrics
49,Directory Metrics
49,Disk Metrics
49,Event Server Metrics
49,Failover Controller Metrics
49,Filesystem Metrics
49,Flume Metrics
49,Flume Channel Metrics
49,Flume Sink Metrics
49,Flume Source Metrics
49,Garbage Collector Metrics
49,HBase Metrics
49,HBase REST Server Metrics
49,HBase RegionServer Replication Peer Metrics
49,HBase Thrift Server Metrics
49,HDFS Metrics
49,HDFS Cache Directive Metrics
49,HDFS Cache Pool Metrics
49,HRegion Metrics
49,HTable Metrics
49,History Server Metrics
49,Hive Metrics
49,Hive Execution Metrics
49,Hive Metastore Server Metrics
49,HiveServer2 Metrics
49,Host Metrics
49,Host Monitor Metrics
49,HttpFS Metrics
49,Hue Metrics
49,Hue Server Metrics
49,Impala Metrics
49,Impala Catalog Server Metrics
49,Impala Daemon Metrics
49,Impala Daemon Resource Pool Metrics
49,Impala Llama ApplicationMaster Metrics
49,Impala Pool Metrics
49,Impala Pool User Metrics
49,Impala Query Metrics
49,Impala StateStore Metrics
49,Isilon Metrics
49,Java KeyStore KMS Metrics
49,JobHistory Server Metrics
49,JobTracker Metrics
49,JournalNode Metrics
49,Kafka Metrics
49,Kafka Broker Metrics
49,Kafka Broker Topic Metrics
49,Kafka Broker Topic Partition Metrics
49,Kafka Consumer Metrics
49,Kafka Consumer Group Metrics
49,Kafka MirrorMaker Metrics
49,Kafka Producer Metrics
49,Kafka Replica Metrics
49,Kerberos Ticket Renewer Metrics
49,Key Management Server Metrics
49,Key Management Server Proxy Metrics
49,Key Trustee KMS Metrics
49,Key Trustee Server Metrics
49,Key-Value Store Indexer Metrics
49,Kudu Metrics
49,Kudu Replica Metrics
49,Lily HBase Indexer Metrics
49,Load Balancer Metrics
49,MapReduce Metrics
49,Master Metrics
49,Monitor Metrics
49,NFS Gateway Metrics
49,NameNode Metrics
49,Navigator Audit Server Metrics
49,Navigator HSM KMS backed by SafeNet Luna HSM Metrics
49,Navigator HSM KMS backed by Thales HSM Metrics
49,Navigator Luna KMS Metastore Metrics
49,Navigator Luna KMS Proxy Metrics
49,Navigator Metadata Server Metrics
49,Navigator Thales KMS Metastore Metrics
49,Navigator Thales KMS Proxy Metrics
49,Network Interface Metrics
49,NodeManager Metrics
49,Oozie Metrics
49,Oozie Server Metrics
49,Passive Database Metrics
49,Passive Key Trustee Server Metrics
49,RegionServer Metrics
49,Reports Manager Metrics
49,ResourceManager Metrics
49,SecondaryNameNode Metrics
49,Sentry Metrics
49,Sentry Server Metrics
49,Server Metrics
49,Service Monitor Metrics
49,Solr Metrics
49,Solr Replica Metrics
49,Solr Server Metrics
49,Solr Shard Metrics
49,Spark Metrics
49,Spark (Standalone) Metrics
49,Sqoop 1 Client Metrics
49,Tablet Server Metrics
49,TaskTracker Metrics
49,Telemetry Publisher Metrics
49,Time Series Table Metrics
49,Tracer Metrics
49,User Metrics
49,WebHCat Server Metrics
49,Worker Metrics
49,YARN (MR2 Included) Metrics
49,YARN Pool Metrics
49,YARN Pool User Metrics
49,ZooKeeper Metrics
49,Disabling Metrics for Specific Roles
49,Performance Management
49,Optimizing Performance in CDH
49,Choosing and Configuring Data Compression
49,Tuning the Solr Server
49,Tuning Spark Applications
49,Tuning YARN
49,Tuning JVM Garbage Collection
49,Resource Management
49,Static Service Pools
49,Linux Control Groups (cgroups)
49,Dynamic Resource Pools
49,YARN (MRv2) and MapReduce (MRv1) Schedulers
49,Configuring the Fair Scheduler
49,Enabling and Disabling Fair Scheduler Preemption
49,Data Storage for Monitoring Data
49,Cluster Utilization Reports
49,Creating a Custom Cluster Utilization Report
49,High Availability
49,HDFS High Availability
49,Introduction to HDFS High Availability
49,Configuring Hardware for HDFS HA
49,Enabling HDFS HA
49,Disabling and Redeploying HDFS HA
49,Configuring Other CDH Components to Use HDFS HA
49,Administering an HDFS High Availability Cluster
49,Changing a Nameservice Name for Highly Available HDFS Using Cloudera Manager
49,MapReduce (MRv1) and YARN (MRv2) High Availability
49,YARN (MRv2) ResourceManager High Availability
49,Work Preserving Recovery for YARN Components
49,MapReduce (MRv1) JobTracker High Availability
49,Cloudera Navigator Key Trustee Server High Availability
49,Enabling Key Trustee KMS High Availability
49,Enabling Navigator HSM KMS High Availability
49,High Availability for Other CDH Components
49,HBase High Availability
49,HBase Read Replicas
49,Oozie High Availability
49,Search High Availability
49,Navigator Data Management in a High Availability Environment
49,Configuring Cloudera Manager for High Availability With a Load Balancer
49,Introduction to Cloudera Manager Deployment Architecture
49,Prerequisites for Setting up Cloudera Manager High Availability
49,Cloudera Manager Failover Protection
49,High-Level Steps to Configure Cloudera Manager High Availability
49,Step 1: Setting Up Hosts and the Load Balancer
49,Step 2: Installing and Configuring Cloudera Manager Server for High Availability
49,Step 3: Installing and Configuring Cloudera Management Service for High Availability
49,Step 4: Automating Failover with Corosync and Pacemaker
49,Database High Availability Configuration
49,TLS and Kerberos Configuration for Cloudera Manager High Availability
49,Backup and Disaster Recovery
49,Port Requirements for Backup and Disaster Recovery
49,Data Replication
49,Designating a Replication Source
49,HDFS Replication
49,Monitoring the Performance of HDFS Replications
49,Hive/Impala Replication
49,Monitoring the Performance of Hive/Impala Replications
49,Replicating Data to Impala Clusters
49,Using Snapshots with Replication
49,Enabling Replication Between Clusters with Kerberos Authentication
49,Replication of Encrypted Data
49,HBase Replication
49,Snapshots
49,Cloudera Manager Snapshot Policies
49,Managing HBase Snapshots
49,Managing HDFS Snapshots
49,BDR Tutorials
49,How To Back Up and Restore Apache Hive Data Using Cloudera Enterprise BDR
49,How To Back Up and Restore HDFS Data Using Cloudera Enterprise BDR
49,BDR Automation Examples
49,Migrating Data between Clusters Using distcp
49,Copying Cluster Data Using DistCp
49,Copying Data between a Secure and an Insecure Cluster using DistCp and WebHDFS
49,Post-migration Verification
49,Backing Up Databases
49,Cloudera Navigator Administration
49,Accessing Storage Using Amazon S3
49,Configuring the Amazon S3 Connector
49,"Using S3 Credentials with YARN, MapReduce, or Spark"
49,Using Fast Upload with Amazon S3
49,Configuring and Managing S3Guard
49,How to Configure a MapReduce Job to Access S3 with an HDFS Credstore
49,Importing Data into Amazon S3 Using Sqoop
49,Accessing Storage Using Microsoft ADLS
49,Configuring ADLS Access Using Cloudera Manager
49,Configuring ADLS Gen1 Connectivity
49,Configuring ADLS Gen2 Connectivity
49,Importing Data into Microsoft Azure Data Lake Store Using Sqoop
49,Configuring Google Cloud Storage Connectivity
49,How To Create a Multitenant Enterprise Data Hub
49,Security
49,Overview
49,Authentication Overview
49,Encryption Overview
49,Encryption Mechanisms
49,Authorization Overview
49,Auditing and Data Governance
49,Authentication
49,Kerberos Security Artifacts Overview
49,Configuring Authentication in Cloudera Manager
49,Cloudera Manager User Accounts
49,Configuring External Authentication and Authorization for Cloudera Manager
49,Enabling Kerberos Authentication for CDH
49,Step 1: Install Cloudera Manager and CDH
49,Step 2: Install JCE Policy Files for AES-256 Encryption
49,Step 3: Create the Kerberos Principal for Cloudera Manager Server
49,Step 4: Enabling Kerberos Using the Wizard
49,Step 5: Create the HDFS Superuser
49,Step 6: Get or Create a Kerberos Principal for Each User Account
49,Step 7: Prepare the Cluster for Each User
49,Step 8: Verify that Kerberos Security is Working
49,Step 9: (Optional) Enable Authentication for HTTP Web Consoles for Hadoop Roles
49,Kerberos Authentication for Non-Default Users
49,Customizing Kerberos Principals
49,Managing Kerberos Credentials Using Cloudera Manager
49,Using a Custom Kerberos Keytab Retrieval Script
49,Adding Trusted Realms to the Cluster
49,Using Auth-to-Local Rules to Isolate Cluster Users
49,Configuring Authentication for Cloudera Navigator
49,Cloudera Navigator and External Authentication
49,Configuring Cloudera Navigator for Active Directory
49,Configuring Cloudera Navigator for LDAP
49,Configuring Cloudera Navigator for SAML
49,Configuring Groups for Cloudera Navigator
49,Configuring Authentication for Other Components
49,Flume Authentication
49,Configuring Kerberos for Flume Thrift Source and Sink Using Cloudera Manager
49,Writing to a Secure HBase Cluster
49,Using Substitution Variables with Flume for Kerberos Artifacts
49,HBase Authentication
49,Configuring Kerberos Authentication for HBase
49,Configuring Secure HBase Replication
49,Configuring the HBase Client TGT Renewal Period
49,Hive Authentication
49,HiveServer2 Security Configuration
49,Using Hive to Run Queries on a Secure HBase Server
49,HttpFS Authentication
49,Hue Authentication
49,Enable Hue to Use Kerberos for Authentication
49,Impala Authentication
49,Enabling Kerberos Authentication for Impala
49,Enabling LDAP Authentication for Impala
49,Using Multiple Authentication Methods with Impala
49,Configuring Impala Delegation for Hue and BI Tools
49,Cloudera Search Authentication
49,Using Kerberos with Cloudera Search
49,Spark Authentication
49,Sqoop1 Authentication
49,ZooKeeper Authentication
49,Configuring a Dedicated MIT KDC for Cross-Realm Trust
49,Integrating MIT Kerberos and Active Directory
49,Hadoop Users (user:group) and Kerberos Principals
49,Mapping Kerberos Principals to Short Names
49,Authorization
49,Cloudera Manager User Roles
49,HDFS Extended ACLs
49,Authorization for HDFS Web UIs
49,Configuring LDAP Group Mappings
49,Authorization With Apache Sentry
49,Configuring HBase Authorization
49,Encrypting Data in Transit
49,Understanding Keystores and Truststores
49,Configuring TLS Encryption for Cloudera Manager and CDH Using Auto-TLS
49,Manually Configuring TLS Encryption for Cloudera Manager
49,Manually Configuring TLS Encryption on the Agent Listening Port
49,Manually Configuring TLS/SSL Encryption for CDH Services
49,"Configuring TLS/SSL for HDFS, YARN and MapReduce"
49,Configuring TLS/SSL for HBase
49,Configuring TLS/SSL for Flume
49,Configuring Encrypted Communication Between HiveServer2 and Client Drivers
49,Configuring TLS/SSL for Hue
49,Configuring TLS/SSL for Impala
49,Configuring TLS/SSL for Oozie
49,Configuring TLS/SSL for Solr
49,Spark Encryption
49,Configuring TLS/SSL for HttpFS
49,Configuring TLS/SSL for Navigator Audit Server
49,Configuring TLS/SSL for Navigator Metadata Server
49,Configuring TLS/SSL for Kafka (Navigator Event Broker)
49,Configuring Encrypted Transport for HDFS
49,Configuring Encrypted Transport for HBase
49,Encrypting Data at Rest
49,Data at Rest Encryption Reference Architecture
49,Data at Rest Encryption Requirements
49,Resource Planning for Data at Rest Encryption
49,HDFS Transparent Encryption
49,Optimizing Performance for HDFS Transparent Encryption
49,Enabling HDFS Encryption Using the Wizard
49,Managing Encryption Keys and Zones
49,Configuring the Key Management Server (KMS)
49,Securing the Key Management Server (KMS)
49,Configuring KMS Access Control Lists (ACLs)
49,Migrating from a Key Trustee KMS to an HSM KMS
49,Migrating Keys from a Java KeyStore to Cloudera Navigator Key Trustee Server
49,Migrating a Key Trustee KMS Server Role Instance to a New Host
49,Configuring CDH Services for HDFS Encryption
49,Cloudera Navigator Key Trustee Server
49,Backing Up and Restoring Key Trustee Server and Clients
49,Initializing Standalone Key Trustee Server
49,Configuring a Mail Transfer Agent for Key Trustee Server
49,Verifying Cloudera Navigator Key Trustee Server Operations
49,Managing Key Trustee Server Organizations
49,Managing Key Trustee Server Certificates
49,Cloudera Navigator Key HSM
49,Initializing Navigator Key HSM
49,HSM-Specific Setup for Cloudera Navigator Key HSM
49,Validating Key HSM Settings
49,Managing the Navigator Key HSM Service
49,Integrating Key HSM with Key Trustee Server
49,Cloudera Navigator Encrypt
49,Registering Cloudera Navigator Encrypt with Key Trustee Server
49,Preparing for Encryption Using Cloudera Navigator Encrypt
49,Encrypting and Decrypting Data Using Cloudera Navigator Encrypt
49,Converting from Device Names to UUIDs for Encrypted Devices
49,Navigator Encrypt Access Control List
49,Maintaining Cloudera Navigator Encrypt
49,Configuring Encryption for Data Spills
49,Configuring Encrypted On-disk File Channels for Flume
49,Impala Security Overview
49,Security Guidelines for Impala
49,Securing Impala Data and Log Files
49,Installation Considerations for Impala Security
49,Securing the Hive Metastore Database
49,Securing the Impala Web User Interface
49,Kudu Security Overview
49,How-To Guides
49,Add Root and Intermediate CAs to Truststore for TLS/SSL
49,Amazon S3 Security
49,Authenticate Kerberos Principals Using Java
49,Check Cluster Security Settings
49,Configure Antivirus Software on CDH Hosts
49,Configure Browser-based Interfaces to Require Authentication (SPNEGO)
49,Configure Browsers for Kerberos Authentication (SPNEGO)
49,Configure Cluster to Use Kerberos Authentication
49,"Convert DER, JKS, PEM Files for TLS/SSL Artifacts"
49,Configure Authentication for Amazon S3
49,Configure Encryption for Amazon S3
49,Configure AWS Credentials
49,Enable Sensitive Data Redaction
49,Log a Security Support Case
49,Obtain and Deploy Keys and Certificates for TLS/SSL
49,Renew and Redistribute Certificates
49,Set Up a Gateway Host to Restrict Access to the Cluster
49,Set Up Access to Cloudera EDH or Altus Director (Microsoft Azure Marketplace)
49,Use Self-Signed Certificates for TLS
49,Troubleshooting Security Issues
49,Error Messages
49,Authentication and Kerberos Issues
49,HDFS Encryption Issues
49,Key Trustee KMS Encryption Issues
49,TLS/SSL Issues
49,"YARN, MRv1, and Linux OS Security"
49,TaskController Error Codes (MRv1)
49,ContainerExecutor Error Codes (YARN)
49,Cloudera Navigator Data Management
49,Overview
49,Search
49,Performing Actions on Entities
49,Auditing
49,Using Audit Events to Understand Cluster Activity
49,Exploring Audit Data
49,Cloudera Navigator Audit Event Reports
49,Analytics
49,Policies
49,Lineage
49,Using the Lineage View
49,Using Lineage to Display Table Schema
49,Generating Lineage Diagrams
49,Business Metadata
49,Defining Managed Properties
49,Adding and Editing Metadata
49,Administration (Navigator Console)
49,Managing Metadata Storage with Purge
49,Administering Navigator User Roles
49,Navigator Configuration and Management
49,Accessing Navigator Data Management Logs
49,Backing Up Cloudera Navigator Data
49,Authentication and Authorization
49,Configuring Cloudera Navigator to work with Hue HA
49,Cloudera Navigator support for Virtual Private Clusters
49,Encryption (TLS/SSL) and Cloudera Navigator
49,Limiting Sensitive Data in Navigator Logs
49,Preventing Concurrent Logins from the Same User
49,Navigator Audit Server Management
49,Setting Up Navigator Audit Server
49,Enabling Audit and Log Collection for Services
49,Configuring Service Auditing Properties
49,Adding Audit Filters
49,Monitoring Navigator Audit Service Health
49,Publishing Audit Events
49,Maintaining Navigator Audit Server
49,Navigator Metadata Server Management
49,Setting Up Navigator Metadata Server
49,Navigator Metadata Server Tuning
49,Configuring and Managing Extraction
49,Hive and Impala Lineage Configuration
49,Configuring the Server for Policy Messages
49,Cloudera Navigator and the Cloud
49,Using Cloudera Navigator with Altus Clusters
49,Configuring Extraction for Altus Clusters on AWS
49,Using Cloudera Navigator with Amazon S3
49,Configuring Extraction for Amazon S3
49,Cloudera Navigator APIs
49,Navigator APIs Overview
49,Applying Metadata to HDFS and Hive Entities using the API
49,Using the Purge APIs for Metadata Maintenance Tasks
49,Cloudera Navigator Reference
49,Lineage Diagram Icons
49,Search Syntax and Properties
49,Service Audit Events
49,Service Metadata Entity Types
49,Metadata Policy Expressions
49,User Roles and Privileges Reference
49,Troubleshooting Navigator Data Management
49,CDH Component Guides
49,Crunch
49,Flume
49,Configuring
49,Configuring the Flume Properties File
49,Files Installed by the Flume RPM and Debian Packages
49,Configuring Flume Security with Kafka
49,Using & Managing
49,Running Flume
49,"Supported Sources, Sinks, and Channels"
49,Flume Kudu Sink
49,Viewing the Flume Documentation
49,HBase
49,Configuring
49,Accessing HBase by using the HBase Shell
49,HBase Online Merge
49,Using MapReduce with HBase
49,Configuring HBase Garbage Collection
49,Configuring the HBase Canary
49,Configuring the Blocksize for HBase
49,Configuring the HBase BlockCache
49,Configuring Quotas
49,Configuring the HBase Scanner Heartbeat
49,Limiting the Speed of Compactions
49,Configuring and Using the HBase REST API
49,Configuring HBase MultiWAL Support
49,Storing Medium Objects (MOBs) in HBase
49,Configuring the Storage Policy for the Write-Ahead Log (WAL)
49,Using & Managing
49,Starting and Stopping HBase
49,Accessing HBase by using the HBase Shell
49,Using HBase Command-Line Utilities
49,Using the HBCK2 Tool to Remediate HBase Clusters
49,Hedged Reads
49,Reading Data from HBase
49,HBase Filtering
49,Writing Data to HBase
49,Importing Data Into HBase
49,Exposing HBase Metrics to a Ganglia Server
49,Using HashTable and SyncTable Tool
49,Security
49,Troubleshooting
49,Hive
49,Installation and Upgrade
49,Configuring
49,Configuring HiveServer2
49,File System Permissions
49,"Starting, Stopping, & Using HS2"
49,Using Hive w/HBase
49,Installing JDBC/ODBC Drivers
49,Setting HADOOP_MAPRED_HOME
49,Using & Managing
49,Managing Hive with Cloudera Manager
49,Ingesting & Querying Data
49,Using Parquet Tables
49,Running Hive on Spark
49,Using HS2 Web UI
49,Using Query Plan Graph View
49,Accessing Table Statistics
49,Managing UDFs
49,Hive ETL Jobs on S3
49,Hive with ADLS
49,Erasure Coding with Hive
49,Removing the Hive Compilation Lock
49,Sqoop HS2 Import
49,Tuning
49,Tuning Hive on Spark
49,Tuning Hive on S3
49,Configuring HS2 HA
49,Enabling Query Vectorization
49,Hive Metastore (HMS)
49,Configuring
49,Configuring HMS
49,Configuring HMS HA
49,Configuring HMS for HDFS HA
49,Configuring Shared Amazon RDS as HMS
49,Using & Managing
49,Starting the Metastore
49,Using Metastore Schema Tool
49,Data Replication
49,Security
49,HCatalog
49,HCatalog Prerequisites
49,Configuration Change on Hosts Used with HCatalog
49,Accessing Table Information with the HCatalog Command-line API
49,Accessing Table Data with MapReduce
49,Accessing Table Data with Pig
49,Accessing Table Information with REST
49,Viewing the HCatalog Documentation
49,Troubleshooting
49,Hue
49,Hue Versions
49,Reference Architecture
49,Installation & Upgrade
49,Using
49,Enable SQL Editor Autocompleter
49,Use Governance-Based Data Discovery
49,Use S3 as Source or Sink in Hue
49,Administration
49,Configuring
49,Customize Hue Web UI
49,Enable Governance-Based Data Discovery
49,Enable S3 Cloud Storage
49,Run Shell Commands
49,Connecting a Database
49,Connect to MySQL or MariaDB
49,Connect to PostgreSQL
49,Connect to Oracle (Parcel)
49,Connect to Oracle (Package)
49,Custom Database Tutorial
49,Migrate the Database
49,Populate the Database
49,Performance Tuning
49,Add Load Balancer
49,Configure High Availability
49,Hue/HDFS High Availability
49,Security
49,User Permissions
49,Create Password Scripts
49,Authenticate Users with LDAP
49,Synchronize with LDAP Server
49,Authenticate Users with SAML
49,Authorize Groups with Sentry
49,Troubleshooting
49,Potential Misconfiguration
49,Unable to connect to database with provided credential
49,Unable to view Snappy-compressed files
49,“Unknown Attribute Name” exception while enabling SAML
49,Invalid query handle
49,Services backed by Postgres fail or hang
49,Downloading query results from Hue takes long time
49,Bad status: 3 (PLAIN auth failed: Error validating LDAP user)
49,502 Proxy Error while accessing Hue from the Load Balancer
49,Hue Load Balancer does not start after enabling TLS
49,Impala
49,Concepts and Architecture
49,Components
49,Developing Applications
49,Role in the Hadoop Ecosystem
49,Deployment Planning
49,Impala Requirements
49,Designing Schemas
49,Tutorials
49,Administration
49,Setting Timeouts
49,Load-Balancing Proxy for HA
49,Managing Disk Space
49,Auditing
49,Viewing Lineage Info
49,SQL Reference
49,Comments
49,Data Types
49,ARRAY Complex Type (CDH 5.5 or higher only)
49,BIGINT
49,BOOLEAN
49,CHAR
49,DECIMAL
49,DOUBLE
49,FLOAT
49,INT
49,MAP Complex Type (CDH 5.5 or higher only)
49,REAL
49,SMALLINT
49,STRING
49,STRUCT Complex Type (CDH 5.5 or higher only)
49,TIMESTAMP
49,Customizing Time Zones
49,TINYINT
49,VARCHAR
49,Complex Types (CDH 5.5 or higher only)
49,Literals
49,SQL Operators
49,Schema Objects and Object Names
49,Aliases
49,Databases
49,Functions
49,Identifiers
49,Tables
49,Views
49,SQL Statements
49,DDL Statements
49,DML Statements
49,ALTER DATABASE
49,ALTER TABLE
49,ALTER VIEW
49,COMMENT
49,COMPUTE STATS
49,CREATE DATABASE
49,CREATE FUNCTION
49,CREATE ROLE
49,CREATE TABLE
49,CREATE VIEW
49,DELETE
49,DESCRIBE
49,DROP DATABASE
49,DROP FUNCTION
49,DROP ROLE
49,DROP STATS
49,DROP TABLE
49,DROP VIEW
49,EXPLAIN
49,GRANT
49,INSERT
49,INVALIDATE METADATA
49,LOAD DATA
49,REFRESH
49,REFRESH AUTHORIZATION
49,REFRESH FUNCTIONS
49,REVOKE
49,SELECT
49,Joins
49,ORDER BY Clause
49,GROUP BY Clause
49,HAVING Clause
49,LIMIT Clause
49,OFFSET Clause
49,UNION Clause
49,Subqueries
49,TABLESAMPLE Clause
49,WITH Clause
49,DISTINCT Operator
49,SET
49,Query Options for the SET Statement
49,ABORT_ON_ERROR
49,ALLOW_ERASURE_CODED_FILES
49,ALLOW_UNSUPPORTED_FORMATS
49,APPX_COUNT_DISTINCT
49,BATCH_SIZE
49,BUFFER_POOL_LIMIT
49,COMPRESSION_CODEC
49,COMPUTE_STATS_MIN_SAMPLE_SIZE
49,DEBUG_ACTION
49,DECIMAL_V2
49,DEFAULT_JOIN_DISTRIBUTION_MODE
49,DEFAULT_SPILLABLE_BUFFER_SIZE
49,DISABLE_CODEGEN
49,DISABLE_CODEGEN_ROWS_THRESHOLD
49,DISABLE_ROW_RUNTIME_FILTERING
49,DISABLE_STREAMING_PREAGGREGATIONS
49,DISABLE_UNSAFE_SPILLS
49,ENABLE_EXPR_REWRITES
49,EXEC_SINGLE_NODE_ROWS_THRESHOLD
49,EXEC_TIME_LIMIT_S
49,EXPLAIN_LEVEL
49,HBASE_CACHE_BLOCKS
49,HBASE_CACHING
49,IDLE_SESSION_TIMEOUT
49,KUDU_READ_MODE
49,LIVE_PROGRESS
49,LIVE_SUMMARY
49,MAX_ERRORS
49,MAX_MEM_ESTIMATE_FOR_ADMISSION
49,MAX_NUM_RUNTIME_FILTERS
49,MAX_ROW_SIZE
49,MAX_SCAN_RANGE_LENGTH
49,MEM_LIMIT
49,MIN_SPILLABLE_BUFFER_SIZE
49,MT_DOP
49,NUM_NODES
49,NUM_ROWS_PRODUCED_LIMIT
49,NUM_SCANNER_THREADS
49,OPTIMIZE_PARTITION_KEY_SCANS
49,PARQUET_COMPRESSION_CODEC
49,PARQUET_ANNOTATE_STRINGS_UTF8
49,PARQUET_ARRAY_RESOLUTION
49,PARQUET_DICTIONARY_FILTERING
49,PARQUET_FALLBACK_SCHEMA_RESOLUTION
49,PARQUET_FILE_SIZE
49,PARQUET_READ_STATISTICS
49,PREFETCH_MODE
49,QUERY_TIMEOUT_S
49,REPLICA_PREFERENCE
49,REQUEST_POOL
49,RESOURCE_TRACE_RATIO
49,RUNTIME_BLOOM_FILTER_SIZE
49,RUNTIME_FILTER_MAX_SIZE
49,RUNTIME_FILTER_MIN_SIZE
49,RUNTIME_FILTER_MODE
49,RUNTIME_FILTER_WAIT_TIME_MS
49,S3_SKIP_INSERT_STAGING
49,SCAN_BYTES_LIMIT
49,SCHEDULE_RANDOM_REPLICA
49,SCRATCH_LIMIT
49,SHUFFLE_DISTINCT_EXPRS
49,SUPPORT_START_OVER
49,SYNC_DDL
49,THREAD_RESERVATION_AGGREGATE_LIMIT
49,THREAD_RESERVATION_LIMIT
49,TIMEZONE
49,TOPN_BYTES_LIMIT
49,SHOW
49,SHUTDOWN
49,TRUNCATE TABLE
49,UPDATE
49,UPSERT
49,USE
49,VALUES
49,Optimizer Hints
49,Built-In Functions
49,Mathematical Functions
49,Bit Functions
49,Type Conversion Functions
49,Date and Time Functions
49,Conditional Functions
49,String Functions
49,Miscellaneous Functions
49,Aggregate Functions
49,APPX_MEDIAN
49,AVG
49,COUNT
49,GROUP_CONCAT
49,MAX
49,MIN
49,NDV
49,"STDDEV, STDDEV_SAMP, STDDEV_POP"
49,SUM
49,"VARIANCE, VARIANCE_SAMP, VARIANCE_POP, VAR_SAMP, VAR_POP"
49,Analytic Functions
49,User-Defined Functions (UDFs)
49,SQL Differences Between Impala and Hive
49,Porting SQL
49,Resource Management
49,Admission Control and Query Queuing
49,Configuring Resource Pools and Admission Control
49,Admission Control Sample Scenario
49,Performance Tuning
49,Performance Best Practices
49,Join Performance
49,Table and Column Statistics
49,Benchmarking
49,Controlling Resource Usage
49,Runtime Filtering
49,HDFS Caching
49,HDFS Block Skew
49,Data Cache for Remote Reads
49,Testing Impala Performance
49,EXPLAIN Plans and Query Profiles
49,Scalability Considerations
49,Scaling Limits and Guidelines
49,Dedicated Coordinators
49,Metadata Management
49,Partitioning
49,File Formats
49,Text Data Files
49,Parquet Data Files
49,ORC Data Files
49,Avro Data Files
49,RCFile Data Files
49,SequenceFile Data Files
49,Using Impala to Query Kudu Tables
49,HBase Tables
49,S3 Tables
49,Configure with Cloudera Manager
49,Configure from Command Line
49,ADLS Tables
49,Logging
49,Impala Client Access
49,The Impala Shell
49,Configuration Options
49,Connecting to impalad
49,Running Commands and SQL Statements
49,Command Reference
49,Configuring Impala to Work with ODBC
49,Configuring Impala to Work with JDBC
49,Troubleshooting Impala
49,Web User Interface
49,Breakpad Minidumps
49,Ports Used by Impala
49,Impala Reserved Words
49,Impala Frequently Asked Questions
49,Kafka
49,Setup
49,Cloudera Manager
49,Clients
49,Brokers
49,Integration
49,Security
49,Managing Multiple Kafka Versions
49,Managing Topics across Multiple Kafka Clusters
49,Setting up an End-to-End Data Streaming Pipeline
49,Developing Kafka Clients
49,Metrics
49,Administration
49,Administration Basics
49,Broker Migration
49,User Limits for Kafka
49,Quotas
49,Kafka Command Line Tools
49,Disk Management
49,JBOD
49,Setup and Migration
49,Delegation Tokens
49,Enable Delegation Tokens
49,Managing Individual Delegation Tokens
49,Rotating the Master Key/Secret
49,Client Authentication
49,Kafka Security Hardening with Zookeeper ACLs
49,Kafka Streams
49,Performance Tuning
49,Handling Large Messages
49,Cluster Sizing
49,Broker Configuration
49,System-Level Broker Tuning
49,Kafka-ZooKeeper Performance Tuning
49,Reference
49,Metrics Reference
49,Useful Shell Command Reference
49,Kafka Public APIs
49,FAQ
49,Kudu
49,Concepts and Architecture
49,Usage Limitations
49,Installation and Upgrade
49,Configuration
49,Administration
49,Developing Applications with Kudu
49,Using Apache Impala with Kudu
49,Using the Hive Metastore with Kudu
49,Schema Design
49,Transaction Semantics
49,Background Tasks
49,Scaling Guide
49,Troubleshooting
49,More Resources
49,Oozie
49,Configuration
49,Configuring an External Database for Oozie
49,Oozie High Availability
49,Configuring Oozie to Use HDFS HA
49,Oozie Authentication
49,Using Sqoop Actions with Oozie
49,Configuring Oozie to Enable MapReduce Jobs To Read/Write from Amazon S3
49,Configuring Oozie to Enable MapReduce Jobs To Read/Write from Microsoft Azure (ADLS)
49,Oozie
49,"Starting, Stopping, and Accessing the Oozie Server"
49,Adding the Oozie Service Using Cloudera Manager
49,Redeploying the Oozie ShareLib
49,Configuring Oozie Data Purge Settings Using Cloudera Manager
49,Dumping and Loading an Oozie Database Using Cloudera Manager
49,Adding Schema to Oozie Using Cloudera Manager
49,Enabling the Oozie Web Console on Managed Clusters
49,Enabling Oozie SLA with Cloudera Manager
49,Setting the Oozie Database Timezone
49,Scheduling in Oozie Using Cron-like Syntax
49,Phoenix
49,Release Notes
49,Prerequisites
49,Installing Apache Phoenix using Cloudera Manager
49,Using Apache Phoenix to Store and Access Data
49,Orchestrating SQL and APIs with Apache Phoenix
49,Configuring Phoenix Query Server
49,Connecting to PQS
49,Creating and Using User-Defined Functions (UDFs) in Phoenix
49,Mapping Phoenix Schemas to HBase Namespaces
49,Associating Tables of a Schema to a Namespace
49,Using Phoenix Client to Load Data
49,Using the Index in Phoenix
49,Understanding Apache Phoenix-Spark Connector
49,Understanding Apache Phoenix-Hive Connector
49,Performance Tuning
49,Frequently Asked Questions
49,Uninstalling Phoenix Parcel
49,Search
49,Search
49,Understanding
49,Search and Other CDH Components
49,Architecture
49,Tasks and Processes
49,Tutorial
49,Validating Search Deployment
49,Preparing to Index Sample Tweets
49,Using MapReduce Batch Indexing to Index Sample Tweets
49,Near Real Time (NRT) Indexing Tweets Using Flume
49,Using Hue with Search
49,Deployment Planning
49,Schemaless Mode
49,Deploying
49,Using Search through a Proxy for High Availability
49,Using Custom JAR Files with Search
49,Cloudera Search Security
49,Enable Kerberos Authentication in Cloudera Search
49,Managing
49,Configuration
49,Collections
49,solrctl Reference
49,Example solrctl Usage
49,Migrating Solr Replicas
49,Backing Up and Restoring
49,ETL with Cloudera Morphlines
49,Example Morphline Usage
49,Indexing Data
49,Near Real Time Indexing
49,Flume NRT Indexing
49,Flume MorphlineSolrSink Configuration Options
49,Flume MorphlineInterceptor Configuration Options
49,Flume Solr UUIDInterceptor Configuration Options
49,Flume Solr BlobHandler Configuration Options
49,Flume Solr BlobDeserializer Configuration Options
49,Lily HBase NRT Indexing
49,Using the Lily HBase NRT Indexer Service
49,Configuring Lily HBase Indexer Security
49,Batch Indexing
49,Spark Indexing
49,MapReduce Indexing
49,MapReduceIndexerTool
49,Lily HBase Batch Indexing
49,FAQ
49,Troubleshooting
49,Configuration and Log Files
49,Identifying Problems
49,Solr Query Returns no Documents when Executed with a Non-Privileged User
49,Sentry
49,Before You Install Sentry
49,Installing and Upgrading the Sentry Service
49,Configuring
49,Sentry High Availability
49,Enabling Sentry Authorization for Impala
49,Configuring Sentry Authorization for Cloudera Search
49,Using & Managing
49,Synchronizing HDFS ACLs and Sentry Permissions
49,Authorization Privilege Model for Hive and Impala
49,Authorization Privilege Model for Cloudera Search
49,Hive SQL Syntax for Use with Sentry
49,Object Ownership
49,Using the Sentry Web Server
49,Sentry Debugging and Failure Scenarios
49,Troubleshooting
49,How-To Guides
49,Enabling High Availability
49,Verify HDFS ACL Sync
49,Managing Table Access in Hue
49,Spark
49,Running Your First Spark Application
49,Troubleshooting for Spark
49,Frequently Asked Questions about Apache Spark in CDH
49,Spark Application Overview
49,Developing Spark Applications
49,Developing and Running a Spark WordCount Application
49,Using Spark Streaming
49,Using Spark SQL
49,Using Spark MLlib
49,Accessing External Storage
49,Accessing Data Stored in Amazon S3 through Spark
49,Accessing Data Stored in Azure Data Lake Store (ADLS) through Spark
49,Accessing Avro Data Files From Spark SQL Applications
49,Accessing Parquet Files From Spark SQL Applications
49,Building Spark Applications
49,Configuring Spark Applications
49,Running Spark Applications
49,Running Spark Applications on YARN
49,Using PySpark
49,Running Spark Python Applications
49,Spark and IPython and Jupyter Notebooks
49,Tuning Spark Applications
49,Spark and Hadoop Integration
49,Building and Running a Crunch Application with Spark
49,File Formats and Compression
49,Parquet
49,Predicate Pushdown in Parquet
49,Avro
49,Data Compression
49,Snappy Compression
49,Glossary
49,"To read this documentation, you must turn JavaScript on."
49,Optimizer Hints in Impala
49,"The Impala SQL dialect supports query hints, for fine-tuning the inner workings of queries. Specify hints as a temporary workaround for expensive queries, where missing statistics or"
49,other factors cause inefficient performance.
49,Hints are most often used for the most resource-intensive kinds of Impala queries:
49,"Join queries involving large tables, where intermediate result sets are transmitted across the network to evaluate the join conditions."
49,"Inserting into partitioned Parquet tables, where many memory buffers could be allocated on each host to hold intermediate results for each partition."
49,Syntax:
49,"In CDH 5.2 / Impala 2.0 and higher, you can specify the hints inside comments that use either the /* */ or -- notation. Specify a + symbol immediately before the hint name. Recently added hints are only available"
49,"using the /* */ and -- notation. For clarity, the /* */ and -- styles are"
49,"used in the syntax and examples throughout this section. With the /* */ or -- notation for hints, specify a + symbol immediately before the first hint name. Multiple hints can be specified separated by commas, for example /* +clustered,shuffle */"
49,SELECT STRAIGHT_JOIN select_list FROM
49,join_left_hand_table
49,JOIN /* +BROADCAST|SHUFFLE */
49,join_right_hand_table
49,remainder_of_query;
49,SELECT select_list FROM
49,join_left_hand_table
49,JOIN -- +BROADCAST|SHUFFLE
49,join_right_hand_table
49,remainder_of_query;
49,INSERT insert_clauses
49,/* +SHUFFLE|NOSHUFFLE */
49,SELECT remainder_of_query;
49,INSERT insert_clauses
49,-- +SHUFFLE|NOSHUFFLE
49,SELECT remainder_of_query;
49,INSERT /* +SHUFFLE|NOSHUFFLE */
49,insert_clauses
49,SELECT remainder_of_query;
49,INSERT -- +SHUFFLE|NOSHUFFLE
49,insert_clauses
49,SELECT remainder_of_query;
49,UPSERT /* +SHUFFLE|NOSHUFFLE */
49,upsert_clauses
49,SELECT remainder_of_query;
49,UPSERT -- +SHUFFLE|NOSHUFFLE
49,upsert_clauses
49,SELECT remainder_of_query;
49,SELECT select_list FROM
49,table_ref
49,/* +{SCHEDULE_CACHE_LOCAL | SCHEDULE_DISK_LOCAL | SCHEDULE_REMOTE}
49,"[,RANDOM_REPLICA] */"
49,remainder_of_query;
49,INSERT insert_clauses
49,-- +CLUSTERED
49,SELECT remainder_of_query;
49,INSERT insert_clauses
49,/* +CLUSTERED */
49,SELECT remainder_of_query;
49,INSERT -- +CLUSTERED
49,insert_clauses
49,SELECT remainder_of_query;
49,INSERT /* +CLUSTERED */
49,insert_clauses
49,SELECT remainder_of_query;
49,UPSERT -- +CLUSTERED
49,upsert_clauses
49,SELECT remainder_of_query;
49,UPSERT /* +CLUSTERED */
49,upsert_clauses
49,SELECT remainder_of_query;
49,CREATE /* +SHUFFLE|NOSHUFFLE */
49,table_clauses
49,AS SELECT remainder_of_query;
49,CREATE -- +SHUFFLE|NOSHUFFLE
49,table_clauses
49,AS SELECT remainder_of_query;
49,CREATE /* +CLUSTERED|NOCLUSTERED */
49,table_clauses
49,AS SELECT remainder_of_query;
49,CREATE -- +CLUSTERED|NOCLUSTERED
49,table_clauses
49,AS SELECT remainder_of_query;
49,"The square bracket style hints are supported for backward compatibility, but the syntax is deprecated and will be removed in a future release. For that reason, any newly added hints are"
49,not available with the square bracket syntax.
49,SELECT STRAIGHT_JOIN select_list FROM
49,join_left_hand_table
49,JOIN [{ /* +BROADCAST */ | /* +SHUFFLE */ }]
49,join_right_hand_table
49,remainder_of_query;
49,INSERT insert_clauses
49,[{ /* +SHUFFLE */ | /* +NOSHUFFLE */ }]
49,[/* +CLUSTERED */]
49,SELECT remainder_of_query;
49,UPSERT [{ /* +SHUFFLE */ | /* +NOSHUFFLE */ }]
49,[/* +CLUSTERED */]
49,upsert_clauses
49,SELECT remainder_of_query;
49,Usage notes:
49,"With both forms of hint syntax, include the STRAIGHT_JOIN keyword immediately after the SELECT and any DISTINCT or ALL keywords to prevent Impala from reordering the tables in a way that makes the join-related hints ineffective."
49,The STRAIGHT_JOIN hint affects the join order of table references in the query block containing the hint. It does not affect the join
49,"order of nested queries, such as views, inline views, or WHERE-clause subqueries. To use this hint for performance tuning of complex queries, apply the hint to all"
49,query blocks that need a fixed join order.
49,"To reduce the need to use hints, run the COMPUTE STATS statement against all tables involved in joins, or used as the source tables for INSERT ... SELECT operations where the destination is a partitioned Parquet table. Do this operation after loading data or making substantial changes to the data within each table."
49,Having up-to-date statistics helps Impala choose more efficient query plans without the need for hinting. See Table and Column Statistics
49,for details and examples.
49,"To see which join strategy is used for a particular query, examine the EXPLAIN output for that query. See Using the EXPLAIN Plan for Performance Tuning for details and examples."
49,Hints for join queries:
49,The /* +BROADCAST */ and /* +SHUFFLE */ hints control the execution strategy for join queries. Specify one of the following
49,constructs immediately after the JOIN keyword in a query:
49,"/* +SHUFFLE */ makes that join operation use the ""partitioned"" technique, which divides up corresponding rows from both tables"
49,"using a hashing algorithm, sending subsets of the rows to other nodes for processing. (The keyword SHUFFLE is used to indicate a ""partitioned"
49,"join"", because that type of join is not related to ""partitioned tables"".) Since the alternative ""broadcast"" join mechanism is the default when"
49,"table and index statistics are unavailable, you might use this hint for queries where broadcast joins are unsuitable; typically, partitioned joins are more efficient for joins between large tables of"
49,similar size.
49,"/* +BROADCAST */ makes that join operation use the ""broadcast"" technique that sends the entire contents of the right-hand table to"
49,"all nodes involved in processing the join. This is the default mode of operation when table and index statistics are unavailable, so you would typically only need it if stale metadata caused Impala"
49,"to mistakenly choose a partitioned join operation. Typically, broadcast joins are more efficient in cases where one table is much smaller than the other. (Put the smaller table on the right side of"
49,the JOIN operator.)
49,Hints for INSERT ... SELECT and CREATE TABLE AS SELECT (CTAS):
49,"When inserting into partitioned tables, such as using the Parquet file format, you can include a hint in the INSERT or"
49,CREATE TABLE AS SELECT(CTAS) statements to fine-tune the overall performance of the operation and its resource usage.
49,"You would only use hints if an INSERT or CTAS into a partitioned table was failing due to capacity limits, or if such an"
49,operation was succeeding but with less-than-optimal performance.
49,/* +SHUFFLE */ and /* +NOSHUFFLE */ Hints
49,"/* +SHUFFLE */ adds an exchange node, before writing the data, which re-partitions the result of the SELECT based on the"
49,"partitioning columns of the target table. With this hint, only one node writes to a partition at a time, minimizing the global number of simultaneous writes and the number of memory buffers holding"
49,"data for individual partitions. This also reduces fragmentation, resulting in fewer files. Thus it reduces overall resource usage of the INSERT or CTAS operation and allows some operations to succeed that otherwise would fail. It does involve some data transfer between the nodes so that the data files for a particular"
49,partition are all written on the same node.
49,Use /* +SHUFFLE */ in cases where an INSERT or CTAS statement fails or runs inefficiently due
49,to all nodes attempting to write data for all partitions.
49,"If the table is unpartitioned or every partitioning expression is constant, then /* +SHUFFLE */ will cause every write to happen on the coordinator"
49,node.
49,/* +NOSHUFFLE */ does not add exchange node before inserting to partitioned tables and disables re-partitioning. So the selected execution plan might be
49,"faster overall, but might also produce a larger number of small data files or exceed capacity limits, causing the INSERT or CTAS"
49,operation to fail.
49,"Impala automatically uses the /* +SHUFFLE */ method if any partition key column in the source table, mentioned in the SELECT"
49,"clause, does not have column statistics. In this case, use the /* +NOSHUFFLE */ hint if you want to override this default behavior."
49,"If column statistics are available for all partition key columns in the source table mentioned in the INSERT ... SELECT or CTAS query, Impala chooses whether to use the /* +SHUFFLE */ or /* +NOSHUFFLE */ technique based on the estimated"
49,"number of distinct values in those columns and the number of nodes involved in the operation. In this case, you might need the /* +SHUFFLE */ or the /* +NOSHUFFLE */ hint to override the execution plan selected by Impala."
49,/* +CLUSTERED */ and /* +NOCLUSTERED */ Hints
49,/* +CLUSTERED */ sorts data by the partition columns before inserting to ensure that only one partition is written at a time per node. Use this hint to
49,"reduce the number of files kept open and the number of buffers kept in memory simultaneously. This technique is primarily useful for inserts into Parquet tables, where the large block size requires"
49,substantial memory to buffer data for multiple output files at once. This hint is available in CDH 5.10 / Impala 2.8 or higher.
49,"Starting in CDH 6.0 / Impala 3.0, /* +CLUSTERED */ is the default behavior for HDFS tables."
49,/* +NOCLUSTERED */ does not sort by primary key before insert. This hint is available in CDH 5.10 /
49,Impala 2.8 or higher.
49,Use this hint when inserting to Kudu tables.
49,"In the versions lower than CDH 6.0 / Impala 3.0, /* +NOCLUSTERED */ is the default in HDFS"
49,tables.
49,Kudu consideration:
49,"Starting from CDH 5.12 / Impala 2.9, the INSERT or UPSERT"
49,operations into Kudu tables automatically add an exchange and a sort node to the plan that partitions and sorts the rows according to the partitioning/primary key scheme of the target table (unless
49,"the number of rows to be inserted is small enough to trigger single node execution). Since Kudu partitions and sorts rows on write, pre-partitioning and sorting takes some of the load off of Kudu and"
49,"helps large INSERT operations to complete without timing out. However, this default behavior may slow down the end-to-end performance of the INSERT or UPSERT operations. Starting from CDH 5.13 / Impala 2.10, you can use the"
49,/* +NOCLUSTERED */ and /* +NOSHUFFLE */ hints together to disable partitioning and sorting before the rows are sent to Kudu.
49,"Additionally, since sorting may consume a large amount of memory, consider setting the MEM_LIMIT query option for those queries."
49,Hints for scheduling of scan ranges (HDFS data blocks or Kudu tablets):
49,"The hints /* +SCHEDULE_CACHE_LOCAL */, /* +SCHEDULE_DISK_LOCAL */, and /*"
49,"+SCHEDULE_REMOTE */ have the same effect as specifying the REPLICA_PREFERENCE query option with the respective option settings of CACHE_LOCAL, DISK_LOCAL, or REMOTE."
49,Specifying the replica preference as a query hint always overrides the query option setting.
49,The hint /* +RANDOM_REPLICA */ is the same as enabling the SCHEDULE_RANDOM_REPLICA query option.
49,"You can use these hints in combination by separating them with commas, for example, /* +SCHEDULE_CACHE_LOCAL,RANDOM_REPLICA */. See"
49,REPLICA_PREFERENCE Query Option (CDH 5.9 or higher only) and SCHEDULE_RANDOM_REPLICA Query Option (CDH 5.7 or higher only) for information about how these settings influence the way Impala processes HDFS
49,data blocks or Kudu tablets.
49,Specifying either the SCHEDULE_RANDOM_REPLICA query option or the corresponding RANDOM_REPLICA query
49,hint enables the random tie-breaking behavior when processing data blocks during the query.
49,Suggestions versus directives:
49,"In early Impala releases, hints were always obeyed and so acted more like directives. Once Impala gained join order optimizations, sometimes join queries were automatically reordered in"
49,"a way that made a hint irrelevant. Therefore, the hints act more like suggestions in Impala 1.2.2 and higher."
49,"To force Impala to follow the hinted execution mechanism for a join query, include the STRAIGHT_JOIN keyword in the SELECT"
49,"statement. See Overriding Join Reordering with STRAIGHT_JOIN for details. When you use this technique, Impala does not reorder the"
49,"joined tables at all, so you must be careful to arrange the join order to put the largest table (or subquery result set) first, then the smallest, second smallest, third smallest, and so on. This"
49,"ordering lets Impala do the most I/O-intensive parts of the query using local reads on the DataNodes, and then reduce the size of the intermediate result set as much as possible as each subsequent"
49,table or subquery result set is joined.
49,Restrictions:
49,"Queries that include subqueries in the WHERE clause can be rewritten internally as join queries. Currently, you cannot apply hints to the joins produced by"
49,these types of queries.
49,"Because hints can prevent queries from taking advantage of new metadata or improvements in query planning, use them only when required to work around performance issues, and be prepared"
49,"to remove them when they are no longer required, such as after a new Impala release or bug fix."
49,"In particular, the /* +BROADCAST */ and /* +SHUFFLE */ hints are expected to be needed much less frequently in Impala 1.2.2"
49,"and higher, because the join order optimization feature in combination with the COMPUTE STATS statement now automatically choose join order and join mechanism without"
49,the need to rewrite the query and add hints. See Performance Considerations for Join Queries for details.
49,Compatibility:
49,The hints embedded within -- comments are compatible with Hive queries. The hints embedded within /* */
49,"comments or [ ] square brackets are not recognized by or not compatible with Hive. For example, Hive raises an error for Impala hints within /*"
49,*/ comments because it does not recognize the Impala hint names.
49,Considerations for views:
49,"If you use a hint in the query that defines a view, the hint is preserved when you query the view. Impala internally rewrites all hints in views to use the -- comment notation, so that Hive can query such views without errors due to unrecognized hint names."
49,Examples:
49,"For example, this query joins a large customer table with a small lookup table of less than 100 rows. The right-hand table can be broadcast efficiently to all nodes involved in the join."
49,"Thus, you would use the /* +broadcast */ hint to force a broadcast join strategy:"
49,"select straight_join customer.address, state_lookup.state_name"
49,from customer join /* +broadcast */ state_lookup
49,on customer.state_id = state_lookup.state_id;
49,This query joins two large tables of unpredictable size. You might benchmark the query with both kinds of hints and find that it is more efficient to transmit portions of each table to
49,"other nodes for processing. Thus, you would use the /* +shuffle */ hint to force a partitioned join strategy:"
49,"select straight_join weather.wind_velocity, geospatial.altitude"
49,from weather join /* +shuffle */ geospatial
49,on weather.lat = geospatial.lat and weather.long = geospatial.long;
49,"For joins involving three or more tables, the hint applies to the tables on either side of that specific JOIN keyword. The STRAIGHT_JOIN keyword ensures that joins are processed in a predictable order from left to right. For example, this query joins t1 and t2 using a partitioned join, then joins that result set to t3 using a broadcast join:"
49,"select straight_join t1.name, t2.id, t3.price"
49,from t1 join /* +shuffle */ t2 join /* +broadcast */ t3
49,on t1.id = t2.id and t2.id = t3.id;
49,Related information:
49,"For more background information about join queries, see Joins in Impala SELECT Statements. For performance considerations, see"
49,Performance Considerations for Join Queries.
49,Categories: Data Analysts | Developers | Impala | Performance | Querying | SQL | Troubleshooting | All Categories
49,VALUES
49,Built-In Functions
49,About Cloudera
49,Resources
49,Contact
49,Careers
49,Press
49,Documentation
49,United States: +1 888 789 1488
49,Outside the US: +1 650 362 0488
49,"© 2021 Cloudera, Inc. All rights reserved. Apache Hadoop and associated open source project names are trademarks of the Apache Software Foundation. For a complete list of trademarks, click here."
49,"If this documentation includes code, including but not limited to, code examples, Cloudera makes this available to you under the terms of the Apache License, Version 2.0, including any required"
49,notices. A copy of the Apache License Version 2.0 can be found here.
49,Terms & Conditions  |  Privacy Policy
49,"Page generated February 13, 2021."
50,Twenty Ways To Optimize Slow MySQL for Faster Insert Rate - SEO Explorer's Blog
50,Pricing
50,API
50,Blog
50,SEO Explorer Blog:
50,English
50,עברית
50,Try us:
50,label
50,Twenty Ways To Optimize Slow MySQL for Faster Insert Rate
50,"If you’re following my blog posts, you read that I had to develop my own database because MySQL insert speed was deteriorating over the 50GB mark. That’s why I tried to optimize for faster insert rate."
50,"The application was inserting at a rate of 50,000 concurrent inserts per second, but it grew worse,  the speed of insert dropped to 6,000 concurrent inserts per second, which is well below what I needed."
50,"I was able to optimize the MySQL performance, so the sustained insert rate was kept around the 100GB mark, but that’s it."
50,"I decided to share the optimization tips I used for optimizations; it may help database administrators who want a faster insert rate into MySQL database. (Even though these tips are written for MySQL, some of them can be used for: MariaDB, Percona MySQL, Microsoft SQL Server)."
50,Will all the methods improve your insert performance?
50,Table of Contents
50,Will all the methods improve your insert performance?How to measure SQL Server performanceOptimizing MySQL InnoDB storage engineBuy a fast serverWhat is a virtual CPUUsing a Dedicated server for MySQLMySQL serverUsing SSD instead of magnetic drivesUsing RAID 5/6 for Database DurabilityTweak MySQL memory settingsinnodb_buffer_pool_sizeinnodb_buffer_pool_instancesOther Memory settingsTweak commit mechanismWhat is MySQL transactionWhat is MySQL commitMySQL inserts with a transactionChanging the commit mechanisminnodb_flush_log_at_trx_commit=1innodb_flush_log_at_trx_commit=0innodb_flush_log_at_trx_commit=2innodb_flush_log_at_timeoutUsing precalculated primary key for stringChanging the Database’s flush methodUsing file system compressionDo you need that index?Dropping the indexUsing partitions to improve MySQL insert slow ratePlacing a table on a different driveUsing MySQL bulk insertRegular insert (single row)MySQL insert multiple rows (Extended inserts)Using replace into or insert ignoreMySQL Replace intoMySQL Insert ignoreInserting from multiple threadsUsing load from file (MySQL bulk insert)Using application-level cacheAdjusting char/varchar collationUsing MySQL clusterUsing a custom engineUsing a custom solutionBonus sectionImprove select speedThread concurrencyUsing replicationUsing prepared statementsDisabling foreign keyConnection reuseNumber of available connectionsSummary
50,"Every database deployment is different, which means that some of the suggestions here can slow down your insert performance; that’s why you need to benchmark each modification to see the effect it has."
50,How to measure SQL Server performance
50,"Before we try to tweak our performance, we must know we improved the performance."
50,"Some optimizations don’t need any special tools, because the time difference will be significant."
50,"For example, when we switched between using single inserts to multiple inserts during data import, it took one task a few hours, and the other task didn’t complete within 24 hours."
50,"For those optimizations that we’re not sure about, and we want to rule out any file caching or buffer pool caching we need a tool to help us."
50,"There are several great tools to help you, for example:"
50,SysBench Benchmark tool
50,mysqladmin – Comes with the default MySQL installation
50,Mytop – Command line tool for monitoring MySQL
50,"There are more applications, of course, and you should discover which ones work best for your testing environment."
50,A blog we like a lot with many MySQL benchmarks is by Percona. Percona is distributing their fork of MySQL server that includes many improvements and the TokuDB engine.
50,Optimizing MySQL InnoDB storage engine
50,"MySQL supports two storage engines: MyISAM and InnoDB table type. This article will focus only on optimizing InnoDB for optimizing insert speed. (because MyISAM table allows for full table locking, it’s a different topic altogether)"
50,MariaDB and Percona MySQL supports TukoDB as well; this will not be covered as well.
50,Buy a fast server
50,"‘The Cloud’ has been a hot topic for the past few years―with a couple clicks, you get a server, and with one click you delete it, a very powerful way to manage your infrastructure."
50,"Besides the downside in costs, though, there’s also a downside in performance. Let’s take, for example, DigitalOcean, one of the leading VPS providers. For $40, you get a VPS that has 8GB of RAM, 4 Virtual CPUs, and 160GB SSD."
50,What is a virtual CPU
50,"It’s important to know that virtual CPU is not the same as a real CPU; to understand the distinction, we need to know what a VPS is."
50,"VPS is an isolated virtual environment that is allocated on a dedicated server running a particular software like Citrix or VMWare. It’s possible to allocate many VPSs on the same server, with each VPS isolated from the others."
50,"So, as an example, a provider would use a computer with X amount of threads and memory and provisions a higher number of VPSs than what the server can accommodate if all VPSs would use a100% CPU all the time."
50,The reason is that the host knows that the VPSs will not use all the CPU at the same time.
50,"Let’s assume each VPS uses the CPU only 50% of the time, which means the web hosting can allocate twice the number of CPUs. Therefore, it’s possible that all VPSs will use more than 50% at one time, which means the virtual CPU will be throttled."
50,"Another option is to throttle the virtual CPU all the time to half or a third of the real CPU, on top or without over-provisioning. This will allow you to provision even more VPSs."
50,"CPU throttling is not a secret; it is why some web hosts offer guaranteed virtual CPU: the virtual CPU will always get 100% of the real CPU. Needless to say, the cost is double the usual cost of VPS."
50,Using a Dedicated server for MySQL
50,"If I use a bare metal server at Hetzner (a good and cheap host), I’ll get either AMD Ryzen 5 3600 Hexa-Core (12 threads) or i7-6700 (8 threads), 64 GB of RAM, and two 512GB NVME SSDs (for the sake of simplicity, we’ll consider them as one, since you will most likely use the two drives in mirror raid for data protection)"
50,"As you can see, the dedicated server costs the same, but is at least four times as powerful."
50,"There are drawbacks to take in consideration, however:"
50,VPS
50,Bare metal
50,Slower
50,Faster
50,Fast deployment
50,Slow deployment
50,Built-in backups
50,Manual backups
50,Easy to restore snapshots
50,No snapshots
50,Webhost responsible for hardware
50,You must debug any hardware issues
50,MySQL server
50,"One of the fastest ways to improve MySQL performance, in general, is to use bare-metal servers, which is a superb option as long as you can manage them."
50,Using SSD instead of magnetic drives
50,"It’s 2020, and there’s no need to use magnetic drives; in all seriousness, don’t unless you don’t need a high-performance database."
50,"A magnetic drive can do around 150 random access writes per second (IOPS), which will limit the number of possible inserts."
50,"An SSD will have between 4,000-100,000 IOPS per second, depending on the model."
50,Using RAID 5/6 for Database Durability
50,"Raid 5 means having at least three hard drives―one drive is the parity, and the others are for the data, so each write will write just a part of the data to the drives and calculate the parity for the last drive."
50,"The parity method allows restoring the RAID array if any drive crashes, even if it’s the parity drive."
50,"The advantage is that each write takes less time, since only part of the data is written; make sure, though, that you use an excellent raid controller that doesn’t slow down because of parity calculations."
50,"In addition, RAID 5 for MySQL will improve reading speed because it reads only a part of the data from each drive."
50,"RAID 6 means there are at least two parity hard drives, and this allows for the creation of bigger arrays, for example, 8+2: Eight data and two parity."
50,"On a personal note, I used ZFS, which should be highly reliable, I created Raid X, which is similar to raid 5, and I had a corrupt drive. I was so glad I used a raid and wanted to recover the array."
50,"I got an error that wasn’t even in Google Search, and data was lost. Fortunately, it was test data, so it was nothing serious. But I dropped ZFS and will not use it again. The fact that I’m not going to use it doesn’t mean you shouldn’t."
50,Tweak MySQL memory settings
50,"MySQL default settings are very modest, and the server will not use more than 1GB of RAM. The reason for that is that MySQL comes pre-configured to support web servers on VPS or modest servers."
50,"The assumption is that the users aren’t tech-savvy, and if you need 50,000 concurrent inserts per second, you will know how to configure the MySQL server."
50,Some of the memory tweaks I used (and am still using on other scenarios):
50,innodb_buffer_pool_size
50,From MySQL documentation:
50,"The size in bytes of the buffer pool, the memory area where InnoDB caches table, index data and query cache (results of select queries). The default value is 134217728 bytes (128MB) according to the reference manual."
50,"Primary memory setting for MySQL, according to Percona, should be 80-90% of total server memory, so in the 64GB example, I will set it to 57GB."
50,"Understand that this value is dynamic, which means it will grow to the maximum as needed."
50,"The more memory available to MySQL means that there’s more space for cache and indexes, which reduces disk IO and improves speed."
50,"Make sure you put a value higher than the amount of memory; by accident once, probably a finger slipped, and I put nine times the amount of free memory. The database was throwing random errors."
50,innodb_buffer_pool_instances
50,From MySQL documentation:
50,"Typically, having multiple buffer pool instances is appropriate for systems that allocate multiple gigabytes to the InnoDB buffer pool, with each instance being one gigabyte or larger"
50,"This setting allows you to have multiple pools (the total size will still be the maximum specified in the previous section), so, for example, let’s say we have set this value to 10, and the innodb_buffer_pool_size is set to 50GB., MySQL will then allocate ten pools of 5GB."
50,Having multiple pools allows for better concurrency control and means that each pool is shared by fewer connections and incurs less locking.
50,Increasing the number of the pool is beneficial in case multiple connections perform heavy operations.
50,Other Memory settings
50,"Just to clarify why I didn’t mention it, MySQL has more flags for memory settings, but they aren’t related to insert speed. They can affect insert performance if the database is used for reading other data while writing."
50,"In that case, any read optimization will allow for more server resources for the insert statements."
50,Tweak commit mechanism
50,"MySQL is ACID compliant (Atomicity, Consistency, Isolation, Durability), which means it has to do certain things in a certain way that can slow down the database."
50,"In some cases, you don’t want ACID and can remove part of it for better performance."
50,What is MySQL transaction
50,"Part of ACID compliance is being able to do a transaction, which means running a set of operations together that either all succeed or all fail."
50,"For example, let’s say we do ten inserts in one database transaction, and one of the inserts fails. The database should “cancel” all the other inserts (this is called a rollback) as if none of our inserts (or any other modification) had occurred."
50,A single transaction can contain one operation or thousands.
50,What is MySQL commit
50,A commit is when the database takes the transaction and makes it permanent.
50,The process of a transaction is:
50,Start transaction
50,Do some modifications
50,Commit transaction
50,"The way MySQL does commit: It has a transaction log, whereby every transaction goes to a log file and it’s committed only from that log file."
50,The transaction log is needed in case of a power outage or any kind of other failure. The database can then resume the transaction from the log file and not lose any data.
50,MySQL inserts with a transaction
50,"After we do an insert, it goes to a transaction log, and from there it’s committed and flushed to the disk, which means that we have our data written two times, once to the transaction log and once to the actual MySQL table."
50,"In specific scenarios where we care more about data integrity that’s a good thing, but if we upload from a file and can always re-upload in case something happened, we are losing speed."
50,Changing the commit mechanism
50,"The flag innodb_flush_log_at_trx_commit controls the way transactions are flushed to the hard drive. There are three possible settings, each with its pros and cons."
50,innodb_flush_log_at_trx_commit=1
50,The default MySQL value: This value is required for full ACID compliance. MySQL writes the transaction to a log file and flushes it to the disk on commit.
50,innodb_flush_log_at_trx_commit=0
50,"With this option, MySQL will write the transaction to the log file and will flush to the disk at a specific interval (once per second)."
50,innodb_flush_log_at_trx_commit=2
50,"With this option, MySQL flushes the transaction to OS buffers, and from the buffers, it flushes to the disk at each interval that will be the fastest."
50,innodb_flush_log_at_timeout
50,"This flag allows you to change the commit timeout from one second to another value, and on some setups, changing this value will benefit performance. I believe it has to do with systems on Magnetic drives with many reads."
50,Using precalculated primary key for string
50,"Let’s say we have a table of Hosts. Naturally, we will want to use the host as the primary key, which makes perfect sense."
50,"The problem with that approach, though, is that we have to use the full string length in every table you want to insert into: A host can be 4 bytes long, or it can be 128 bytes long. Inserting the full-length string will, obviously, impact performance and storage."
50,"The problem becomes worse if we use the URL itself as a primary key, which can be one byte to 1024 bytes long (and even more)."
50,"The solution is to use a hashed primary key. Instead of using the actual string value, use a hash. Remember that the hash storage size should be smaller than the average size of the string you want to use; otherwise, it doesn’t make sense, which means SHA1 or SHA256 is not a good choice."
50,Changing the Database’s flush method
50,"The flag innodb_flush_method specifies how MySQL will flush the data, and the default is O_SYNC, which means all the data is also cached in the OS IO cache."
50,"The flag O_DIRECT tells MySQL to write the data directly without using the OS IO cache, and this might speed up the insert rate."
50,Using file system compression
50,"Some filesystems support compression (like ZFS), which means that storing MySQL data on compressed partitions may speed the insert rate. The reason is that if the data compresses well, there will be less data to write, which can speed up the insert rate."
50,Do you need that index?
50,"Inserting to a table that has an index will degrade performance because MySQL has to calculate the index on every insert. In case there are multiple indexes, they will impact insert performance even more."
50,"Check every index if it’s needed, and try to use as few as possible."
50,"BTW, when I considered using custom solutions that promised consistent insert rate, they required me to have only a primary key without indexes, which was a no-go for me."
50,Dropping the index
50,"In case you have one or more indexes on the table (Primary key is not considered an index for this advice), you have a bulk insert, and you know that no one will try to read the table you insert into, it may be better to drop all the indexes and add them once the insert is complete, which may be faster."
50,"This solution is scenario dependent. If it’s possible to read from the table while inserting, this is not a viable solution."
50,Using partitions to improve MySQL insert slow rate
50,"MySQL supports table partitions, which means the table is split into X mini tables (the DBA controls X). The one big table is actually divided into many small ones."
50,"Some people claim it reduced their performance; some claimed it improved it, but as I said in the beginning, it depends on your solution, so make sure to benchmark it."
50,"Before using MySQL partitioning feature make sure your version supports it, according to MySQL documentation it’s supported by: MySQL Community Edition, MySQL Enterprise Edition and MySQL Cluster CGE. It’s not supported by MySQL Standard Edition."
50,Placing a table on a different drive
50,"It’s possible to place a table on a different drive, whether you use multiple RAID 5/6 or simply standalone drives. Placing a table on a different drive means it doesn’t share the hard drive performance and bottlenecks with tables stored on the main drive."
50,Using MySQL bulk insert
50,"If you have a bunch of data (for example when inserting from a file), you can insert the data one records at a time:"
50,Regular insert (single row)
50,Insert into table values ()
50,"This method is inherently slow; in one database, I had the wrong memory setting and had to export data using the flag –skip-extended-insert, which creates the dump file with a single insert per line."
50,"Needless to say, the import was very slow, and after 24 hours it was still inserting, so I stopped it, did a regular export, and loaded the data, which was then using bulk inserts, this time it was many times faster, and took only an hour."
50,MySQL insert multiple rows (Extended inserts)
50,The alternative is to insert multiple rows using the syntax of many inserts per query (this is also called extended inserts):
50,"Insert into table values (),(),()"
50,"The limitation of many inserts per query is the value of –max_allowed_packet, which limits the maximum size of a single command."
50,"You should experiment with the best number of rows per command: I limited it at 400 rows per insert, but I didn’t see any improvement beyond that point."
50,Using replace into or insert ignore
50,"Trying to insert a row with an existing primary key will cause an error, which requires you to perform a select before doing the actual insert. This will, however, slow down the insert further if you want to do a bulk insert."
50,MySQL Replace into
50,"Replace the row into will overwrite in case the primary key already exists; this removes the need to do a select before insert, you can treat this type of insert as insert and update, or you can treat it duplicate key update."
50,MySQL Insert ignore
50,Insert ignore will not insert the row in case the primary key already exists; this removes the need to do a select before insert.
50,Inserting from multiple threads
50,"In case the data you insert does not rely on previous data, it’s possible to insert the data from multiple threads, and this may allow for faster inserts."
50,"Some things to watch for are deadlocks (threads concurrency). In my case, one of the apps could crash because of a soft deadlock break, so I added a handler for that situation to retry and insert the data."
50,"Since I used PHP to insert data into MySQL, I ran my application a number of times, as PHP support for multi-threading is not optimal. When I needed a better performance I used a C++ application and used MySQL C++ connector."
50,Using load from file (MySQL bulk insert)
50,"Using load from file (load data infile method) allows you to upload data from a formatted file and perform multiple rows insert in a single file. You simply specify which table to upload to and the data format, which is a CSV, the syntax is:"
50,LOAD DATA
50,[LOW_PRIORITY | CONCURRENT] [LOCAL]
50,INFILE 'file_name'
50,[REPLACE | IGNORE]
50,INTO TABLE tbl_name
50,"[PARTITION (partition_name [, partition_name] ...)]"
50,[CHARACTER SET charset_name]
50,[{FIELDS | COLUMNS}
50,[TERMINATED BY 'string']
50,[[OPTIONALLY] ENCLOSED BY 'char']
50,[ESCAPED BY 'char']
50,[LINES
50,[STARTING BY 'string']
50,[TERMINATED BY 'string']
50,[IGNORE number {LINES | ROWS}]
50,[(col_name_or_user_var
50,"[, col_name_or_user_var] ...)]"
50,[SET col_name={expr | DEFAULT}
50,"[, col_name={expr | DEFAULT}] ...]"
50,"The MySQL bulk data insert performance is incredibly fast vs other insert methods, but it can’t be used in case the data needs to be processed before inserting into the SQL server database."
50,Using application-level cache
50,"The data I inserted had many lookups. For example, if I inserted web links, I had a table for hosts and table for URL prefixes, which means the hosts could recur many times."
50,"I created a map that held all the hosts and all other lookups that were already inserted. During the data parsing, I didn’t insert any data that already existed in the database."
50,Adjusting char/varchar collation
50,"When working with strings, check each string to determine if you need it to be Unicode or ASCII."
50,"Ascii character is one byte, so a 255 characters string will take 255 bytes."
50,"Unicode is needed to support any language that is not English, and a Unicode char takes 2 bytes. Therefore, a Unicode string is double the size of a regular string, even if it’s in English."
50,"Some collation uses utf8mb4, in which every character is 4 bytes, so, inserting collations that are 2 or 4 bytes per character will take longer."
50,"In my case, URLs and hash primary keys are ASCII only, so I changed the collation accordingly."
50,Using MySQL cluster
50,"MySQL NDB Cluster (Network Database) is the technology that powers MySQL distributed database. This means the database is composed of multiple servers (each server is called a node), which allows for faster insert rate The downside, though, is that it’s harder to manage and costs more money."
50,"I calculated that for my needs I’d have to pay between 10,000-30,000 dollars per month just for hosting of 10TB of data which will also support the insert speed I need."
50,Using a custom engine
50,"MySQL uses InnoDB as the default engine. There are more engines on the market, for example, TokuDB. I don’t have experience with it, but it’s possible that it may allow for better insert performance."
50,Using a custom solution
50,"Unfortunately, with all the optimizations I discussed, I had to create my own solution, a custom database tailored just for my needs, which can do 300,000 concurrent inserts per second without degradation."
50,"I know there are several custom solutions besides MySQL, but I didn’t test any of them because I preferred to implement my own rather than use a 3rd party product with limited support."
50,Bonus section
50,We decided to add several extra items beyond our twenty suggested methods for further InnoDB performance optimization tips.
50,Improve select speed
50,"What goes in, must come out. Selecting data from the database means the database has to spend more time locking tables and rows and will have fewer resources for the inserts."
50,"To improve select performance, you can read our other article about the subject of optimization for  improving MySQL select speed."
50,Thread concurrency
50,"When inserting data to the same table in parallel, the threads may be waiting because another thread has locked the resource it needs, you can check that by inspecting thread states, see how many threads are waiting on a lock."
50,"If you get a deadlock error, you know you have a locking issue, and you need to revise your database design or insert methodology. (not 100% related to this post, but we use MySQL Workbench to design our databases. It’s free and easy to use)"
50,Using replication
50,"Using replication is more of a design solution. Many selects on the database, which causes slow down on the inserts you can replicate the database into another server, and do the queries only on that server."
50,"This way, you split the load between two servers, one for inserts one for selects."
50,Using prepared statements
50,"When sending a command to MySQL, the server has to parse it and prepare a plan."
50,"When using prepared statements, you can cache that parse and plan to avoid calculating it again, but you need to measure your use case to see if it improves performance."
50,Disabling foreign key
50,"A foreign key is an index that is used to enforce data integrity this is a design used when doing database normalisation. When inserting data into normalized tables, it will cause an error when inserting data without matching IDs on other tables."
50,Doing so also causes an index lookup for every insert. Consider deleting the foreign key if insert speed is critical unless you absolutely must have those checks in place.
50,Connection reuse
50,Do you reuse a single connection or close it and create it immediately?
50,The best way is to keep the same connection open as long as possible. The reason is that opening and closing database connections takes time and resources from both the MySQL client and server and reduce insert time.
50,Number of available connections
50,"With some systems connections that can’t be reused, it’s essential to make sure that MySQL is configured to support enough connections. Otherwise, new connections may wait for resources or fail all together."
50,One thing to keep in mind that MySQL maintains a connection pool. Having too many connections can put a strain on the available memory.
50,Summary
50,"There are many possibilities to improve slow inserts and improve insert speed. But because every database is different, the DBA must always test to check which option works best when doing database tuning."
50,"Further, optimization that is good today may be incorrect down the road when the data size increases or the database schema changes."
50,Good luck
50,vote
50,Article Rating
50,Share via:
50,Facebook
50,Twitter
50,LinkedIn
50,More
50,Related posts:
50,Why would I want to build a database?
50,Fast Insert Performance Custom Database
50,Weird case of MySQL index that doesn’t function correctly
50,Compiling TensorFlow on CentOS 8
50,Subscribe
50,Login
50,Notify of
50,new follow-up comments
50,new replies to my comments
50,[+]
50,Name*
50,Email*
50,Website
50,[+]
50,Name*
50,Email*
50,Website
50,0 Comments
50,Inline Feedbacks
50,View all comments
50,Product
50,Pricing
50,API
50,URL Classification
50,Backlinks explorer
50,Keywords explorer
50,Legal info
50,Terms and conditions
50,Privacy policy
50,Refund policy
50,Affiliate disclaimer
50,General
50,SEO API Details
50,Blog
50,Free code
50,Company
50,About us
50,Contact us
50,Facebook page
50,Twitter
50,Linkedin
50,Youtube
50,Soundcloud
50,"wpDiscuz00Would love your thoughts, please comment.x()x| ReplyInsert"
50,Facebook
50,Twitter
50,LinkedIn
50,More Networks
50,Share via
50,Facebook
50,Twitter
50,LinkedIn
50,Mix
50,Email
50,Print
50,Copy Link
50,Powered by Social Snap
50,Copy link
50,CopyCopied
50,Powered by Social Snap
51,MySQL - Force index degraded performance over time - Database Administrators Stack Exchange
51,Stack Exchange Network
51,"Stack Exchange network consists of 176 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
51,Visit Stack Exchange
51,Loading…
51,Tour
51,Start here for a quick overview of the site
51,Help Center
51,Detailed answers to any questions you might have
51,Meta
51,Discuss the workings and policies of this site
51,About Us
51,Learn more about Stack Overflow the company
51,Business
51,Learn more about hiring developers or posting ads with us
51,Log in
51,Sign up
51,current community
51,Database Administrators
51,help
51,chat
51,Database Administrators Meta
51,your communities
51,Sign up or log in to customize your list.
51,more stack exchange communities
51,company blog
51,Unblock your team by capturing collective knowledge that anyone can find.
51,A private collaboration & knowledge sharing platform.
51,Learn more about Teams
51,"“If there is one thing developers like less than writing documentation, it’s responding to unnecessary escalations […] and too many escalations wear down the developers.”"
51,Tom LimoncelliSite Reliability Engineering Manager at Stack Overflow
51,Read blog post
51,"“We needed a better place to store all the questions and answers that people were repeatedly asking, and we discovered Stack Overflow for Teams.”"
51,Suyog RaoDirector of Engineering at Elastic Cloud
51,Listen to podcast
51,"“We use Stack Overflow for Teams internally when onboarding new developers, and when new developers ask questions, everyone can benefit from the shared knowledge.”"
51,Roberta ArcoverdePrincipal Software Developer at Stack Overflow
51,Listen to podcast
51,Home
51,Public
51,Questions
51,Tags
51,Users
51,Unanswered
51,Find a Job
51,Jobs
51,Companies
51,Teams
51,Stack Overflow for Teams
51,– Collaborate and share knowledge with a private group.
51,Create a free Team
51,What is Teams?
51,Teams
51,What’s this?
51,Create free Team
51,Teams
51,Q&A for work
51,Connect and share knowledge within a single location that is structured and easy to search.
51,Learn more
51,MySQL - Force index degraded performance over time
51,Ask Question
51,Asked
51,7 months ago
51,Active
51,7 months ago
51,Viewed
51,87 times
51,Last year we migrated data from our old product environment into a new environment with our new product.
51,"Since the new product was still fresh, we found that some of the new queries were very slow after the migration. and we fixed several of them by adding the 'FORCE INDEX(SOME_INDEX)' to the queries join clauses. this immediately solved our problem and sped up the queries dramatically."
51,"Lately we see some performance degradation in the new product again, and the same queries we optimized last year using the 'FORCE INDEX(SOME_INDEX)' are not faster without the force index part of it (basically using the original query from last year is now faster)"
51,"Someone at the office threw to the air the idea that mysql keeps his own statistics of its indexes and as the mysql server keeps working it become more efficient in creating the flow of the query. (meaning that at the beginning it has less statistics and thats why hinting to force index helps it, and after a year of production usage its own statistics are enough)."
51,Is this correct? is there any other plausible explanation for this?
51,mysql performance index mysql-5.7 index-tuning
51,Share
51,Improve this question
51,Follow
51,edited Aug 25 '20 at 18:13
51,Urbanleg
51,asked Aug 25 '20 at 15:11
51,UrbanlegUrbanleg
51,33511 gold badge33 silver badges1313 bronze badges
51,It may help to tag the version number of MySQL you are using.
51,– bbaird
51,Aug 25 '20 at 16:36
51,Add a comment
51,1 Answer
51,Active
51,Oldest
51,Votes
51,"Without any index hint, the Optimizer uses statistics to decide which index to use."
51,And it updates the statistics in a reasonably (but not always) intelligent manner.
51,"With a hint, such as FORCE INDEX, it ignores the statistics and uses the given index if at all possible."
51,Probably the FORCE was beneficial at first.
51,"But later the data in the table shifted due to new rows, changing distribution of values, or the phase of the moon."
51,And now that index is no longer optimal.
51,"Your experience is why I warn people against using hints, saying ""it may help today, but hurt tomorrow."""
51,"If you would like to discuss the specific query, please provide SHOW CREATE TABLE and EXPLAIN SELECT..."
51,"It may be that a different composite query, without the hint, will work even better than what you had or have."
51,More on the history...
51,"The Optimizer was heavily worked on in 5.6, 5.7, and 8.0."
51,"A big change was moving to a ""cost-based"" model."
51,"In theory, it produces better results."
51,"In practice, it rarely changes anything."
51,"And, as perhaps your case shows, it sometimes hurts."
51,The presumption is that the cost-based model is usually better than the previous algorithm.
51,The statistics are updated when any of these happens:
51,The table grows by 10%.
51,(This partially addresses your coworker's theory.)
51,You run ANALYZE TABLE.
51,"In rare cases, this is a ""fix"" for a bad index choice, but it may or may not stick."
51,Some flavors of ALTER.
51,Share
51,Improve this answer
51,Follow
51,answered Aug 25 '20 at 23:21
51,Rick JamesRick James
51,59.8k44 gold badges3636 silver badges8686 bronze badges
51,Add a comment
51,Your Answer
51,"Thanks for contributing an answer to Database Administrators Stack Exchange!Please be sure to answer the question. Provide details and share your research!But avoid …Asking for help, clarification, or responding to other answers.Making statements based on opinion; back them up with references or personal experience.To learn more, see our tips on writing great answers."
51,Draft saved
51,Draft discarded
51,Sign up or log in
51,Sign up using Google
51,Sign up using Facebook
51,Sign up using Email and Password
51,Submit
51,Post as a guest
51,Name
51,Email
51,"Required, but never shown"
51,Post as a guest
51,Name
51,Email
51,"Required, but never shown"
51,Post Your Answer
51,Discard
51,"By clicking “Post Your Answer”, you agree to our terms of service, privacy policy and cookie policy"
51,Not the answer you're looking for? Browse other questions tagged mysql performance index mysql-5.7 index-tuning
51,or ask your own question.
51,The Overflow Blog
51,Level Up: Creative Coding with p5.js – parts 4 and 5
51,Understanding quantum computing through drunken walks
51,Featured on Meta
51,"Stack Overflow for Teams is now free for up to 50 users, forever"
51,Related
51,Filtered index statistics refresh threshold
51,SQL Index order and performance based on cardinality and data
51,SQL Server 2008 - Question about index behaviour
51,MySQL requires FORCE INDEX on huge table and simple SELECTs
51,"MySQL 5.1 -> 5.6 problem, select distinct order by limit query stops using index"
51,Select performance problems with conditional index vs non-conditional on large volume table
51,Is it worth the time to review indexes suggested by index tuning advisers?
51,Worse performance after create empty table
51,MariaDB (MySQL) slow query when primary key range combined with fulltext index
51,MySQL - single heavily indexed table queries runs fast until the first join with another table
51,Hot Network Questions
51,Can I ask my supervisor how long would it take to complete my PhD before beginning my PhD studies and post getting acceptance letter?
51,70s(?) film with rich folks using guns to burn trees for fun?
51,Do 90% of employees who accept a counteroffer still end up leaving after a year?
51,Why split the resistance on either side of an LED?
51,How did early laser printers get by with so little memory?
51,Which form of spelling is preferred when publishing for an English speaking audience?
51,Passive Cooling Techniques for Apartment Buildings
51,Why does pentatonic + diatonic embellishments sound so much better than just using the diatonic for guitar solos?
51,Model if-else statement
51,I am getting the wrong result for north coordinate
51,What kind of problems can a flight have if passenger weight is miscalculated?
51,How can I see if my oscillator works without using an oscilloscope?
51,Should the SpaceX Starship static fire tests more closely match actual flight parameters?
51,Is Yoda committing a fallacy?
51,What does “strongly correlated” mean?
51,How to handle when a player wants the DM to hard-bend the story for his/her epic idea of outcome?
51,What accounted for the cost of ENIAC?
51,Is lithium grease suitable for lubricating the steel headset cups when pressing them into the aluminium frame?
51,Does department receive funding for each PhD candidate
51,Is there a problem with grep command? I am getting characters that don't match my regular expression
51,Naturally occurring examples of badly behaved categories
51,"How would you, as a teacher, decide if a particular topic/ concept should be tested as multiple-choice questions or as free-response questions?"
51,Suspicious GitHub fork
51,Why does the First Amendment apply to states?
51,more hot questions
51,Question feed
51,Subscribe to RSS
51,Question feed
51,"To subscribe to this RSS feed, copy and paste this URL into your RSS reader."
51,lang-sql
51,Database Administrators
51,Tour
51,Help
51,Chat
51,Contact
51,Feedback
51,Mobile
51,Company
51,Stack Overflow
51,For Teams
51,Advertise With Us
51,Hire a Developer
51,Developer Jobs
51,About
51,Press
51,Legal
51,Privacy Policy
51,Terms of Service
51,Cookie Settings
51,Cookie Policy
51,Stack Exchange Network
51,Technology
51,Life / Arts
51,Culture / Recreation
51,Science
51,Other
51,Stack Overflow
51,Server Fault
51,Super User
51,Web Applications
51,Ask Ubuntu
51,Webmasters
51,Game Development
51,TeX - LaTeX
51,Software Engineering
51,Unix & Linux
51,Ask Different (Apple)
51,WordPress Development
51,Geographic Information Systems
51,Electrical Engineering
51,Android Enthusiasts
51,Information Security
51,Database Administrators
51,Drupal Answers
51,SharePoint
51,User Experience
51,Mathematica
51,Salesforce
51,ExpressionEngine® Answers
51,Stack Overflow em Português
51,Blender
51,Network Engineering
51,Cryptography
51,Code Review
51,Magento
51,Software Recommendations
51,Signal Processing
51,Emacs
51,Raspberry Pi
51,Stack Overflow на русском
51,Code Golf
51,Stack Overflow en español
51,Ethereum
51,Data Science
51,Arduino
51,Bitcoin
51,Software Quality Assurance & Testing
51,Sound Design
51,Windows Phone
51,more (28)
51,Photography
51,Science Fiction & Fantasy
51,Graphic Design
51,Movies & TV
51,Music: Practice & Theory
51,Worldbuilding
51,Video Production
51,Seasoned Advice (cooking)
51,Home Improvement
51,Personal Finance & Money
51,Academia
51,Law
51,Physical Fitness
51,Gardening & Landscaping
51,Parenting
51,more (10)
51,English Language & Usage
51,Skeptics
51,Mi Yodeya (Judaism)
51,Travel
51,Christianity
51,English Language Learners
51,Japanese Language
51,Chinese Language
51,French Language
51,German Language
51,Biblical Hermeneutics
51,History
51,Spanish Language
51,Islam
51,Русский язык
51,Russian Language
51,Arqade (gaming)
51,Bicycles
51,Role-playing Games
51,Anime & Manga
51,Puzzling
51,Motor Vehicle Maintenance & Repair
51,Board & Card Games
51,Bricks
51,Homebrewing
51,Martial Arts
51,The Great Outdoors
51,Poker
51,Chess
51,Sports
51,more (16)
51,MathOverflow
51,Mathematics
51,Cross Validated (stats)
51,Theoretical Computer Science
51,Physics
51,Chemistry
51,Biology
51,Computer Science
51,Philosophy
51,Linguistics
51,Psychology & Neuroscience
51,Computational Science
51,more (10)
51,Meta Stack Exchange
51,Stack Apps
51,API
51,Data
51,Blog
51,Facebook
51,Twitter
51,LinkedIn
51,Instagram
51,site design / logo © 2021 Stack Exchange Inc; user contributions licensed under cc by-sa.
51,rev 2021.4.14.39087
51,Database Administrators Stack Exchange works best with JavaScript enabled
51,Your privacy
51,"By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy."
51,Accept all cookies
51,Customize settings
52,mysql - MariaDB 10.2 Performance Tuning - Lock wait timeout Issues - Stack Overflow
52,Stack Overflow
52,About
52,Products
52,For Teams
52,Stack Overflow
52,Public questions & answers
52,Stack Overflow for Teams
52,Where developers & technologists share private knowledge with coworkers
52,Jobs
52,Programming & related technical career opportunities
52,Talent
52,Recruit tech talent & build your employer brand
52,Advertising
52,Reach developers & technologists worldwide
52,About the company
52,Loading…
52,Log in
52,Sign up
52,current community
52,Stack Overflow
52,help
52,chat
52,Meta Stack Overflow
52,your communities
52,Sign up or log in to customize your list.
52,more stack exchange communities
52,company blog
52,Unblock your team by capturing collective knowledge that anyone can find.
52,A private collaboration & knowledge sharing platform.
52,Learn more about Teams
52,"“If there is one thing developers like less than writing documentation, it’s responding to unnecessary escalations […] and too many escalations wear down the developers.”"
52,Tom LimoncelliSite Reliability Engineering Manager at Stack Overflow
52,Read blog post
52,"“We needed a better place to store all the questions and answers that people were repeatedly asking, and we discovered Stack Overflow for Teams.”"
52,Suyog RaoDirector of Engineering at Elastic Cloud
52,Listen to podcast
52,"“We use Stack Overflow for Teams internally when onboarding new developers, and when new developers ask questions, everyone can benefit from the shared knowledge.”"
52,Roberta ArcoverdePrincipal Software Developer at Stack Overflow
52,Listen to podcast
52,Home
52,Public
52,Questions
52,Tags
52,Users
52,Find a Job
52,Jobs
52,Companies
52,Teams
52,Stack Overflow for Teams
52,– Collaborate and share knowledge with a private group.
52,Create a free Team
52,What is Teams?
52,Teams
52,What’s this?
52,Create free Team
52,Teams
52,Q&A for work
52,Connect and share knowledge within a single location that is structured and easy to search.
52,Learn more
52,MariaDB 10.2 Performance Tuning - Lock wait timeout Issues
52,Ask Question
52,Asked
52,11 months ago
52,Active
52,11 months ago
52,Viewed
52,1k times
52,I have 4 instance database-clustering (2 of them are mariadb 10.2 server and 2 of them is garbd server)
52,"We use update-heavy queries in our databases, dealing with a huge tables which some of the tables have over 20 million records. Also we have lots of java application which use database and these threads mostly make ""update"" processes in tables. In the peak_time we use about 200 threads."
52,"Our problem is that we have ""Deadlock"" in our applications. I believe that I can figure out by making some performance tuning in database configurations. My last configuration is below;"
52,DB1: x.x.x.x
52,DB2: y.y.y.y
52,Garbd-1: w.w.w.w
52,Garbd-2: z.z.z.z
52,DB-1 : my.cnf
52,[client-server]
52,socket=/app/mysql/mysql.sock
52,port=3306
52,[mysqld]
52,user=mysql
52,port=3306
52,bind-address=0.0.0.0
52,socket=/app/mysql/mysql.sock
52,skip-name-resolve
52,lower_case_table_names=1
52,server-id=1
52,event-scheduler=ON
52,datadir=/app/mysql
52,pid-file=/app/mysql/db1.pid
52,log-error=/app/mysql/db1.err
52,log_bin_trust_function_creators=1
52,query_cache_type=0
52,query_cache_size=0
52,#query_cache_limit = 4M
52,default_table_type = InnoDB
52,table_open_cache = 4096
52,open_files_limit = 8192
52,max_connections = 800
52,wait_timeout=100
52,interactive_timeout=100
52,#net_read_timeout=3600
52,#net_write_timeout=3600
52,max_heap_table_size=64M
52,tmp_table_size=64M
52,thread_cache_size=256
52,sort_buffer_size = 2M
52,join_buffer_size = 256K
52,read_buffer_size=128K
52,read_rnd_buffer_size = 256M
52,transaction-isolation=READ-COMMITTED
52,log_warnings
52,slow_query_log
52,long_query_time=5
52,# Physical RAM is 32G (8-core)
52,innodb_buffer_pool_size =24G
52,innodb_buffer_pool_instances=6
52,key_buffer_size=512M
52,nnodb_write_io_threads=8
52,innodb_read_io_threads=8
52,innodb_thread_concurrency = 16
52,#innodb_data_file_path = ibdata1:1G:autoextend
52,#innodb_autoextend_increment=128M
52,#innodb_file_per_table
52,innodb_flush_log_at_trx_commit=2
52,#sync_binlog=1
52,innodb_log_buffer_size = 16M
52,innodb_log_file_size = 2G
52,innodb_max_dirty_pages_pct = 80
52,innodb_flush_method=O_DIRECT
52,innodb_lock_wait_timeout = 120
52,binlog_format=ROW
52,innodb_autoinc_lock_mode=2
52,[mysqldump]
52,quick
52,max_allowed_packet = 512M
52,[mysql]
52,no-auto-rehash
52,[mysqld_safe]
52,datadir=/app/mysql
52,pid-file=/app/mysql/db1.pid
52,log-error=/app/mysql/db1.err
52,[mariadb-10.2]
52,wsrep_on=ON
52,wsrep_provider=/usr/lib64/galera/libgalera_smm.so
52,"wsrep_provider_options=""gcache.size=10G;gmcast.listen_addr=tcp://0.0.0.0:4567;socket.checksum=2"""
52,"wsrep_cluster_address=""gcomm://10.92.221.215,10.92.221.216,10.92.223.215,10.92.223.216"""
52,wsrep_cluster_name='galera_cluster'
52,wsrep_node_address='10.92.221.215'
52,wsrep_node_name='ivrocsdbp00'
52,wsrep_sst_method=rsync
52,wsrep_sst_auth=sst_user:dbpass
52,"These are also configured in second server, too with a server-id=2 and db2.pid, db2.err changes."
52,Is these changes might solve my problem?
52,I want to know that these parameters are well-configured ?
52,We have 32 GB RAM (8-core) in the servers.
52,Thanks for your kind help.
52,mysql mariadb database-performance database-deadlocks my.cnf
52,Share
52,Follow
52,asked May 7 '20 at 19:14
52,user13493257user13493257
52,5144 bronze badges
52,"To discuss deadlocks and lock-wait-timeout, we need to see the conflicting transactions, plus SHOW CREATE TABLE."
52,– Rick James
52,May 8 '20 at 4:15
52,Additional information request from DB-1.
52,Any SSD or NVME devices on MySQL Host server?
52,Post on pastebin.com and share the links.
52,"From your SSH login root, Text results of:"
52,B) SHOW GLOBAL STATUS;
52,after minimum 24 hours UPTIME C) SHOW GLOBAL VARIABLES;
52,D) SHOW FULL PROCESSLIST; E) complete MySQLTuner report
52,"AND Optional very helpful information, if available includes -"
52,"htop OR top 	for most active apps,"
52,ulimit -a
52,"for a Linux/Unix list of limits,"
52,"iostat -xm 5 3	for IOPS by device and core/cpu count,"
52,for server workload tuning analysis to provide suggestions.
52,– Wilson Hauck
52,May 9 '20 at 14:27
52,Add a comment
52,5 Answers
52,Active
52,Oldest
52,Votes
52,I'm guessing nnodb_write_io_threads is a typo as mysqld wouldn't start with it.
52,I would start with innodb_thread_concurrency = 0 to de-restrict the thread concurrency throttling.
52,"Are your UPDATEs running fully indexed, ideally with PK in the WHERE?"
52,Is there a good reason you have innodb_file_per_table commented out?
52,"It might be worth verifying that your table_open_cache isn't too small (look at the process list and see if you regularly see queries in state of ""opening|closing tables""."
52,"Are these servers VMs, and are they on a busy/overbooked host? I have seen very interesting spurious deadlocks in situations where clock ticks are unstable and unpredictable, e.g. on heavily overbooked virtual machines. There is some interesting thread racing happening under those conditions that seems to be completely un-reproducible on bare metal. Bumping up the instance size in the cloud environment usually cures the problem because bigger instances tend to be on less overbooked hosts."
52,Share
52,Follow
52,answered May 7 '20 at 19:57
52,Gordan BobicGordan Bobic
52,"1,32899 silver badges1111 bronze badges"
52,Add a comment
52,Thanks for your responding.
52,I have corrected my typo and changed my configuration files as innodb_thread_concurrency = 0 as you mentioned.
52,"I tried to configure very low-performing-consuming parameters and I have read about that innodb_file_per_table parameters could effect the RAM usage, so I decided to comment it out. Actually I couldn't figure out what this parameters use for clearly."
52,Let me tell you my some observation after I have restarted both DB servers.
52,"First of all I stopped all application that making connection to db-1 (Actually we are making all db connections through DB1), then restarted both DB-servers. First of all, let me clear that there was no any traffic when I started applications."
52,"After I start all appcalitons that we use, I realised that I was seeing about 120 processes that created by our user in PROCESSLIST in the INFORMATION_SCHEMA. Also I know that some of these connections don't make anything but using connnection_pool."
52,"I was set up 100s wait_timeout, after 100s later, PROCESSLIST down to ""27"" and threads_cache became 92."
52,"I also know that these 27 connection was used by some applications that running some check queries in spesific time interval. (10sec, 4sec, 2sec) So when they run queries , their connection_time return 0 and count again as they are never closed."
52,"I believe that in the peak_time, we are gonna need thread_cache_size, so configuring this parameter as 256 would be good for our system."
52,I know it is so hard to understand for now but can you please check my innodb engine status and give me some advice as I am not very good at concept of database management systems.
52,=====================================
52,2020-05-08 01:10:28 0x7fd8cc55c700 INNODB MONITOR OUTPUT
52,=====================================
52,Per second averages calculated from the last 7 seconds
52,-----------------
52,BACKGROUND THREAD
52,-----------------
52,"srv_master_thread loops: 4897 srv_active, 0 srv_shutdown, 371 srv_idle"
52,srv_master_thread log flush and writes: 5266
52,----------
52,SEMAPHORES
52,----------
52,OS WAIT ARRAY INFO: reservation count 34869
52,OS WAIT ARRAY INFO: signal count 121009
52,"RW-shared spins 0, rounds 214984, OS waits 15858"
52,"RW-excl spins 0, rounds 1540716, OS waits 11401"
52,"RW-sx spins 52309, rounds 782230, OS waits 3893"
52,"Spin rounds per wait: 214984.00 RW-shared, 1540716.00 RW-excl, 14.95 RW-sx"
52,------------
52,TRANSACTIONS
52,------------
52,Trx id counter 211830460
52,Purge done for trx's n:o < 211830450 undo n:o < 0 state: running but idle
52,History list length 1
52,LIST OF TRANSACTIONS FOR EACH SESSION:
52,"---TRANSACTION 422083415170480, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415166264, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415162048, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415157832, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415153616, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415149400, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415145184, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415140968, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415136752, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415132536, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415128320, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415124104, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415119888, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415115672, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415111456, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415107240, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415103024, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415098808, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415094592, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415090376, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415086160, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415081944, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415077728, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415073512, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415069296, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415065080, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415060864, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415056648, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,"---TRANSACTION 422083415052432, not started"
52,"0 lock struct(s), heap size 1136, 0 row lock(s)"
52,--------
52,FILE I/O
52,--------
52,I/O thread 0 state: waiting for completed aio requests (insert buffer thread)
52,I/O thread 1 state: waiting for completed aio requests (log thread)
52,I/O thread 2 state: waiting for completed aio requests (read thread)
52,I/O thread 3 state: waiting for completed aio requests (read thread)
52,I/O thread 4 state: waiting for completed aio requests (read thread)
52,I/O thread 5 state: waiting for completed aio requests (read thread)
52,I/O thread 6 state: waiting for completed aio requests (read thread)
52,I/O thread 7 state: waiting for completed aio requests (read thread)
52,I/O thread 8 state: waiting for completed aio requests (read thread)
52,I/O thread 9 state: waiting for completed aio requests (read thread)
52,I/O thread 10 state: waiting for completed aio requests (write thread)
52,I/O thread 11 state: waiting for completed aio requests (write thread)
52,I/O thread 12 state: waiting for completed aio requests (write thread)
52,I/O thread 13 state: waiting for completed aio requests (write thread)
52,I/O thread 14 state: waiting for completed aio requests (write thread)
52,I/O thread 15 state: waiting for completed aio requests (write thread)
52,I/O thread 16 state: waiting for completed aio requests (write thread)
52,I/O thread 17 state: waiting for completed aio requests (write thread)
52,"Pending normal aio reads: [0, 0, 0, 0, 0, 0, 0, 0] , aio writes: [0, 0, 0, 0, 0, 0, 0, 0] ,"
52,"ibuf aio reads:, log i/o's:, sync i/o's:"
52,Pending flushes (fsync) log: 0; buffer pool: 0
52,"363895 OS file reads, 105906 OS file writes, 18722 OS fsyncs"
52,"0.00 reads/s, 0 avg bytes/read, 6.14 writes/s, 2.43 fsyncs/s"
52,-------------------------------------
52,INSERT BUFFER AND ADAPTIVE HASH INDEX
52,-------------------------------------
52,"Ibuf: size 1, free list len 13430, seg size 13432, 2410 merges"
52,merged operations:
52,"insert 72630, delete mark 0, delete 0"
52,discarded operations:
52,"insert 0, delete mark 0, delete 0"
52,"Hash table size 6374293, node heap has 108 buffer(s)"
52,"Hash table size 6374293, node heap has 64 buffer(s)"
52,"Hash table size 6374293, node heap has 64 buffer(s)"
52,"Hash table size 6374293, node heap has 18 buffer(s)"
52,"Hash table size 6374293, node heap has 10 buffer(s)"
52,"Hash table size 6374293, node heap has 72 buffer(s)"
52,"Hash table size 6374293, node heap has 4 buffer(s)"
52,"Hash table size 6374293, node heap has 634 buffer(s)"
52,"3473.22 hash searches/s, 553.49 non-hash searches/s"
52,---
52,LOG
52,---
52,Log sequence number 129572121784
52,Log flushed up to
52,129572121784
52,Pages flushed up to 129572121784
52,Last checkpoint at
52,129572120321
52,"0 pending log flushes, 0 pending chkp writes"
52,"3127 log i/o's done, 0.71 log i/o's/second"
52,----------------------
52,BUFFER POOL AND MEMORY
52,----------------------
52,Total large memory allocated 26398949376
52,Dictionary memory allocated 278784
52,Buffer pool size
52,1572672
52,Free buffers
52,1200805
52,Database pages
52,370893
52,Old database pages 137029
52,Modified db pages
52,Percent of dirty pages(LRU & free pages): 0.000
52,Max dirty pages percent: 80.000
52,Pending reads 0
52,"Pending writes: LRU 0, flush list 0, single page 0"
52,"Pages made young 2349, not young 0"
52,"0.00 youngs/s, 0.00 non-youngs/s"
52,"Pages read 363786, created 7107, written 94745"
52,"0.00 reads/s, 0.00 creates/s, 4.43 writes/s"
52,"Buffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000"
52,"Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s"
52,"LRU len: 370893, unzip_LRU len: 0"
52,"I/O sum[0]:cur[24], unzip sum[0]:cur[0]"
52,----------------------
52,INDIVIDUAL BUFFER POOL INFO
52,----------------------
52,---BUFFER POOL 0
52,Buffer pool size
52,262112
52,Free buffers
52,198770
52,Database pages
52,63178
52,Old database pages 23341
52,Modified db pages
52,Percent of dirty pages(LRU & free pages): 0.000
52,Max dirty pages percent: 80.000
52,Pending reads 0
52,"Pending writes: LRU 0, flush list 0, single page 0"
52,"Pages made young 372, not young 0"
52,"0.00 youngs/s, 0.00 non-youngs/s"
52,"Pages read 62294, created 884, written 22682"
52,"0.00 reads/s, 0.00 creates/s, 2.14 writes/s"
52,"Buffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000"
52,"Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s"
52,"LRU len: 63178, unzip_LRU len: 0"
52,"I/O sum[0]:cur[4], unzip sum[0]:cur[0]"
52,---BUFFER POOL 1
52,Buffer pool size
52,262112
52,Free buffers
52,198593
52,Database pages
52,63353
52,Old database pages 23406
52,Modified db pages
52,Percent of dirty pages(LRU & free pages): 0.000
52,Max dirty pages percent: 80.000
52,Pending reads 0
52,"Pending writes: LRU 0, flush list 0, single page 0"
52,"Pages made young 365, not young 0"
52,"0.00 youngs/s, 0.00 non-youngs/s"
52,"Pages read 62005, created 1348, written 17745"
52,"0.00 reads/s, 0.00 creates/s, 0.86 writes/s"
52,"Buffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000"
52,"Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s"
52,"LRU len: 63353, unzip_LRU len: 0"
52,"I/O sum[0]:cur[4], unzip sum[0]:cur[0]"
52,---BUFFER POOL 2
52,Buffer pool size
52,262112
52,Free buffers
52,202987
52,Database pages
52,58961
52,Old database pages 21784
52,Modified db pages
52,Percent of dirty pages(LRU & free pages): 0.000
52,Max dirty pages percent: 80.000
52,Pending reads 0
52,"Pending writes: LRU 0, flush list 0, single page 0"
52,"Pages made young 388, not young 0"
52,"0.00 youngs/s, 0.00 non-youngs/s"
52,"Pages read 58042, created 919, written 15919"
52,"0.00 reads/s, 0.00 creates/s, 1.00 writes/s"
52,"Buffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000"
52,"Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s"
52,"LRU len: 58961, unzip_LRU len: 0"
52,"I/O sum[0]:cur[4], unzip sum[0]:cur[0]"
52,---BUFFER POOL 3
52,Buffer pool size
52,262112
52,Free buffers
52,199895
52,Database pages
52,62058
52,Old database pages 22928
52,Modified db pages
52,Percent of dirty pages(LRU & free pages): 0.000
52,Max dirty pages percent: 80.000
52,Pending reads 0
52,"Pending writes: LRU 0, flush list 0, single page 0"
52,"Pages made young 439, not young 0"
52,"0.00 youngs/s, 0.00 non-youngs/s"
52,"Pages read 60791, created 1267, written 12163"
52,"0.00 reads/s, 0.00 creates/s, 0.00 writes/s"
52,"Buffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000"
52,"Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s"
52,"LRU len: 62058, unzip_LRU len: 0"
52,"I/O sum[0]:cur[4], unzip sum[0]:cur[0]"
52,---BUFFER POOL 4
52,Buffer pool size
52,262112
52,Free buffers
52,201239
52,Database pages
52,60712
52,Old database pages 22431
52,Modified db pages
52,Percent of dirty pages(LRU & free pages): 0.000
52,Max dirty pages percent: 80.000
52,Pending reads 0
52,"Pending writes: LRU 0, flush list 0, single page 0"
52,"Pages made young 382, not young 0"
52,"0.00 youngs/s, 0.00 non-youngs/s"
52,"Pages read 59190, created 1522, written 12539"
52,"0.00 reads/s, 0.00 creates/s, 0.43 writes/s"
52,"Buffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000"
52,"Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s"
52,"LRU len: 60712, unzip_LRU len: 0"
52,"I/O sum[0]:cur[4], unzip sum[0]:cur[0]"
52,---BUFFER POOL 5
52,Buffer pool size
52,262112
52,Free buffers
52,199321
52,Database pages
52,62631
52,Old database pages 23139
52,Modified db pages
52,Percent of dirty pages(LRU & free pages): 0.000
52,Max dirty pages percent: 80.000
52,Pending reads 0
52,"Pending writes: LRU 0, flush list 0, single page 0"
52,"Pages made young 403, not young 0"
52,"0.00 youngs/s, 0.00 non-youngs/s"
52,"Pages read 61464, created 1167, written 13697"
52,"0.00 reads/s, 0.00 creates/s, 0.00 writes/s"
52,"Buffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000"
52,"Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s"
52,"LRU len: 62631, unzip_LRU len: 0"
52,"I/O sum[0]:cur[4], unzip sum[0]:cur[0]"
52,--------------
52,ROW OPERATIONS
52,--------------
52,"0 queries inside InnoDB, 0 queries in queue"
52,0 read views open inside InnoDB
52,"Process ID=31986, Main thread ID=140569828304640, state: sleeping"
52,"Number of rows inserted 276741, updated 1143, deleted 276686, read 179703655"
52,"0.00 inserts/s, 0.29 updates/s, 0.00 deletes/s, 109971.15 reads/s"
52,"Number of system rows inserted 0, updated 0, deleted 0, read 0"
52,"0.00 inserts/s, 0.00 updates/s, 0.00 deletes/s, 0.00 reads/s"
52,----------------------------
52,END OF INNODB MONITOR OUTPUT
52,============================
52,"I will share threads status and INNODB ENGINE STATUS after 1-2 day later, too."
52,I hope we won't have face with any lock wait timeout issues.
52,Have a nice day.
52,Share
52,Follow
52,answered May 7 '20 at 22:52
52,user13493257user13493257
52,5144 bronze badges
52,Add a comment
52,Servers are running over 48 hours and let me share tuning-primer results in DB-1.
52,-- MYSQL PERFORMANCE TUNING PRIMER --
52,- By: Matthew Montgomery -
52,MySQL Version 10.2.27-MariaDB-log x86_64
52,Uptime = 2 days 3 hrs 54 min 46 sec
52,Avg. qps = 26
52,Total Questions = 5022294
52,Threads Connected = 17
52,Server has been running for over 48hrs.
52,It should be safe to follow these recommendations
52,To find out more information on how each of these
52,runtime variables effects performance visit:
52,http://dev.mysql.com/doc/refman/10.2/en/server-system-variables.html
52,Visit http://www.mysql.com/products/enterprise/advisors.html
52,for info about MySQL's Enterprise Monitoring and Advisory Service
52,SLOW QUERIES
52,The slow query log is NOT enabled.
52,Current long_query_time = 5.000000 sec.
52,You have 10 out of 5022308 that take longer than 5.000000 sec. to complete
52,Your long_query_time seems to be fine
52,BINARY UPDATE LOG
52,The binary update log is NOT enabled.
52,You will not be able to do point in time recovery
52,See http://dev.mysql.com/doc/refman/10.2/en/point-in-time-recovery.html
52,WORKER THREADS
52,Current thread_cache_size = 256
52,Current threads_cached = 103
52,Current threads_per_sec = 0
52,Historic threads_per_sec = 0
52,Your thread_cache_size is fine
52,MAX CONNECTIONS
52,Current max_connections = 800
52,Current threads_connected = 17
52,Historic max_used_connections = 120
52,The number of used connections is 15% of the configured maximum.
52,Your max_connections variable seems to be fine.
52,INNODB STATUS
52,Current InnoDB index space = 4.29 G
52,Current InnoDB data space = 4.72 G
52,Current InnoDB buffer pool free = 69 %
52,Current innodb_buffer_pool_size = 24.00 G
52,Depending on how much space your innodb indexes take up it may be safe
52,to increase this value to up to 2 / 3 of total system memory
52,MEMORY USAGE
52,Max Memory Ever Allocated : 54.82 G
52,Configured Max Per-thread Buffers : 202.07 G
52,Configured Max Global Buffers : 24.51 G
52,Configured Max Memory Limit : 226.59 G
52,Physical Memory : 31.26 G
52,Max memory limit exceeds 90% of physical memory
52,KEY BUFFER
52,Current MyISAM index space = 638 K
52,Current key_buffer_size = 512 M
52,Key cache miss rate is 1 : 144181
52,Key buffer free ratio = 81 %
52,Your key_buffer_size seems to be too high.
52,Perhaps you can use these resources elsewhere
52,QUERY CACHE
52,Query cache is supported but not enabled
52,Perhaps you should set the query_cache_size
52,SORT OPERATIONS
52,Current sort_buffer_size = 2 M
52,Current read_rnd_buffer_size = 256 M
52,Sort buffer seems to be fine
52,JOINS
52,Current join_buffer_size = 260.00 K
52,You have had 373030 queries where a join could not use an index properly
52,"You should enable ""log-queries-not-using-indexes"""
52,Then look for non indexed joins in the slow query log.
52,If you are unable to optimize your queries you may want to increase your
52,join_buffer_size to accommodate larger joins in one pass.
52,Note! This script will still suggest raising the join_buffer_size when
52,ANY joins not using indexes are found.
52,OPEN FILES LIMIT
52,Current open_files_limit = 9031 files
52,The open_files_limit should typically be set to at least 2x-3x
52,that of table_cache if you have heavy MyISAM usage.
52,Your open_files_limit value seems to be fine
52,TABLE CACHE
52,Current table_open_cache = 4096 tables
52,Current table_definition_cache = 400 tables
52,You have a total of 186 tables
52,You have 428 open tables.
52,The table_cache value seems to be fine
52,TEMP TABLES
52,Current max_heap_table_size = 64 M
52,Current tmp_table_size = 64 M
52,"Of 490764 temp tables, 0% were created on disk"
52,Created disk tmp tables ratio seems fine
52,TABLE SCANS
52,Current read_buffer_size = 128 K
52,Current table scan ratio = 34 : 1
52,read_buffer_size seems to be fine
52,TABLE LOCKING
52,Current Lock Wait ratio = 1 : 383
52,You may benefit from selective use of InnoDB.
52,If you have long running SELECT's against MyISAM tables and perform
52,frequent updates consider setting 'low_priority_updates=1'
52,If you have a high concurrency of inserts on Dynamic row-length tables
52,consider setting 'concurrent_insert=ALWAYS'.
52,Should I decrease some parameters in my.cnf ? Any suggestion please ?
52,Thanks
52,Share
52,Follow
52,answered May 10 '20 at 0:43
52,user13493257user13493257
52,5144 bronze badges
52,"In the SORT OPERATIONS section of this report, this line"
52,Current read_rnd_buffer_size = 256 M - indicates your rrbs is 256Meg.
52,SET GLOBAL read_rnd_buffer_size=256*1024;
52,for a 256K rrbs to significantly reduce handler_read_rnd_next RPS.
52,In your my.cnf make the change for future more reasonable RAM usage and to reduce CPU busy.
52,– Wilson Hauck
52,May 10 '20 at 13:41
52,"Hello, I actually misconfigured this parameters. I think default value for this parameters is 256K, right ? I will set as you mentioned."
52,– user13493257
52,May 10 '20 at 14:24
52,"You are correct, read_rnd_buffer_size in MariaDB reference manual is 256K."
52,"If you could provide the additional information requested, additional suggestions will be provided."
52,"View profile, Network profile for contact info and our free downloadable Utility Scripts to assist with performance tuning."
52,– Wilson Hauck
52,May 10 '20 at 20:40
52,Is your response time improved significantly with the 256K read_rnd_buffer_size rather than the 256Meg you had been running?
52,"Upvotes would be nice, if we helped you."
52,– Wilson Hauck
52,May 24 '20 at 14:25
52,Add a comment
52,-- MYSQL PERFORMANCE TUNING PRIMER --
52,- By: Matthew Montgomery -
52,MySQL Version 10.2.27-MariaDB-log x86_64
52,Uptime = 2 days 9 hrs 24 min 19 sec
52,Avg. qps = 69
52,Total Questions = 14381364
52,Threads Connected = 24
52,Server has been running for over 48hrs.
52,It should be safe to follow these recommendations
52,To find out more information on how each of these
52,runtime variables effects performance visit:
52,http://dev.mysql.com/doc/refman/10.2/en/server-system-variables.html
52,Visit http://www.mysql.com/products/enterprise/advisors.html
52,for info about MySQL's Enterprise Monitoring and Advisory Service
52,SLOW QUERIES
52,The slow query log is NOT enabled.
52,Current long_query_time = 5.000000 sec.
52,You have 608 out of 14381378 that take longer than 5.000000 sec. to complete
52,Your long_query_time seems to be fine
52,BINARY UPDATE LOG
52,The binary update log is NOT enabled.
52,You will not be able to do point in time recovery
52,See http://dev.mysql.com/doc/refman/10.2/en/point-in-time-recovery.html
52,WORKER THREADS
52,Current thread_cache_size = 256
52,Current threads_cached = 109
52,Current threads_per_sec = 0
52,Historic threads_per_sec = 0
52,Your thread_cache_size is fine
52,MAX CONNECTIONS
52,Current max_connections = 800
52,Current threads_connected = 24
52,Historic max_used_connections = 156
52,The number of used connections is 19% of the configured maximum.
52,Your max_connections variable seems to be fine.
52,INNODB STATUS
52,Current InnoDB index space = 5.79 G
52,Current InnoDB data space = 6.29 G
52,Current InnoDB buffer pool free = 63 %
52,Current innodb_buffer_pool_size = 24.00 G
52,Depending on how much space your innodb indexes take up it may be safe
52,to increase this value to up to 2 / 3 of total system memory
52,MEMORY USAGE
52,Max Memory Ever Allocated : 24.95 G
52,Configured Max Per-thread Buffers : 2.27 G
52,Configured Max Global Buffers : 24.51 G
52,Configured Max Memory Limit : 26.78 G
52,Physical Memory : 31.26 G
52,Max memory limit seem to be within acceptable norms
52,KEY BUFFER
52,Current MyISAM index space = 684 K
52,Current key_buffer_size = 512 M
52,Key cache miss rate is 1 : 111228
52,Key buffer free ratio = 81 %
52,Your key_buffer_size seems to be too high.
52,Perhaps you can use these resources elsewhere
52,QUERY CACHE
52,Query cache is supported but not enabled
52,Perhaps you should set the query_cache_size
52,SORT OPERATIONS
52,Current sort_buffer_size = 2 M
52,Current read_rnd_buffer_size = 256 K
52,Sort buffer seems to be fine
52,JOINS
52,Current join_buffer_size = 260.00 K
52,You have had 419644 queries where a join could not use an index properly
52,"You should enable ""log-queries-not-using-indexes"""
52,Then look for non indexed joins in the slow query log.
52,If you are unable to optimize your queries you may want to increase your
52,join_buffer_size to accommodate larger joins in one pass.
52,Note! This script will still suggest raising the join_buffer_size when
52,ANY joins not using indexes are found.
52,OPEN FILES LIMIT
52,Current open_files_limit = 9031 files
52,The open_files_limit should typically be set to at least 2x-3x
52,that of table_cache if you have heavy MyISAM usage.
52,Your open_files_limit value seems to be fine
52,TABLE CACHE
52,Current table_open_cache = 4096 tables
52,Current table_definition_cache = 400 tables
52,You have a total of 186 tables
52,You have 1303 open tables.
52,The table_cache value seems to be fine
52,TEMP TABLES
52,Current max_heap_table_size = 64 M
52,Current tmp_table_size = 64 M
52,"Of 842280 temp tables, 0% were created on disk"
52,Created disk tmp tables ratio seems fine
52,TABLE SCANS
52,Current read_buffer_size = 128 K
52,Current table scan ratio = 50 : 1
52,read_buffer_size seems to be fine
52,TABLE LOCKING
52,Current Lock Wait ratio = 1 : 29
52,You may benefit from selective use of InnoDB.
52,If you have long running SELECT's against MyISAM tables and perform
52,frequent updates consider setting 'low_priority_updates=1'
52,If you have a high concurrency of inserts on Dynamic row-length tables
52,consider setting 'concurrent_insert=ALWAYS'.
52,Any suggest please ?
52,Share
52,Follow
52,answered May 15 '20 at 20:15
52,user13493257user13493257
52,5144 bronze badges
52,Add a comment
52,"I also wanna status my table spesifications. We usually had deadlocks in 3 major tables which are cp_resource_allocation, cp_pending, cp_batch."
52,CREATE TABLE `cp_resource_allocation` (
52,"`CAMPAIGN_ID` int(11) NOT NULL,"
52,"`RESOURCE_QUOTA` int(11) NOT NULL DEFAULT 0,"
52,"`RESOURCE_ALLOCATED` int(11) NOT NULL DEFAULT 0,"
52,"`DAILY_ATTEMPT` int(11) NOT NULL DEFAULT 0,"
52,"`DAILY_SUCCESSFUL` int(11) NOT NULL DEFAULT 0,"
52,"`DAILY_RESERVED` int(11) NOT NULL DEFAULT 0,"
52,PRIMARY KEY (`CAMPAIGN_ID`)
52,) ENGINE=InnoDB DEFAULT CHARSET=latin1 COMMENT='Campaign Resource Allocation Table'
52,CREATE TABLE `cp_pending` (
52,"`RID` int(20) NOT NULL AUTO_INCREMENT,"
52,"`CAMPAIGN` int(10) NOT NULL,"
52,"`BATCH` int(11) NOT NULL,"
52,"`DEST_ADDR` char(20) CHARACTER SET latin1 NOT NULL COMMENT 'Aranan / Gönderilen adres.',"
52,"`TRIES_MADE` int(10) NOT NULL DEFAULT 0 COMMENT 'Specifies current retry count for the Called Number.',"
52,"`SELECT_LOCK` int(10) NOT NULL DEFAULT 0 COMMENT 'NULL’dan farklı ise kayıt seçilmiştir. ',"
52,"`SELECT_TIME` datetime DEFAULT NULL COMMENT 'Seçim zamanı.',"
52,`OUTCOME` int(10) NOT NULL DEFAULT 0 COMMENT 'İşlem durumu.
52,"0 ise işlem devam ediyor. 1 ise işlem başarılı. 2 ise işlem başarısız. 3 ise işlem iptal. 4 ise numara blacklist''de. 5 ise HLR''a tetik konmuş.',"
52,"`APP_INT1` int(10) DEFAULT NULL COMMENT 'Uygulamaya özel.',"
52,"`APP_INT2` int(10) unsigned zerofill DEFAULT NULL COMMENT 'Uygulamaya özel..',"
52,"`CID` int(11) DEFAULT NULL,"
52,"`CC_SERVER_ADDRESS` char(45) CHARACTER SET latin1 DEFAULT NULL,"
52,"`CALL_START_TIME` datetime DEFAULT NULL,"
52,"`CALL_ANSWER_TIME` datetime DEFAULT NULL,"
52,"`CALL_END_TIME` datetime DEFAULT NULL,"
52,"`CAUSE_VALUE` int(11) DEFAULT NULL COMMENT 'RELEASE CAUSE VALUE',"
52,"`CALL_REQUEST_TIME` datetime DEFAULT NULL COMMENT 'time when a call manager retrieves this record',"
52,"`CALL_CONNECT_DURATION` int(11) NOT NULL DEFAULT 0 COMMENT 'CALL_END_TIME - CALL_ANSWER_TIME',"
52,"`LIST_TYPE` int(11) NOT NULL DEFAULT 0 COMMENT 'List Group Type;\\n0 Start List. \\n1 Main List.\\n2 End List. \\n',"
52,"`CALL_ACTIVE_DURATION` int(11) NOT NULL DEFAULT 0 COMMENT 'CALL_END_TIME-CALL_START_TIME',"
52,"`DTMF` varchar(5) CHARACTER SET latin1 DEFAULT NULL,"
52,"`app_str1` varchar(2048) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,"
52,"`APP_STR2` varchar(2048) CHARACTER SET latin1 DEFAULT NULL,"
52,"`APP_STR3` varchar(2048) CHARACTER SET latin1 DEFAULT NULL,"
52,"`APP_STR4` varchar(80) CHARACTER SET latin1 DEFAULT NULL,"
52,"`APP_STR5` varchar(80) CHARACTER SET latin1 DEFAULT NULL,"
52,"`LAST_HLR_UPDATE` datetime DEFAULT NULL,"
52,"`TTS_STATUS` int(11) DEFAULT 0 COMMENT '0: initial, 1: Converted, 2: Convert failed, 3: Uploaded, 4: Upload Error, 5: Partial Upload Error',"
52,"`TTS_SELECT_LOCK` varchar(100) DEFAULT NULL,"
52,"PRIMARY KEY (`RID`) USING BTREE,"
52,"KEY `campaign` (`CAMPAIGN`),"
52,"KEY `ilock` (`SELECT_LOCK`) USING BTREE,"
52,"KEY `iDestAddr` (`DEST_ADDR`),"
52,"KEY `ix_cp_pending_2` (`DEST_ADDR`,`OUTCOME`),"
52,"KEY `ix_test` (`DEST_ADDR`,`CAMPAIGN`,`OUTCOME`),"
52,"KEY `ix_batch` (`OUTCOME`,`BATCH`),"
52,"KEY `ix_time` (`SELECT_TIME`,`OUTCOME`),"
52,"KEY `ix_mrcp_tts_client` (`TTS_STATUS`,`CAMPAIGN`,`TTS_SELECT_LOCK`)"
52,) ENGINE=InnoDB AUTO_INCREMENT=148583478 DEFAULT CHARSET=utf8mb4
52,CREATE TABLE `cp_batch` (
52,"`BATCH_ID` int(10) NOT NULL AUTO_INCREMENT,"
52,"`BATCH_NAME` varchar(200) NOT NULL,"
52,"`BATCH_STATUS` tinyint(3) unsigned NOT NULL COMMENT 'Batch status indicator. 1-Loading, 2-Active, 3-Aborted, 4-Completed, 5-Suspended, 6-Deleted',"
52,"`CAMPAIGN_ID` int(10) NOT NULL,"
52,"`LIST_ID` int(10) DEFAULT NULL,"
52,"`OPERATION_TIME` timestamp NOT NULL DEFAULT current_timestamp(),"
52,"`INPUT_FILE` varchar(200) DEFAULT NULL COMMENT 'If this batch was created automatically by text file placed in disk folder, the name of that file.',"
52,"`NUM_DESTINATIONS` int(10) unsigned NOT NULL DEFAULT 0 COMMENT 'Total number of destinations used for this batch.',"
52,"`NUM_SUCCESS` int(10) unsigned NOT NULL DEFAULT 0 COMMENT 'Total number of successfull calls',"
52,"`NUM_CANCEL` int(10) unsigned NOT NULL DEFAULT 0 COMMENT 'Total number of canceled calls\n',"
52,"`REMAINING_DESTINATIONS` int(10) unsigned DEFAULT NULL,"
52,"`NUM_UNTRIED` int(10) NOT NULL COMMENT 'Number of destinations to which no call attempt has been made yet.',"
52,"`NUM_BLACKLIST` int(11) NOT NULL DEFAULT 0 COMMENT 'total number of blacklisted calls',"
52,"`COMPLETION_REASON` int(10) unsigned DEFAULT NULL COMMENT 'Indicates how the campaign was completed (end date, max tries, etc.)',"
52,"`DEPLETION_TIME` datetime DEFAULT NULL COMMENT 'The time when all numbers in this batch have been processed. Set by CallMgr.',"
52,"`IS_DEPLETED` int(10) unsigned NOT NULL DEFAULT 0 COMMENT 'Shows if all numbers in this batch have been processed. 1: All numbers were processed 0: No',"
52,"`MIN_RID` bigint(20) NOT NULL DEFAULT 0 COMMENT 'beginning of rid for this campaign',"
52,"`MAX_RID` bigint(20) NOT NULL DEFAULT 0 COMMENT 'last rid for this campaign',"
52,"`CURRENT_RID` bigint(20) NOT NULL DEFAULT 0 COMMENT 'current pointer for this campaign',"
52,"`TRIES_MADE` int(11) NOT NULL DEFAULT 0,"
52,"`START_NOTIF` int(11) NOT NULL DEFAULT 0,"
52,"`IS_EXPANDABLE` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'default:0, initial:1',"
52,"PRIMARY KEY (`BATCH_ID`),"
52,"KEY `ix1` (`BATCH_STATUS`,`IS_DEPLETED`),"
52,"KEY `ix2_campaign` (`CAMPAIGN_ID`),"
52,KEY `batch_index` (`BATCH_NAME`)
52,) ENGINE=InnoDB AUTO_INCREMENT=1588730 DEFAULT CHARSET=latin1
52,"We haven't face with any deadlock yet since yesterday. Do you see any suspect value in ""create table"" parameters ?"
52,Thanks.
52,Share
52,Follow
52,answered May 8 '20 at 10:45
52,user13493257user13493257
52,5144 bronze badges
52,Should your DEFAULT CHARSET be the same for all three tables?
52,Please provide TEXT results from EXPLAIN for one query experiencing deadlocks.
52,– Wilson Hauck
52,May 10 '20 at 13:05
52,Please answer the comment above asking about the DEFAULT CHARSET inconsistency.
52,– Wilson Hauck
52,May 30 '20 at 21:13
52,Add a comment
52,Your Answer
52,"Thanks for contributing an answer to Stack Overflow!Please be sure to answer the question. Provide details and share your research!But avoid …Asking for help, clarification, or responding to other answers.Making statements based on opinion; back them up with references or personal experience.To learn more, see our tips on writing great answers."
52,Draft saved
52,Draft discarded
52,Sign up or log in
52,Sign up using Google
52,Sign up using Facebook
52,Sign up using Email and Password
52,Submit
52,Post as a guest
52,Name
52,Email
52,"Required, but never shown"
52,Post as a guest
52,Name
52,Email
52,"Required, but never shown"
52,Post Your Answer
52,Discard
52,"By clicking “Post Your Answer”, you agree to our terms of service, privacy policy and cookie policy"
52,Not the answer you're looking for? Browse other questions tagged mysql mariadb database-performance database-deadlocks my.cnf
52,or ask your own question.
52,The Overflow Blog
52,Level Up: Creative Coding with p5.js – parts 4 and 5
52,Understanding quantum computing through drunken walks
52,Featured on Meta
52,"Stack Overflow for Teams is now free for up to 50 users, forever"
52,Downvotes Survey results
52,Outdated Answers: results from use-case survey
52,Visit chat
52,Related
52,querying huge database table takes too much of time in mysql
52,Starting MySQL.Manager of pid-file quit without updating fi error
52,303
52,Getting “Lock wait timeout exceeded; try restarting transaction” even though I'm not using a transaction
52,280
52,How to debug Lock wait timeout exceeded on MySQL?
52,Drupal database performance tuning - switching particular tables to InnoDB from MyISAM
52,Optimal my.cnf recommendation
52,(2006) MySQL server has gone away
52,Parameter database MariaDB my.cnf
52,Maria MySQL upgraded from 10.1.x to 10.2.x but now has long semaphore waits
52,Hot Network Questions
52,I am getting the wrong result for north coordinate
52,"In a .desktop file, what is the ""%U"" variable?"
52,Unital *-homomorphisms between matrices
52,"Film where a preternaturally talented hitman falls in love with a woman with the same skills, but she doesn't know it yet"
52,"ATM in Vatican City: ""Inserito scidulam quaeso ut faciundam cognoscas rationem"""
52,Which form of spelling is preferred when publishing for an English speaking audience?
52,What's stopping me from inventing a planet with oceans of mercury?
52,AES GCM : is it acceptable to return the wrong plaintext if the tag is incorrect?
52,Z80 to x86 asm translator?
52,What are possible applications of deep learning to research mathematics
52,Access argument passed to environment with etoolbox \AtBeginEnvironment
52,"Singing hit the pitch, but it still sounds terrible. What is my problem? How I can solve it?"
52,Would it be advisable to email a potential employer letting them know that you are about to take another offer?
52,Implementation of the trigonometric functions for real time application
52,"Possible meanings of ""IN CASE OF FIRE, DO NOT USE ELEVATOR."""
52,Suspicious GitHub fork
52,"How does the nonsense word ""frabjous"" conform to English phonotactics?"
52,Is an ECS viable in garbage collected languages?
52,What is the origin of the idea that moral realism requires a god?
52,"Why are countries consistently ranked across ""nice"" lists?"
52,Calculate Home Primes
52,Idiom meaning inferring too much from the available evidence
52,"Company about to send me an offer, but they changed their mind at the last moment"
52,How likely is a lack of DNA match with a distant relative?
52,more hot questions
52,Question feed
52,Subscribe to RSS
52,Question feed
52,"To subscribe to this RSS feed, copy and paste this URL into your RSS reader."
52,lang-sql
52,Stack Overflow
52,Questions
52,Jobs
52,Developer Jobs Directory
52,Salary Calculator
52,Help
52,Mobile
52,Products
52,Teams
52,Talent
52,Advertising
52,Enterprise
52,Company
52,About
52,Press
52,Work Here
52,Legal
52,Privacy Policy
52,Terms of Service
52,Contact Us
52,Cookie Settings
52,Cookie Policy
52,Stack Exchange Network
52,Technology
52,Life / Arts
52,Culture / Recreation
52,Science
52,Other
52,Stack Overflow
52,Server Fault
52,Super User
52,Web Applications
52,Ask Ubuntu
52,Webmasters
52,Game Development
52,TeX - LaTeX
52,Software Engineering
52,Unix & Linux
52,Ask Different (Apple)
52,WordPress Development
52,Geographic Information Systems
52,Electrical Engineering
52,Android Enthusiasts
52,Information Security
52,Database Administrators
52,Drupal Answers
52,SharePoint
52,User Experience
52,Mathematica
52,Salesforce
52,ExpressionEngine® Answers
52,Stack Overflow em Português
52,Blender
52,Network Engineering
52,Cryptography
52,Code Review
52,Magento
52,Software Recommendations
52,Signal Processing
52,Emacs
52,Raspberry Pi
52,Stack Overflow на русском
52,Code Golf
52,Stack Overflow en español
52,Ethereum
52,Data Science
52,Arduino
52,Bitcoin
52,Software Quality Assurance & Testing
52,Sound Design
52,Windows Phone
52,more (28)
52,Photography
52,Science Fiction & Fantasy
52,Graphic Design
52,Movies & TV
52,Music: Practice & Theory
52,Worldbuilding
52,Video Production
52,Seasoned Advice (cooking)
52,Home Improvement
52,Personal Finance & Money
52,Academia
52,Law
52,Physical Fitness
52,Gardening & Landscaping
52,Parenting
52,more (10)
52,English Language & Usage
52,Skeptics
52,Mi Yodeya (Judaism)
52,Travel
52,Christianity
52,English Language Learners
52,Japanese Language
52,Chinese Language
52,French Language
52,German Language
52,Biblical Hermeneutics
52,History
52,Spanish Language
52,Islam
52,Русский язык
52,Russian Language
52,Arqade (gaming)
52,Bicycles
52,Role-playing Games
52,Anime & Manga
52,Puzzling
52,Motor Vehicle Maintenance & Repair
52,Board & Card Games
52,Bricks
52,Homebrewing
52,Martial Arts
52,The Great Outdoors
52,Poker
52,Chess
52,Sports
52,more (16)
52,MathOverflow
52,Mathematics
52,Cross Validated (stats)
52,Theoretical Computer Science
52,Physics
52,Chemistry
52,Biology
52,Computer Science
52,Philosophy
52,Linguistics
52,Psychology & Neuroscience
52,Computational Science
52,more (10)
52,Meta Stack Exchange
52,Stack Apps
52,API
52,Data
52,Blog
52,Facebook
52,Twitter
52,LinkedIn
52,Instagram
52,site design / logo © 2021 Stack Exchange Inc; user contributions licensed under cc by-sa.
52,rev 2021.4.14.39087
52,Stack Overflow works best with JavaScript enabled
52,Your privacy
52,"By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy."
52,Accept all cookies
52,Customize settings
54,Progress® DataDirect®
54,<![endif]-->
54,Progress DataDirect Connect Series for ODBC: Version 7.1.6
54,Panel Progress
54,Table of Contents
54,Index
54,odbc
57,Optimizing Performance - MythTV Official Wiki
57,Optimizing Performance
57,From MythTV Official Wiki
57,"Jump to:					navigation, 					search"
57,This HOWTO aims to collect the multitude of tips regarding optimizing performance of your system for use with MythTV.
57,Contents
57,1 File Systems
57,1.1 Local File Systems
57,1.1.1 Put the database on a separate spindle
57,1.1.2 General Tips For Any File System
57,1.1.2.1 Combat Fragmentation
57,1.1.2.2 Disabling File Access Time Logging
57,"1.1.2.3 Using ""relatime"" Mount Option"
57,1.1.3 XFS-Specific Tips
57,1.1.3.1 Combat Fragmentation
57,1.1.3.2 Optimizing XFS on RAID Arrays
57,1.1.3.2.1 Examples
57,1.1.3.3 Further Information
57,1.2 Network File Systems
57,1.2.1 Disable NFS file attribute caching
57,1.2.2 NFS servers
57,1.3 RemoteFS
57,2 Devices
57,2.1 Capture Cards
57,2.2 Video Cards & Hardware Accelerated Video
57,2.2.1 VDPAU
57,2.2.2 VAAPI
57,2.2.3 CrystalHD
57,2.3 CPU / Processor
57,2.3.1 Clock Speed Throttling
57,3 Networks
57,3.1 Wireless Networks
57,3.2 Ethernet Full-Duplex Mode
57,4 Operating System
57,4.1 Kernel Selection
57,4.2 Kernel Configuration
57,4.2.1 Processor Family
57,4.2.2 IDE Controller
57,4.2.3 Preemptible Kernel
57,4.2.4 Timer Frequency
57,4.3 Realtime Threads
57,4.4 PCI Latency
57,4.5 RTC Maximum Frequency
57,4.6 Linux Distribution Selection
57,5 Other Software
57,5.1 MythTV
57,5.1.1 Multiple Machines
57,5.1.2 Frontend Playback Profiles
57,5.1.3 Recording Settings
57,5.1.4 Mythfilldatabase Scheduling
57,5.2 MySQL Database Tweaks
57,5.3 XORG CPU Hogging
57,5.4 Lightweight Window Managers
57,File Systems
57,Local File Systems
57,Put the database on a separate spindle
57,MythTV reads and writes large video files and it reads and writes small bits of metadata from a database. The small bits of metadata are accessed frequently enough that the seeks can more than halve the overall performance of access to the large video files MythTV also uses. Past the experimentation point it makes a lot of sense to allocate one small disk to the OS and the database and use a separate drive or drives for everything else. The database is generally under /var unless you have moved it.
57,General Tips For Any File System
57,Combat Fragmentation
57,"Fragmentation happens when the data placement of files is not contiguous on disk, causing time-consuming head seeks when reading or writing a file."
57,"MythTV recordings on disk can become quite fragmented, due to several factors, such as the fact that MythTV writes large files over a very long period of time, the fact that recording files may have drastically different sizes, and the fact that many MythTV systems have multiple capture cards--allowing for recording multiple shows at once."
57,"Note, also, that any time MythTV is recording multiple shows to a single filesystem (even if in different directories and/or in different Storage Groups), the recordings will necessarily be fragmented."
57,"Configuring multiple local filesystems within MythTV's Storage Groups will allow MythTV to write recordings to separate filesystems, thereby minimizing fragmentation."
57,"Therefore, the best approach to combat fragmentation is to ensure each computer running mythbackend has at least as many local (and available) filesystems as capture cards."
57,"If using a combination of local and network-mounted filesystems, you may need to adjust the Storage Groups Weighting to cause MythTV to write to network-mounted filesystems (though doing so may negatively impact performance, meaning the use of a sufficient number of local filesystems or the use of only network-mounted filesystems is preferred)."
57,"The availability of a filesystem is somewhat dependent on that filesystem having space available for writing (i.e. having 2 filesystems for 2 capture cards with one filesystem completely full and the other only half full will not help prevent fragmentation, though if both are full, autoexpiration should allow either to be used)."
57,"Fragmentation can be measured by the ""filefrag"" command on most any filesystem."
57,Disabling File Access Time Logging
57,Most filesystems log the access times of files.
57,"Generally this file metadata shouldn't be necessary, however, if for some strange reason you experience problems, then don't apply this tweak."
57,"To disable the logging of file access times, add the ""noatime"" and ""nodiratime"" options to your /etc/fstab:"
57,# 1.5 TB RAID 5 array. Large file optimization: 64m of prealloc
57,# NO logging of access times: improves performance
57,# NO block devices or suid progs allowed: improves security
57,/dev/md0
57,/terabyte
57,"xfs defaults,noatime,nodiratime,nosuid,nodev,allocsize=64m 0"
57,"If you get something like the following, the mount option is not supported for your filesystem:"
57,"mount: wrong fs type, bad option, bad superblock on /dev/md0,"
57,"missing codepage or helper program, or other error"
57,In some cases useful info is found in syslog - try
57,dmesg | tail
57,or so
57,# dmesg | tail would return something like:
57,YOUR_FILESYSTEM_TYPE: unknown mount option [noatime].
57,"Using ""relatime"" Mount Option"
57,"You may also wish to look into the ""relatime"" mount option to improve performance, but still have file access times updated."
57,This is an alternative to using noatime and nodiratime.
57,"For more information on this (and related discussion), see: Linux: Replacing atime With relatime"
57,XFS-Specific Tips
57,Combat Fragmentation
57,"Under XFS, an additional command can be used to measure filesystem fragmentation: ""xfs_bmap""."
57,The xfs filesystem has a mount option which can help combat this fragmentation: allocsize
57,allocsize=size
57,Sets the buffered I/O end-of-file preallocation size when
57,doing delayed allocation writeout (default size is 64KiB).
57,Valid values for this option are page size (typically 4KiB)
57,"through to 1GiB, inclusive, in power-of-2 increments."
57,"This can be added to /etc/fstab, for example:"
57,/dev/sd1
57,/video
57,xfs
57,"defaults,allocsize=64m 0 0"
57,"This essentially causes xfs to speculatively preallocate 64m of space at a time for a file when writing, and can greatly reduce fragmentation. MythTV syncs the file it is writing to disk regularly to prevent the filesystem for freezing up for long periods of time writing large chunks of data that MythTV is generating and so preventing smooth simultaneous playback of the same or different file from that filesystem."
57,"For files which are already heavily fragmented, the xfs_fsr command (from the xfsdump package) can be used to defragment individual files, or an entire filesystem."
57,Run the following command to determine how fragmented your filesystem is:
57,xfs_db -c frag -r /dev/sda1
57,xfs_fsr with no parameters will run for two hours. The -t parameter specifies how long it runs in seconds. It keeps track of where it got up to and can be run repeatedly. It can be added to our crontab to periodically defragment your disk. Add the following to /etc/crontab:
57,30 1 * * * root /usr/sbin/xfs_fsr -t 21600 >/dev/null 2>&1
57,to run it every night at 1:30 for 6 hours.
57,"Don't forget to see the complete XFS_Filesystem wiki page that includes general info about XFS, defragmenting, disk checking and maintenance, etc."
57,Optimizing XFS on RAID Arrays
57,Some more RAID specific tweaks for XFS were found in this helpful article: Optimizing XFS on RAID Arrays.
57,This section is a slightly reformatted version of that article.
57,"Please note the author of that article incorrectly used the term ""block size"" in some places when he really meant ""stripe size"" or ""chunk size""."
57,block size refers to the filesystem's unit of data transfer.
57,This is set at format time for the filesystem.
57,"The default value is 4096 bytes (4 KiB), the minimum is 512, and the maximum is 65536 (64 KiB)."
57,XFS on Linux
57,currently only supports pagesize or smaller blocks.
57,chunk size or stripe size refers to the RAID array's unit of data transfer.
57,"This is set during RAID array creation time for the array (in software raid, the --chunk=X option to mdadm)."
57,"For me, mkfs.xfs complained when using chunk size=256KB, and block size=4096 bytes and specifying a sunit & swidth calculated using the block size."
57,The values it mentions are correct if calculated using the chunk size.
57,"Therefore, this section assumes sunit & swidth calculated using chunk size."
57,"If you are having trouble, try my"
57,script.
57,XFS has builtin optimizations for reading data from RAID arrays.
57,These options can be specified at mkfs time or at mount time (you can even set them while the system is running using the mount -o remount command) and can affect the performance of your system.
57,"There are two parameters for tweaking how XFS handles your RAID arrays (there are actually four, but you only need to use these two): sunit and swidth."
57,sunit is the stripe unit and swidth is the stripe width.
57,The stripe unit sits on a single disk while the stripe width spans the entire array; in this way the sunit is similar to the stripe size of your array.
57,"Before you begin, you’ll need to know:"
57,What type of RAID array you’re using
57,The number of disks in the array
57,The stripe size (aka the chunk size in software RAID).
57,"For RAID{1,0,10} arrays, the number of “disks” is equal to the number of spindles."
57,"For RAID{5,6} arrays, the number of disks is equal to N-1 for RAID5 and N-2 for RAID6, where N is the number of spindles."
57,"If you guessed that the sunit is equal to your stripe size, you’re almost correct."
57,"The sunit is measured in 512-byte block units (from the mount man page), so for a 64kB stripe size your sunit=128, for 256kB use sunit=512."
57,"As mentioned before, the swidth spans the entire array, but is also measured in 512-byte blocks, so you’ll want to multiply the number of disks calculated above by the sunit for your stripe size."
57,Note:
57,"If you have not formatted your xfs partition yet, you may set a blocksize in mkfs.xfs using the -b size=X option."
57,The default is usually 4096 bytes (4 KiB) on most systems. (Remember blocksize is limited to your system's memory pagesize. blocksize <= pagesize).
57,"If already created, use the following command, and look for bsize=X in output:"
57,# replace /dev/md0 with your device's name
57,xfs_info /dev/md0
57,Examples
57,Calculate the correct values for your system using these examples as a guideline.
57,A 4-disk RAID0 array with a 64kB stripe size will have a sunit of 128 and a swidth of 4*128=512.
57,Your mkfs.xfs and mount commands would thus be:
57,#Note that you should only need to use one of these.
57,You may also add the sunit and swidth options to /etc/fstab to make the second one permanent.
57,"mkfs.xfs -d sunit=128,swidth=512 /dev/whatever"
57,"mount -o remount,sunit=128,swidth=512"
57,An 8-disk RAID6 array with a 256kB stripe size will have a sunit of 512 and a swidth of (8-2)*512=3072.
57,Your commands would thus be:
57,"mkfs.xfs -d sunit=512,swidth=3072 /dev/whatever"
57,"mount -o remount,sunit=512,swidth=3072"
57,Further Information
57,"If you are having trouble figuring this out (as I did at first), here is a useful bash script to help you."
57,"It only requires that you have the ""bc"" bash calculator program installed."
57,"To get it in Ubuntu, use ""sudo apt-get install bc""."
57,"Put the following in a text editor, and chmod +x it, tweak values to match your system and run."
57,#!/bin/bash
57,BLOCKSIZE=4096 # Make sure this is in bytes
57,CHUNKSIZE=256
57,# Make sure this is in KiB
57,NUMSPINDLES=8
57,RAID_TYPE=6
57,"RAID_DEVICE_NAME=""/dev/md0"" # Specify device name for your RAID device"
57,"FSLABEL=""mythtv"" # specify filesystem label for generating mkfs line here"
57,"case ""$RAID_TYPE"" in"
57,RAID_DISKS=${NUMSPINDLES};
57,RAID_DISKS=${NUMSPINDLES};
57,10)
57,RAID_DISKS=${NUMSPINDLES};
57,"RAID_DISKS=`echo ""${NUMSPINDLES} - 1"" | bc`;"
57,"RAID_DISKS=`echo ""${NUMSPINDLES} - 2"" | bc`;"
57,"echo ""Please specify RAID_TYPE as one of: 0, 1, 10, 5, or 6."""
57,exit
57,esac
57,"SUNIT=`echo ""${CHUNKSIZE} * 1024 / 512"" | bc`"
57,"SWIDTH=`echo ""$RAID_DISKS * ${SUNIT}"" | bc`"
57,"echo ""System blocksize=${BLOCKSIZE}"""
57,"echo ""Chunk Size=${CHUNKSIZE} KiB"""
57,"echo ""NumSpindles=${NUMSPINDLES}"""
57,"echo ""RAID Type=${RAID_TYPE}"""
57,"echo ""RAID Disks (usable for data)=${RAID_DISKS}"""
57,"echo ""Calculated values:"""
57,"echo ""Stripe Unit=${SUNIT}"""
57,"echo -e ""Stripe Width=${SWIDTH}\n"""
57,"echo ""mkfs line:"""
57,"echo -e ""mkfs.xfs -b size=${BLOCKSIZE} -d sunit=${SUNIT},swidth=${SWIDTH} -L ${FSLABEL} ${RAID_DEVICE_NAME}\n"""
57,"echo ""mount line:"""
57,"echo -e ""mount -o remount,sunit=${SUNIT},swidth=${SWIDTH}\n"""
57,"echo ""Add these options to your /etc/fstab to make permanent:"""
57,"echo ""sunit=${SUNIT},swidth=${SWIDTH}"""
57,Please refer to the following references for more details and other useful tweaks to improve the performance of your XFS filesystem:
57,Filesystem Performance Tweaking with XFS
57,Optimizing XFS on RAID Arrays
57,Network File Systems
57,Disable NFS file attribute caching
57,"if you are using SMB (not CIFS), you can try the ttl option using ""-o ttl=100"" which should set your timeout lower than the default."
57,"The default is supposed to be 1000ms which equals 1 second, but one user has reported that setting ttl=100 corrected the issue for him, so SMB users can give it a try."
57,NFS servers
57,"Ensure that your NFS server is running in 'async' mode (configured in /etc/exports). The default for many NFS servers is 'async', but recent versions of debian now default to 'sync', which can result in very low throughput and the dreaded ""TFW, Error: Write() -- IOBOUND"" errors. Example of setting async in /etc/exports:"
57,/mnt/store
57,"192.168.1.3/32(rw,async,udp)"
57,"There are a few other NFS mount options that can help, such as ""intr"", ""nfsvers=3"", ""actimeo=0"" , ""noatime"" and ""tcp""/""udp""."
57,"You can read the man pages for a more detailed description, but suggestions are below."
57,"nfsvers=3 - This tells the client to use NFS v3, which is better than NFS v2."
57,"Of course, the server has to also support it."
57,actimeo=0 - disable this attribute caching to allow the frontend to see updates from the backend quicker.
57,The problem has been seen where LiveTV fails to transition from one program to another.
57,The cache file attribute prevents the frontend from opening the new file promptly.
57,This also causes more load on the server if that is a issue.
57,"tcp - This tells NFS to use TCP instead of UDP. It has been reported to improve performance for some, but has also caused repeated 3-5 second filesystem freeze-ups for others. If you have a network with only 1000-mbit clients or suffer performance problems with udp, you can try this. Poor performance with tcp may be improved by setting rsize and wsize to appropriate values (usuall 32KB)."
57,"udp - This tell NFS to use UDP instead of TCP. This is the traditional mechanism for NFS to utilize. For networks containing wifi or 100-mbit clients this is probably the best option. If you get video stuttering with either tcp or udp, try the other one."
57,"rsize=32768,wsize=32768 - These tell your nfs client to use a particular block size, 32KB in this case."
57,"Modern NFS clients auto-negotiate a block size so this isn't really necessary for udp where a block size of 32KB is generally auto-negotiated. However with tcp the auto-negotiated block size can be too large resulting in very high latency. On Linux, check the output of /proc/mounts and if rsize or wsize depart very far from 32KB, you probably do want to set this."
57,intr - Makes I/O to a NFS mounted filesystem interuptable if the server is down.
57,If not given the I/O becomes a uninteruptable sleep which causes the process to be impossible to kill until the server comes up again. This is a no-op on newer Linux kernels and you must use async instead.
57,"async - Like intr this allows you to kill a process that has a file open on an unresponsive NFS server. On Linux this should always be used for A/V only volumes, unless you are using a kernel that still supports intr. Otherwise, you can end up with a permanently hung backend or frontend if a remote volume goes down for some reason."
57,"soft - If the NFS server becomes unavailable the NFS client will generate ""soft"" errors instead of hanging. Some software will handle this well, other much less well. In the later case file corruption will result. For a file system solely used for A/V data this is safe and can avoid the a frontend or backend entering uninterruptible sleep."
57,Example /etc/fstab entry:
57,"server:/mythtv/rec0 /mythtv/rec0 nfs async,nfsvers=3,actimeo=0,tcp,rsize=32768,wsize=32768"
57,"server:/mythtv/rec1 /mythtv/rec1 nfs async,nfsvers=3,actimeo=0,udp"
57,RemoteFS
57,This is a fuse based file system that may be well suited to providing network access at remote frontends typically required. I have utilised this on my own setup and have found it to be faster / more responsive than the previous NFS setup I was using (regardless of options used above). NFS provides lots of sophisticated features for handling large numbers of users accessing via different types of network links and therefore latencies etc. these features are unlikely to be of any benefit for typically LAN connected systems especially as they are often just ro.
57,The author appears to have done quite a lot of performance testing which you can see here https://sourceforge.net/apps/mediawiki/remotefs/index.php?title=Development:Performance_Tests Certainly worthy of testing / review IMHO.
57,Devices
57,Capture Cards
57,"For backend machines, or machines that are a combination frontend/backend, the type of capture card used will impact performance."
57,"With a typical analog capture card, such as the popular bttv cards, the CPU must encode the raw video to MPEG-4 or RTjpeg on the fly."
57,"When watching live TV on a combination frontend/backend machine, the machine has to both encode AND decode the video stream simultaneously."
57,Luckily there are two options:
57,"Hardware MPEG-2 Capture Cards, such as the popular Hauppauge PVR-150 and PVR-500."
57,"Digital tuners, such as the pcHDTV HD-5500, which work with both OTA 8VSB signals as well as QAM for digital cable systems."
57,"With cards of this type, the machine's CPU doesn't have to encode the incoming video."
57,"Instead, it simply receives the MPEG-2 stream from the card and dumps it to disk."
57,"This makes the recording process a simple operation, with relatively low resource usage."
57,Video Cards & Hardware Accelerated Video
57,Several options are available for accelerating video output:
57,VDPAU
57,"VDPAU is currently NVIDIA-only for the time being, but provides for GPU-accelerated decode of MPEG-1, MPEG-2, H.264, and VC-1 bitstreams, as well as post-processing of decoded video including temporal and spatial deinterlacing, inverse telecine, and noise reduction."
57,VAAPI
57,CrystalHD
57,CPU / Processor
57,Clock Speed Throttling
57,There are several conditions in which your computer's CPU may be scaled down from its maximum clock speed:
57,A laptop or notebook has scaled down the CPU automatically due to being unplugged from an AC power source and running on the battery
57,"The system has detected an unsafe thermal condition, and has scaled back the clock speed to avoid damage"
57,The CPU speed has been configured incorrectly in the BIOS
57,The CPU speed has been manually configured to a lower speed at runtime
57,You can check your CPU's current operating frequency by running the command:
57,cat /proc/cpuinfo
57,"If your system is slowing down because it is at its thermal limits, the only real option is to beef up your cooling capacity."
57,"This could be in the form of a larger heatsink, a larger fan, or even liquid cooling."
57,"A CPU that is incorrectly configured in the BIOS should be easy to check and easy to fix, but take care that you don't unintentionally overclock it in the process."
57,"Changing a manual control or overriding an automatic speed control will likely be distribution-dependent, or subject to your choice of adjustment tools."
57,Networks
57,Wireless Networks
57,"While it is possible to run MythTV over a wireless network, you may find that you have better performance when using a wired connection."
57,"With a wireless connection, your bandwidth & latency are dependent on your distance from the access point, interference from other devices, the number of wireless users on the network, and the capabilities of your equipment at both ends."
57,"If you find that you have trouble with skips and/or dropouts when watching content on a wireless front end, it would be good to test the same setup with a wired connection to determine if the network is the problem."
57,Ethernet Full-Duplex Mode
57,Make sure that your ethernet adapters are running in full duplex mode.
57,Check your current configuration with this command:
57,ethtool eth0
57,Typically both sides will be configured for autonegotiation by default and you will get the best possible connection automatically but there are conditions--typically involving old or buggy hardware--when this may not happen.
57,"The following can be used to disable autonegotiation and force a 100base-T network adapter into full duplex mode, when autonegotiation is failing."
57,ethtool -s eth0 speed 100 duplex full autoneg off
57,"This problem can exhibit itself with ""IOBOUND"" errors in your logs."
57,"Note: To use full-duplex mode, your network card must be connected to a switch (not a hub) and the switch must be configured to allow full-duplex operation (almost always the default) on the ports that are being used."
57,"By definition, a network switch supports full duplex operation and a network hub (sometimes referred to as a repeater) does not."
57,"If you are connecting to a hub, full-duplex operation will not be possible."
57,"Most switches support using 100base-T (Fast Ethernet) as well as 10base-T, while most hubs will only use 10base-T, and while a few 100base-T hubs (and 10base-T switches) do exist, they are quite rare."
57,Gigabit switches can reliably be expected to handle both fast ethernet and normal ethernet connections in addition to the gigabit ethernet speeds.
57,Do not disable autonegotiation if things are currently working correctly.
57,"This will only create new problems, not prevent future ones."
57,Forced connections can't advertise what they are capable of so the autonegotiating side must assume half-duplex.
57,You will actually be creating a problem if the now forced connection was already full-duplex.
57,It should be noted that most consumer-level switches and home routers do not support manual port configuration and this will result in them selecting a half-duplex connection if the remote end is no longer participating in connection negotiation.
57,"Nearly all of the time, using autonegotiation on all of the equipment will give you the best possible results."
57,If you encounter problems with autonegotiation you can opt to manually configure settings for that device but it is highly recommended that you manually configure every piece of equipment on that segment as well.
57,Operating System
57,Kernel Selection
57,Many Linux distributions have alternative pre-compiled kernels that you can use.
57,"Depending on how sensitive your setup is to latency, you can test out different ones to see if they solve your issues and reduce latency caused by the kernel scheduler."
57,Decoding and playback on low powered machines are more sensitive to latency.
57,This article on Ubuntu describe various different kernels.
57,Kernel Configuration
57,"If you're compiling your own kernel, you might want to try out the following options:"
57,Processor Family
57,"Ensure that the ""Processor Family"" (in ""Processor Type and Features"") is configured correctly."
57,IDE Controller
57,"Ensure that the correct IDE controller is set (in ""Device Drivers->ATA/ATAPI support->PCI IDE chipset support"")."
57,"There is a generic IDE controller driver in the kernel that will handle many different chipsets, but it's performance is sub-par."
57,Preemptible Kernel
57,Kernel preemption allows high priority threads to interrupt even kernel operations -- this ensures the lowest possible latency when responding to important events. (Note: apparently some IVTV drivers show stability problems with a preemptible kernel.)
57,Timer Frequency
57,"Increasing the scheduler's timer frequency to 1000Hz can reduce latency between multiple threads of execution (at a small cost to overall performance), e.g. when recording/playing multiple video streams."
57,Generally you will want to pick 300Hz which is neatly divisible by both 50Hz (PAL) and 60Hz (NTSC) because of the frame rates involved in displaying your media.
57,"On some machines you may hear an annoying high-pitched ""whistle"": reduce the frequency to 250Hz or lower to avoid this."
57,Realtime Threads
57,"The mythfrontend & mythtv threads can be configured to run with ""realtime"" priorities - if the frontend is configured this way, and if sufficient privileges are available to the user running mythfrontend."
57,"The HOWTO has an excellent section on how to set your system up to enable this (look for ""Enabling real-time scheduling of the display thread."")"
57,"You will also need to select ""Enable Realtime Priority Threads"" in the General Playback frontend setup dialogue."
57,"Realtime threads can help smooth out video and audio, because the system scheduler gives very high priority to mythtv."
57,"For more information on how this works, see the Real-Time chapter in Robert Love's great Linux Kernel Development book."
57,PCI Latency
57,Incorrect or less-than-optimal settings of PCI Latency can cause performance-related problems. See the page PCI Latency
57,RTC Maximum Frequency
57,See Adjusting_the_RTC_Interrupt_Frequency
57,Linux Distribution Selection
57,"At a more fundamental level, your choice of a Linux distribution can have a large impact on the overall performance of your Myth machine."
57,"Most ""modern"" distributions (Fedora, Ubuntu, etc) come with default installations intended to give the best initial user experience by providing support for scores of devices & programs, with automation wherever possible."
57,"The downside to this, is that these default installations have large kernels and large numbers of background processes running to support this usage."
57,"While any distribution can be whittled down to meet a more focused need, it takes an effort to do so."
57,"An alternative approach, is to select a distribution such as Gentoo that provides you with a blank slate by default. This allows you to add only the components you need, ensuring a clean system with minimal effort."
57,Other Software
57,MythTV
57,Multiple Machines
57,One great feature of the MythTV architecture is that the recording and playback functions are split between two applications - the backend and the frontend.
57,"While they can both be run on the same machine, one of the easiest performance boosts is to simply split these tasks between two machines."
57,"If desired, it is even possible to set up an additional backend machine to assist with recording and/or commercial flagging & transcoding tasks."
57,"This sort of arrangement may be beneficial if the backend machines are low-power (unable to keep up with the transcoding jobs), and is a good way to ensure that post-processing operations do not interfere with active recordings."
57,Frontend Playback Profiles
57,The choice of an appropriate playback profile can make a huge difference in the perceived performance of your MythTV frontend.
57,"The playback profile decides which video decoder will be used, how the on-screen display is rendered, and which video filters (deinterlacing, etc) are used."
57,"The playback profile also dictates how hardware acceleration is used, which is especially important on low-end PCs or machines processing HD content."
57,Recording Settings
57,"Part of ""optimizing"" is determining what you actually need, not just making something as good as it can get."
57,"For example, if you only watch recordings on your iPod and nothing else, you might as well configure your tuners to record TV at 320x240 resolution."
57,"Doing this will allow faster processing (commercial removal, etc), and the reduced file sizes will let you store more videos and copy them to/from your devices faster."
57,"Likewise, if you intend to burn a significant number of your recordings to DVD, you could save your backend a lot of work by saving your recordings directly to a DVD-compliant resolution, and in DVD-compliant MPEG-2 if your capture card supports it. (More information is available in the MythArchive page.)"
57,"Even if you watch your recordings from a frontend on your TV, you may still find it worthwhile to play with the recording settings. You may find that a lower audio bitrate eliminates hiss in the audio track, or that a lower resolution with an equivalent bitrate produces fewer objectionable video artifacts."
57,"Or, if your frontend isn't very powerful, or your network is congested, a lower bitrate may help make smooth video possible where it otherwise would not have been."
57,Mythfilldatabase Scheduling
57,"By default, Mythfilldatabase runs automatically every 24 hours to keep your listings up to date."
57,"Mythfilldatabase is known for being able to saturate I/O systems (See Troubleshooting:Mythfilldatabase_IO_bottleneck), which can cause problems on heavily used or low-power backends."
57,"If you have recording or playback issues during the default script timeslot (2AM-5AM), you can manually adjust the script's schedule via the frontend setup menu to better suit your particular usage."
57,"Running the database and MySQL on a different machine is another way to alleviate these issues, to ensure consistent performance."
57,MySQL Database Tweaks
57,MySQL v8 (Ubuntu 20.04+) see *** below Deprecations
57,Taken from this thread in mythtv-users.
57,Add the following to the [mysqld] section of /etc/my.cnf to see improvements in database speed for MythTV as well as MythWeb. Check your default values using 'mysql> show global variables;'
57,key_buffer = 48M
57,max_allowed_packet = 8M
57,table_cache = 128 # this setting is deprecated in mysql 5.6.23 and will prevent mysql from starting
57,sort_buffer_size = 48M
57,net_buffer_length = 1M
57,thread_cache_size = 4
57,query_cache_type = 1 ***
57,query_cache_size = 4M ***
57,"N.B. Turning on the query_cache (query_cache_type=1) can cause problems with if you have a newer server (>=5.7) and mixed clients (some below 5.7, some above)."
57,"For example, the current Raspbian distro (Jessie) only ships with a v5.5 MySQL client."
57,See this MySQL bug.
57,There are also example my.cnf files included with your MySQL installation that have suggested values based on the amount of memory your system has.
57,Information about them can be found in the MySQL documentation.
57,There is a great Perl script available at mysqltuner.pl. It will look through many of the MySQL server settings and report on variables that need to be changed. Hints on usage [1]
57,XORG CPU Hogging
57,"Under some circumstances, X can use huge amounts of CPU. This may be fixed in some cases by increasing its priority above the base value of 0 (i.e. to a negative value). e.g. renice -2 [pid for X]."
57,"If you must renice a process, do so in small steps."
57,Raising applications above the priority of mechanisms like kjournald or ksoftirqd can have adverse side-effects.
57,"A second way of lowering Xorg CPU usage (especially when decoding is accelerated with XvMC or VDPAU) with nVidia cards, is to add"
57,Option
57,"""UseEvents"""
57,"""True"""
57,to the Device section of your Xorg.conf.
57,"(warning: although this works well for watching hd content, it's considered unstable for 3D software like gaming, etc... )"
57,Lightweight Window Managers
57,"While KDE & Gnome provide for a nice user experience, they also bring along a lot of baggage which is unnecessary for a dedicated Myth machine."
57,"Switching to a lightweight window manager such as WindowMaker,Fluxbox, or Ratpoison will reduce startup times and give you more available system resources at runtime."
57,"Retrieved from ""http://www.mythtv.org/wiki?title=Optimizing_Performance&oldid=65284"""
57,Category: HOWTO
57,Navigation menu
57,Personal tools
57,Log in
57,Namespaces
57,Page
57,Discussion
57,Variants
57,Views
57,Read
57,View source
57,View history
57,More
57,Search
57,Wiki Navigation
57,Main pageRecent changesRandom pagepopularpagesWanted pagesHelp
57,Official Resources
57,Official WebsiteOfficial ForumOfficial DocumentationUser ManualIRC FAQMailing List ArchivesBug TrackerSource Code Repository
57,Tools
57,What links hereRelated changesSpecial pagesPermanent linkPage information
57,Print/export
57,Create a bookDownload as PDFPrintable version
57,"This page was last modified on 6 May 2020, at 20:34."
57,Privacy policy
57,About MythTV Official Wiki
57,Disclaimers
58,Example: Deploying WordPress and MySQL with Persistent Volumes | KubernetesExample: Deploying WordPress and MySQL with Persistent Volumes | KubernetesDocumentationKubernetes BlogTrainingPartnersCommunityCase StudiesVersionsv1.21
58,v1.20
58,v1.19
58,v1.18
58,v1.17English中文 Chinese
58,한국어 Korean
58,日本語 Japanese
58,HomeAvailable Documentation VersionsGetting startedRelease notes and version skewv1.21 Release Notes
58,Kubernetes version and version skew support policyLearning environmentProduction environmentContainer runtimesInstalling Kubernetes with deployment toolsBootstrapping clusters with kubeadmInstalling kubeadm
58,Troubleshooting kubeadm
58,Creating a cluster with kubeadm
58,Customizing control plane configuration with kubeadm
58,Options for Highly Available topology
58,Creating Highly Available clusters with kubeadm
58,Set up a High Availability etcd cluster with kubeadm
58,Configuring each kubelet in your cluster using kubeadm
58,Dual-stack support with kubeadmInstalling Kubernetes with kops
58,Installing Kubernetes with KubesprayTurnkey Cloud SolutionsWindows in KubernetesIntro to Windows support in Kubernetes
58,Guide for scheduling Windows containers in KubernetesBest practicesConsiderations for large clusters
58,Running in multiple zones
58,Validate node setup
58,PKI certificates and requirementsConceptsOverviewWhat is Kubernetes?
58,Kubernetes Components
58,The Kubernetes APIWorking with Kubernetes ObjectsUnderstanding Kubernetes Objects
58,Kubernetes Object Management
58,Object Names and IDs
58,Namespaces
58,Labels and Selectors
58,Annotations
58,Field Selectors
58,Recommended LabelsCluster ArchitectureNodes
58,Control Plane-Node Communication
58,Controllers
58,Cloud Controller ManagerContainersImages
58,Container Environment
58,Runtime Class
58,Container Lifecycle HooksWorkloadsPodsPod Lifecycle
58,Init Containers
58,Pod Topology Spread Constraints
58,Disruptions
58,Ephemeral ContainersWorkload ResourcesDeployments
58,ReplicaSet
58,StatefulSets
58,DaemonSet
58,Jobs
58,Garbage Collection
58,TTL Controller for Finished Resources
58,CronJob
58,"ReplicationControllerServices, Load Balancing, and NetworkingService"
58,Topology-aware traffic routing with topology keys
58,DNS for Services and Pods
58,Connecting Applications with Services
58,Ingress
58,Ingress Controllers
58,EndpointSlices
58,Service Internal Traffic Policy
58,Topology Aware Hints
58,Network Policies
58,Adding entries to Pod /etc/hosts with HostAliases
58,IPv4/IPv6 dual-stackStorageVolumes
58,Persistent Volumes
58,Volume Snapshots
58,CSI Volume Cloning
58,Storage Classes
58,Volume Snapshot Classes
58,Dynamic Volume Provisioning
58,Storage Capacity
58,Ephemeral Volumes
58,Node-specific Volume Limits
58,Volume Health MonitoringConfigurationConfiguration Best Practices
58,ConfigMaps
58,Secrets
58,Managing Resources for Containers
58,Organizing Cluster Access Using kubeconfig Files
58,Pod Priority and PreemptionSecurityOverview of Cloud Native Security
58,Pod Security Standards
58,Controlling Access to the Kubernetes APIPoliciesLimit Ranges
58,Resource Quotas
58,Pod Security Policies
58,Process ID Limits And Reservations
58,Node Resource ManagersScheduling and EvictionKubernetes Scheduler
58,Assigning Pods to Nodes
58,Resource Bin Packing for Extended Resources
58,Taints and Tolerations
58,Pod Overhead
58,Eviction Policy
58,Scheduling Framework
58,Scheduler Performance TuningCluster AdministrationCertificates
58,Managing Resources
58,Cluster Networking
58,Logging Architecture
58,Metrics For Kubernetes System Components
58,System Logs
58,Garbage collection for container images
58,Proxies in Kubernetes
58,API Priority and Fairness
58,Installing AddonsExtending KubernetesExtending the Kubernetes APICustom Resources
58,"Extending the Kubernetes API with the aggregation layerCompute, Storage, and Networking ExtensionsNetwork Plugins"
58,Device PluginsOperator pattern
58,Service CatalogTasksInstall ToolsInstall and Set Up kubectl on Linux
58,Install and Set Up kubectl on macOS
58,Install and Set Up kubectl on WindowsAdminister a ClusterAdministration with kubeadmCertificate Management with kubeadm
58,Configuring a cgroup driver
58,Upgrading kubeadm clusters
58,Adding Windows nodes
58,Upgrading Windows nodesMigrating from dockershimCheck whether Dockershim deprecation affects you
58,"Migrating telemetry and security agents from dockershimCertificatesManage Memory, CPU, and API ResourcesConfigure Default Memory Requests and Limits for a Namespace"
58,Configure Default CPU Requests and Limits for a Namespace
58,Configure Minimum and Maximum Memory Constraints for a Namespace
58,Configure Minimum and Maximum CPU Constraints for a Namespace
58,Configure Memory and CPU Quotas for a Namespace
58,Configure a Pod Quota for a NamespaceInstall a Network Policy ProviderUse Calico for NetworkPolicy
58,Use Cilium for NetworkPolicy
58,Use Kube-router for NetworkPolicy
58,Romana for NetworkPolicy
58,Weave Net for NetworkPolicyAccess Clusters Using the Kubernetes API
58,Access Services Running on Clusters
58,Advertise Extended Resources for a Node
58,Autoscale the DNS Service in a Cluster
58,Change the default StorageClass
58,Change the Reclaim Policy of a PersistentVolume
58,Cloud Controller Manager Administration
58,Configure Out of Resource Handling
58,Configure Quotas for API Objects
58,Control CPU Management Policies on the Node
58,Control Topology Management Policies on a node
58,Customizing DNS Service
58,Debugging DNS Resolution
58,Declare Network Policy
58,Developing Cloud Controller Manager
58,Enable Or Disable A Kubernetes API
58,Enabling Service Topology
58,Enabling Topology Aware Hints
58,Encrypting Secret Data at Rest
58,Guaranteed Scheduling For Critical Add-On Pods
58,IP Masquerade Agent User Guide
58,Limit Storage Consumption
58,Memory Manager
58,Migrate Replicated Control Plane To Use Cloud Controller Manager
58,Namespaces Walkthrough
58,Operating etcd clusters for Kubernetes
58,Reconfigure a Node's Kubelet in a Live Cluster
58,Reserve Compute Resources for System Daemons
58,Safely Drain a Node
58,Securing a Cluster
58,Set Kubelet parameters via a config file
58,Set up High-Availability Kubernetes Masters
58,Share a Cluster with Namespaces
58,Upgrade A Cluster
58,Using a KMS provider for data encryption
58,Using CoreDNS for Service Discovery
58,Using NodeLocal DNSCache in Kubernetes clusters
58,Using sysctls in a Kubernetes ClusterConfigure Pods and ContainersAssign Memory Resources to Containers and Pods
58,Assign CPU Resources to Containers and Pods
58,Configure GMSA for Windows Pods and containers
58,Configure RunAsUserName for Windows pods and containers
58,Configure Quality of Service for Pods
58,Assign Extended Resources to a Container
58,Configure a Pod to Use a Volume for Storage
58,Configure a Pod to Use a PersistentVolume for Storage
58,Configure a Pod to Use a Projected Volume for Storage
58,Configure a Security Context for a Pod or Container
58,Configure Service Accounts for Pods
58,Pull an Image from a Private Registry
58,"Configure Liveness, Readiness and Startup Probes"
58,Assign Pods to Nodes
58,Assign Pods to Nodes using Node Affinity
58,Configure Pod Initialization
58,Attach Handlers to Container Lifecycle Events
58,Configure a Pod to Use a ConfigMap
58,Share Process Namespace between Containers in a Pod
58,Create static Pods
58,Translate a Docker Compose File to Kubernetes ResourcesManage Kubernetes ObjectsDeclarative Management of Kubernetes Objects Using Configuration Files
58,Declarative Management of Kubernetes Objects Using Kustomize
58,Managing Kubernetes Objects Using Imperative Commands
58,Imperative Management of Kubernetes Objects Using Configuration Files
58,Update API Objects in Place Using kubectl patchManaging SecretsManaging Secret using kubectl
58,Managing Secret using Configuration File
58,Managing Secret using KustomizeInject Data Into ApplicationsDefine a Command and Arguments for a Container
58,Define Dependent Environment Variables
58,Define Environment Variables for a Container
58,Expose Pod Information to Containers Through Environment Variables
58,Expose Pod Information to Containers Through Files
58,Distribute Credentials Securely Using SecretsRun ApplicationsRun a Stateless Application Using a Deployment
58,Run a Single-Instance Stateful Application
58,Run a Replicated Stateful Application
58,Scale a StatefulSet
58,Delete a StatefulSet
58,Force Delete StatefulSet Pods
58,Horizontal Pod Autoscaler
58,Horizontal Pod Autoscaler Walkthrough
58,Specifying a Disruption Budget for your Application
58,Accessing the Kubernetes API from a PodRun JobsRunning Automated Tasks with a CronJob
58,Coarse Parallel Processing Using a Work Queue
58,Fine Parallel Processing Using a Work Queue
58,Indexed Job for Parallel Processing with Static Work Assignment
58,Parallel Processing using ExpansionsAccess Applications in a ClusterWeb UI (Dashboard)
58,Accessing Clusters
58,Configure Access to Multiple Clusters
58,Use Port Forwarding to Access Applications in a Cluster
58,Use a Service to Access an Application in a Cluster
58,Connect a Frontend to a Backend Using Services
58,Create an External Load Balancer
58,List All Container Images Running in a Cluster
58,Set up Ingress on Minikube with the NGINX Ingress Controller
58,Communicate Between Containers in the Same Pod Using a Shared Volume
58,"Configure DNS for a ClusterMonitoring, Logging, and DebuggingApplication Introspection and Debugging"
58,Auditing
58,Debug a StatefulSet
58,Debug Init Containers
58,Debug Pods and ReplicationControllers
58,Debug Running Pods
58,Debug Services
58,Debugging Kubernetes nodes with crictl
58,Determine the Reason for Pod Failure
58,Developing and debugging services locally
58,Get a Shell to a Running Container
58,Logging Using Stackdriver
58,Monitor Node Health
58,Resource metrics pipeline
58,Tools for Monitoring Resources
58,Troubleshoot Applications
58,Troubleshoot Clusters
58,TroubleshootingExtend KubernetesConfigure the Aggregation LayerUse Custom ResourcesExtend the Kubernetes API with CustomResourceDefinitions
58,Versions in CustomResourceDefinitionsSet up an Extension API Server
58,Configure Multiple Schedulers
58,Use an HTTP Proxy to Access the Kubernetes API
58,Set up Konnectivity serviceTLSConfigure Certificate Rotation for the Kubelet
58,Manage TLS Certificates in a Cluster
58,Manual Rotation of CA CertificatesManage Cluster DaemonsPerform a Rolling Update on a DaemonSet
58,Perform a Rollback on a DaemonSetService CatalogInstall Service Catalog using Helm
58,Install Service Catalog using SCNetworkingValidate IPv4/IPv6 dual-stackConfigure a kubelet image credential provider
58,Extend kubectl with plugins
58,Manage HugePages
58,Schedule GPUsTutorialsHello MinikubeLearn Kubernetes BasicsCreate a ClusterUsing Minikube to Create a Cluster
58,Interactive Tutorial - Creating a ClusterDeploy an AppUsing kubectl to Create a Deployment
58,Interactive Tutorial - Deploying an AppExplore Your AppViewing Pods and Nodes
58,Interactive Tutorial - Exploring Your AppExpose Your App PubliclyUsing a Service to Expose Your App
58,Interactive Tutorial - Exposing Your AppScale Your AppRunning Multiple Instances of Your App
58,Interactive Tutorial - Scaling Your AppUpdate Your AppPerforming a Rolling Update
58,"Interactive Tutorial - Updating Your AppConfigurationExample: Configuring a Java MicroserviceExternalizing config using MicroProfile, ConfigMaps and Secrets"
58,Interactive Tutorial - Configuring a Java MicroserviceConfiguring Redis using a ConfigMapStateless ApplicationsExposing an External IP Address to Access an Application in a Cluster
58,Example: Deploying PHP Guestbook application with MongoDBStateful ApplicationsStatefulSet Basics
58,Example: Deploying WordPress and MySQL with Persistent Volumes
58,Example: Deploying Cassandra with a StatefulSet
58,"Running ZooKeeper, A Distributed System CoordinatorClustersRestrict a Container's Access to Resources with AppArmor"
58,Restrict a Container's Syscalls with SeccompServicesUsing Source IPReferenceGlossaryAPI OverviewKubernetes API Concepts
58,Server-Side Apply
58,Client Libraries
58,Kubernetes Deprecation Policy
58,Deprecated API Migration Guide
58,Kubernetes API health endpointsAPI Access ControlAuthenticating
58,Authenticating with Bootstrap Tokens
58,Certificate Signing Requests
58,Using Admission Controllers
58,Dynamic Admission Control
58,Managing Service Accounts
58,Authorization Overview
58,Using RBAC Authorization
58,Using ABAC Authorization
58,Using Node Authorization
58,"Webhook ModeWell-Known Labels, Annotations and TaintsKubernetes APIWorkload ResourcesPod"
58,EphemeralContainers
58,PodTemplate
58,ReplicationController
58,ReplicaSet
58,Deployment
58,StatefulSet
58,ControllerRevision
58,DaemonSet
58,Job
58,CronJob
58,HorizontalPodAutoscaler
58,HorizontalPodAutoscaler v2beta2
58,PriorityClassService ResourcesService
58,Endpoints
58,EndpointSlice
58,Ingress
58,IngressClassConfig and Storage ResourcesConfigMap
58,Secret
58,Volume
58,PersistentVolumeClaim
58,PersistentVolume
58,StorageClass
58,VolumeAttachment
58,CSIDriver
58,CSINode
58,CSIStorageCapacity v1beta1Authentication ResourcesServiceAccount
58,TokenRequest
58,TokenReview
58,CertificateSigningRequestAuthorization ResourcesLocalSubjectAccessReview
58,SelfSubjectAccessReview
58,SelfSubjectRulesReview
58,SubjectAccessReview
58,ClusterRole
58,ClusterRoleBinding
58,Role
58,RoleBindingPolicy ResourcesLimitRange
58,ResourceQuota
58,NetworkPolicy
58,PodDisruptionBudget
58,PodSecurityPolicy v1beta1Extend ResourcesCustomResourceDefinition
58,MutatingWebhookConfiguration
58,ValidatingWebhookConfigurationCluster ResourcesNode
58,Namespace
58,Event
58,APIService
58,Lease
58,RuntimeClass
58,FlowSchema v1beta1
58,PriorityLevelConfiguration v1beta1
58,Binding
58,ComponentStatusCommon DefinitionsDeleteOptions
58,LabelSelector
58,ListMeta
58,LocalObjectReference
58,NodeSelectorRequirement
58,ObjectFieldSelector
58,ObjectMeta
58,ObjectReference
58,Patch
58,Quantity
58,ResourceFieldSelector
58,Status
58,TypedLocalObjectReferenceCommon ParametersKubernetes Issues and SecurityKubernetes Issue Tracker
58,Kubernetes Security and Disclosure InformationSetup toolsKubeadmkubeadm init
58,kubeadm join
58,kubeadm upgrade
58,kubeadm config
58,kubeadm reset
58,kubeadm token
58,kubeadm version
58,kubeadm alpha
58,kubeadm certs
58,kubeadm init phase
58,kubeadm join phase
58,kubeadm kubeconfig
58,kubeadm reset phase
58,kubeadm upgrade phase
58,Implementation detailsComponent toolsFeature Gates
58,kubelet
58,kube-apiserver
58,kube-controller-manager
58,kube-proxy
58,kube-scheduler
58,Kubelet authentication/authorization
58,kubelet.config.k8s.io/v1beta1
58,TLS bootstrappingkubectlOverview of kubectl
58,JSONPath Support
58,kubectl
58,kubectl Cheat Sheet
58,kubectl Commands
58,kubectl for Docker Users
58,kubectl Usage ConventionsConfiguration APIsClient Authentication (v1beta1)
58,kube-apiserver Audit Configuration (v1)
58,kube-proxy Configuration (v1alpha1)
58,kube-scheduler Configuration (v1beta1)
58,kube-scheduler Policy Configuration (v1)
58,Kubelet Configuration (v1beta1)
58,WebhookAdmission Configuration (v1)SchedulingScheduling Policies
58,Scheduler ConfigurationOther ToolsContributeSuggesting content improvementsContributing new contentOverview
58,Opening a pull request
58,Documenting for a release
58,Blogs and case studiesReviewing changesReviewing pull requests
58,For approvers and reviewersLocalizing Kubernetes documentationParticipating in SIG DocsRoles and responsibilities
58,PR wranglersDocumentation style overviewContent guide
58,Style guide
58,Writing a new topic
58,Page content types
58,Content organization
58,Custom Hugo ShortcodesReference Docs OverviewContributing to the Upstream Kubernetes Code
58,Quickstart
58,Generating Reference Documentation for the Kubernetes API
58,Generating Reference Documentation for kubectl Commands
58,Generating Reference Pages for Kubernetes Components and Tools
58,"Advanced contributingDocs smoke test pageKubernetes DocumentationTutorialsStateful ApplicationsExample: Deploying WordPress and MySQL with Persistent VolumesExample: Deploying WordPress and MySQL with Persistent VolumesThis tutorial shows you how to deploy a WordPress site and a MySQL database using Minikube. Both applications use PersistentVolumes and PersistentVolumeClaims to store data.A PersistentVolume (PV) is a piece of storage in the cluster that has been manually provisioned by an administrator, or dynamically provisioned by Kubernetes using a StorageClass. A PersistentVolumeClaim (PVC) is a request for storage by a user that can be fulfilled by a PV. PersistentVolumes and PersistentVolumeClaims are independent from Pod lifecycles and preserve data through restarting, rescheduling, and even deleting Pods.Warning: This deployment is not suitable for production use cases, as it uses single instance WordPress and MySQL Pods. Consider using WordPress Helm Chart to deploy WordPress in production.Note: The files provided in this tutorial are using GA Deployment APIs and are specific to kubernetes version 1.9 and later. If you wish to use this tutorial with an earlier version of Kubernetes, please update the API version appropriately, or reference earlier versions of this tutorial.ObjectivesCreate PersistentVolumeClaims and PersistentVolumesCreate a kustomization.yaml witha Secret generatorMySQL resource configsWordPress resource configsApply the kustomization directory by kubectl apply -k ./Clean upBefore you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must"
58,be configured to communicate with your cluster. If you do not already have a
58,"cluster, you can create one by using"
58,minikube
58,"or you can use one of these Kubernetes playgrounds:KatacodaPlay with KubernetesTo check the version, enter kubectl version."
58,"The example shown on this page works with kubectl 1.14 and above.Download the following configuration files:mysql-deployment.yamlwordpress-deployment.yamlCreate PersistentVolumeClaims and PersistentVolumesMySQL and Wordpress each require a PersistentVolume to store data. Their PersistentVolumeClaims will be created at the deployment step.Many cluster environments have a default StorageClass installed. When a StorageClass is not specified in the PersistentVolumeClaim, the cluster's default StorageClass is used instead.When a PersistentVolumeClaim is created, a PersistentVolume is dynamically provisioned based on the StorageClass configuration.Warning: In local clusters, the default StorageClass uses the hostPath provisioner. hostPath volumes are only suitable for development and testing. With hostPath volumes, your data lives in /tmp on the node the Pod is scheduled onto and does not move between nodes. If a Pod dies and gets scheduled to another node in the cluster, or the node is rebooted, the data is lost.Note: If you are bringing up a cluster that needs to use the hostPath provisioner, the --enable-hostpath-provisioner flag must be set in the controller-manager component.Note: If you have a Kubernetes cluster running on Google Kubernetes Engine, please follow this guide.Create a kustomization.yamlAdd a Secret generatorA Secret is an object that stores a piece of sensitive data like a password or key. Since 1.14, kubectl supports the management of Kubernetes objects using a kustomization file. You can create a Secret by generators in kustomization.yaml.Add a Secret generator in kustomization.yaml from the following command. You will need to replace YOUR_PASSWORD with the password you want to use.cat <<EOF >./kustomization.yaml"
58,secretGenerator:
58,- name: mysql-pass
58,literals:
58,- password=YOUR_PASSWORD
58,EOF
58,Add resource configs for MySQL and WordPressThe following manifest describes a single-instance MySQL Deployment. The MySQL container mounts the PersistentVolume at /var/lib/mysql. The MYSQL_ROOT_PASSWORD environment variable sets the database password from the Secret.application/wordpress/mysql-deployment.yaml
58,apiVersion: v1
58,kind: Service
58,metadata:
58,name: wordpress-mysql
58,labels:
58,app: wordpress
58,spec:
58,ports:
58,- port: 3306
58,selector:
58,app: wordpress
58,tier: mysql
58,clusterIP: None
58,---
58,apiVersion: v1
58,kind: PersistentVolumeClaim
58,metadata:
58,name: mysql-pv-claim
58,labels:
58,app: wordpress
58,spec:
58,accessModes:
58,- ReadWriteOnce
58,resources:
58,requests:
58,storage: 20Gi
58,---
58,apiVersion: apps/v1
58,kind: Deployment
58,metadata:
58,name: wordpress-mysql
58,labels:
58,app: wordpress
58,spec:
58,selector:
58,matchLabels:
58,app: wordpress
58,tier: mysql
58,strategy:
58,type: Recreate
58,template:
58,metadata:
58,labels:
58,app: wordpress
58,tier: mysql
58,spec:
58,containers:
58,- image: mysql:5.6
58,name: mysql
58,env:
58,- name: MYSQL_ROOT_PASSWORD
58,valueFrom:
58,secretKeyRef:
58,name: mysql-pass
58,key: password
58,ports:
58,- containerPort: 3306
58,name: mysql
58,volumeMounts:
58,- name: mysql-persistent-storage
58,mountPath: /var/lib/mysql
58,volumes:
58,- name: mysql-persistent-storage
58,persistentVolumeClaim:
58,claimName: mysql-pv-claim
58,The following manifest describes a single-instance WordPress Deployment. The WordPress container mounts the
58,PersistentVolume at /var/www/html for website data files. The WORDPRESS_DB_HOST environment variable sets
58,"the name of the MySQL Service defined above, and WordPress will access the database by Service. The"
58,WORDPRESS_DB_PASSWORD environment variable sets the database password from the Secret kustomize generated.application/wordpress/wordpress-deployment.yaml
58,apiVersion: v1
58,kind: Service
58,metadata:
58,name: wordpress
58,labels:
58,app: wordpress
58,spec:
58,ports:
58,- port: 80
58,selector:
58,app: wordpress
58,tier: frontend
58,type: LoadBalancer
58,---
58,apiVersion: v1
58,kind: PersistentVolumeClaim
58,metadata:
58,name: wp-pv-claim
58,labels:
58,app: wordpress
58,spec:
58,accessModes:
58,- ReadWriteOnce
58,resources:
58,requests:
58,storage: 20Gi
58,---
58,apiVersion: apps/v1
58,kind: Deployment
58,metadata:
58,name: wordpress
58,labels:
58,app: wordpress
58,spec:
58,selector:
58,matchLabels:
58,app: wordpress
58,tier: frontend
58,strategy:
58,type: Recreate
58,template:
58,metadata:
58,labels:
58,app: wordpress
58,tier: frontend
58,spec:
58,containers:
58,- image: wordpress:4.8-apache
58,name: wordpress
58,env:
58,- name: WORDPRESS_DB_HOST
58,value: wordpress-mysql
58,- name: WORDPRESS_DB_PASSWORD
58,valueFrom:
58,secretKeyRef:
58,name: mysql-pass
58,key: password
58,ports:
58,- containerPort: 80
58,name: wordpress
58,volumeMounts:
58,- name: wordpress-persistent-storage
58,mountPath: /var/www/html
58,volumes:
58,- name: wordpress-persistent-storage
58,persistentVolumeClaim:
58,claimName: wp-pv-claim
58,Download the MySQL deployment configuration file.curl -LO https://k8s.io/examples/application/wordpress/mysql-deployment.yaml
58,Download the WordPress configuration file.curl -LO https://k8s.io/examples/application/wordpress/wordpress-deployment.yaml
58,Add them to kustomization.yaml file.cat <<EOF >>./kustomization.yaml
58,resources:
58,- mysql-deployment.yaml
58,- wordpress-deployment.yaml
58,EOF
58,Apply and VerifyThe kustomization.yaml contains all the resources for deploying a WordPress site and a
58,MySQL database. You can apply the directory bykubectl apply -k ./
58,Now you can verify that all objects exist.Verify that the Secret exists by running the following command:kubectl get secrets
58,The response should be like this:NAME
58,TYPE
58,DATA
58,AGE
58,mysql-pass-c57bb4t7mf
58,Opaque
58,Verify that a PersistentVolume got dynamically provisioned.kubectl get pvc
58,Note: It can take up to a few minutes for the PVs to be provisioned and bound.The response should be like this:NAME
58,STATUS
58,VOLUME
58,CAPACITY
58,ACCESS MODES
58,STORAGECLASS
58,AGE
58,mysql-pv-claim
58,Bound
58,pvc-8cbd7b2e-4044-11e9-b2bb-42010a800002
58,20Gi
58,RWO
58,standard
58,77s
58,wp-pv-claim
58,Bound
58,pvc-8cd0df54-4044-11e9-b2bb-42010a800002
58,20Gi
58,RWO
58,standard
58,77s
58,Verify that the Pod is running by running the following command:kubectl get pods
58,Note: It can take up to a few minutes for the Pod's Status to be RUNNING.The response should be like this:NAME
58,READY
58,STATUS
58,RESTARTS
58,AGE
58,wordpress-mysql-1894417608-x5dzt
58,1/1
58,Running
58,40s
58,Verify that the Service is running by running the following command:kubectl get services wordpress
58,The response should be like this:NAME
58,TYPE
58,CLUSTER-IP
58,EXTERNAL-IP
58,PORT(S)
58,AGE
58,wordpress
58,LoadBalancer
58,10.0.0.89
58,<pending>
58,80:32406/TCP
58,Note: Minikube can only expose Services through NodePort. The EXTERNAL-IP is always pending.Run the following command to get the IP Address for the WordPress Service:minikube service wordpress --url
58,The response should be like this:http://1.2.3.4:32406
58,"Copy the IP address, and load the page in your browser to view your site.You should see the WordPress set up page similar to the following screenshot.Warning: Do not leave your WordPress installation on this page. If another user finds it, they can set up a website on your instance and use it to serve malicious content.Either install WordPress by creating a username and password or delete your instance.Cleaning upRun the following command to delete your Secret, Deployments, Services and PersistentVolumeClaims:kubectl delete -k ./"
58,What's nextLearn more about Introspection and DebuggingLearn more about JobsLearn more about Port ForwardingLearn how to Get a Shell to a ContainerFeedbackWas this page helpful?Yes
58,"NoThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on"
58,Stack Overflow.
58,Open an issue in the GitHub repo if you want to
58,report a problem
58,"suggest an improvement.Last modified June 20, 2020 at 12:45 PM PST: Correct wordpress service command output (966215f88) Edit this page"
58,Create child page
58,Create an issue
58,Print entire sectionObjectivesBefore you beginCreate PersistentVolumeClaims and PersistentVolumesCreate a kustomization.yamlAdd a Secret generatorAdd resource configs for MySQL and WordPressApply and VerifyCleaning upWhat's nextHome
58,Blog
58,Training
58,Partners
58,Community
58,"Case Studies© 2021 The Kubernetes Authors | Documentation Distributed under CC BY 4.0Copyright © 2021 The Linux Foundation ®. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our Trademark Usage pageICP license: 京ICP备17074266号-3"
59,TIBCO Support Portal
59,LoadingÃ—Sorry to interruptCSS ErrorRefresh
60,How to Speed Up WordPress and Boost Performance: Top WordPress Optimization Tricks [2021 Edition] • Crunchify
60,Skip to main content Skip to primary sidebar Skip to footerAdditional menuCrunchifyLargest free Technical and Blogging resource site for Beginner. We help clients transform their great ideas into reality!Java
60,Abstract Class & Method
60,Static Methods & Variables
60,Java Reflection Tutorial
60,Java eNum Introduction
60,What is Java Interface
60,Top 10 Java Interview Q&A
60,HTTP GET/POST request
60,Java8 Tutorials
60,Java Prod Ready Utilities
60,Spring MVC
60,1st Spring MVC Tutorial
60,1st Spring Boot Tutorial
60,All Spring Boot Tutorials
60,All Maven Tutorials
60,Eclipse IDE Tutorials
60,Apache Tomcat Tutorials
60,Tutorials
60,Ansible Tutorials
60,GitHub Tutorials
60,MacOS Handy Tips
60,IntelliJ IDE Tutorials
60,JSON Tutorials
60,JavaScript Tutorials
60,WordPress
60,Start Your 1st Blog
60,15 WP Optimization Tips
60,Speed up WordPress
60,SEO Tips & Tricks
60,Create 1st WP Plugin
60,WordPress Plugin Hacks
60,Create Custom Post Type
60,WordPress Beginner Guide
60,Google Adsense Tutorials
60,Genesis WP
60,Add Grid to HomePage
60,Modify 404 Page
60,functions.php Hacks
60,.htaccess Tricks
60,style.css Tricks
60,Deals
60,Pro
60,WordPress Services
60,Crunchy Sharing Plugin
60,AIO Optimizer Plugin
60,Login / My Account
60,Cart / Checkout
60,Advertise
60,Contact
60,Sitemap
60,Advertisement
60,Crunchify
60,WordPress Optimization and Tutorials
60,How to Speed Up WordPress and Boost Performance: Top WordPress Optimiz ...How to Speed Up WordPress and Boost Performance: Top WordPress Optimization Tricks [2021 Edition]
60,"Last Updated on December 31st, 2020 by   App Shah"
60,42 comments
60,There are literally hundreds of possible ways a user can speed up their WordPress website and increase pageviews. Steps in this tutorial will optimize WordPress performance and increase site speed.
60,Do you have any of below questions?
60,10 WordPress Setting Changes to Optimize Your Site’s PerformanceWordPress Performance Optimization Best Practices.How to Optimize WordPress for Better Performance?WordPress Performance Optimization TipsHow to Speed up Your WordPress Site.Run a Site Speed Diagnosis. Understanding how fast your website loads is the next step towards improving site performance.Speed up WordPress to make sure your website content is delivered the fastest.
60,"From buying traffic (not suggested) to updating your websites server for maximum speed (highly recommended), ensuring that you use proper methods is the best way to increase traffic."
60,"I won’t promise you a million visitors a month using these methods, but they will at least ensure you are using best practices."
60,"Unlike other lists this article isn’t going to tell you how to write Search Engine Optimized content, instead it should work as the foundation for your new or existing site before you decide to create new or more content."
60,"Using these methods you can speed up your sites performance, which in turn SHOULD lead to more traffic."
60,Step-1. Leveraging Browser Caching via .htaccess
60,Here is a complete tutorial on How to Speed up WordPress Leveraging Browser Caching via .htaccess.
60,Step-2. Use Caching
60,"You can cache all of your WordPress pages.  When you cache, it saves a copy of each page and serves that copy up instead of running all of the php code each time someone requests a page."
60,This can be really helpful. I use WP Super Cache plugin and really like it. Detailed instruction on how to set it up correctly.
60,"The only down side of caching is that it takes a little while to set up, and it can also make updating your site more confusing because sometimes you are looking at a cached version of your site when you think you are looking at a live version. But once you work through those kinks, caching is great."
60,Get your copy.
60,"Step-3. Minify Your Sites CSS, JS and remove unnecessary Java Scripts"
60,"With websites, I believe less is more.  This is undeniably true of loading times. Your sites stylesheets and other CSS forms have a lot of blank white space, the more space you have the more system resources your server uses to search your sites CSS."
60,Complete tutorial on how to remove unnecessary JavaScripts and Remove JS and CSS if you are using Crayon Syntax Highlighter.
60,Step-4. Choose a Good Web Host
60,"This might seem like a very obvious one, but a lot of the times people try to save a few bucks and compromise quality. It is not worth it. Pay the extra few dollars and get a better web host."
60,One that is reliable and have strong servers. If you have millions of visitors / month then it’s not a bad idea to go for VPS hosting or Dedicated Server hosting.
60,Step-5. Optimize Your MySQL Database
60,Plugin: WP Sweep.
60,"When you delete unused plugins they don’t necessarily disappear from your sites database, by using this plugin you can find those unneeded tables and delete them, freeing up database space that didn’t need to be used."
60,"The less database tables you have, the faster your site will run. Here’s the clean options plugin."
60,Articles:
60,Cleanup DB after deleting DisqusWooCommerce Setting
60,Top-6. Do a Plugin Check and Remove Inactive Plugins
60,If you are using more than 15 plugins on your blog then you need to check to see if you really need all of those. If you don’t then you need to delete them. Also remove the inactivate plugins from the site.
60,"While checking for plugins, make sure that you use the proper format when including the plugin in your template files."
60,Instead of including the code like this:
60,<?php wp_page_navi(); ?>
60,You should add it like this:
60,<?php if (function_exists('wp_page_navi')) { wp_page_navi(); } ?>
60,"Having it this way ensures that WordPress will only pull that code on the page if this plugin is active. If you use the first way of coding, and you inactivate the plugin, your page will load with error and sometimes it even displays on the page that there is an error."
60,Step-7. Use Content Delivery Network
60,At Crunchify we use MaxCDN as our Content Delivery Network partner. MaxCDN is one of the best Content Delivery Network (CDN) provider so far I came across.
60,But make sure to avoid duplicate content issue.
60,Step-8. Removed loading unwanted files
60,Please read detailed tutorial on how to got rid-off top 9 files out of 10 resources which were loading on all Crunchify pages.
60,"As always, be sure to backup your website and MySQL databases before you use any of these methods, just to be on the safe side. Do you have other tips? Feel free to share them with our readers in the comments section."
60,"Join the DiscussionIf you liked this article, then please share it on social media. Still have any questions about an article, leave us a comment."
60,Share:
60,Other Popular Articles...
60,Better Optimize WordPress Database – All in One Guide
60,WordPress Plugins We Use on Crunchify: Favorite and must have Plugins – Want to Know Why?
60,8 Best Essential and Useful WordPress Plugins for Beginners [2021 Edition]
60,15 Essential Settings and Optimization Tasks After Installing WordPress for First Time
60,Gonzales WordPress Plugin – Remove unwanted CSS and JS Loading for Better Performance Optimization Goal
60,On-Page SEO Techniques to Rank First on Google Search Result Page
60,WordPress Optimization and Tutorials WordPress Plugins
60,I want to...
60,Learn SEO
60,Optimize WordPress
60,Plugins we use
60,Java Production Ready Utils
60,"About App ShahI'm an Engineer by profession, Blogger by passion & Founder of Crunchify, LLC, the largest free blogging & technical resource site for beginners. Love SEO, SaaS, #webperf, WordPress, Java. With over 16 millions+ pageviews/month, Crunchify has changed the life of over thousands of individual around the globe teaching Java & Web Tech for FREE. Get latest update on"
60,and .
60,Subscribe To Newsletter…
60,"Get Early Access To New Articles, Plugins, Discount Codes And Brief Updates About What's New With Crunchify! Join Over 16 Million Monthly Readers..."
60,Primary Sidebar
60,Over 16 million readers
60,Get fresh content from Crunchify
60,Top Tech Tutorials
60,Simplest Spring MVC Hello World
60,Spring Boot Tutorial
60,NEW
60,Install Docker on Linux
60,Build RESTful Service using Jersey JAX-RS
60,Top 10 Java Interview Q&A
60,Install & Configure Prometheus on Linux
60,NEW
60,Install Ansible on Linux
60,Race Condition in Java Multi-Threading
60,Sort a HashMap by Key & Value
60,Reverse a String in Java
60,NEW
60,Implement a LinkedList Class From Scratch
60,Memcached Java Client
60,Basic Java Tech
60,Singleton Pattern
60,Java Caching
60,LinkedList Iterator
60,Java Abstract
60,Java Static Intro
60,Java Interface
60,Github OAuth
60,Sorting Algorithm
60,Semaphore & Mutex
60,Java Reflection
60,Java NIO (Non-blocking)
60,SOAP vs REST
60,.zip file by Maven
60,"Modern, Secure & Fast Managed WordPress Hosting."
60,Check it out.
60,Useful WordPress Guide
60,NEW
60,Start 1st WordPress Blog
60,15 Essential Optimization Tips
60,Leverage .htaccess to Speed up WordPress
60,Stop loading unnecessary Files on Site
60,Top 5 Basic SEO Tips
60,Importance of Keyword Research
60,Better cleanup WordPress Header Section
60,Fix cPanel CPU issue
60,Google Form as ultimate WordPress Contact Form
60,Load WordPress Fonts Locally (Speed Tips)
60,16 proven ways to get Quality Backlinks
60,Better Upgrade to PHP 7.1
60,NEW
60,Secure WordPress Login Area
60,Cloak
60,Affiliate Links without WordPress plugin
60,WORDPRESS TUNING TIPS
60,Install WP Locally
60,WordPress CPT
60,Disable Cron Jobs
60,Modify 404 Page
60,Scroll To Top
60,GenesisWP Hooks
60,Add Bitly Shortlink
60,Adsense without Plugin
60,Plugins we Use
60,Top Backup Plugins
60,Domain Authority Tips
60,Interlinking Tips
60,Setup Forum
60,FooterTop Tech Categories…
60,Java & J2EE
60,Eclipse IDE Tutorials
60,Android Dev Tutorials
60,Apache Tomcat Tutorials
60,Design & Dev
60,Interview Questions Answers
60,JavaScript
60,Spring MVC and Spring Boot Tutorials
60,Maven
60,Top Blogging Categories…
60,SEO 101 Tutorials
60,WordPress Optimization and Tutorials
60,Genesis WP
60,Blogging
60,Making Money Online
60,functions.php Hacks
60,WebHosting
60,style.css Hacks
60,WooCommerce
60,Start A Blog
60,Advertise
60,Sitemap
60,Setup
60,Forum
60,Affiliate
60,Discount Code
60,"2021 Crunchify, LLC."
60,Hosted at Kinsta  •  Built on Genesis Themes.
60,About  •  DCMA Disclaimer and Privacy Policy.
60,Noticed a bug? Let us know.
61,How do I tune Artifactory for heavy loads?
61,Products
61,Solutions
61,Resources
61,Services
61,Pricing
61,Start For FreeProducts
61,JFrog Platform
61,JFrog Artifactory
61,JFrog Xray
61,JFrog Pipelines
61,JFrog Distribution
61,JFrog Mission Control
61,JFrog Container Registry
61,Solutions
61,JFrog for Banking and Financial Services
61,JFrog for The Automotive Industry
61,JFrog for HealthCare
61,JFrog for the Technology and Software Industries
61,Artifact Management
61,JFrog for Security and Compliance
61,JFrog for Continuous Integration and Continuous Delivery (CI/CD)
61,Resources
61,Resource Center
61,Blog
61,User Guides
61,DevOps Tools
61,Integration
61,Academy
61,Customer Zone
61,Knowledge base
61,Upcoming Webinars
61,Services
61,Support
61,Ticket Portal
61,Consulting
61,MyJFrog Customer Portal
61,Certification
61,Pricing
61,Industry
61,Financial Services
61,End-to-End DevOps for Banking and Financial Software Development
61,Learn More
61,Automotive Industry
61,Scalable DevOps for Automotive Companies and OEMs
61,Learn More
61,Healthcare Services
61,Trusted Software Releases for Healthcare Companies
61,Learn More
61,Technology & Software
61,DevOps Automation for Technology and Software Companies
61,Learn More
61,Use Case
61,Artifact Management
61,Scalable DevOps for Software Artifact Management
61,Learn More
61,Security & Compliance
61,DevOps Automation for Security and Compliance Management
61,Learn More
61,CI/CD
61,Software Development Pipeline Automation and Management
61,Learn More
61,ProfessionalServices
61,Consulting
61,Leaping to Enterprise DevOps
61,See More
61,Certification
61,Become a JFrog Artifactory Certified DevOps Engineer
61,See More
61,Support
61,Get Support
61,24/7 R&D Level Support
61,See More
61,Ticket Portal
61,Existing customers? Get direct help from our team
61,Log In
61,Account
61,MyJFrog - Customer Portal
61,Manage your Cloud subscriptions
61,Log In
61,Resources
61,Resource Center
61,"Webinars, articles, white papers, screencasts, use cases,"
61,and more
61,See More
61,User Guides
61,Technical documentation about JFrog products
61,See More
61,DevOps Tools
61,Accelerating software releases
61,Read More
61,Blog
61,"The latest DevOps trends, news on JFrog products, launches and announcements"
61,Read Now
61,JFrog Academy
61,"Self-paced, free training"
61,for JFrog solutions
61,See More
61,Knowledge Base
61,Comprehensive self-service portal
61,See More
61,Upcoming Webinars
61,Join our leading tech experts
61,to enrich your knowledge
61,See More
61,Integrations
61,All of the technologies that integrate with JFrog
61,Read Now
61,Customer Zone
61,All the resources you need to manage and troubleshoot your JFrog products
61,See More
61,Platform
61,The JFrog Platform
61,End-to-end Software Management and Releases
61,Learn More
61,Products
61,JFrog Artifactory
61,Enterprise Universal Artifact Repository
61,Learn More
61,JFrog Pipelines
61,Universal CI/CD DevOps Pipeline for the enterprise
61,Learn More
61,JFrog Mission Control
61,Centralized Global Artifact Management
61,Learn More
61,JFrog Xray
61,Container Security and Universal
61,Artifact Analysis
61,Learn More
61,JFrog Distribution
61,For Trusted Software Releases
61,Learn More
61,JFrog Container Registry
61,"Powerful, Hybrid Docker and Helm Registry"
61,Learn More
61,Find Other Useful Articles:
61,Product
61,All
61,General
61,Artifactory
61,Bintray
61,Mission Control
61,Xray
61,Enterprise Plus
61,Distribution
61,Access
61,JFrog Pipelines
61,Category
61,All
61,Filter
61,Search
61,How do I tune Artifactory for heavy loads?
61,Ariel Kabov 2020-09-03 07:19
61,Relevant Versions: Artifactory 7 and above.A tuning guide for previous versions is available here.
61,"Artifactory comes with a predefined set of default configurations and parameters. The default Artifactory should handle up to ~200 concurrent connections well.If you believe your Artifactory server is under-utilized, or in order to allow it to handle more processes at a given moment, it is possible to tune Artifactory to support a higher load.While it is always possible to scale horizontally by adding additional nodes to your HA cluster, here we will focus on a more vertical scale."
61,"Recommendation: The more crucial Artifactory becomes in your organization, the more crucial will be to have a monitoring system to look over Artifactory.You may read further at Monitoring and Optimizing Artifactory Performance."
61,JVM Memory
61,"By default, Artifactory comes with a predefined JVM memory limit. To modify the JVM memory allocation, please refer to the Product Configuration section that is part of the installation guide. Be advised to follow our hardware recommendations.When increasing the JVM memory allocation, make sure you leave at least 30% of the total RAM to the OS and other services."
61,Database connections
61,We can alter the maximum connections an Artifactory node can open to the DB by modifying the Artifactory System YAML.
61,Default values:artifactory:  database:    maxOpenConnections: 100...    access:  database:    maxOpenConnections: 100...metadata:  database:    maxOpenConnections: 100
61,Tuning example:artifactory:  database:    maxOpenConnections: 300...    access:  database:    maxOpenConnections: 300...metadata:  database:    maxOpenConnections: 300
61,"Important: The Artifactory maxOpenConnections parameter is being used also by the Artifactory Session Management mechanism.This means once the above example is used, the Artifactory node will open up to 1200 DB connections. Therefore we need to make sure the DB can accommodate the total number of connections all Artifactory nodes can open.As a rule of thumb we will require from the DB a number of connections based on:Total # of connections = (number of nodes) * ((artifactory.database.maxOpenConnections * 2) + access.database.maxOpenConnections + metadata.database.maxOpenConnections) + 50;"
61,*The extra 50 connections are to provide extra breathing room in situations where all DB connection pools are exhausted.
61,Tomcat HTTP Connections / Threads
61,"Artifactory runs on top of Apache Tomcat, which manages the incoming HTTP connection pools.This sets the number of concurrent HTTP connections Artifactory can serve.We can override the default thread pool limit by modifying the Artifactory System YAML."
61,Default values:artifactory:  tomcat:    connector:      maxThreads: 200...access:  tomcat:    connector:      maxThreads: 50
61,Tuning example:artifactory:  tomcat:    connector:      maxThreads: 600...access:  tomcat:    connector:      maxThreads: 150
61,"Important: When modifying the Access maxThreads, it is required to update the $JFROG_HOME/artifactory/var/etc/artifactory/artifactory.system.properties file with:artifactory.access.client.max.connections = <VALUE>This is to modify the internal HTTP connection pool Artifactory uses to internally interact with Access."
61,Artifactory async Thread Pool
61,"One of the most important thread pools in Artifactory is the “async” thread pool. This one defines the number of processes that can run in parallel.In addition to configuring the total number of parallel processes, we can also modify the maximum number of processes that can be queued.This is configured in $JFROG_HOME/artifactory/var/etc/artifactory/artifactory.system.properties."
61,Default values:(this means the machines CPU cores times 4)artifactory.async.corePoolSize = (4 * Runtime.getRuntime().availableProcessors()) artifactory.async.poolMaxQueueSize = 10000
61,Tuning example:(Shouldn’t be more than 8x the machine CPU cores)artifactory.async.corePoolSize = 128artifactory.async.poolMaxQueueSize = 100000
61,Garbage Collection
61,"By default, the Artifactory Garbage Collection is configured to run every 4 hours.The GC is a very resource-consuming operation, and if you see correlations between the running period of the GC to slow performance, we would recommend you to alter the Artifactory GC (not related JVM GC) to run at non-peak hours."
61,HTTP Client
61,"Artifactory manages a separate connection pool for outgoing HTTP requests per remote repository.This connection pool is limited by default to 50 concurrent connections, and up to 50 concurrent connections per unique route."
61,This is configured in $JFROG_HOME/artifactory/var/etc/artifactory/artifactory.system.properties.
61,Default values:artifactory.http.client.max.total.connections = 50artifactory.http.client.max.connections.per.route = 50
61,Tuning example:artifactory.http.client.max.total.connections = 150artifactory.http.client.max.connections.per.route = 120
61,Bypassing the Router
61,"The Artifactory 7 System Architecture provides us a flexible way to modify the flow a request will be processed by the Artifactory services.You can benefit from an improved performance by bypassing the Router service for API requests to Artifactory.This can be achieved using a Reverse Proxy such as NGINX or Apache HTTPD.By having a reverse proxy to redirect requests from $JFROG_URL/artifactory directly to $ARTIFACTORY_NODE:8081/artifactory, you will bypass the Router service.When under high load this can help to better distribute the requests in advance."
61,Filestore Configurations
61,"Artifactory supports different backend storage configurations to store the Artifactory filestore.For scenarios where the configured storage is not local, a performance benefit can be using a large Cache-FS provider mounted locally for each node.Cached files will be served quickly, and therefore having a large Cache FS provider will be a performance gain."
61,"Some filestore providers allow tuning and modifications of parameters. For instance: Eventual: ""numberOfThreads"".Eventual-Cluster: ""maxWorkers"".Remote: ""maxConnections""."
61,Read more at the Best Practices for Managing Your Artifactory Filestore white paper.
61,SHARE:
61,Release Fast Or Die
61,ProductsArtifactory
61,Xray
61,Pipelines
61,Distribution
61,Container Registry
61,JFrog Platform
61,ResourcesBlog
61,Events
61,User Guide
61,DevOps Tools
61,Open Source
61,Featured
61,CompanyAbout
61,Management
61,Investor Relations
61,Partners
61,Customers
61,Careers
61,Press
61,Contact Us
61,Brand Guidelines
61,CommunitySolutions
61,Foundations
61,Programs
61,Community Forum
61,© 2021 JFrog Ltd All Rights Reserved
61,Terms of Use
61,Cookies Policy
61,Privacy Policy
61,Accessibility Mode
61,Success
61,Your action was successful
61,Continue
61,Success
61,Your action was successful
61,Get Started
61,Oops...
61,Something went wrong
61,Please try again later
61,Continue
61,Information
61,Modal Message
61,Continue
61,Click Here
61,请点这里
63,MySQL and Oracle Database for Developers and Designers - Course | UCSC Silicon Valley Extension
63,Skip to main content
63,Academics
63,Courses
63,Open for Enrollment
63,Short Courses - 30 Days or Less
63,New Courses
63,Spring Courses
63,View all Courses
63,Popular Specializations
63,Social Media Marketing
63,Python
63,Mobile Application Development
63,Administrative Professionals
63,View all Specializations
63,Popular Certificates
63,Database and Data Analytics
63,Project and Program Management
63,Computer Programming
63,Human Resources Management
63,View all Certificates
63,Popular Series
63,Digital Marketing Science
63,Sales Operation Science
63,Paralegal Studies
63,Legal Studies
63,View all Series
63,Open Campus / Concurrent Enrollment
63,"The Open Campus Program, administered by UCSC Extension, allows you to enroll in courses offered on the UC Santa Cruz campus without being formally admitted to a degree program."
63,Learn More
63,Silicon Valley Startup Series
63,"The Silicon Valley Startup series, offered in partnership with Silicon Valley Ignite, brings you the wisdom of business development experts in highly interactive, mentored courses for the new entrepreneur and the seasoned executive."
63,Learn More
63,Workforce
63,International
63,Info
63,Academic Calendar
63,Policies
63,Students with Disabilities
63,FAQ
63,Title IX External link
63,Resources
63,Forms
63,Concurrent Enrollment
63,Career Services
63,Tutoring Services
63,Course Catalogs
63,About
63,About UCSC Extension
63,Areas of Study
63,Instructors
63,Our UC Value
63,Digital Badges
63,Work for Us
63,Blog
63,Contact
63,Login
63,Course
63,MySQL and Oracle Database for Developers and Designers | DBDA.X409
63,Oracle and MySQL are both reliable database engines commonly used for storing and serving data as web content. They are popular among developers of open source platforms and projects on the Web. High volume major websites use them. They also have a significant user base in the enterprise database market. This course is intended for DB developers and designers who want to learn MySQL and Oracle technology in depth.
63,"The course begins by reviewing the basic SQL queries, DDL and DML operations, data retrieval from multiple tables, and different types of storage engines in databases. It then introduces the aggregate, the index merge, data manipulation, and stored procedures in MySQL. You will learn to write complex queries and get hands-on experience with advanced features such as creating sub programs, data security, triggers, and dynamic SQL. You will also learn a performance tuning strategy, server configuration, loading techniques and the application architecture for efficient database design. This is a hands-on lab-based course designed to help students master MySQL features and tune for performance."
63,Learning Outcomes:
63,"At the conclusion of the course, you should be able to:"
63,Perform DDL and DML operations using SQL commands
63,Develop and manage database stored procedures including best practices
63,Develop Database Triggers to automate database operations
63,Understand Database Partitions and create tables with different types of partitions for improving database performance
63,Perform exception handling and error handling capabilities in both Oracle and MYSQL
63,"Understand different strategies used for improving database performance through Database Indexes, Optimizer, Explain Plan, and database hints"
63,Topics include:
63,Review of MYSQL and Oracle database essentials
63,"Understanding MYSQL storage engines, transactions and features of the database and how it differs from Oracle"
63,Performing DDL and DML operations using SQL commands
63,Retrieving data from multiple tables using JOINS
63,"Writing complex queries using JOINS, SUBQUERIES and nested SUB QURIES"
63,"MYSQL functions including single-row, multiple-row, group and aggregate functions"
63,Understanding Oracle and MYSQL optimizer and index merge method
63,Developing and managing database stored procedures including best practices
63,Exception handling and error handling capabilities in both Oracle and MYSQL
63,Utilizing database triggers to automate database operations
63,Oracle and MYSQL performance enhancements with queries and indexes
63,Database loading techniques and their effects on performance
63,Skills Needed: Students should have prior knowledge of the installation and basic operation of MySQL.
63,Have a question about this course?
63,First Name*
63,Last Name*
63,Email*
63,Phone Number
63,Question
63,Submit
63,Speak to a student services representative. Call (408) 861-3860
63,This course is related to the following programs:
63,Certificate Program in Database and Data Analytics
63,Course Availability Notification
63,Please use this form to be notified when this course is open for enrollment.
63,First Name*
63,Last Name*
63,Email*
63,Phone Number
63,Course Code
63,Course
63,Submit
63,Contact Us
63,Speak to a student services representative.
63,Call (408) 861-3860
63,Envelope extension@ucsc.edu
63,Subscribe
63,"Stay up to date on new courses, upcoming events, and alumni activities."
63,Info
63,Academic Calendar
63,Policies
63,Students with Disabilities
63,FAQ
63,Title IX External link
63,Resources
63,Forms
63,Workforce Training
63,Concurrent Enrollment
63,Career Services
63,Course Catalogs
63,About
63,Areas of Study
63,Instructors
63,Our UC Value
63,Digital Badges
63,Work for Us
63,Contact Us
63,Map Icon Map & Directions
63,Phone Icon (408) 861-3700
63,Envelope Icon extension@ucsc.edu
63,"Map Marker Icon 3175 Bowers Ave, Santa Clara, CA"
63,LinkedIn
63,YouTube
63,Instagram
63,Twitter
63,Facebook
63,©2021 UCSC Silicon Valley Extension and its licensors. All rights reserved.
64,Loading
65,Contact Support
67,How to Force Index on a SQL Server Query? - Interview Question of the Week #281 - SQL Authority with Pinal Dave
67,"April Discount: Comprehensive Database Performance Health Check | TestimonialsConsultingTrainingFree VideosAll ArticlesInterview Questions and AnswersSQL Tips and TricksSQL PerformanceSQL PuzzleBig DataBlog StatsSQL BooksSearch SQLAuthorityDownloadsHire MeHealth CheckTrainingHow to Force Index on a SQL Server Query? – Interview Question of the Week #281June 14, 2020Pinal DaveSQL Interview Questions and Answers2 CommentsQuestion: How to Force Index on a SQL Server Query?Answer: I personally do not like to force an index on any query. As a matter of fact, I have enough bad experience with this one. I always recommend in my Comprehensive Database Performance Health Check, I always discuss why index hints and query hints are not recommended in my health check consulting engagement.Here is how you can force an index to be used with a query with the help of an index hint.SELECT *"
67,FROM [WideWorldImporters].[Sales].[Invoices]
67,WITH(INDEX([FK_Sales_Invoices_AccountsPersonID]))
67,"WHERE CustomerID = 191In the above query, we are forcing the index FK_Sales_Invoices_AccountsPersonID to the index. We can always pass the name of the index in the WITH clause and that index will be used for the query. If the index does not exist, it will give you an error, so it is a good idea to check if the index exists before the query is executed.It is not required that you have to use the name of the index in the SQL Query. You can also use the number as well.SELECT *"
67,FROM [WideWorldImporters].[Sales].[Invoices]
67,WITH(INDEX(0))
67,"WHERE CustomerID = 191For example, the above query will use the clustered index (if exists) or a heap to retrieve data from the table.You can replace the index id zero with the other index number (greater than 1) which will represent the non-clustered index on the table.SELECT *"
67,FROM [WideWorldImporters].[Sales].[Invoices]
67,WITH(INDEX(2))
67,"WHERE CustomerID = 191For example, the above query will use the first non clustered index and use it to retrieve the data. The id of the index you can check in the sys.index table.UPDATE: If a clustered index exists, INDEX(0) forces a clustered index scan and INDEX(1) forces a clustered index scan or seek. If no clustered index exists, INDEX(0) forces a table scan and INDEX(1) is interpreted as an error. (Thanks to Carsten Saastamoinen-Jakobsen)Once again, I want to reiterate that the index hint is not a good way to get started with the coding. You need to make sure that your query is re-written in such a way that it uses the most optimal index.Reference: Pinal Dave (https://blog.sqlauthority.com)"
67,"Clustered Index, Query Hint, SQL Index, SQL Scripts, SQL ServerPrevious PostCan a Database Have Multiple Files with Extension MDF? – Interview Question of the Week #279Next PostDoes BIT Datatype Equal to 1 or TRUE in SQL Server? – Interview Question of the Week #282Related PostsSQL SERVER – GROUP BY Columns with XMLPATH – Comma Delimit Multiple RowsOctober 3, 2014Interview Question of the Week #007 – How to Reindex Every Table of the Database?February 15, 2015SQL SERVER – Get the List of Object Dependencies – sp_depends and information_schema.routinesFebruary 4, 2010 2 Comments. Leave new"
67,"Carsten Saastamoinen-Jakobsen June 21, 2020 12:20 pmSELECT * FROM [WideWorldImporters].[Sales].[Invoices] WITH(INDEX(1)) WHERE CustomerID = 191For example, the above query will use the first non clustered index and use it to retrieve the data.It’s wrong! The rules are:If a clustered index exists, INDEX(0) forces a clustered index scan and INDEX(1) forces a clustered index scan or seek. If no clustered index exists, INDEX(0) forces a table scan and INDEX(1) is interpreted as an error. Reply"
67,"Pinal Dave June 21, 2020 1:12 pmYou are correct. I wanted to know use index(2) but ended up using index 1 in example. So now to make my blog correct, I will add your note there.Thanks Carsten. Reply"
67,Leave a Reply Cancel reply
67,"Pinal Dave is an SQL Server Performance Tuning Expert and independent consultant with over 17 years of hands-on experience. He holds a Masters of Science degree and numerous database certifications.Pinal has authored 12 SQL Server database books and 37 Pluralsight courses. To freely share his knowledge and help others build their expertise, Pinal has also written more than 5,500 database tech articles on his blog at https://blog.sqlauthority.com.Pinal is an experienced and dedicated professional with a deep commitment to flawless customer service. If you need help with any SQL Server Performance Tuning Issues, please feel free to reach out at pinal@sqlauthority.com.Pinal is also a CrossFit Level 1 Trainer (CF-L1) and CrossFit Level 2 Trainer (CF-L2).Nupur Dave is a social media enthusiast and an independent consultant. She primarily focuses on the database domain, helping clients build short and long term multi-channel campaigns to drive leads for their sales pipeline.Exclusive NewsletterWebsite Is your SQL Server running slow and you want to speed it up without sharing server credentials? In my Comprehensive Database Performance Health Check, we can work together remotely and resolve your biggest performance troublemakers in less than 4 hours.Once you learn my business secrets, you will fix the majority of problems in the future.Have you ever opened any PowerPoint deck when you face SQL Server Performance Tuning emergencies? SQL Server Performance Tuning Practical Workshop is my MOST popular training with no PowerPoint presentations and 100% practical demonstrations.Essentially I share my business secrets to optimize SQL Server performance."
67,SQL Interview Q & ATestimonialsSearchPrivacy Policy© 2006 – 2021 All rights reserved. pinal @ SQLAuthority.com
67,Menu
67,Go to mobile version
68,FreeBSD Network Performance Tuning @ Calomel.org
68,home
68,rss
68,search
68,"April 05, 2021"
68,FreeBSD Tuning and Optimization
68,performance modifications for 1gig and 10gig networks
68,The default install of FreeBSD and TrueOS is quite fast and will work well
68,the majority of the time. If you installed either FreeBSD without any
68,"modifications you will not be disappointed. But, what if you wanted to get the"
68,most out of your install or you simply want to understand more about the OS ?
68,"In this post we offer some ideas to tune, tweak and optimize FreeBSD's network"
68,stack to get the most out of the operating system. Further down on the page we
68,offer proofs to show gained performance and lower latency as well as links to
68,the graphing tools we used so you can do the same.
68,"FreeBSD is fast, but hardware is important"
68,If you want to achieve optimized network throughput you need to use good
68,"hardware. As Monty Python taught us, it is daft to build a castle in a"
68,"swamp. Cheap hardware will cause nothing but misery. High latency, low"
68,throughput and poor driver support not to mention inconsistent performance
68,under load. A case in point is the built in network port on motherboards. The
68,"chipset may negotiate at one(1) gigabit, but it will not perform well under"
68,stress.
68,The Network Tuning and Performance
68,Guide uses both hardware setups and similar network modifications. The
68,first is an example of a one(1) gigabit machine for home or office use. The
68,second is a rack mounted server for trunked ten(10) gigabit and forty(40)
68,gigabit high speed networks. Both hardware configurations are actively
68,supported in production in the storage and https web server locations. For more
68,"information on SSL speeds, please take a look at the AES-NI SSL Performance benchmarks and our Chelsio Unified Wire Adapter on"
68,FreeBSD notes.
68,## Home or Office server (close to silent)
68,Processor
68,": Intel Core i7-6700 Skylake @ 3.40GHz , 65 watt"
68,CPU Cooler
68,: Noctua NH-D9L Dual Tower CPU Cooler
68,Motherboard
68,: Asus Z270-A LGA 1151
68,Memory
68,: Kingston HyperX FURY DDR4 32GB 2133MHz (HX421C14FBK4/32)
68,Video
68,: Intel HD Graphics 530 integrated graphics on CPU
68,Hard Drive
68,": Samsung 960 EVO Series, 1TB PCIe NVMe, M.2 Internal SSD (MZ-V6E1T0BW)"
68,"HGST Ultrastar He10 HUH721010ALE604 10TB, two(2) drives, ZFS RAID1 mirror"
68,"Power Supply : EVGA SuperNOVA 650 P2, 80+ PLATINUM 650W"
68,Case
68,: Corsair Carbide Series Air 540 with Arctic F12 PWM PST Fans
68,"Network Card : Chelsio T520-BT, Dual port RJ-45 / 10GBase-T, 20 watts (PCIe 3 x8)"
68,-OR-
68,"Intel I350-T2 Server Adapter, 4.4 watts (PCIe v2.1 x4)"
68,"NOTE: Though we prefer Chelsio on FreeBSD, the Intel I350-T2 is an affordable,"
68,fast and stable line rate NIC which uses the FreeBSD igb(4) driver.
68,## Rack mounted server
68,Processor
68,": Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz 95 Watt, 12 Core"
68,Motherboard
68,": SMC, Intel C612 chipset"
68,Memory
68,": 128 GB, DDR4-2400MHz, registered ECC memory w/ Thermal Sensor"
68,Chassis
68,": SMC, 2U, 24 bay (2.5"") with SAS3 expander backplane"
68,Controller
68,": LSI MegaRAID SAS3 9300-4i4e, 12 Gigabit/sec HBA"
68,Hard Drive
68,": 24x Mushkin MKNSSDRE1TB Reactor 1TB SSDs, mirrored ZFS root, LZ4 compression"
68,"Network Card : Chelsio T520-CR, 10GBASE-SR, LC Duplex (PCI Express x8)"
68,Transceiver
68,": Chelsio SFP+ 10Gbit, SM10G-SR (850nm wavelength)"
68,-OR-
68,Network Card : Myricom Myri-10G 10G-PCIE2-8B2-2S (PCI Express x8)
68,Transceiver
68,: Myricom Myri-10G SFP+ 10GBase-SR optical fiber (850nm wavelength)
68,Switches
68,: Arista 7150S-52
68,Both switches were able to saturate
68,: Force10 S4810
68,a bidirectional 10gig interface
68,The /boot/loader.conf
68,The /boot/loader.conf is where we setup the specifics for our network cards
68,and some hashes. We tried to completely comment each of the options in the
68,file. Directives which are commented out are not used and only included for
68,reference. You should be able to copy and paste the following text into your
68,loader.conf if you wish.
68,# Calomel.org
68,-|-
68,April 2021
68,# https://calomel.org/freebsd_network_tuning.html
68,# ZFS root boot config
68,"zfs_load=""YES"""
68,"vfs.root.mountfrom=""zfs:zroot"""
68,"# Pf firewall kernel modules, preload"
68,"pf_load=""YES"""
68,"pflog_load=""YES"""
68,"# ZFS: the maximum upper limit of RAM used for dirty, ""modified"", uncommitted"
68,# data which vfs.zfs.dirty_data_max can not exceed. The server has 64GB of RAM
68,"# in which we will allow up to 16GB, if needed, to cache incoming data before"
68,# TXG commit to the PCIe NVMe array. Note: the dirty_data cache is part of the
68,"# Adaptive Replacement Cache (ARC) and can be viewed in ""top"" as the ""Anon"""
68,# value under ARC.
68,"vfs.zfs.dirty_data_max_max=""17179869184"""
68,"# (default 4294967296, 4GB)"
68,# ZFS: max percentage of total server RAM allowed to be dirty (Anon in top).
68,# 25% of 64 GBytes of RAM is 16MB which is equal to vfs.zfs.dirty_data_max_max .
68,"vfs.zfs.dirty_data_max_percent=""25"""
68,# (default 10 percent)
68,# hostcache cache limit is the number of ip addresses in the hostcache list.
68,# Setting the value to zero(0) stops any ip address connection information from
68,"# being cached and negates the need for ""net.inet.tcp.hostcache.expire"". We"
68,# find disabling the hostcache increases burst data rates if a subnet was
68,# incorrectly graded as slow on a previous connection. A host cache entry is
68,"# the client's cached tcp connection details and metrics (TTL, SSTRESH and"
68,# VARTTL) the server can use to improve future performance of connections
68,"# between the same two hosts. When a tcp connection is completed, our server"
68,# will cache information about the connection until an expire timeout. If a new
68,"# connection between the same client is initiated before the cache has expired,"
68,# the connection will use the cached connection details to setup the
68,# connection's internal variables. This pre-cached setup allows the client and
68,# server to reach optimal performance significantly faster because the server
68,# will not need to go through the usual steps of re-learning the optimal
68,# parameters for the connection. To view the current host cache stats use
68,"# ""sysctl net.inet.tcp.hostcache.list"""
68,"net.inet.tcp.hostcache.enable=""0"""
68,"net.inet.tcp.hostcache.cachelimit=""0"""
68,"# Drive Labels. A diskid or gptid is a long, unique string assigned to drives"
68,# which we find are difficult to relate to. We prefer to disable diskid's and
68,"# gptid's and use GPT Labels, like gpt/disk0, or the raw device names, like"
68,"# nvd0p2 for the first NVMe drive, second partition. Use ""glabel status"" to"
68,# display a map of GPT Labels to raw device names in order to identify the
68,"# physical drive location. When adding new drives, try to use gpt labels"
68,"# instead of raw device names in case the drives move to different SATA, SAS or"
68,# SCSI interface ports.
68,"kern.geom.label.disk_ident.enable=""0"" # (default 1) diskid/DISK-ABC0123..."
68,"kern.geom.label.gptid.enable=""0"""
68,# (default 1) gptid/123abc-abc123...
68,"# Disable Hyper Threading (HT), also known as Intel's proprietary simultaneous"
68,# multithreading (SMT) because implementations typically share TLBs and L1
68,# caches between threads which is a security concern. SMT is likely to slow
68,# down workloads not specifically optimized for SMT if you have a CPU with more
68,"# than two(2) real CPU cores. Secondly, multi-queue network cards are as much"
68,# as 20% slower when network queues are bound to both real CPU cores and SMT
68,# virtual cores due to interrupt processing collisions.
68,"machdep.hyperthreading_allowed=""0"""
68,"# (default 1, allow Hyper Threading (HT))"
68,# Enable the optimized version of the soreceive() kernel socket interface for
68,# stream (TCP) sockets. soreceive_stream() only does one sockbuf unlock/lock
68,# per receive independent of the length of data to be moved into the uio
68,# compared to soreceive() which unlocks/locks per *mbuf*. soreceive_stream()
68,# can significantly reduced CPU usage and lock contention when receiving fast
68,"# TCP streams. Additional gains are obtained when the receiving application,"
68,"# like a web server, is using SO_RCVLOWAT to batch up some data before a read"
68,# (and wakeup) is done. NOTE: disable net.inet.tcp.soreceive_stream when using
68,"# rndc to update BIND DNS records otherwise the following error will trigger,"
68,"# ""rndc: recv failed: host unreachable""."
68,"net.inet.tcp.soreceive_stream=""1"""
68,# (default 0)
68,"# NETISR: by default, FreeBSD uses a single thread to process all network"
68,# traffic in accordance with the strong ordering requirements found in some
68,"# protocols, such as TCP. In order to increase potential packet processing"
68,"# concurrency, net.isr.maxthreads can be define as ""-1"" which will"
68,# automatically enable netisr threads equal to the number of CPU cores in the
68,"# machine. Now, all CPU cores can be used for packet processing and the system"
68,# will not be limited to a single thread running on a single CPU core.
68,"# The Intel igb(4) driver with queues autoconfigured (hw.igb.num_queues=""0"")"
68,# and msix enabled (hw.igb.enable_msix=1) will create the maximum number of
68,"# queues limited by the Intel igb hardware, msix messages and the number of"
68,# CPUs in the system. Once the igb interface maximum number of queues is
68,"# defined, an interrupt handler is bound to each of those queues on their"
68,# respective seperate CPU cores. The igb driver then creates a separate
68,# single-threaded taskqueue for each queue and each queue's interrupt handler
68,# sends work to its associated taskqueue when the interrupt fires. Those
68,# taskqueues are on the same CPU core where the ethernet packets were received
68,# and processed by the driver. All IP (and above) processing for that packet
68,# will be performed on the same CPU the queue interrupt was bound to thus
68,# gaining CPU affinity for that flow.
68,# A single net.isr workflow on a Core i5 CPU can process ~4Gbit/sec of traffic
68,# which is adequate for a dual 1Gbit/sec firewall interface. On a system
68,"# supporting mostly non-ordered protocols such as UDP (HTTP/3, Google's QUIC or"
68,# NTPd) you may want to assign more queues and bind them to their own CPU core.
68,"# For a 10GBit/sec interface, we recommend a modern CPU with at least four(4)"
68,"# real CPU cores and enable net.isr.maxthreads=""-1"". Use ""netstat -Q"" to check"
68,"# bindings and work streams. ""vmstat -i"" for interrupts per network queue."
68,# https://lists.freebsd.org/pipermail/freebsd-net/2014-April/038470.html
68,# Do Not enable net.isr.maxthreads on Chelsio T5/T4 cards.
68,"net.isr.maxthreads=""-1"""
68,"# (default 1, single threaded)"
68,# NETISR: Kernel network dispatch service. Enforced ordering will limit the
68,"# opportunity for concurrency, but maintain the strong ordering requirement"
68,# found in protocols such as TCP. Of related concern is CPU affinity; it is
68,# desirable to process all data associated with a particular stream on the same
68,# CPU core over time in order to avoid acquiring locks associated with the
68,"# connection on different CPUs, keep connection data in one L1/L2 cache, and to"
68,# generally encourage associated user threads to live on the same CPU as the
68,# stream. It's also desirable to avoid lock migration and contention where
68,# locks are associated with more than one flow.
68,"# By default, FreeBSD uses a single net.isr thread (net.isr.maxthreads=""1"") for"
68,# strict protocol ordering and we can bind that thread to CPU0 to take
68,"# advantage of CPU affinity. When net.isr.maxthreads=""-1"" each thread will be"
68,"# bound to its own CPU core. Use ""netstat -Q"" to check bindings and"
68,# workstreams. https://blog.cloudflare.com/how-to-receive-a-million-packets/
68,# Do Not enable net.isr.bindthreads on Chelsio T5/T4 cards.
68,"net.isr.bindthreads=""1"""
68,"# (default 0, runs randomly on any one cpu core)"
68,# PF: Increase the size of the pf(4) source nodes hashtable from 32k to 1M. As
68,"# the amount of remote source addresses starts to reach 100K, Pf will begin to"
68,# be the limiting factor with regards to packet throughput on the network
68,"# interfaces. By increasing the hashtable to 1M, Pf can sustain upwards of 80%"
68,# of the maximum packets per second throughput with more than a million source
68,"# addresses. Also set ""src-nodes 1000000"" in /etc/pf.conf . The hashtable"
68,# increase is necessary for HTTP/3 UDP traffic due to the sheer number of
68,# malicious UDP packets creating states.
68,# https://www.bsdcan.org/2016/schedule/attachments/365_Improving%20PF
68,"net.pf.source_nodes_hashsize=""1048576"""
68,# (default 32768)
68,###
68,######
68,######### OFF BELOW HERE #########
68,"# Other options not used, but included for future reference."
68,# Disable UDP/IPv4 and UDP/IPv6 checksum offloading to network card
68,"#hw.hn.enable_udp4cs=""0"""
68,"# (default 1, enabled)"
68,"#hw.hn.enable_udp6cs=""0"""
68,"# (default 1, enabled)"
68,"#hw.hn.trust_hostudp=""0"""
68,"# (default 1, enabled)"
68,# H-TCP Congestion Control for a more aggressive increase in sending speed on
68,"# higher latency, high bandwidth networks with minimal packet loss."
68,"#cc_htcp_load=""YES"""
68,# RACK TCP Stack: Netflix's TCP Recent ACKnowledgment (Recent ACK) and Tail
68,# Loss Probe (TLP) for improved Retransmit TimeOut response.
68,"#tcp_rack_load=""YES"""
68,# CUBIC Congestion Control improves TCP-friendliness and RTT-fairness. The
68,# window growth function of CUBIC is governed by a cubic function in terms of
68,# the elapsed time since the last loss event.
68,# https://labs.ripe.net/Members/gih/bbr-tcp
68,"#cc_cubic_load=""YES"""
68,"# CAIA Delay-Gradient (CDG) is a temporal, delay-based TCP congestion control"
68,"#cc_cdg_load=""YES"""
68,# Maximum Send Queue Length: common recommendations are to set the interface
68,# buffer size to the number of packets the interface can transmit (send) in 50
68,# milliseconds _OR_ 256 packets times the number of interfaces in the machine;
68,# whichever value is greater. To calculate a size of a 50 millisecond buffer
68,# for a 60 megabit network take the bandwidth in megabits divided by 8 bits
68,"# divided by the MTU times 50 millisecond times 1000, 60/8/1460*50*1000=256.84"
68,"# packets in 50 milliseconds. OR, if the box has two(2) interfaces take 256"
68,# packets times two(2) NICs to equal 512 packets.
68,512 is greater then 256.84
68,# so set to 512.
68,"# Our preference, if and only if you regularly reach your maximum upload"
68,"# bandwidth, is to define the interface queue length as two(2) times the value"
68,"# set in the interface transmit descriptor ring, ""hw.igb.txd"". If"
68,"# hw.igb.txd=""1024"" then set the net.link.ifqmaxlen=""2048""."
68,# An indirect result of increasing the interface queue is the buffer acts like
68,# a large TCP initial congestion window (init_cwnd) by allowing a network stack
68,# to burst packets at the start of a connection. Do not to set to zero(0) or
68,"# the network will stop working due to ""no network buffers"" available. Do not"
68,# set the interface buffer ludicrously large to avoid buffer bloat.
68,"#net.link.ifqmaxlen=""2048"""
68,# (default 50)
68,# accf accept filters are used so the server will not have to context switch
68,# several times before performing the initial parsing of the request. This
68,# could decrease server load by reducing the amount of CPU time to handle
68,# incoming requests.
68,buffer incoming connections until complete HTTP requests
68,"# arrive (nginx apache) for nginx http add, ""listen 127.0.0.1:80"
68,"# accept_filter=httpready;"""
68,"#accf_http_load=""YES"""
68,# A FreeBSD accept_data filter can be used to protect https HTTP/2 (TLS) web
68,"# servers, proxies, and accelerators. When a remote client connects to an Nginx"
68,# https (TCP port 443) service the FreeBSD network stack negotiates the TCP
68,"# connection. Without an accept_filter, the Nginx daemon immediately accept()'s"
68,# the connection and will process the client data stream no matter how small or
68,# slow the transfer is. This means Nginx will waste resources on clients who
68,"# never send any requests, send partial requests, immediately disconnect or"
68,"# time out. With an accept_filter, the FreeBSD kernel still does the TCP"
68,# handshake but now the accept_filter will wait for the remote client to send a
68,# full request before ever notifying the nginx deamon of the new connection.
68,# The result is the Nginx deamon can focus on serving active client connections
68,# using its resources more efficiently. The accept_filter does not affect the
68,# latency or speed of client requests to Nginx because the Nginx daemon is
68,# notified of a complete client request at the same time as not using a filter.
68,"# For nginx https servers add ""listen 127.0.0.1:443 ssl http2"
68,"# accept_filter=dataready;"" to the nginx.conf ."
68,# https://savagedlight.me/2015/08/23/eli5-freebsd-accept-filters/
68,"#accf_data_load=""YES"""
68,"# Asynchronous I/O, or non-blocking I/O is a form of input/output processing"
68,# permitting other processing to continue before the transmission has finished.
68,# AIO is used for accelerating Nginx on ZFS. Check for our tutorials on both.
68,# FreeBSD 11.0 removed the aio kernel module
68,"#aio_load=""YES"""
68,"# qlimit for igmp, arp, ether and ip6 queues only (netstat -Q) (default 256)"
68,"#net.isr.defaultqlimit=""2048"" # (default 256)"
68,# enable /dev/crypto for IPSEC of custom seeding using the AES-NI Intel
68,# hardware cpu support
68,"#aesni_load=""YES"""
68,# load the Intel PRO/1000 PCI Express kernel module on boot
68,"#if_em_load=""YES"""
68,# load the Myri10GE kernel module on boot
68,"#if_mxge_load=""YES"""
68,# load the Chelsio T520 (cxl) kernel module on boot
68,"#t5fw_cfg_load=""YES"""
68,"#if_cxgbe_load=""YES"""
68,# load the PF CARP module
68,"#if_carp_load=""YES"""
68,# Wait for full DNS request accept filter (unbound)
68,"#accf_dns_load=""YES"""
68,# Advanced Host Controller Interface (AHCI)
68,"#ahci_load=""YES"""
68,######################################### intel igb tuning ##############
68,"# Intel igb(4) kernel driver, preload"
68,"#if_igb_load=""YES"""
68,# Intel igb(4): netmap is natively supported on the following network devices
68,"# on FreeBSD: em(4), igb(4), ixgbe(4), lem(4), re(4)"
68,# Once of the best upgrades for a network server is to replace the network
68,# interface with an efficient network card. The on-board chipsets use a
68,# significant amount of CPU time. By simply installing an Intel i350 network
68,# card you can reduce CPU time and interrupt processing and reduce latency.
68,# Intel igb(4): Message Signaled Interrupts (MSI-X) provide multiple interrupt
68,"# vectors, which allow multiple interrupts to be handled simultaneously and"
68,# loadbalanced across multiple cores. This improvement helps improve CPU
68,# utilization and lowers latency.
68,"# Verify MSI-X is being used by the NIC using ""dmesg | grep -i msi"" with the"
68,"# output looking similar to, ""igb0: Using MSIX interrupts with 5 vectors"" for a"
68,"# two(2) port, four(4) queue Intel i350-T2 network card."
68,"#hw.igb.enable_msix=""1"""
68,# (default 1)
68,# Intel igb(4): Adaptive interrupt Moderation adjusts the interrupt rate
68,# dynamically based on packet size and throughput and reduces system load for
68,"# igb(4). Enabling AIM, and the separate MSIX option, will result in"
68,# significantly better efficiency in the network stack.
68,"#hw.igb.enable_aim=""1"""
68,# (default 1)
68,# Intel igb(4): FreeBSD puts an upper limit on the number of received packets a
68,# network card can concurrently process to 100 packets per cycle. This limit is
68,# in place because of inefficiencies in IRQ sharing when the network card is
68,# using the same IRQ as another device. When the Intel network card is assigned
68,# a unique IRQ (dmesg) and MSI-X is enabled through the driver
68,# (hw.igb.enable_msix=1) then interrupt scheduling is significantly more
68,# efficient and the NIC can be allowed to process packets as fast as they are
68,"# received. A value of ""-1"" means unlimited packet processing and sets the same"
68,# value to dev.igb.0.rx_processing_limit and dev.igb.1.rx_processing_limit .
68,# Option removed in FreeBSD 12
68,"#hw.igb.rx_process_limit=""-1"""
68,# (default 100 packets to process concurrently)
68,# Intel igb(4): The Intel i350-T2 dual port NIC supports up to eight(8)
68,"# input/output queues per network port, the card has two(2) network ports."
68,# Multiple transmit and receive queues in network hardware allow network
68,# traffic streams to be distributed into queues. Queues can be mapped by the
68,# FreeBSD network card driver to specific processor cores leading to reduced
68,# CPU cache misses. Queues also distribute the workload over multiple CPU
68,"# cores, process network traffic in parallel and prevent network traffic or"
68,# interrupt processing from overwhelming a single CPU core.
68,# http://www.intel.com/content/dam/doc/white-paper/improving-network-performance-in-multi-core-systems-paper.pdf
68,# For a firewall under heavy CPU load we recommend setting the number of
68,# network queues equal to the total number of real CPU cores in the machine
68,"# divided by the number of active network ports. For example, a firewall with"
68,# four(4) real CPU cores and an i350-T2 dual port NIC should use two(2) queues
68,# per network port (hw.igb.num_queues=2). This equals a total of four(4)
68,# network queues over two(2) network ports which map to to four(4) real CPU
68,# cores. A FreeBSD server with four(4) real CPU cores and a single network port
68,"# should use four(4) network queues (hw.igb.num_queues=4). Or, set"
68,# hw.igb.num_queues to zero(0) to allow the FreeBSD driver to automatically set
68,# the number of network queues to the number of CPU cores. It is not recommend
68,# to allow more network queues than real CPU cores per network port.
68,"# Query total interrupts per queue with ""vmstat -i"" and use ""top -CHIPS"" to"
68,# watch CPU usage per igb0:que. Multiple network queues will trigger more total
68,"# interrupts compared to a single network queue, but the processing of each of"
68,# those queues will be spread over multiple CPU cores allowing the system to
68,# handle increased network traffic loads.
68,"#hw.igb.num_queues=""2"""
68,"# (default 0 , queues equal the number of CPU real cores)"
68,# Intel igb(4): Intel PRO 1000 network chipsets support a maximum of 4096 Rx
68,# and 4096 Tx descriptors. Two cases when you could change the amount of
68,# descriptors are: 1) Low RAM and 2) CPU or bus saturation. If the system RAM
68,"# is too low you can drop the amount of descriptors to 128, but the system may"
68,# drop packets if it can not processes the packets fast enough. If you have a
68,# large number of packets incoming and they are being processed too slowly then
68,# you can increase to the descriptors up to 4096. Increasing descriptors is
68,# only a hack because the system is too slow to processes the packets in a
68,# timely manner. You should look into getting a faster CPU with a wider PCI bus
68,"# or identifying why the receiving application is so slow. Use ""netstat -ihw 1"""
68,# and look for idrops. Note that each received packet requires one Receive
68,"# Descriptor, and each descriptor uses 2 KB of memory. A setting of ""1024"" is"
68,# the most efficient value. https://fasterdata.es.net/host-tuning/nic-tuning/
68,"#hw.igb.rxd=""4096"""
68,# (default 1024)
68,"#hw.igb.txd=""4096"""
68,# (default 1024)
68,# maximum number of interrupts per second generated by single igb(4) (default
68,# 8000). FreeBSD 10 supports the new drivers which reduces interrupts
68,# significantly.
68,"#hw.igb.max_interrupt_rate=""16000"" # (default 8000)"
68,# Intel igb(4): using older intel drivers and jumbo frames caused memory
68,# fragmentation as header splitting wouldn't allocate jumbo clusters. The
68,"# current intel drivers do not seem to have these issues, so headers splitting"
68,# is disabled by default.
68,#hw.igb.header_split=0 # (default 0)
68,######################################### intel igb tuning ##############
68,# IPv6: disable automatically adding an IPv6 link-local address to interfaces.
68,# The link-local address conflict in Pf leading to issues with a global ipv6
68,# address and the link-local address.
68,"#net.inet6.ip6.auto_linklocal=""0"""
68,"# (default 1, add fe80:: address)"
68,# thermal sensors for intel or amd cpus
68,"#coretemp_load=""YES"""
68,"#amdtemp_load=""YES"""
68,# higher HZ settings have a negative impact on machine performance due to
68,# handling more timer interrupts resulting in more context switches and cache
68,# flushes (default 1000).
68,Lower HZ settings can have a detrimental effect on
68,# ZFS.
68,# http://lists.freebsd.org/pipermail/freebsd-questions/2005-April/083482.html
68,# Also take a look into kern.sched.interact and kern.sched.slice in
68,# /etc/sysctl.conf
68,#kern.hz=1000
68,# increase the number of network mbufs the system is willing to allocate.
68,Each
68,"# cluster represents approximately 2K of memory, so a value of 524288"
68,# represents 1GB of kernel memory reserved for network buffers. (default
68,# 492680)
68,"#kern.ipc.nmbclusters=""492680"""
68,"#kern.ipc.nmbjumbop=""246339"""
68,# maximum number of interrupts per second on any interrupt level (vmstat -i for
68,"# total rate). If you still see Interrupt Storm detected messages, increase the"
68,# limit to a higher number and look for the culprit.
68,For 10gig NIC's set to
68,# 9000 and use large MTU. (default 1000)
68,"#hw.intr_storm_threshold=""9000"""
68,"# Size of the syncache hash table, must be a power of 2 (default 512)"
68,"#net.inet.tcp.syncache.hashsize=""1024"""
68,# Limit the number of entries permitted in each bucket of the hash table. (default 30)
68,"#net.inet.tcp.syncache.bucketlimit=""100"""
68,# number of hash table buckets to handle incoming tcp connections. a value of
68,# 65536 allows the system to handle millions incoming connections. each tcp
68,# entry in the hash table on x86_64 uses 252 bytes of ram.
68,vmstat -z | egrep
68,"# ""ITEM|tcpcb"" (default 65536 which is ~16 million connections)"
68,"#net.inet.tcp.tcbhashsize=""65536"""
68,"# when booting, display the ascii art FreeBSD Orb with the two horns on top."
68,"# Just a cosmetic preference over ""beastie"", the multicolored daemon with"
68,# pitchfork and oversized shoes.
68,"#loader_logo=""orb"""
68,# How many seconds to sit at the boot menu before booting the server. Reduce
68,"# this value for a faster booting machine or set to ""-1"" for no delay. For a"
68,"# server, you may want to increase this time if you have the BIOS auto boot"
68,# after a power outage or brownout. By increasing the delay you allow more time
68,"# for the power grid to stabilize and UPS batteries to re-charge. Ideally, you"
68,# want to avoid the system fast booting into the OS and mounting the file
68,# system only to power off due to another brownout. If you are at the console
68,# during boot you can always hit enter to bypass this delay.
68,"#autoboot_delay=""60"""
68,# (default 10) seconds
68,"#autoboot_delay=""-1"""
68,# (default 10) seconds
68,"# NOTE regarding ""net.isr.*"" : Processor affinity can effectively reduce cache"
68,# problems but it does not curb the persistent load-balancing problem.[1]
68,# Processor affinity becomes more complicated in systems with non-uniform
68,# architectures. A system with two dual-core hyper-threaded CPUs presents a
68,# challenge to a scheduling algorithm. There is complete affinity between two
68,"# virtual CPUs implemented on the same core via hyper-threading, partial"
68,# affinity between two cores on the same physical chip (as the cores share
68,"# some, but not all, cache), and no affinity between separate physical chips."
68,# https://github.com/freebsd/freebsd/blob/master/sys/net/netisr.c
68,"# qlimit for igmp, arp, ether and ip6 queues only (netstat -Q) (default 256)"
68,"#net.isr.defaultqlimit=""256"""
68,"# limit per-workstream queues (use ""netstat -Q"" if Qdrop is greater then 0"
68,# increase this directive) (default 10240)
68,"#net.isr.maxqlimit=""10240"""
68,# SIFTR (Statistical Information For TCP Research) is a kernel module which
68,# logs a range of statistics on active TCP connections to a log file in comma
68,# separated format. Only useful for researching tcp flows as it does add some
68,# processing load to the system.
68,# http://manpages.ubuntu.com/manpages/precise/man4/siftr.4freebsd.html
68,"#siftr_load=""YES"""
68,### EOF ###
68,The /etc/sysctl.conf
68,The /etc/sysctl.conf is the primary optimization file. Everything from
68,"congestion control to buffer changes can be found here. Again, each option we"
68,changed is fully commented and may also have a link to a research study for
68,more information. Directives which are commented out are not used and included
68,for reference. This is a large file so take some time to look through each
68,option and understand why we made the change from default.
68,# Calomel.org
68,-|-
68,April 2021
68,# https://calomel.org/freebsd_network_tuning.html
68,# TCP Tuning: The throughput of connection is limited by two windows: the
68,# (Initial) Congestion Window and the TCP Receive Window (RWIN). The Congestion
68,"# Window avoids exceeding the capacity of the network (RACK, CAIA, H-TCP or"
68,# NewReno congestion control); and the Receive Window avoids exceeding the
68,# capacity of the receiver to process data (flow control). When our server is
68,# able to process packets as fast as they are received we want to allow the
68,"# remote sending host to send data as fast as the network, Congestion Window,"
68,# will allow. https://en.wikipedia.org/wiki/TCP_tuning
68,"# IPC Socket Buffer: the maximum combined socket buffer size, in bytes, defined"
68,# by SO_SNDBUF and SO_RCVBUF. kern.ipc.maxsockbuf is also used to define the
68,# window scaling factor (wscale in tcpdump) our server will advertise. The
68,# window scaling factor is defined as the maximum volume of data allowed in
68,# transit before the recieving server is required to send an ACK packet
68,# (acknowledgment) to the sending server. FreeBSD's default maxsockbuf value is
68,# two(2) megabytes which corresponds to a window scaling factor (wscale) of
68,"# six(6) allowing the remote sender to transmit up to 2^6 x 65,535 bytes ="
68,"# 4,194,240 bytes (4MB) in flight, on the network before requiring an ACK"
68,"# packet from our server. In order to support the throughput of modern, long"
68,# fat networks (LFN) with variable latency we suggest increasing the maximum
68,"# socket buffer to at least 16MB if the system has enough RAM. ""netstat -m"""
68,# displays the amount of network buffers used. Increase kern.ipc.maxsockbuf if
68,"# the counters for ""mbufs denied"" or ""mbufs delayed"" are greater than zero(0)."
68,# https://en.wikipedia.org/wiki/TCP_window_scale_option
68,# https://en.wikipedia.org/wiki/Bandwidth-delay_product
68,# speed:
68,1 Gbit
68,maxsockbuf:
68,2MB
68,wscale:
68,in-flight:
68,2^6*65KB =
68,4MB (default)
68,# speed:
68,2 Gbit
68,maxsockbuf:
68,4MB
68,wscale:
68,in-flight:
68,2^7*65KB =
68,8MB
68,# speed:
68,10 Gbit
68,maxsockbuf:
68,16MB
68,wscale:
68,in-flight:
68,2^9*65KB =
68,32MB
68,# speed:
68,40 Gbit
68,maxsockbuf: 150MB
68,wscale: 12
68,in-flight: 2^12*65KB =
68,260MB
68,# speed: 100 Gbit
68,maxsockbuf: 600MB
68,wscale: 14
68,in-flight: 2^14*65KB = 1064MB
68,#kern.ipc.maxsockbuf=2097152
68,# (wscale
68,6 ; default)
68,#kern.ipc.maxsockbuf=4194304
68,# (wscale
68,#kern.ipc.maxsockbuf=16777216
68,# (wscale
68,#kern.ipc.maxsockbuf=157286400
68,# (wscale 12)
68,kern.ipc.maxsockbuf=614400000
68,# (wscale 14)
68,# TCP Buffers: Larger buffers and TCP Large Window Extensions (RFC1323) can
68,# help alleviate the long fat network (LFN) problem caused by insufficient
68,# window size; limited to 65535 bytes without RFC 1323 scaling. Verify the
68,"# window scaling extension is enabled with net.inet.tcp.rfc1323=1, which is"
68,# default. Both the client and server must support RFC 1323 to take advantage
68,# of scalable buffers. A network connection at 100Mbit/sec with a latency of 10
68,# milliseconds has a bandwidth-delay product of 125 kilobytes
68,# ((100*10^6*10*10^-3)/8=125000) which is the same BDP of a 1Gbit LAN with
68,# one(1) millisecond latency ((1000*10^6*1*10^-3)/8=125000 bytes). As the
68,# latency and/or throughput increase so does the BDP. If the connection needs
68,# more buffer space the kernel will dynamically increase these network buffer
68,# values by net.inet.tcp.sendbuf_inc and net.inet.tcp.recvbuf_inc increments.
68,"# Use ""netstat -an"" to watch Recv-Q and Send-Q as the kernel increases the"
68,# network buffer up to net.inet.tcp.recvbuf_max and net.inet.tcp.sendbuf_max .
68,# https://en.wikipedia.org/wiki/Bandwidth-delay_product
68,#net.inet.tcp.recvbuf_inc=65536
68,# (default 16384)
68,net.inet.tcp.recvbuf_max=4194304
68,# (default 2097152)
68,net.inet.tcp.recvspace=65536
68,# (default 65536)
68,net.inet.tcp.sendbuf_inc=65536
68,# (default 8192)
68,net.inet.tcp.sendbuf_max=4194304
68,# (default 2097152)
68,net.inet.tcp.sendspace=65536
68,# (default 32768)
68,# maximum segment size (MSS) specifies the largest payload of data in a single
68,# IPv4 TCP segment. RFC 6691 states the maximum segment size should equal the
68,"# effective MTU minus the fixed IP and TCP headers, but before subtracting IP"
68,# options like TCP timestamps. Path MTU Discovery (PMTUD) is not supported by
68,# all internet paths and can lead to increased connection setup latency so the
68,# MMS can be defined manually.
68,"# Option 1 - Maximum Payload - To construct the maximum MMS, start with an"
68,# ethernet frame size of 1514 bytes and subtract 14 bytes for the ethernet
68,# header for an interface MTU of 1500 bytes. Then subtract 20 bytes for the IP
68,# header and 20 bytes for the TCP header to equal an Maximum Segment Size (MSS)
68,# of tcp.mssdflt=1460 bytes. With net.inet.tcp.rfc1323 enabled the packet
68,# payload is reduced by a further 12 bytes and the MSS is reduced from
68,# tcp.mssdflt=1460 bytes to a packet payload of 1448 bytes total. An MMS of
68,# 1448 bytes has a 95.64% packet efficiency (1448/1514=0.9564).
68,# Option 2 - No Frags - Google states the HTTP/3 QUIC (Quick UDP Internet
68,# Connection) IPv4 datagram should be no larger than 1280 octets to attempt to
68,# avoid any packet fragmentation over any Internet path. To follow Google's
68,# no-fragment UDP policy for TCP packets set FreeBSD's MSS to 1240 bytes. To
68,# construct Google's no-fragment datagram start with an ethernet frame size of
68,# 1294 bytes and subtract 14 bytes for the ethernet header to equal Google's
68,# recommended PMTU size of 1280 bytes. Then subtract 20 bytes for the IP header
68,"# and 20 bytes for the TCP header to equal tcp.mssdflt=1240 bytes. Then, before"
68,"# the packet is sent, FreeBSD will set the TCP timestamp (rfc1323) on the"
68,# packet reducing the true packet payload (MSS) another 12 bytes from
68,# tcp.mssdflt=1240 bytes to 1228 bytes which has an 94.89% packet efficiency
68,# (1228/1294=0.9489). https://tools.ietf.org/html/draft-ietf-quic-transport-20
68,# Broken packets: IP fragmentation is flawed
68,# https://blog.cloudflare.com/ip-fragmentation-is-broken/
68,# FYI: PF with an outgoing scrub rule will re-package the packet using an MTU
68,"# of 1460 by default, thus overriding the mssdflt setting wasting CPU time and"
68,# adding latency.
68,net.inet.tcp.mssdflt=1460
68,# Option 1 (default 536)
68,#net.inet.tcp.mssdflt=1240
68,# Option 2 (default 536)
68,"# minimum, maximum segment size (mMSS) specifies the smallest payload of data"
68,# in a single IPv4 TCP segment our system will agree to send when negotiating
68,# with the client. RFC 6691 states that a minimum MTU size of 576 bytes must be
68,# supported and the MSS option should equal the effective MTU minus the fixed
68,"# IP and TCP headers, but without subtracting IP or TCP options. To construct"
68,# the minimum MSS start with a frame size of 590 bytes and subtract 14 bytes
68,# for the ethernet header to equal the RFC 6691 recomended MTU size of 576
68,# bytes. Continue by subtracting 20 bytes for the IP header and 20 bytes for
68,"# the TCP header to equal tcp.minmss=536 bytes. Then, before the packet is"
68,"# sent, FreeBSD will set the TCP timestamp (rfc1323) on the packet reducing the"
68,# true packet payload (MSS) another 12 bytes from tcp.minmss=536 bytes to 524
68,# bytes which is 90.9% packet efficiency (524/576=0.909). The default mMMS is
68,# only 84% efficient (216/256=0.84).
68,net.inet.tcp.minmss=536
68,# (default 216)
68,# TCP Slow start gradually increases the data send rate until the TCP
68,"# congestion algorithm (CDG, H-TCP) calculates the networks maximum carrying"
68,# capacity without dropping packets. TCP Congestion Control with Appropriate
68,# Byte Counting (ABC) allows our server to increase the maximum congestion
68,"# window exponentially by the amount of data ACKed, but limits the maximum"
68,# increment per ACK to (abc_l_var * maxseg) bytes. An abc_l_var of 44 times a
68,# maxseg of 1460 bytes would allow slow start to increase the congestion window
68,# by more than 64 kilobytes per step; 65535 bytes is the TCP receive buffer
68,# size of most hosts without TCP window scaling.
68,net.inet.tcp.abc_l_var=44
68,# (default 2) if net.inet.tcp.mssdflt = 1460
68,#net.inet.tcp.abc_l_var=52
68,# (default 2) if net.inet.tcp.mssdflt = 1240
68,# Initial Congestion Window (initcwnd) limits the amount of segments TCP can
68,# send onto the network before receiving an ACK from the other machine.
68,# Increasing the TCP Initial Congestion Window will reduce data transfer
68,# latency during the slow start phase of a TCP connection. The initial
68,"# congestion window should be increased to speed up short, burst connections in"
68,# order to send the most data in the shortest time frame without overloading
68,# any network buffers. Google's study reported sixteen(16) segments as showing
68,# the lowest latency initial congestion window. Also test 44 segments which is
68,"# 65535 bytes, the TCP receive buffer size of most hosts without TCP window"
68,# scaling.
68,# https://developers.google.com/speed/pagespeed/service/tcp_initcwnd_paper.pdf
68,net.inet.tcp.initcwnd_segments=44
68,# (default 10 for FreeBSD 11.2) if net.inet.tcp.mssdflt = 1460
68,#net.inet.tcp.initcwnd_segments=52
68,# (default 10 for FreeBSD 11.2) if net.inet.tcp.mssdflt = 1240
68,#net.inet.tcp.experimental.initcwnd10=1
68,# (default
68,1 for FreeBSD 10.1)
68,#net.inet.tcp.experimental.initcwnd10=1
68,# (default
68,0 for FreeBSD
68,9.2)
68,#net.inet.tcp.local_slowstart_flightsize=16
68,# (default
68,4 for FreeBSD
68,9.1)
68,#net.inet.tcp.slowstart_flightsize=16
68,# (default
68,4 for FreeBSD
68,9.1)
68,# RFC 8511 TCP Alternative Backoff with ECN (ABE) for FreeBSD's default
68,"# congestion control mechanism, NewReno. The reception of a Congestion"
68,# Experienced (CE) Explicit Congestion Notification (ECN) event indicates that
68,"# an Active Queue Management (AQM) mechanism is used at the bottleneck, thus an"
68,# assumption can be made that the bottleneck network queue is transient. The
68,# feedback of this signal allows the TCP sender-side ECN reaction in congestion
68,# avoidance to reduce the Congestion Window (cwnd) by a less aggressive 20%
68,# rather than the NewReno default of 50% when inferred packet loss is detected.
68,# The goal is more packets on the wire using greater network capacity while
68,# minimizing actual packet loss. https://tools.ietf.org/html/rfc8511
68,net.inet.tcp.cc.abe=1
68,"# (default 0, disabled)"
68,# RFC 6675 increases the accuracy of TCP Fast Recovery when combined with
68,# Selective Acknowledgement (net.inet.tcp.sack.enable=1). TCP loss recovery is
68,"# enhanced by computing ""pipe"", a sender side estimation of the number of bytes"
68,# still outstanding on the network. Fast Recovery is augmented by sending data
68,"# on each ACK as necessary to prevent ""pipe"" from falling below the slow-start"
68,# threshold (ssthresh). The TCP window size and SACK-based decisions are still
68,"# determined by the congestion control algorithm; CDG, CUBIC or H-TCP if"
68,"# enabled, newreno by default."
68,net.inet.tcp.rfc6675_pipe=1
68,# (default 0)
68,# Reduce the amount of SYN/ACKs the server will re-transmit to an ip address
68,# whom did not respond to the first SYN/ACK. On a client's initial connection
68,# our server will always send a SYN/ACK in response to the client's initial
68,"# SYN. Limiting retranstited SYN/ACKS reduces local syn cache size and a ""SYN"
68,"# flood"" DoS attack's collateral damage by not sending SYN/ACKs back to spoofed"
68,"# ips, multiple times. If we do continue to send SYN/ACKs to spoofed IPs they"
68,"# may send RST's back to us and an ""amplification"" attack would begin against"
68,# our host. If you do not wish to send retransmits at all then set to zero(0)
68,# especially if you are under a SYN attack. If our first SYN/ACK gets dropped
68,# the client will re-send another SYN if they still want to connect. Also set
68,"# ""net.inet.tcp.msl"" to two(2) times the average round trip time of a client,"
68,"# but no lower then 2000ms (2s). Test with ""netstat -s -p tcp"" and look under"
68,# syncache entries. http://www.ouah.org/spank.txt
68,# https://people.freebsd.org/~jlemon/papers/syncache.pdf
68,net.inet.tcp.syncache.rexmtlimit=0
68,# (default 3)
68,# IP fragments require CPU processing time and system memory to reassemble. Due
68,# to multiple attacks vectors ip fragmentation can contribute to and that
68,"# fragmentation can be used to evade packet inspection and auditing, we will"
68,# not accept IPv4 or IPv6 fragments. Comment out these directives when
68,# supporting traffic which generates fragments by design; like NFS and certain
68,# preternatural functions of the Sony PS4 gaming console.
68,# https://en.wikipedia.org/wiki/IP_fragmentation_attack
68,# https://www.freebsd.org/security/advisories/FreeBSD-SA-18:10.ip.asc
68,net.inet.ip.maxfragpackets=0
68,# (default 63474)
68,net.inet.ip.maxfragsperpacket=0
68,# (default 16)
68,net.inet6.ip6.maxfragpackets=0
68,# (default 507715)
68,net.inet6.ip6.maxfrags=0
68,# (default 507715)
68,# Syncookies have advantages and disadvantages. Syncookies are useful if you
68,# are being DoS attacked as this method helps filter the proper clients from
68,"# the attack machines. But, since the TCP options from the initial SYN are not"
68,"# saved in syncookies, the tcp options are not applied to the connection,"
68,"# precluding use of features like window scale, timestamps, or exact MSS"
68,"# sizing. As the returning ACK establishes the connection, it may be possible"
68,# for an attacker to ACK flood a machine in an attempt to create a connection.
68,# Another benefit to overflowing to the point of getting a valid SYN cookie is
68,# the attacker can include data payload. Now that the attacker can send data to
68,"# a FreeBSD network daemon, even using a spoofed source IP address, they can"
68,# have FreeBSD do processing on the data which is not something the attacker
68,# could do without having SYN cookies. Even though syncookies are helpful
68,"# during a DoS, we are going to disable syncookies at this time."
68,net.inet.tcp.syncookies=0
68,# (default 1)
68,# RFC 6528 Initial Sequence Numbers (ISN) refer to the unique 32-bit sequence
68,# number assigned to each new Transmission Control Protocol (TCP) connection.
68,"# The TCP protocol assigns an ISN to each new byte, beginning with 0 and"
68,# incrementally adding a secret number every four seconds until the limit is
68,# exhausted. In continuous communication all available ISN options could be
68,# used up in a few hours. Normally a new secret number is only chosen after the
68,# ISN limit has been exceeded. In order to defend against Sequence Number
68,# Attacks the ISN secret key should not be used sufficiently often that it
68,"# would be regarded as predictable, and thus insecure. Reseeding the ISN will"
68,"# break TIME_WAIT recycling for a few minutes. BUT, for the more paranoid,"
68,# simply choose a random number of seconds in which a new ISN secret should be
68,# generated.
68,https://tools.ietf.org/html/rfc6528
68,net.inet.tcp.isn_reseed_interval=4500
68,"# (default 0, disabled)"
68,"# TCP segmentation offload (TSO), also called large segment offload (LSO),"
68,# should be disabled on NAT firewalls and routers. TSO/LSO works by queuing up
68,# large 64KB buffers and letting the network interface card (NIC) split them
68,# into separate packets. The problem is the NIC can build a packet that is the
68,"# wrong size and would be dropped by a switch or the receiving machine, like"
68,# for NFS fragmented traffic. If the packet is dropped the overall sending
68,# bandwidth is reduced significantly. You can also disable TSO in /etc/rc.conf
68,"# using the ""-tso"" directive after the network card configuration; for example,"
68,"# ifconfig_igb0=""inet 10.10.10.1 netmask 255.255.255.0 -tso"". Verify TSO is off"
68,"# on the hardware by making sure TSO4 and TSO6 are not seen in the ""options="""
68,# section using ifconfig.
68,# http://www.peerwisdom.org/2013/04/03/large-send-offload-and-network-performance/
68,net.inet.tcp.tso=0
68,# (default 1)
68,# Intel i350-T2 igb(4): flow control manages the rate of data transmission
68,# between two nodes preventing a fast sender from overwhelming a slow receiver.
68,"# Ethernet ""PAUSE"" frames will pause transmission of all traffic types on a"
68,"# physical link, not just the individual flow causing the problem. By disabling"
68,# physical link flow control the link instead relies on native TCP or QUIC UDP
68,# internal congestion control which is peer based on IP address and more fair
68,# to each flow. The options are: (0=No Flow Control) (1=Receive Pause)
68,"# (2=Transmit Pause) (3=Full Flow Control, Default). A value of zero(0)"
68,# disables ethernet flow control on the Intel igb(4) interface.
68,# http://virtualthreads.blogspot.com/2006/02/beware-ethernet-flow-control.html
68,dev.igb.0.fc=0
68,# (default 3)
68,# Intel i350-T2 igb(4): the rx_budget sets the maximum number of receive
68,"# packets to process in an interrupt. If the budget is reached, the"
68,# remaining/pending packets will be processed later in a scheduled taskqueue.
68,# The default of zero(0) indicates a FreeBSD 12 default of sixteen(16) frames
68,# can be accepted at a time which is less than 24 kilobytes. If the server is
68,# not CPU limited and also receiving an agglomeration of QUIC HTTP/3 UDP
68,"# packets, we advise increasing the budget to a maximum of 65535 packets. ""man"
68,"# iflib"" for more information."
68,dev.igb.0.iflib.rx_budget=65535
68,"# (default 0, which is 16 frames)"
68,dev.igb.1.iflib.rx_budget=65535
68,"# (default 0, which is 16 frames)"
68,# Fortuna pseudorandom number generator (PRNG) maximum event size is also
68,# referred to as the minimum pool size. Fortuna has a main generator which
68,# supplies the OS with PRNG data. The Fortuna generator is seeded by 32
68,# separate 'Fortuna' accumulation pools which each have to be filled with at
68,# least 'minpoolsize' bytes before being able to seed the OS with random bits.
68,"# On FreeBSD, the default 'minpoolsize' of 64 bytes is an estimate of the"
68,# minimum amount of bytes a new pool should contain to provide at least 128
68,"# bits of entropy. After a pool is used in a generator reseed, that pool is"
68,# reset to an empty string and must reach 'minpoolsize' bytes again before
68,# being used as a seed. Increasing the 'minpoolsize' allows higher entropy into
68,# the accumulation pools before being assimilated by the generator.
68,# The Fortuna authors state 64 bytes is safe enough even if an attacker
68,"# influences some random source data. To be a bit more paranoid, we increase"
68,# the 'minpoolsize' to 128 bytes so each pool will provide an absolute minimum
68,"# of 256 bits of entropy, but realistically closer to 1024 bits of entropy, for"
68,# each of the 32 Fortuna accumulation pools. Values of 128 bytes and 256 bytes
68,# are reasonable when coupled with a dedicated hardware based PRNG like the
68,# fast source Intel Secure Key RNG (PURE_RDRAND). Do not make the pool value
68,# too large as this will delay the reseed even if very good random sources are
68,# available. https://www.schneier.com/academic/paperfiles/fortuna.pdf
68,"# FYI: on FreeBSD 11, values over 64 can incur additional reboot time to"
68,"# populate the pools during the ""Feeding entropy:"" boot stage. For example, a"
68,# pool size value of 256 can add an additional 90 seconds to boot the machine.
68,# FreeBSD 12 has been patched to not incur the boot delay issue with larger
68,# pool values.
68,kern.random.fortuna.minpoolsize=128
68,# (default 64)
68,"# Entropy is the amount of order, disorder or chaos observed in a system which"
68,# can be observed by FreeBSD and fed though Fortuna to the accumulation pools.
68,# Setting the harvest.mask to 67583 allows the OS to harvest entropy from any
68,"# source including peripherals, network traffic, the universal memory allocator"
68,"# (UMA) and interrupts (SWI), but be warned, setting the harvest mask to 67583"
68,# will limit network throughput to less than a gigabit even on modern hardware.
68,# When running a ten(10) gigabit network with more than four(4) real CPU cores
68,# and more than four(4) network card queues it is recommended to reduce the
68,"# harvest mask to 33119 to disable UMA. FS_ATIME, INTERRUPT and NET_ETHER"
68,# entropy sources in order to achieve peak packets per second (PPS). By
68,"# default, Fortuna will use a CPU's 'Intel Secure Key RNG' if available in"
68,"# hardware (PURE_RDRAND). Use ""sysctl kern.random.harvest"" to check the"
68,# symbolic entropy sources being polled; disabled items are listed in square
68,# brackets. A harvest mask of 33119 is only around four(4%) more efficient than
68,# the default mask of 33247 at the maximum packets per second of the interface.
68,#kern.random.harvest.mask=351
68,"# (default 511,"
68,FreeBSD 11 and 12 without Intel Secure Key RNG)
68,#kern.random.harvest.mask=65887
68,"# (default 66047, FreeBSD 12 with Intel Secure Key RNG)"
68,kern.random.harvest.mask=33119
68,"# (default 33247, FreeBSD 13 with Intel Secure Key RNG)"
68,# HardenedBSD and DoS mitigation
68,hw.kbd.keymap_restrict_change=4
68,# disallow keymap changes for non-privileged users (default 0)
68,kern.elf32.allow_wx=0
68,"# enforce W^X memory mapping policy for 32 bit user processes (default 1, disabled)"
68,kern.elf64.allow_wx=0
68,"# enforce W^X memory mapping policy for 64 bit user processes (default 1, disabled)"
68,kern.ipc.shm_use_phys=1
68,"# lock shared memory into RAM and prevent it from being paged out to swap (default 0, disabled)"
68,kern.msgbuf_show_timestamp=1
68,# display timestamp in msgbuf (default 0)
68,kern.randompid=1
68,"# calculate PIDs by the modulus of an integer, set to one(1) to auto random (default 0)"
68,net.bpf.optimize_writers=1
68,# bpf is write-only unless program explicitly specifies the read filter (default 0)
68,net.inet.icmp.drop_redirect=1
68,# no redirected ICMP packets (default 0)
68,net.inet.ip.check_interface=1
68,# verify packet arrives on correct interface (default 0)
68,net.inet.ip.portrange.first=32768
68,# use ports 32768 to portrange.last for outgoing connections (default 10000)
68,net.inet.ip.portrange.randomcps=9999 # use random port allocation if less than this many ports per second are allocated (default 10)
68,net.inet.ip.portrange.randomtime=1 # seconds to use sequental port allocation before switching back to random (default 45 secs)
68,net.inet.ip.random_id=1
68,# assign a random IP id to each packet leaving the system (default 0)
68,net.inet.ip.redirect=0
68,# do not send IP redirects (default 1)
68,net.inet6.ip6.redirect=0
68,# do not send IPv6 redirects (default 1)
68,net.inet.tcp.blackhole=2
68,# drop tcp packets destined for closed ports (default 0)
68,net.inet.tcp.drop_synfin=1
68,# SYN/FIN packets get dropped on initial connection (default 0)
68,"net.inet.tcp.fast_finwait2_recycle=1 # recycle FIN/WAIT states quickly, helps against DoS, but may cause false RST (default 0)"
68,"net.inet.tcp.fastopen.client_enable=0 # disable TCP Fast Open client side, enforce three way TCP handshake (default 1, enabled)"
68,"net.inet.tcp.fastopen.server_enable=0 # disable TCP Fast Open server side, enforce three way TCP handshake (default 0)"
68,"net.inet.tcp.finwait2_timeout=1000 # TCP FIN_WAIT_2 timeout waiting for client FIN packet before state close (default 60000, 60 sec)"
68,net.inet.tcp.icmp_may_rst=0
68,# icmp may not send RST to avoid spoofed icmp/udp floods (default 1)
68,net.inet.tcp.keepcnt=2
68,# amount of tcp keep alive probe failures before socket is forced closed (default 8)
68,net.inet.tcp.keepidle=62000
68,"# time before starting tcp keep alive probes on an idle, TCP connection (default 7200000, 7200 secs)"
68,net.inet.tcp.keepinit=5000
68,"# tcp keep alive client reply timeout (default 75000, 75 secs)"
68,net.inet.tcp.msl=2500
68,"# Maximum Segment Lifetime, time the connection spends in TIME_WAIT state (default 30000, 2*MSL = 60 sec)"
68,net.inet.tcp.path_mtu_discovery=0
68,"# disable for mtu=1500 as most paths drop ICMP type 3 packets, but keep enabled for mtu=9000 (default 1)"
68,net.inet.udp.blackhole=1
68,# drop udp packets destined for closed sockets (default 0)
68,net.inet.udp.recvspace=1048576
68,"# UDP receive space, HTTP/3 webserver, ""netstat -sn -p udp"" and increase if full socket buffers (default 42080)"
68,security.bsd.hardlink_check_gid=1
68,"# unprivileged processes may not create hard links to files owned by other groups, DISABLE for mailman (default 0)"
68,security.bsd.hardlink_check_uid=1
68,"# unprivileged processes may not create hard links to files owned by other users,"
68,DISABLE for mailman (default 0)
68,security.bsd.see_other_gids=0
68,# groups only see their own processes. root can see all (default 1)
68,security.bsd.see_other_uids=0
68,# users only see their own processes. root can see all (default 1)
68,security.bsd.stack_guard_page=1
68,"# insert a stack guard page ahead of growable segments, stack smashing protection (SSP) (default 0)"
68,security.bsd.unprivileged_proc_debug=0 # unprivileged processes may not use process debugging (default 1)
68,security.bsd.unprivileged_read_msgbuf=0 # unprivileged processes may not read the kernel message buffer (default 1)
68,# ZFS Tuning for PCIe NVMe M.2 and 64GB system RAM
68,# book: FreeBSD Mastery: ZFS By Michael W Lucas and Allan Jude
68,# https://www.pugetsystems.com/labs/articles/Samsung-950-Pro-M-2-Additional-Cooling-Testing-795/
68,# http://dtrace.org/blogs/ahl/2012/12/13/zfs-fundamentals-transaction-groups/
68,# http://dtrace.org/blogs/ahl/2013/12/27/zfs-fundamentals-the-write-throttle/
68,# http://bit.csc.lsu.edu/~fchen/publications/papers/hpca11.pdf
68,# http://dtrace.org/blogs/ahl/2014/08/31/openzfs-tuning/
68,# https://www.freebsd.org/doc/handbook/zfs-advanced.html
68,# https://calomel.org/zfs_freebsd_root_install.html
68,# NVMe drive
68,: Samsung 960 EVO 1TB PCIe 3.0 ×4 NVMe M.2 (MZ-V6E1T0BW)
68,# before zfs tuning : reads 1.87 GB/s
68,writes 1.86 GB/s
68,scrub 1.88 GB/s
68,19.2K IOPs
68,after zfs tuning : reads 3.11 GB/s
68,writes 1.95 GB/s
68,scrub 3.11 GB/s
68,3.8K IOPs
68,# NVMe drive
68,: ADATA XPG SX8200 Pro 1TB PCIe 3.0 ×4 NVMe M.2 (ASX8200PNP-1TT-C)
68,# before zfs tuning : reads 1.88 GB/s
68,writes 1.86 GB/s
68,scrub 1.88 GB/s
68,19.3K IOPs
68,after zfs tuning : reads 3.27 GB/s
68,writes 2.62 GB/s
68,scrub 3.27 GB/s
68,2.6K IOPs
68,vfs.zfs.delay_min_dirty_percent=98
68,"# write throttle when dirty ""modified"" data reaches 98% of dirty_data_max (default 60%)"
68,vfs.zfs.dirty_data_sync_percent=95
68,# force commit Transaction Group (TXG) if dirty_data reaches 95% of dirty_data_max (default 20%)
68,vfs.zfs.min_auto_ashift=12
68,"# newly created pool ashift, set to 12 for 4K and 13 for 8k alignment, zdb (default 9, 512 byte, ashift=9)"
68,vfs.zfs.trim.txg_batch=128
68,# max number of TRIMs per top-level vdev (default 32)
68,vfs.zfs.txg.timeout=75
68,"# force commit Transaction Group (TXG) at 75 secs, increase to aggregated more data (default 5 sec)"
68,vfs.zfs.vdev.def_queue_depth=128
68,# max number of outstanding I/Os per top-level vdev (default 32)
68,vfs.zfs.vdev.write_gap_limit=0
68,"# max gap between any two aggregated writes, 0 to minimize frags (default 4096, 4KB)"
68,ZFS Tuning: The plan is to use large amounts of RAM for dirty_data_max to
68,# buffer incoming data before ZFS must commit the data in the next Transaction
68,# Group (TXG) to the physical drives in the pool. TXG commits are sequential by
68,# design; the incoming random write traffic cached between TXG commits is
68,# sequential when written to disk. When the server is able to keep more dirty
68,"# ""modified"" data in RAM before the next TXG commit, there is a greater chance"
68,# of long sequential writes without holes. These long sequential stripes of
68,# written data also result in significantly faster sequential reads.
68,# ZFS will trigger a forced TXG commit when either the temporal limit
68,# txg.timeout or the dirty data capacity limit dirty_data_sync_pct is reached.
68,# Increasing these two(2) limits will allow the system to collect more
68,# uncommitted data in RAM in order to write to the vdev in efficient sequential
68,"# stripes. But, understand, if the server losses power or crashes we lose all"
68,# dirty data in RAM not previously committed; so make sure to be on an
68,"# Uninterruptible Power Supply (UPS). A manual ""sync"" as well as a ""shutdown"""
68,"# or ""poweroff"" will always force a commit of all data in RAM to disk."
68,"# Dirty ""modified"" data in RAM can be read from, written to and modified even"
68,"# before the data is committed to disk. If the data set is rapidly changing,"
68,"# like during database transactions or bittorrent traffic, the changes will be"
68,# made solely to RAM in between TXG commits. Only the latest copy of the data
68,# in RAM will be written to disk on TXG commit which is a good argument for an
68,# extended txg.timeout.
68,# The number of outstanding I/Os per top-level vdev should be set to the
68,# maximum Queue Depth of the storage device times the number of threads
68,# supported by the storage device. According to the white sheets for NVMe
68,"# devices, the queue depth is 32 and concurrent thread support is four (QD 32"
68,# Thread 4). Set the vdev.def_queue_depth to a queue depth of 32 (Q32) times
68,# four(4) threads times one(1) NVMe drive to equal 128 max number of outstanding
68,# I/Os per top-level vdev. (32_queues_*_4_threads_*_1_drive=128).
68,"# Make sure to never, ever reach the dirty_data_sync_pct capacity limit"
68,"# especially if the zfs logbias is set to ""latency"". Logbias latency will"
68,# double write the same incoming data to ZIL and to the disk when
68,# dirty_data_sync_pct is reached halving throughput. Take a look at zfs logbias
68,"# ""throughput"" to avoid these double writes. When the server is accepting data"
68,# on a 1Gbit network interface the dirty_data_sync_pct should be larger than
68,# the true incoming throughput of the network times the txg.timeout; 118MB/sec
68,# times 75 seconds will require 8.85 gigabytes of dirty_data_max RAM space
68,# which is well below 95% of dirty_data_max at 15.2 gigabytes.
68,"# When determining the size of the dirty_data_max look at the amount of fast,"
68,# first and second tier cache available in the NVMe drives. All of the data in
68,# a completely filled dirty_data_max cache should be able to be committed to
68,# the drive well before the next txg.timeout even if a saturated network is
68,# concurrently writing data to dirty_data_max.
68,# The ADATA XPG SX8200 Pro 1TB NVMe has 165 gigabytes of first tier SLC cache
68,"# and 500 gigabytes of of second tier MLC, dynamic cache. The SX8200 can write"
68,# at 2.62 gigabytes per second to the first tier SLC cache when the drive is
68,"# properly cooled, meaning 14.16 gigabytes of dirty_data_sync_pct can be"
68,"# committed to the NVMe drive in five(5) seconds, well before the next"
68,# txg.timeout of 75 seconds.
68,# The Samsung 960 EVO NVMe 1TB has six(6) gigabytes of first tier cache plus
68,"# thirty six(36) gigabytes of second tier, dynamic cache. The 960 EVO can write"
68,# at 1.95 gigabytes per second when the drive is properly cooled meaning 14.16
68,# gigabytes of dirty_data_sync_pct can be committed to the NVMe drive in
68,"# seven(7) seconds, well before the next txg.timeout of 75 seconds."
68,"# NVMe M.2 Cooling: Enzotech BMR-C1 passive copper heat sinks (14mm x 14mm x 14mm,"
68,"# C1100 forged copper, 8-pack) work well to cool our NVMe drives. User four(4)"
68,"# heatsinks per NVMe drive, one 14mm x 14mm copper square per silicon chip."
68,"# The ZFS commit logic order is strictly sync_read, sync_write, async_read,"
68,# async_write and finally scrub/resilver .
68,###
68,######
68,######### OFF BELOW HERE #########
68,# ZFS Tuning
68,#vfs.zfs.delay_scale=500000
68,"# (default 500000 ns, nanoseconds)"
68,#vfs.zfs.dirty_data_max=17179869184
68,"# dirty_data can use up to 16GB RAM, equal to dirty_data_max_max (default, 10% of RAM or up to 4GB)"
68,#vfs.zfs.dirty_data_sync=12348030976
68,"# force commit Transaction Group (TXG) if dirty_data reaches 11.5GB (default 67108864, 64MB, FreeBSD 12.0; replaced by vfs.zfs.dirty_data_sync_pct on FreeBSD 12.1)"
68,#vfs.zfs.no_scrub_prefetch=0
68,# disable prefetch on scrubs (default 0)
68,#vfs.zfs.nopwrite_enabled=1
68,"# enable nopwrite feature, requires sha256 / sha512 checksums (default 1)"
68,#vfs.zfs.prefetch_disable=0
68,"# file-level prefetching, disable if zfs-stats prefetch stats below 10% (default 0 if RAM greater than 4GB)"
68,#vfs.zfs.resilver_delay=2
68,"# number of pause ticks to delay resilver on a busy pool (default 2, kern.hz 1000 ticks/sec / 2 = 500 IOPS)"
68,#vfs.zfs.scrub_delay=4
68,"# number of pause ticks to delay scrub on a busy pool (default 4, kern.hz 1000 ticks/sec / 4 = 250 IOPS)"
68,#vfs.zfs.sync_pass_rewrite=2
68,# rewrite new bps starting in this pass (default 2)
68,#vfs.zfs.trim.txg_delay=2
68,"# delay TRIMs by up to this many TXGs, trim.txg_delay * txg.timeout ~= 240 secs (default 32, 32*5secs=160 secs)"
68,#vfs.zfs.vdev.aggregation_limit=1048576
68,"# aggregated eight(8) TXGs into a single sequential TXG, make divisible by largest pool recordsize (default 131072, 128KB, FreeBSD 12.0; default 1048576 on FreeBSD 12.1)"
68,#vfs.zfs.vdev.async_read_max_active=3
68,# max async_read I/O requests per device in pool (default 3)
68,#vfs.zfs.vdev.async_read_min_active=1
68,# min async_read I/O requests per device in pool (default 1)
68,#vfs.zfs.vdev.async_write_active_max_dirty_percent=60 # percent dirty_data_max cached when max_active I/Os are all active (default 60%)
68,#vfs.zfs.vdev.async_write_active_min_dirty_percent=30 # percent dirty_data_max cached before linearly rising to max_active I/Os (default 30%)
68,#vfs.zfs.vdev.async_write_max_active=10
68,# max async_write I/O requests per device in pool (default 10)
68,#vfs.zfs.vdev.async_write_min_active=1
68,# min async_write I/O requests per device in pool (default 1)
68,#vfs.zfs.vdev.max_active=1000
68,# max I/Os of any type active per device in pool (default 1000)
68,#vfs.zfs.vdev.read_gap_limit=32768
68,"# max gap between any two reads being aggregated (default 32768, 32KB)"
68,#vfs.zfs.vdev.scrub_max_active=2
68,# max scrub I/Os active on each device (default 2)
68,#vfs.zfs.vdev.scrub_min_active=1
68,# min scrub I/Os active on each device (default 1)
68,#vfs.zfs.vdev.sync_read_max_active=10
68,# max sync_read I/O requests per device in pool (default 10)
68,#vfs.zfs.vdev.sync_read_min_active=10
68,# min sync_read I/O requests per device in pool (default 10)
68,#vfs.zfs.vdev.sync_write_max_active=10
68,# max sync_write I/O requests per device in pool (default 10)
68,#vfs.zfs.vdev.sync_write_min_active=10
68,# min sync_write I/O requests per device in pool (default 10)
68,#vfs.zfs.vdev.trim_max_active=64
68,# max trim I/O requests per device in pool (default 64)
68,#vfs.zfs.vdev.write_gap_limit=4096
68,"# max gap between any two writes being aggregated, 16K bittorrent, 4k nfs, 4k mysql (default 4096, 4KB)"
68,# ZFS L2ARC tuning - If you have read intensive workloads and limited RAM make
68,# sure to use an SSD for your L2ARC. Verify noprefetch is enabled(1) and
68,"# increase the speed at which the system can fill the L2ARC device. By default,"
68,# when the L2ARC is being populated FreeBSD will only write at 16MB/sec to the
68,# SSD. 16MB calculated by adding the speed of write_boost and write_max.
68,# 16MB/sec is too slow as many SSD's made today which can easily sustain
68,# 500MB/sec. It is recommend to set both write_boost and write_max to at least
68,"# 256MB each so the L2ARC can be quickly seeded. Contrary to myth, enterprise"
68,# class SSDs can last for many years under constant read/write abuse of a web
68,# server.
68,#vfs.zfs.l2arc_noprefetch=1
68,# (default 1)
68,#vfs.zfs.l2arc_write_boost=268435456 # (default 8388608)
68,#vfs.zfs.l2arc_write_max=268435456
68,# (default 8388608)
68,# General Security and DoS mitigation
68,#hw.hn.enable_udp4cs=1
68,# Offload UDP/IPv4 checksum to network card (default 1)
68,#hw.hn.enable_udp6cs=1
68,# Offload UDP/IPv6 checksum to network card (default 1)
68,#hw.ixl.enable_tx_fc_filter=1
68,"# filter out Ethertype 0x8808, flow control frames (default 1)"
68,#net.bpf.optimize_writers=0
68,# bpf are write-only unless program explicitly specifies the read filter (default 0)
68,#net.bpf.zerocopy_enable=0
68,"# zero-copy BPF buffers, breaks dhcpd ! (default 0)"
68,#net.inet.icmp.bmcastecho=0
68,# do not respond to ICMP packets sent to IP broadcast addresses (default 0)
68,#net.inet.icmp.log_redirect=0
68,# do not log redirected ICMP packet attempts (default 0)
68,#net.inet.icmp.maskfake=0
68,# do not fake reply to ICMP Address Mask Request packets (default 0)
68,#net.inet.icmp.maskrepl=0
68,# replies are not sent for ICMP address mask requests (default 0)
68,#net.inet.ip.accept_sourceroute=0
68,# drop source routed packets since they can not be trusted (default 0)
68,#net.inet.ip.portrange.randomized=1 # randomize outgoing upper ports (default 1)
68,#net.inet.ip.process_options=1
68,# process IP options in the incoming packets (default 1)
68,#net.inet.ip.sourceroute=0
68,# if source routed packets are accepted the route data is ignored (default 0)
68,#net.inet.ip.stealth=0
68,# do not reduce the TTL by one(1) when a packets goes through the firewall (default 0)
68,#net.inet.tcp.always_keepalive=1
68,"# tcp keep alive detection for dead peers, keepalive can be spoofed (default 1)"
68,#net.inet.tcp.ecn.enable=1
68,# Explicit Congestion Notification (ECN) allowed for incoming and outgoing connections (default 2)
68,#net.inet.tcp.keepintvl=75000
68,"# time between tcp.keepcnt keep alive probes (default 75000, 75 secs)"
68,#net.inet.tcp.maxtcptw=50000
68,# max number of tcp time_wait states for closing connections (default ~27767)
68,#net.inet.tcp.nolocaltimewait=0
68,# remove TIME_WAIT states for the loopback interface (default 0)
68,#net.inet.tcp.reass.maxqueuelen=100 # Max number of TCP Segments per Reassembly Queue (default 100)
68,#net.inet.tcp.rexmit_min=30
68,"# reduce unnecessary TCP retransmissions by increasing timeout, min+slop (default 30 ms)"
68,#net.inet.tcp.rexmit_slop=200
68,"# reduce the TCP retransmit timer, min+slop (default 200ms)"
68,#net.inet.udp.checksum=1
68,# hardware should generate UDP checksums (default 1)
68,#net.inet.udp.maxdgram=16384
68,# Maximum outgoing UDP datagram size to match MTU of localhost (default 9216)
68,#net.inet.sctp.blackhole=2
68,# drop stcp packets destined for closed ports (default 0)
68,# RACK TCP Stack: Netflix's TCP Recent ACKnowledgment (Recent ACK) and Tail
68,# Loss Probe (TLP) for improved Retransmit TimeOut response. RACK uses the
68,"# notion of time, instead of packet or sequence counts, to detect TCP losses"
68,# for connections supporting per-packet timestamps and selective acknowledgment
68,# (SACK). Connections that do not support SACK are automatically serviced by
68,"# the default, base FreeBSD TCP stack. Use ""sysctl"
68,"# net.inet.tcp.functions_available"" to show available TCP stacks loaded by the"
68,# kernel. FYI: introduced in FreeBSD 12; the kernel must be rebuilt with
68,# additional TCP stacks (makeoptions WITH_EXTRA_TCP_STACKS=1) and the high
68,# precision TCP timer (options TCPHPTS).
68,# https://tools.ietf.org/html/draft-ietf-tcpm-rack-04
68,#net.inet.tcp.functions_default=rack
68,# (default freebsd)
68,# RACK TCP Stack: The method used for Tail Loss Probe (TLP) calculations.
68,# https://tools.ietf.org/html/draft-ietf-tcpm-rack-04
68,# FYI: Needs Testing
68,#net.inet.tcp.rack.tlpmethod=3
68,"# (default 2, 0=no-de-ack-comp, 1=ID, 2=2.1, 3=2.2)"
68,"# RACK TCP Stack: send a Reset (RST) packet as soon as all data is sent and,"
68,"# perhaps, before all pending data is acknowledged (ACK) by the client. This"
68,# may help on busy servers to close connections quickly thus freeing up
68,# resources.
68,# FYI: Needs Testing
68,#net.inet.tcp.rack.data_after_close=0
68,# (default 1)
68,# H-TCP congestion control: The Hamilton TCP (HighSpeed-TCP) algorithm is a
68,# packet loss based congestion control and is more aggressive pushing up to max
68,# bandwidth (total BDP) and favors hosts with lower TTL / VARTTL than
68,"# ""newreno"". The default congrestion control ""newreno"" works well in most"
68,# conditions and enabling H-TCP may only gain a you few percentage points of
68,# throughput.
68,# http://www.sigcomm.org/sites/default/files/ccr/papers/2008/July/1384609-1384613.pdf
68,"# make sure to also add 'cc_htcp_load=""YES""' to /boot/loader.conf then check"
68,"# available congestion control options with ""sysctl net.inet.tcp.cc.available"""
68,#net.inet.tcp.cc.algorithm=htcp
68,# (default newreno)
68,# H-TCP congestion control: adaptive back off will increase bandwidth
68,# utilization by adjusting the additive-increase/multiplicative-decrease (AIMD)
68,# backoff parameter according to the amount of buffers available on the path.
68,# adaptive backoff ensures no queue along the path will remain completely empty
68,# after a packet loss event which increases buffer efficiency.
68,#net.inet.tcp.cc.htcp.adaptive_backoff=1
68,# (default 0 ; disabled)
68,# H-TCP congestion control: RTT scaling will increase the fairness between
68,# competing TCP flows traversing different RTT paths through a common
68,# bottleneck. rtt_scaling increases the Congestion Window Size (CWND)
68,# independent of path round-trip time (RTT) leading to lower latency for
68,# interactive sessions when the connection is saturated by bulk data transfers.
68,# Default is 0 (disabled)
68,#net.inet.tcp.cc.htcp.rtt_scaling=1
68,# (default 0 ; disabled)
68,# CAIA-Delay Gradient (CDG) is a hybrid TCP congestion control algorithm which
68,# reacts to both packet loss and inferred queuing delay. CDG attempts to
68,"# operate as a temporal, delay-based algorithm where possible while utilizing"
68,# heuristics to detect loss-based TCP cross traffic and compete effectively
68,# against other packet loss based congestion controls on the network.
68,"# During time-based operation, CDG uses a delay-gradient based probabilistic"
68,# backoff mechanism to infer non-congestion related packet losses. CDG
68,# periodically switches to loss-based operation when it detects a configurable
68,# number of consecutive delay-based backoffs have had no measurable effect.
68,"# During packet loss-based operation, CDG essentially reverts to"
68,"# cc_newreno-like behaviour. CDG oscillates between temporal, delay-based"
68,# operation and packet loss-based operation as dictated by network conditions.
68,"# Load the kernel module by adding 'cc_cdg_load=""YES""' to /boot/loader.conf and"
68,"# on next reboot verify the available congestion control options with ""sysctl"
68,"# net.inet.tcp.cc.available"""
68,# http://caia.swin.edu.au/cv/dahayes/content/networking2011-cdg-preprint.pdf
68,# http://caia.swin.edu.au/reports/110729A/CAIA-TR-110729A.pdf
68,# https://lwn.net/Articles/645115/
68,#net.inet.tcp.cc.algorithm=cdg
68,# (default newreno)
68,# CAIA-Delay Gradient (CDG) alpha_inc enables an experimental mode where the
68,# CDG window increase factor (alpha) is increased by one(1) MSS every alpha_inc
68,# RTTs during congestion avoidance mode. Setting alpha_inc to 1 results in the
68,# most aggressive growth of the window increase factor over time while a higher
68,# alpha_inc value results in slower growth.
68,#net.inet.tcp.cc.cdg.alpha_inc=1
68,"# (default 0, experimental mode disabled)"
68,# CUBIC congestion control: is a time based congestion control algorithm
68,"# optimized for high speed, high latency networks and a decent choice for"
68,# networks with minimal packet loss; most hard wired internet connections are
68,# in this catagory. CUBIC can improve startup throughput of bulk data transfers
68,# and burst transfers of a web server by up to 2x compared to packet loss based
68,# algorithms like newreno and H-TCP. FreeBSD 11.1 updated CUBIC code to match
68,"# the 2016 RFC including the slow start algorithm, HyStart. CUBIC Hystart uses"
68,"# two heuristics, based on RTT, to exit slow start earlier, but before losses"
68,"# start to occur. Add 'cc_cubic_load=""YES""' to /boot/loader.conf and check"
68,"# available congestion control options with ""sysctl net.inet.tcp.cc.available""."
68,# https://labs.ripe.net/Members/gih/bbr-tcp
68,#net.inet.tcp.cc.algorithm=cubic
68,# (default newreno)
68,# Firewall: Ip Forwarding to allow packets to traverse between interfaces and
68,"# is used for firewalls, bridges and routers. When fast IP forwarding is also"
68,"# enabled, IP packets are forwarded directly to the appropriate network"
68,"# interface with direct processing to completion, which greatly improves the"
68,"# throughput. All packets for local IP addresses, non-unicast, or with IP"
68,# options are handled by the normal IP input processing path. All features of
68,# the normal (slow) IP forwarding path are supported by fast forwarding
68,"# including firewall (through pfil(9) hooks) checking, except ipsec tunnel"
68,# brokering. The IP fast forwarding path does not generate ICMP redirect or
68,"# source quench messages though. Compared to normal IP forwarding, fast"
68,# forwarding can give a speedup of 40 to 60% in packet forwarding performance
68,# which is great for interactive connections like online games or VOIP where
68,# low latency is critical. These options are already enabled if
68,"# gateway_enable=""YES"" is in /etc/rc.conf"
68,#net.inet.ip.forwarding=1
68,# (default 0)
68,#net.inet.ip.fastforwarding=1
68,# (default 0)
68,FreeBSD 11 enabled fastforwarding by default
68,#net.inet6.ip6.forwarding=1
68,# (default 0)
68,# Increase the localhost buffer space as well as the maximum incoming and
68,# outgoing raw IP datagram size to 16384 bytes (2^14 bytes) which is the same
68,"# as the MTU for the localhost interface, ""ifconfig lo0"". The larger buffer"
68,"# space should allow services which listen on localhost, like web or database"
68,"# servers, to more efficiently move data to the network buffers."
68,#net.inet.raw.maxdgram=16384
68,# (default 9216)
68,#net.inet.raw.recvspace=16384
68,# (default 9216)
68,#net.local.stream.sendspace=16384
68,# (default 8192)
68,#net.local.stream.recvspace=16384
68,# (default 8192)
68,# The TCPT_REXMT timer is used to force retransmissions. TCP has the
68,# TCPT_REXMT timer set whenever segments have been sent for which ACKs are
68,"# expected, but not yet received. If an ACK is received which advances"
68,"# tp->snd_una, then the retransmit timer is cleared (if there are no more"
68,# outstanding segments) or reset to the base value (if there are more ACKs
68,"# expected). Whenever the retransmit timer goes off, we retransmit one"
68,"# unacknowledged segment, and do a backoff on the retransmit timer."
68,# net.inet.tcp.persmax=60000 # (default 60000)
68,# net.inet.tcp.persmin=5000
68,# (default 5000)
68,# Drop TCP options from 3rd and later retransmitted SYN
68,# net.inet.tcp.rexmit_drop_options=0
68,# (default 0)
68,# Enable tcp_drain routine for extra help when low on mbufs
68,# net.inet.tcp.do_tcpdrain=1 # (default 1)
68,# Myricom mxge(4): the maximum number of slices the driver will attempt to
68,# enable if enough system resources are available at boot. A slice is comprised
68,# of a set of receive queues and an associated interrupt thread. Multiple
68,# slices should be used when the network traffic is being limited by the
68,"# processing speed of a single CPU core. When using multiple slices, the NIC"
68,# hashes traffic to different slices based on the value of
68,# hw.mxge.rss_hashtype. Using multiple slices requires that your motherboard
68,# and Myri10GE NIC both be capable of MSI-X. The maximum number of slices
68,# is limited to the number of real CPU cores divided by the number of mxge
68,# network ports.
68,"#hw.mxge.max_slices=""1"""
68,"# (default 1, which uses a single cpu core)"
68,"# Myricom mxge(4): when multiple slices are enabled, the hash type determines"
68,# how incoming traffic is steered to each slice. A slice is comprised of a set
68,# of receive queues and an associated interrupt thread. Hashing is disabled
68,"# when using a single slice (hw.mxge.max_slices=1). The options are: =""1"""
68,"# hashes on the source and destination IPv4 addresses. =""2"" hashes on the"
68,# source and destination IPv4 addresses and also TCP source and destination
68,"# ports. =""4"" is the default and hashes on the TCP or UDP source ports. A value"
68,"# to ""4"" will more evenly distribute the flows over the slices. A value of ""1"""
68,# will lock client source ips to a single slice.
68,"#hw.mxge.rss_hash_type=""4"""
68,# (default 4)
68,# Myricom mxge(4): flow control manages the rate of data transmission between
68,# two nodes preventing a fast sender from overwhelming a slow receiver.
68,"# Ethernet ""PAUSE"" frames pause transmission of all traffic on a physical link,"
68,# not just the individual flow causing the problem. By disabling physical link
68,# flow control the link instead relies on TCP's internal flow control which is
68,# peer based on IP address and more fair to each flow. The mxge options are:
68,"# (0=No Flow Control) (1=Full Flow Control, Default). A value of zero(0)"
68,# disables ethernet flow control on the Myricom mxge(4) interface.
68,# http://virtualthreads.blogspot.com/2006/02/beware-ethernet-flow-control.html
68,#hw.mxge.flow_control_enabled=0
68,"# (default 1, enabled)"
68,# The number of frames the NIC's receive (rx) queue will accept before
68,# triggering a kernel inturrupt. If the NIC's queue is full and the kernel can
68,"# not process the packets fast enough then the packets are dropped. Use ""sysctl"
68,"# net.inet.ip.intr_queue_drops"" and ""netstat -Q"" and increase the queue_maxlen"
68,# if queue_drops is greater then zero(0). The real problem is the CPU or NIC is
68,"# not fast enough to handle the traffic, but if you are already at the limit of"
68,# your network then increasing these values will help.
68,#net.inet.ip.intr_queue_maxlen=2048
68,# (default 256)
68,#net.route.netisr_maxqlen=2048
68,# (default 256)
68,# Intel igb(4): freebsd limits the the number of received packets a network
68,# card can process to 100 packets per interrupt cycle. This limit is in place
68,# because of inefficiencies in IRQ sharing when the network card is using the
68,# same IRQ as another device. When the Intel network card is assigned a unique
68,# IRQ (dmesg) and MSI-X is enabled through the driver (hw.igb.enable_msix=1)
68,# then interrupt scheduling is significantly more efficient and the NIC can be
68,"# allowed to process packets as fast as they are received. A value of ""-1"""
68,# means unlimited packet processing. There is no need to set these options if
68,# hw.igb.rx_process_limit is already defined.
68,#dev.igb.0.rx_processing_limit=-1
68,# (default 100)
68,#dev.igb.1.rx_processing_limit=-1
68,# (default 100)
68,# Intel igb(4): Energy-Efficient Ethernet (EEE) is intended to reduce system
68,# power consumption up to 80% by setting the interface to a low power mode
68,# during periods of network inactivity. When the NIC is in low power mode this
68,# allows the CPU longer periods of time to also go into a sleep state thus
68,# lowering overall power usage. The problem is EEE can cause periodic packet
68,# loss and latency spikes when the interface transitions from low power mode.
68,# Packet loss from EEE will not show up in the missed_packets or dropped
68,"# counter because the packet was not dropped, but lost by the network card"
68,# during the transition phase. The Intel i350-T2 only requires 4.4 watts with
68,# both network ports active so we recommend disabling EEE especially on a
68,# server unless power usage is of higher priority. Verify DMA Coalesce is
68,# disabled (dev.igb.0.dmac=0) which is the default. WARNING: enabling EEE will
68,# significantly delay DHCP leases and the network interface will flip a few
68,# times on boot. https://en.wikipedia.org/wiki/Energy-Efficient_Ethernet
68,#dev.igb.0.eee_disabled=1
68,"# (default 0, enabled)"
68,#dev.igb.1.eee_disabled=1
68,"# (default 0, enabled)"
68,# Spoofed packet attacks may be used to overload the kernel route cache. A
68,# spoofed packet attack uses random source IPs to cause the kernel to generate
68,"# a temporary cached route in the route table, Route cache is an extraneous"
68,# caching layer mapping interfaces to routes to IPs and saves a lookup to the
68,# Forward Information Base (FIB); a routing table within the network stack. The
68,# IPv4 routing cache was intended to eliminate a FIB lookup and increase
68,"# performance. While a good idea in principle, unfortunately it provided a very"
68,# small performance boost in less than 10% of connections and opens up the
68,# possibility of a DoS vector. Setting rtexpire and rtminexpire to ten(10)
68,# seconds should be sufficient to protect the route table from attack.
68,# http://www.es.freebsd.org/doc/handbook/securing-freebsd.html
68,# Route cache options were removed in FreeBSD 11.0
68,#net.inet.ip.rtexpire=10
68,# (default 3600)
68,#net.inet.ip.rtminexpire=10
68,# (default 10
68,#net.inet.ip.rtmaxcache=128
68,# (default 128 )
68,"# somaxconn is the OS buffer, backlog queue depth for accepting new incoming TCP"
68,"# connections. An application will have its own, separate max queue length"
68,"# (maxqlen) which can be checked with ""netstat -Lan"". The default is 128"
68,# connections per application thread. Lets say your Nginx web server normally
68,# receives 100 connections/sec and is single threaded application. If clients
68,# are bursting in at a total of 250 connections/sec you may want to set the
68,# somaxconn at 512 to be a 512 deep connection buffer so the extra 122 clients
68,# (250-128=122) do not get denied service since you would have 412
68,"# (512-100=412) extra queue slots. Also, a large listen queue will do a better"
68,"# job of avoiding Denial of Service (DoS) attacks if, and only if, your"
68,# application can handle the TCP load at the cost of more RAM and CPU time.
68,# Nginx sets is backlog queue to the same as the OS somaxconn by default.
68,"# Note: ""kern.ipc.somaxconn"" is not shown in ""sysctl -a"" output, but searching"
68,"# for ""kern.ipc.soacceptqueue"" gives the same value and both directives stand"
68,# for the same buffer value.
68,#kern.ipc.soacceptqueue=1024
68,# (default 128 ; same as kern.ipc.somaxconn)
68,# The TCP window scale (rfc3390) option is used to increase the TCP receive
68,"# window size above its maximum value of 65,535 bytes (64k). TCP Time Stamps"
68,"# (rfc1323) allow nearly every segment, including retransmissions, to be"
68,# accurately timed at negligible computational cost. Both options should be
68,# enabled by default. Enhancing TCP Loss Recovery (rfc3042) says on packet
68,"# loss, trigger the fast retransmit algorithm instead of tcp timeout."
68,#net.inet.tcp.rfc1323=1
68,# (default 1)
68,#net.inet.tcp.rfc3042=1
68,# (default 1)
68,#net.inet.tcp.rfc3390=1
68,# (default 1)
68,# FreeBSD limits the maximum number of TCP reset (RST) and ICMP Unreachable
68,# packets the server will send every second. Limiting reply packets helps curb
68,# the effects of Brute-force TCP denial of service (DoS) attacks and UDP port
68,"# scans. Also, when Pf firewall client states expire FreeBSD will send out RST"
68,"# packets to tell the client the connection is closed. By default, FreeBSD will"
68,# send out 200 packets per second.
68,#net.inet.icmp.icmplim=1
68,# (default 200)
68,#net.inet.icmp.icmplim_output=0
68,# (default 1)
68,# Selective Acknowledgment (SACK) allows the receiver to inform the sender of
68,# packets which have been received and if any packets were dropped. The sender
68,# can then selectively retransmit the missing data without needing to
68,# retransmit entire blocks of data that have already been received
68,# successfully. SACK option is not mandatory and support must be negotiated
68,# when the connection is established using TCP header options. An attacker
68,# downloading large files can abuse SACK by asking for many random segments to
68,# be retransmitted. The server in response wastes system resources trying to
68,# fulfill superfluous requests. If you are serving small files to low latency
68,# clients then SACK can be disabled. If you see issues of flows randomly
68,"# pausing, try disabling SACK to see if there is equipment in the path which"
68,# does not handle SACK correctly.
68,#net.inet.tcp.sack.enable=1
68,# (default 1)
68,"# host cache is the client's cached tcp connection details and metrics (TTL,"
68,# SSTRESH and VARTTL) the server can use to improve future performance of
68,"# connections between the same two hosts. When a tcp connection is completed,"
68,# our server will cache information about the connection until an expire
68,# timeout. If a new connection between the same client is initiated before the
68,"# cache has expired, the connection will use the cached connection details to"
68,# setup the connection's internal variables. This pre-cached setup allows the
68,# client and server to reach optimal performance significantly faster because
68,# the server will not need to go through the usual steps of re-learning the
68,"# optimal parameters for the connection. Unfortunately, this can also make"
68,# performance worse because the hostcache will apply the exception case to
68,"# every new connection from a client within the expire time. In other words, in"
68,"# some cases, one person surfing your site from a mobile phone who has some"
68,# random packet loss can reduce your server's performance to this visitor even
68,# when their temporary loss has cleared.
68,3900 seconds allows clients who
68,# connect regularly to stay in our hostcache. To view the current host cache
68,"# stats use ""sysctl net.inet.tcp.hostcache.list"" . If you have"
68,"# ""net.inet.tcp.hostcache.cachelimit=0"" like in our /boot/loader.conf example"
68,# then this expire time is negated and not uesd.
68,#net.inet.tcp.hostcache.expire=3900
68,# (default 3600)
68,"# By default, acks are delayed by 100 ms or sent every other packet in order to"
68,# improve the chance of being added to another returned data packet which is
68,# full. This method can cut the number of tiny packets flowing across the
68,"# network and is efficient. But, delayed ACKs cause issues on modern, short"
68,"# hop, low latency networks. TCP works by increasing the congestion window,"
68,"# which is the amount of data currently traveling on the wire, based on the"
68,# number of ACKs received per time frame. Delaying the timing of the ACKs
68,"# received results in less data on the wire, time in TCP slowstart is doubled"
68,# and in congestion avoidance after packet loss the congestion window growth is
68,# slowed.
68,Setting delacktime higher then 100 will to slow downloads as ACKs
68,# are queued too long. On low latecy 10gig links we find a value of 20ms is
68,# optimal. http://www.tel.uva.es/personales/ignmig/pdfs/ogonzalez_NOC05.pdf
68,#net.inet.tcp.delayed_ack=1
68,# (default 1)
68,#net.inet.tcp.delacktime=20
68,# (default 100)
68,# security settings for jailed environments. it is generally a good idea to
68,# separately jail any service which is accessible by an external client like
68,# the web or mail server. This is especially true for public facing services.
68,"# take a look at ezjail, http://forums.freebsd.org/showthread.php?t=16860"
68,#security.jail.allow_raw_sockets=1
68,# (default 0)
68,#security.jail.enforce_statfs=2
68,# (default 2)
68,#security.jail.set_hostname_allowed=0
68,# (default 1)
68,#security.jail.socket_unixiproute_only=1 # (default 1)
68,#security.jail.sysvipc_allowed=0
68,# (default 0)
68,#security.jail.chflags_allowed=0
68,# (default 0)
68,# decrease the scheduler maximum time slice for lower latency program calls.
68,"# by default we use stathz/10 which equals thirteen(13). also, decrease the"
68,# scheduler maximum time for interactive programs as this is a dedicated
68,"# server (default 30). Also make sure you look into ""kern.hz=100"" in /boot/loader.conf"
68,#kern.sched.interact=5 # (default 30)
68,#kern.sched.slice=3
68,# (default 12)
68,# threads per process
68,#kern.threads.max_threads_per_proc=9000
68,"# create core dump file on ""exited on signal 6"""
68,#kern.coredump=1
68,# (default 1)
68,#kern.sugid_coredump=1
68,# (default 0)
68,"#kern.corefile=""/tmp/%N.core"" # (default %N.core)"
68,# TCP keep alive can help detecting network errors and signaling connection
68,"# problems. Keep alives will increase signaling bandwidth used, but as"
68,"# bandwidth utilized by signaling channels is low from its nature, the increase"
68,# is insignificant. the system will disconnect a dead TCP connection when the
68,# remote peer is dead or unresponsive for: 10000 + (5000 x 8) = 50000 msec (50
68,# sec)
68,#net.inet.tcp.keepidle=10000
68,# (default 7200000 )
68,#net.inet.tcp.keepintvl=5000
68,# (default 75000 )
68,#net.inet.tcp.always_keepalive=1 # (default 1)
68,# UFS hard drive read ahead equivalent to 4 MiB at 32KiB block size. Easily
68,# increases read speeds from 60 MB/sec to 80 MB/sec on a single spinning hard
68,# drive. Samsung 830 SSD drives went from 310 MB/sec to 372 MB/sec (SATA 6).
68,# use Bonnie++ to performance test file system I/O
68,#vfs.read_max=128
68,# global limit for number of sockets in the system. If kern.ipc.numopensockets
68,# plus net.inet.tcp.maxtcptw is close to kern.ipc.maxsockets then increase this
68,# value
68,#kern.ipc.maxsockets = 25600
68,# spread tcp timer callout load evenly across cpus. We did not see any speed
68,# benefit from enabling per cpu timers. The default is off(0)
68,#net.inet.tcp.per_cpu_timers = 0
68,# seeding cryptographic random number generators is provided by the /dev/random
68,"# device, which provides psudo ""real"" randomness. The arc4random(3) library call"
68,# provides a pseudo-random sequence which is generally reckoned to be suitable
68,# for simple cryptographic use. The OpenSSL library also provides functions for
68,# managing randomness via functions such as RAND_bytes(3) and RAND_add(3). Note
68,# that OpenSSL uses the random device /dev/random for seeding automatically.
68,# http://manpages.ubuntu.com/manpages/lucid/man4/random.4freebsd.html
68,#kern.random.yarrow.gengateinterval=10
68,# default 10 [4..64]
68,#kern.random.yarrow.bins=10
68,# default 10 [2..16]
68,#kern.random.yarrow.fastthresh=192
68,# default 192 [64..256]
68,#kern.random.yarrow.slowthresh=256
68,# default 256 [64..256]
68,#kern.random.yarrow.slowoverthresh=2
68,# default 2 [1..5]
68,#kern.random.sys.seeded=1
68,# default 1
68,#kern.random.sys.harvest.ethernet=1
68,# default 1
68,#kern.random.sys.harvest.point_to_point=1 # default 1
68,#kern.random.sys.harvest.interrupt=1
68,# default 1
68,#kern.random.sys.harvest.swi=0
68,# default 0 and actually does nothing when enabled
68,# IPv6 Security
68,# For more info see http://www.fosslc.org/drupal/content/security-implications-ipv6
68,# Disable Node info replies
68,# To see this vulnerability in action run `ping6 -a sglAac ::1` or `ping6 -w ::1` on unprotected node
68,#net.inet6.icmp6.nodeinfo=0
68,# Turn on IPv6 privacy extensions
68,# For more info see proposal http://unix.derkeiler.com/Mailing-Lists/FreeBSD/net/2008-06/msg00103.html
68,#net.inet6.ip6.use_tempaddr=1
68,#net.inet6.ip6.prefer_tempaddr=1
68,# Disable ICMP redirect
68,#net.inet6.icmp6.rediraccept=0
68,# Disable acceptation of RA and auto linklocal generation if you don't use them
68,##net.inet6.ip6.accept_rtadv=0
68,##net.inet6.ip6.auto_linklocal=0
68,### EOF ###
68,OPTIONAL: Enable the Pf firewall and disable LRO and TSO support
68,The following is an example /etc/rc.conf configuration file with some
68,commonly used directives. For a NAT firewall or router it is a good idea to
68,disable large receive offload (LRO) and TCP segmentation offload (TSO) on the
68,network interface. Take a look through each option and see if any of the
68,directives will work in your environment.
68,# Calomel.org
68,-|-
68,April 2021
68,# https://calomel.org/freebsd_network_tuning.html
68,"zfs_enable=""YES"""
68,# enable the ZFS filesystem
68,"clear_tmp_enable=""YES"""
68,# clear /tmp on boot
68,"gateway_enable=""YES"""
68,"# enable firewall/router mode, allow packets to pass between interfaces"
68,"keyrate=""250.34"""
68,# keyboard delay to 250 ms and repeat to 34 cps
68,# PF firewall
68,"pf_enable=""YES"""
68,# Enable PF (load kernel module as required)
68,"pf_rules=""/etc/pf.conf"""
68,# rule set definition file for pf
68,"pf_flags="""""
68,# additional flags for pfctl start up
68,"pflog_enable=""YES"""
68,# start pflogd(8)
68,"pflog_logfile=""/var/log/pflog"" # where pflogd should store the logfile"
68,"pflog_flags="""""
68,# additional flags for pflogd start up
68,"hostname=""calomel"""
68,"# IPv6, force enable IPv6 interfaces before dhcp intilization"
68,"#ipv6_activate_all_interfaces=""YES"""
68,"# DHCP, enable the ISC dual stack dhcp client"
68,"#dhclient_program=""/usr/local/sbin/dual-dhclient"""
68,# Internet: Disable large receive offload (LRO) and TCP segmentation offload
68,# (TSO) support if this server is a Network Address Translation (NAT) firewall
68,# or router. Depending on the network interface you may need to force disable
68,# transmit checksums (-txcsum) in order to disable TCP segmentation offload
68,"# (TSO) even if ""-tso"" is defined. Chelsio cards require ""-txcsum"" in order to"
68,"# also disable TSO as seen in the logs, ""cxl0: tso4 disabled due to -txcsum."""
68,# Receive and Transmit hardware checksum support is safe to keep enabled on a
68,"# firewall (rxcsum and txcsum). But, we would argue, the firmware on consumer"
68,"# grade one(1) gigabit network interfaces are probably years out of date, so"
68,# you may want to concider disabling hardware checksum support as to not incur
68,# firmware vulnerabilities and driver-to-hardware inefficiencies at the cost of
68,# a negligible increase in CPU usage.
68,"ifconfig_igb0=""dhcp ether 00:07:43:2a:4b:6c -rxcsum -rxcsum6 -txcsum -txcsum6 -lro -tso -vlanhwtso"""
68,"#ifconfig_igb0_ipv6=""inet6 dhcp accept_rtadv -rxcsum6 -txcsum6"""
68,"#ifconfig_igb0=""dhcp -rxcsum -rxcsum6 -txcsum -txcsum6 -lro -tso -vlanhwtso"""
68,"# LAN: define any private, non-routable IPv4 and IPv6 address. Disable LRO,"
68,# TSO and hardware checksum support.
68,"ifconfig_igb1=""inet 10.10.10.1/24 -rxcsum -rxcsum6 -txcsum -txcsum6 -lro -tso -vlanhwtso"""
68,"#ifconfig_igb1_ipv6=""inet6 fddd::1/64 -rxcsum6 -txcsum6"""
68,"#ifconfig_igb1=""inet 10.10.10.1/24 -rxcsum -rxcsum6 -txcsum -txcsum6 -lro -tso -vlanhwtso"""
68,# daemons disabled
68,"dumpdev=""NO"""
68,"sendmail_enable=""NONE"""
68,# daemons enabled
68,"#chronyd_enable=""YES"""
68,"#dhcpd_enable=""YES"""
68,"#dhcpd_flags=""igb1"""
68,"#entropy_file=""/var/db/entropy-file"""
68,"#unbound_enable=""YES"""
68,"#postfix_enable=""YES"""
68,"#sshd_enable=""YES"""
68,"#syslogd_flags=""-ss"""
68,### DISABLED FOR REFERENCE ###
68,# deamons
68,"#postgrey_enable=""YES"""
68,"#postgrey_flags=""--greylist-text=\""GREYLIST\"" --delay=870 --unix=/var/run/postgrey/postgrey.sock"""
68,# ipv6 lan static
68,"#ipv6_activate_all_interfaces=""YES"""
68,"#ifconfig_igb1_ipv6=""inet6 fddd::1/64 -lro -tso"""
68,"#ipv6_defaultrouter=""fddd::1"""
68,"# wireless, https://calomel.org/freebsd_wireless_access_point.html"
68,"#wlans_ath0=""wlan0"""
68,"#create_args_wlan0=""wlanmode hostap"""
68,"#hostapd_enable=""YES"""
68,"#ifconfig_wlan0=""inet 10.0.100.1 netmask 255.255.255.0"""
68,# Security Level (kern.securelevel) Note: updates cannot be installed when the
68,# system securelevel is greater than zero.
68,"#kern_securelevel_enable=""YES"""
68,"#kern_securelevel=""2"""
68,### EOF ###
68,OPTIONAL: Rebuilding the Kernel to use Recent ACKnowledgment (RACK)
68,If you wish to use the new Netflix RACK TCP stack the kernel must be rebuilt
68,with additional TCP stacks and the high speed kernel timer. The rebuild process
68,takes ~20 minutes on an Intel Core i7-8750H with a Samsung 970 PRO NVMe M.2 2TB .
68,# Calomel.org
68,-|-
68,April 2021
68,# https://calomel.org/freebsd_network_tuning.html
68,# Add the RACK TCP stack options to boot/loader and sysctl.conf
68,"echo ""tcp_rack_load=\""YES\"""" >> /boot/loader.conf"
68,"echo ""net.inet.tcp.functions_default=rack"" >> /etc/sysctl.conf"
68,# SVN Checkout the latest 12.1 source tree with current patches applied
68,/usr/bin/svnlite checkout https://svn.freebsd.org/base/releng/12.1 /usr/src/
68,"# Create a new kernel config called ""CALOMEL"". Add the RACK tcp stack to the GENERIC kernel."
68,"echo -e ""include GENERIC\nident"
68,"CALOMEL\nmakeoptions WITH_EXTRA_TCP_STACKS=1\noptions TCPHPTS"" > /usr/src/sys/amd64/conf/CALOMEL"
68,"# Build the new kernel called ""CALOMEL"" and install"
68,cd /usr/src && make buildkernel KERNCONF=CALOMEL && make installkernel KERNCONF=CALOMEL && echo SUCCESS
68,"# If you need to free up space, remove the entire kernel source tree"
68,cd; rm -rf /usr/src/ && mkdir /usr/src && chown root:wheel /usr/src
68,"# Reboot the server to use the new ""CALOMEL"" kernel by default"
68,reboot
68,"# After reboot, verify the new ""CALOMEL"" named kernel loaded, patches"
68,# applied (p6) and SVN revision number (r349243) similar to the following...
68,uname -a
68,FreeBSD Rick-n-Morty 12.1-RELEASE-p0 FreeBSD 12.1-RELEASE-p6 r123456 CALOMEL amd64
68,^^^^^^^
68,"# Then check the available congestion control options with,"
68,"# ""sysctl net.inet.tcp.functions_available"". The rack TCP stack"
68,# should have an asterisk next to it signifying RACK is the
68,# default TCP stack.
68,net.inet.tcp.functions_available:
68,Stack
68,D Alias
68,PCB count
68,freebsd
68,freebsd
68,rack
68,* rack
68,750
68,^^^^
68,Do these optimizations really make a difference for a web server ?
68,"Lets take a look at the web server performance for our server, calomel.org"
68,before and after modifications. Keep in mind the graphs are the result of the
68,"exact same hardware, the same network, the same files and access is 100% public"
68,requests. We are only graphing successful requests (code 200) and no code 301
68,redirections or errors.
68,With Nginx you can setup the log lines to tell you how long it took to
68,fulfill the client request and complete the data transfer to the client. We
68,have multiple examples of how to setup this log format on our Nginx Secure Web Server page. Using the log data we can
68,graph the nginx performance times with our Web Server Distribution
68,Performance Perl script.
68,BEFORE: FreeBSD 10 defaults. The following graph displays the time in
68,100 millisecond increments against the number of completed object transfers for
68,"the last ten thousand (10,000) log lines. The log was collected before speed"
68,"optimizations using a default FreeBSD 10 install. Keep in mind an ""object"" is an"
68,"html, jpg, css or any other file. File sizes range from 24 kilobytes up to 350"
68,kilobytes. On the left hand side is the tall vertical line at 0 seconds going
68,up to 7142 objects and a few smaller lines on the bottom left. The tall line at
68,"0 seconds tells us for the last 10,000 objects served, the web server was able"
68,to send and complete the network transfer in zero(0) seconds ( i.e.
68,less then
68,100 milliseconds) 71.42% of the time. (7142/10000*100=71.42). Keep in mind
68,calomel.org is a SSL enabled site and this time also includes the https
68,negotiation phase.
68,calomel@freebsd10:
68,./calomel_http_log_distribution_performance.pl
68,.:.
68,Calomel Webserver Distribution Statistics
68,"Log lines: 10000, Search string(s):"
68,__________________________________________________________________
68,7142 |.................................................................
68,|.................................................................
68,6157 |_________________________________________________________________
68,|.................................................................
68,5172 |.................................................................
68,|.................................................................
68,4679 |.................................................................
68,|.................................................................
68,3694 |_________________________________________________________________
68,|.................................................................
68,2709 |.................................................................
68,|.................................................................
68,2216 |.................................................................
68,|.................................................................
68,1231 |_________________________________________________________________
68,|||...............................................................
68,246 ||||..............................................................
68,|||||||||||||||||||||||||||||_|||_|_||_|_|_|_|_|__|_______________
68,Time: |^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|
68,0.5
68,1.0
68,1.5
68,2.0
68,2.5
68,3.0
68,3.5
68,4.0
68,4.5
68,4.6
68,5.0
68,5.5
68,6.0
68,AFTER: FreeBSD 10 optimization. The following graph shows the results
68,after applying the optimizations found at the beginning of this page. Compared
68,"to the previous graph, we see our FreeBSD based Nginx server is able to serve"
68,96.6% of the objects in less then 100ms. (9669/10000*100=96.69) compared to
68,just 71.42% before. FreeBSD and Nginx are fast and we just made them 35%
68,faster.
68,calomel@freebsd10:
68,/storage/tools/web_server_distribution_performance.pl
68,.:.
68,Calomel Webserver Distribution Statistics
68,"Log lines: 10000, Search string(s):"
68,__________________________________________________________________
68,9669 |.................................................................
68,|.................................................................
68,8181 |.................................................................
68,|.................................................................
68,7685 |.................................................................
68,|.................................................................
68,6198 |_________________________________________________________________
68,|.................................................................
68,5702 |.................................................................
68,|.................................................................
68,4214 |.................................................................
68,|.................................................................
68,3718 |_________________________________________________________________
68,|.................................................................
68,2231 |.................................................................
68,|.................................................................
68,1239 |_________________________________________________________________
68,|.................................................................
68,247 |.................................................................
68,|||_|____|________________________________________________________
68,Time: |^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|
68,0.5
68,1.0
68,1.5
68,2.0
68,2.5
68,3.0
68,3.5
68,4.0
68,4.5
68,4.6
68,5.0
68,5.5
68,6.0
68,OpenBSD 5.x rough comparison: Here is the Web Server Distribution
68,Performance Perl script run on our log when we were using OpenBSD.
68,"OpenBSD is very secure and has the latest Pf, but it is not considered the"
68,fastest of operating systems. Notice the graph shows 1042 requests completed
68,in 0.1 seconds (i.e. less then 200 milliseconds) which equals 10.42% out of a
68,total of 10000. With the FreeBSD optimizations enabled the server is able to
68,complete transactions more than 18 times faster then OpenBSD.
68,"((9660/.1)/(1042/0.2)=18.54). We respect OpenBSD for being secure, but we have"
68,to admit it is not suited for a high speed - low latency server.
68,calomel@openbsd5.x:
68,./calomel_http_log_distribution_performance.pl
68,.:.
68,Calomel Webserver Distribution Performance Statistics
68,"Log lines: 10000, Search string(s):"
68,__________________________________________________________________
68,1042 .|................................................................
68,.|................................................................
68,898 _|________________________________________________________________
68,.||...............................................................
68,754 .|||..............................................................
68,_|||______________________________________________________________
68,682 .||||.............................................................
68,.||||.............................................................
68,539 _||||_____________________________________________________________
68,.||||..||.........................................................
68,395 .||||||||.........................................................
68,_|||||||||________________________________________________________
68,251 .||||||||||.......................................................
68,.|||||||||||......................................................
68,179 _||||||||||||_____________________________________________________
68,.||||||||||||.....................................................
68,35 ||||||||||||||||||||....||........................................
68,||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
68,Time: |^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|^^^^|
68,0.5
68,1.0
68,1.5
68,2.0
68,2.5
68,3.0
68,3.5
68,4.0
68,4.5
68,4.6
68,5.0
68,5.5
68,6.0
68,Do these optimizations help a firewall too ?
68,Yes. Take a look at the Network Tuning
68,and Performance Guide where we show our FreeBSD firewall passing as much as
68,10 gigabit of data through the machine with low latency and minimal CPU usage.
69,Deep Dive into the New Features of Apache Spark 3.0 - Databricks
69,SAIS 2020
69,Agenda
69,Speakers
69,Training
69,Sponsors
69,Special Events
69,Women at Summit
69,Financial Services
69,Government and Education
69,Healthcare and Life Sciences
69,Media and Entertainment
69,Retail and Consumer Goods
69,Job Board
69,FAQ
69,WATCH KEYNOTES
69,SAIS 2020
69,Agenda
69,Speakers
69,Training
69,Sponsors
69,Special Events
69,Women at Summit
69,Financial Services
69,Government and Education
69,Healthcare and Life Sciences
69,Media and Entertainment
69,Retail and Consumer Goods
69,Job Board
69,FAQ
69,WATCH KEYNOTES
69,Deep Dive into the New Features of Apache Spark 3.0Download Slides
69,"Continuing with the objectives to make Spark faster, easier, and smarter, Apache Spark 3.0 extends its scope with more than 3000 resolved JIRAs. We will talk about the exciting new developments in the Spark 3.0 as well as some other major initiatives that are coming in the future. In this talk, we want to share with the community many of the more important changes with the examples and demos."
69,"The following features are covered: accelerator-aware scheduling, adaptive query execution, dynamic partition pruning, join hints, new query explain, better ANSI compliance, observable metrics, new UI for structured streaming, new UDAF and built-in functions, new unified interface for Pandas UDF, and various enhancements in the built-in data sources [e.g., parquet, ORC and JDBC]."
69,Watch more Spark + AI sessions here
69,Try Databricks for free
69,Video Transcript
69,About us and our Open Source contributions
69,"Hello, everyone. Today, Wenchen and I are glad to share with you the latest"
69,"updates about the upcoming release, Spark 3.0."
69,So I’m Xiao Li. Both Wenchen and I are
69,working for Databricks. We focus on the open-source developments. Both of us are Spark
69,committers and PMC members.
69,About Databricks
69,Databricks provides a unified data analytics platform to accelerate
69,"your data-driven animation. We are a global company with more than 5,000 customers across"
69,"various industries, and we have more than 450 partners worldwide. And most of you might"
69,"have heard of Databricks as original creator of Spark, Delta Lake, MLflow, and Koalas."
69,These are open-source projects that are leading innovation in the fields of data and machine
69,learnings. We continue to contribute and nurture this open-source community.
69,Spark 3.0 Highlights
69,"In Spark 3.0, the whole community resolved more than 3,400 JIRAs. Spark SQL"
69,"and the Core are the new core module, and all the other components are built on Spark"
69,"SQL and the Core. Today, the pull requests for Spark SQL and the core constitute"
69,"more than 60% of Spark 3.0. In the last few releases, the percentage keeps going up. Today,"
69,we will focus on the key features in both Spark SQL and the Core.
69,This release delivered
69,"many new capabilities, performance gains, and extended compatibility for the Spark ecosystem."
69,This is a combination of the tremendous contributions from the open-source community. It is impossible
69,"to discuss the new features within 16 minutes. We resolved more than 3,400 JIRAs. Even in"
69,"this light, I did my best, but I only can put 24 new Spark 3.0 features."
69,"Today, we would like to present some of them. First, let us talk about the"
69,performance-related features.
69,Spark 3.0 Enhanced Performance
69,High performance is one of the major advantages when people
69,select Spark as their computation engine. This release keeps enhancing the performance
69,"for interactive, batch, streaming, and [inaudible] workloads. Here, I will first cover four of"
69,"the performance features in SQL query compilers. Later, Wenchen will talk about the performance enhancement for building data sources."
69,The four major features in query compilers include
69,a new framework for adaptive query execution and a new runtime filtering for dynamic partition
69,"pruning. And also, we greatly reduce the overhead of our query compiler by more than a half,"
69,especially on the optimizer overhead and the SQL cache synchronization. Supporting a complete
69,set of join hints is another useful features many people are waiting for.
69,Adaptive query
69,"execution was available at the previous releases. However, the previous framework has a few"
69,"major drawbacks. Very few companies are using it in the production systems. In this release,"
69,Databricks and the [inaudible] work together and redesigned the new framework and resolved
69,all the known issues.
69,Let us talk about what we did in this release.
69,Spark Catalyst Optimizer
69,Michael Armbrust.
69,"is the creator of Spark SQL and also, Catalyst Optimizer. In the initial release of Spark"
69,"SQL, all the optimizer rules are heuristic-based. To generate good query plans, the query optimizer"
69,"needs to understand the data characteristics. Then in Spark 2.x, we introduced a cost-based"
69,"optimizer. However, in most cases, data statistics are commonly absent, especially when statistics collection is even more expensive than the data processing in the [search?]. Even if"
69,"the statistics are available, the statistics are likely out of date. Based on the storage and the compute separation in Spark, the characteristics of data [rival?] is unpredictable. The costs"
69,are often misestimated due to the different deployment environment and the black box user-defined
69,"functions. We are unable to estimate the cost for the UDF. Basically, in many cases, Spark"
69,optimizer is enabled to generate the best plan due to this limitation.
69,Adaptive Query Execution
69,For all these
69,"reasons, runtime adaptivity becomes more critical for Spark than the traditional systems."
69,So this release introduced a new adaptive query execution framework called
69,AQE. The basic idea of adaptive planning is simple. We optimize the execution plan using
69,the existing rules of the optimizer and the planner after we collect more accurate statistics
69,from the finished plans.
69,The red line shows the new logics we added in this release. Instead
69,"of directly optimizing the execution plans, we send back the unfinished plan segments"
69,and then use an existing optimizer and planner to optimize them at the end and build a new
69,execution plan. This release includes three adaptive features. We can convert the soft
69,"merge join to broadcast hash join, based on the runtime statistics. We can shrink the"
69,number of reducers after over-partitioning. We can also handle the skew join at runtime.
69,"If you want to know more details, please read the blog post I posted here."
69,"Today, I will briefly explain them one by one."
69,Maybe most of you already
69,"learn many performance tuning tips. For example, to make your join faster, you might guide"
69,your optimizer to choose a broadcast hash join instead of the sort merge join. You can
69,"increase the spark.sql.autobroadcastjointhreshold or use a broadcast join hint. However, it"
69,is hard to tune it. You might hit out of memory exceptions and even get worse performance.
69,"Even if it works now, it is hard to maintain over time because it is sensitive to your"
69,data workloads.
69,You might be wondering why Spark is unable to make the wise choice by
69,itself. I can easily list multiple reasons.
69,the statistics might
69,be missing or out of date.
69,the file is compressed.
69,"the file format is column-based, so the file"
69,size does not represent the actual data volume.
69,the filters could be compressed
69,(the filters) might also contain the black box UDFs.
69,"The whole query fragments might be large,"
69,"complex, and it is hard to estimate the actual data volume for Spark to make the best choice."
69,Convert Sort Merge Join to Broadcast Hash Join
69,So this is an example to show how AQE converts a sort merge join to
69,"a broadcast hash join at runtime. First, execute the leave stages. Query the statistics from"
69,the shuffle operators which materialize the query fragments. You can see the actual size
69,of stage two is much smaller than the estimated size reduced from 30 megabytes to 8 megabytes
69,so we can optimize the remaining plan and change the join algorithm from sort merge
69,join to broadcast hash join.
69,Another popular performance tuning tip is to tune the configuration
69,"spark.sql.shuffle.partitions. The default value is a magic number, 200. Previously,"
69,"the original default is 8. Later, it was increased to 200. I believe no one knows the reason"
69,"why it become 200 instead of 50, 400, or 2,000."
69,"It is very hard to tune it, to be"
69,"honest. Because it is a global configuration, it is almost impossible to decide the best"
69,"value for every query’s fragment using a single configuration, especially when your query"
69,plan is huge and complex.
69,"If you set it to very small values, the partition will be huge,"
69,and the aggregation and the sort might need to spew the data to the disk. If the configuration
69,"values are too big, the partition will be small. But the number of partitions is big."
69,It will cause inefficient IO and the performance bottleneck could be the task scheduler. Then
69,"it will slow down everybody. Also, it is very hard to maintain over time."
69,Dynamically Coalesce Shuffle Partitions
69,Until you can
69,"solve it in a smart way, we can first increase our initial partition number to a big one."
69,"After we execute the leave query stage, we can know the actual size of each partition."
69,Then we can automatically correlate the nearby partitions and automatically reduce the number
69,of partitions to a smaller number. This example shows how we reduce the number of partitions
69,from 50 to 5 at runtime. And we added the actual coalesce at runtime.
69,Data Skew
69,One more popular performance tuning tip is about data skew. Data skew is
69,very annoying. You could see some long-running or frozen task and a lot of disks spinning
69,and a very low resource authorization rate in most nodes and even out of memory. Our
69,Spark community might tell you many different ways to solve such a typical performance problem.
69,You can find the skew value and the right queries to handle the skew value separately.
69,"And also, you can add the actual skew keys that can remove the data skew, either new"
69,"columns or some existing columns. Anyway, you have to manually rewrite your queries,"
69,"and this is annoying and sensitive to your workloads, too, which could be changed over"
69,time.
69,"This is an example without the skew optimization. Because of data skew,"
69,"after the shuffle, the shuffle partition, A0, will be very large. If we do a join on"
69,"these two tables, the whole performance bottleneck is to join the values for this specific partition,"
69,"A0. For this partition, A0, the cost of shuffle, sort, and merge are much bigger than the other"
69,partitions. Everyone is waiting for the partition 0 to complete and slow down the execution
69,of the whole query.
69,Our adaptive query execution can handle it very well in the data skew case.
69,"After executing the leaf stages (stages one and stage two), we can optimize our queries"
69,"with a skew shuffle reader. Basically, it will split the skew partitions into smaller"
69,subpartitions after we realize some shuffle partitions are too big.
69,Let us use same example to show how to resolve it using adaptive query
69,"execution. After realizing partitions are too large, AQE will add a skew reader to automatically"
69,"split table A’s partition part 0 to three segments: split 0, split 1, and split 2. Then"
69,it will also duplicate another side for table B. Then we will have three copies for table
69,B’s part 0.
69,"After this step, we can parallelize the shuffle reading, sorting, merging for"
69,this split partition A0. We can avoid generating very big partition for the sort merge join.
69,"Overall, it will be much faster."
69,"Based on a terabyte of TPC-DS benchmark, without statistics, Spark 3.0 can make Q7 eight times faster and also achieve two times fast"
69,and speed up for Q5 and more than 1.1 speed up for another 26 queries. So this is just
69,"the beginning. In the future releases, we will continue to improve the compiler and"
69,introduce more new adaptive rules.
69,Dynamic Partition Pruning
69,The second performance features I want to highlight is dynamic partition pruning.
69,"So this is another runtime optimization rule. Basically, dynamic partition pruning is to"
69,avoid partition scanning based on the queried results of the other query fragments. It is
69,important for star schema queries. We can achieve a significant speed up in TPC-DS
69,queries.
69,"So this is a number, in a TPC-DS benchmark, 60 out of 102 queries show a significant"
69,speed up between 2 times and 18 times. It is to prune the partitions that joins read
69,from the fact table T1 by identifying those partitions that result from filtering the
69,"dimension table, T2."
69,"Let us explain it step by step. First, we will do the filter push down"
69,"in the left side. And on the right side, we can generate a new filter for the partition"
69,column PP because join P is a partition column. Then we get the query results of the left
69,"side. We can reuse our query results and generate the lists of constant values, EPP, and filter"
69,"result. Now, we can push down the in filter in the right side. This will avoid scanning"
69,"all the partitions of the huge fact table, T1. For this example, we can avoid scanning"
69,"90% of partitioning. With this dynamic partition pruning, we can achieve 33 times speed up."
69,JOIN Optimizer Hints
69,So the last performance feature is join hints. Join hints are very common
69,"optimizer hints. It can influence the optimizer to choose an expected join strategies. Previously,"
69,"we already have a broadcast hash join. In this release, we also add the hints for the"
69,"other three join strategies: sort merge join, shuffle hash join, and the shuffle nested"
69,loop join.
69,"Please remember, this should be used very carefully. It is difficult to manage"
69,over time because it is sensitive to your workloads. If your workloads’ patterns are
69,"not stable, the hint could even make your query much slower."
69,Here are examples how to
69,use these hints in the SQL queries. You also can do the same thing in the DataFrame API.
69,"When we decide the join strategies, [our leads are different here?]."
69,So a broadcast
69,"hash join requires one side to be small, no shuffle, no sort, so it performs very fast."
69,"For the shuffle hash join, it needs to shuffle the data but no sort is needed. So it can"
69,handle the large tables but will still hit out of memory if the data is skewed.
69,Sort
69,merge join is much more robust. It can handle any data size. It needs to shuffle and salt
69,data slower in most cases when the table size is small compared with a broadcast hash join.
69,"And also, shuffle nested loop join, it doesn’t require the join keys, unlike the other three"
69,join strategies.
69,Richer APIs: new features and simplify development
69,"To enable new use cases and simplify the Spark application development,"
69,this release delivers a new capability and enhanced interesting features.
69,Pandas UDF
69,"Let’s, first,"
69,talk about Pandas UDF. This is a pretty popular performance features for the PySpark users.
69,So let us talk about the history of UDF support in PySpark. In the first release of Python
69,"support, 2013, we already support Python lambda functions for RDD API. Then in 2014, users"
69,"can register Python UDF for Spark SQL. Starting from Spark 2.0, Python UDF registration is"
69,"session-based. And then next year, users can register the use of Java UDF in Python API."
69,"In 2018, we introduced Pandas UDF. In this release, we redesigned the interface for Pandas"
69,UDF by using the Python tab hints and added more tabs for the Pandas UDFs.
69,To adjust our compatibility with the old Pandas UDFs from Apache Spark 2.0
69,"with the Python 2.6 and above, Python [inaudible] such as pandas.Series, Pandas DataFrame, cube"
69,"hole, and the iterator can be used to impress new Pandas UDF types. For example, in Spark"
69,"2.3, we have a Scala UDF. The input is a pandas.Series and its output is also pandas.Series. In Spark"
69,"2.0, we do not require users to remember any UDF types. You just need to specify the input"
69,"and the output types. In Spark 2.3, we also have a Grouped Map Pandas UDF, so input is"
69,"a Pandas DataFrame, and the output is also Pandas DataFrames."
69,Old vs New Pandas UDF interface
69,This slide shows the difference between the old and the new interface. The
69,same here. The new interface can also be used for the existing Grouped Aggregate Pandas
69,"UDFs. In addition, the old Pandas UDF was split into two API categories: Pandas UDFs"
69,and Pandas function APIs. You can treat Pandas UDFs in the same way that you use the other
69,PySpark column instance.
69,"For example, here, calculate the values. You are calling the"
69,Pandas UDF calculate. We do support the new Pandas UDF types from iterators of series
69,to iterator other series and from iterators of multiple series to iterator of series.
69,So this is useful for [inaudible] state initialization of your Pandas UDFs and also useful for Pandas
69,UDF parquet.
69,"However, you can now use Pandas function APIs with this column instance. Here"
69,"are these two examples: map Pandas function API and the core group, the map Pandas UDF,"
69,the APIs. These APIs are newly added in these units.
69,Back to Wenchen
69,"So next, Wenchen will go over the remaining"
69,features and provide a deep dive into accumulator with Scalar. Please welcome Wenchen.
69,"Thanks, Xiao, for the first half of the talk. Now, let me take over from"
69,here and introduce the remaining Spark 3.0 features.
69,Accelerator-aware Scheduling
69,"I will start with, straight away,"
69,"our scheduler. In 2018 Spark Summit, we already announced the new project [inaudible]. As"
69,"you’re now aware, our scheduler is part of this project. It can be widely used for executing"
69,"special workloads. In this release, we support standalone, YARN, and Kubernetes scheduler"
69,"scheduler backend. So far, users need to specify the require resources using a [inaudible]"
69,configs.
69,"In the future, we will support the job, stage, and task levels. To further understand"
69,"this feature, let’s look at the workflow. Ideally, the cost manager should be able to"
69,"automatically discover resources, like GPUs. When the user submits an application with"
69,"resource request, Spark should pass the resources request to a cluster manager and then the cluster"
69,manager cooperates to allocate and launch executors with the required resources. After
69,"Spark job is submitted, Spark should schedule tasks on available executors, and the cluster"
69,manager should track the results usage and perform dynamic resource allocation.
69,"For example,"
69,"when there are too many pending tasks, the cluster manager should allocate more executors"
69,"to run more tasks at the same time. When a task is running, the user shall be able to"
69,"retrieve the assigned resources and use them in their code. In the meanwhile, cluster manager"
69,shall monitor and recover failed executions.
69,"Now, let’s look at how can a cluster manager discover resources and how"
69,can users request resources.
69,"As an admin of the cluster, I can specify a script to auto discover executors. The discovery script can be specified separately on Java as executors."
69,We also provided an example to auto discover Nvidia GPU resources. You can adjust
69,"this example script for other kinds of resources. Then as a user of Spark, I can request resources"
69,at the application level. I can use the config spark.executor.resource.{resourceName}.amount
69,and the corresponding config for Java to specify the executors amount on the Java and executors.
69,"Also, I can use the config spark.task.resource.{resourceName}.amount to specify the executors required by each"
69,"task. As I mentioned earlier, we will support more time-proven labor later, like the job"
69,or stage labor. Please stay tuned.
69,Retrieve Assigned Accelerators
69,"Next, we’ll see how you can leverage the assigned executors to actually"
69,"execute your workloads, which is probably the most important part to the users. So as"
69,"a user of Spark, I can retrieve the assigned executors from the task content. Here is an"
69,example in PySpark. The contents of resources returns a map from the resource name to resource
69,"info. In the example, we request for GPUs, and we can take the GPU address from the resource"
69,map. Then we launch the TensorFlow to train my model within GPUs. Spark will take care
69,"of the resource allocation and acceleration and also monitor the executors here for failure recovery, which makes my life much easier."
69,Cluster Manager Support
69,"As I mentioned earlier, the executor aware"
69,"of scheduling support has been added to standalone, YARN, and Kubernetes cost manager. You can"
69,"check the Spark JIRA tickets to see more details. Unfortunately, the Mesos support is still"
69,not available. We’d really appreciate it if any Mesos expert has interest and is willing
69,to help the Spark community to add the Mesos support. Please leave a comment in the [inaudible]
69,if you want to work on it. Thanks in advance.
69,Improved Spark Web UI for Accelerators
69,"Last but not the least, we also improved the Spark Web UI to show all"
69,"the discovery resources on the executor page. In this page, we can see that there are GPUs"
69,available on the executor one. You can check the Web UI to see how many executors
69,"are available in the cluster, so you can better schedule your jobs."
69,32 New Built-in Functions
69,"In this release, we also"
69,introduced 32 new built-in functions and add high auto functions in the Scalar API. The
69,Spark community pays a lot of attention to compatibility. We have investigated many other
69,"ecosystems, like the PostgreSQL, and implemented many commonly used functions in Spark."
69,"Hopefully,"
69,these new built-in functions can make it faster to build your queries as you don’t need to
69,waste time to learn a lot of UDFs.
69,"Due to the time limitations, I can’t go over all"
69,"the functions here, so let me just introduce some map type functions as an example."
69,"When you deal with map type values, it’s common to get the keys and values"
69,"for the map as an array. There are two functions, map keys and map values can do"
69,this for you. The example is from the Databricks runtime notebook. Or you may want to do
69,"something more complicated, like creating a new map by transforming the original map"
69,"where it’s a keys and a map values functions. So if there are two functions, transform keys"
69,"and transform values can do this for you, and you just need to write a handler function"
69,to specify the transformation logic.
69,"As I mentioned earlier, the functions"
69,also have Scalar APIs rather than the SQL API. Here is an example about how to do the
69,"same thing, but it’s a Scalar API. You can just write a normal Scala function, which"
69,takes the [kernel?] objects as the input to have the same effect as the SQL API.
69,Monitoring and Debuggability
69,This release also includes many enhancements and makes the monitoring
69,more comprehensive and stable. We can make it easier to close out and get back to your
69,Spark applications.
69,Structured Streaming UI
69,The first feature I will talk to you about is the new UI for the Spark
69,"streaming. Here, the drive to show it– Spark streaming was initially introduced in Spark"
69,2.0. This release has the dedicated– it was Spark web UI for inspection of these streaming
69,"jobs. This UI offers two sets of statistics: one, abbreviate information of [completed?]"
69,"streaming queries and two, detailed statistics information about the streaming query including"
69,"the input rate, processor rate, input loads, [inaudible], operation duration and others."
69,"More specifically, the input rate and processor rate means how many records per second the"
69,streaming software produces and the Spark streaming engine processes. It can give you
69,a sense about if the streaming engine is fast enough to process the continuous input data.
69,"Similarly, you can tell it from the past duration as well. If many batch takes more time than"
69,"the micro-batch [inaudible], it means the engine is not fast enough to process your"
69,"data, and you may need to enable the [inaudible] feature to make the source produce the data"
69,slower.
69,And so operating time is also a very useful matrix. It tells you the time spent
69,on each operator so that you can know where is the bottleneck in your query.
69,DDL and DML enhancements
69,We also have many different enhancements in DDL and DML commands. Let
69,me talk about the improvements in the EXPLAIN command as an example. This is a typical output
69,of the EXPLAIN command. You have many operators in the query plan tree and some operators
69,have other additional information. Reading plans is critical for understanding and attuning
69,"queries. The existing solution looks [inaudible], and, as a stream of each operator, can be"
69,very wide or even truncated. And it becomes wider and wider each release as we add more
69,and more information in the operator to help debugging.
69,"This release, we enhance the EXPLAIN command with a new formatted mode and also provided a capability to dump the plans to"
69,the files. You can see it becomes much easier to read and understand. So here is a very
69,simple plan tree at the beginning. Then follows a detailed section for each operator. This
69,makes it very easy to get an overview of the query by looking at the plan tree. It also
69,makes it very easy to see the details of each operator as the information is now stacked
69,"vertically. And in the end, there is a section to show all the subqueries. In the future"
69,"releases, we will add more and more useful information for each operator."
69,"This release, we also introduced a new API to define your own metrics to observe data quality. Data quality is very important to many applications. It’s usually easy to"
69,"define metrics for data quality by some [other?] function, for example, but it’s also hard"
69,"to calculate the metrics, especially for streaming queries."
69,"For example, you want to keep monitoring"
69,"the data quality of your streaming source. You can simply define the metrics as the percentage of the error records. Then you can do two things. Make it a habit. One, code observe"
69,method of the streaming error rate to define your metrics with a name and the start
69,"of stream. So this example, the name is data quality and the matrix, it just will count"
69,the error record and see how many percent of it in the total lookups.
69,"Two, you add a"
69,"listener to watch the streaming process events, and in the case of your matrix, the name,"
69,"do whatever you want to do, such as sending an email if there are more than 5% error data."
69,SQL Compatibility
69,"Now, let’s move to the next topic. SQL compatibility is also super critical"
69,"for workloads mapped from the other database systems through Spark SQL. In this release,"
69,we introduced the ANSI store assignment policy for table insertion. We added runtime overall
69,checking with respect to ANSI results keywords into the parser. We also switched the calendar
69,to the widely-used calendar which is the ISO and SQL standard.
69,Let’s look at how the first
69,two features can help you to enforce data quality. I say more about the assignment.
69,It’s something like assigning a value to a variable in programming language. In the SQL
69,"world, it is table insertion or upsert, which is kind of assigning values to a table column."
69,"Now, let’s see an example."
69,"Assume there is a table with two columns, I and J, which are type int and"
69,"type string. If we write a int value to the string column, it’s totally okay. It’s totally"
69,"safe. However, if we write a string value to the int column, it’s risky. The string"
69,"value is very likely to not be in integer form, and Spark will fail and worry about"
69,it.
69,"If you do believe your string values are safe to be inserted into an int column, you"
69,can add a cast manually to bypass the type check in Spark.
69,We can also write long type
69,"of values to the int column, and Spark will do the overflow check at runtime. If your"
69,"input data is invalid, Spark will be show exception at runtime to tell you about it."
69,"In this example, the integer one is okay, but the larger value below can’t fit the integer"
69,"type, and you’ll receive this error if you run this table insertion command which tells"
69,you about the overflow problem.
69,Built-in Data Source Enhancements
69,"Also, this release enhances built-in data sources. For example, to"
69,"populate data source, we can’t do nested column and filter pushdown. Also, we support"
69,[inaudible] for CSV files. This release also introduced a new [inaudible] resource and
69,also a new [inaudible] resource for testing and benchmarking. Let me further introduce
69,the origin of nested columns in Parquet and ORC resource.
69,The first one is a kind of [baloney?].
69,"[inaudible] like [inaudible] and [ORC?], we can skip reading some [inaudible] in the blocks"
69,if they don’t contain the columns we need. This [technique?] can be applied to nested
69,columns as well in Spark 2.0. To check if your query– in Spark 3.0. To check if your
69,"query can benefit from this [inaudible] or not, you can run the EXPLAIN command and see"
69,if the read schema over the file scan note strips the [inaudible] nested columns. In
69,"this example, only the nested column is inserted, so the read schema only contains A."
69,"[inaudible] is also a very popular technical [inaudible]. Similarly,"
69,you can also check the expand result and see if the [pushed?] filters over the file scan
69,"note contains the name of column filters. In this example, we do have a filter where"
69,"it’s nested column A, and it does appear in the [pushed?] filter, which makes this version"
69,happen in this query.
69,Catalog plugin API
69,This release also expands other efforts on the extensibility and ecosystem
69,"like the v2 API enhancements, Java 11, Hadoop, and [inaudible] support. [inaudible]"
69,API. This release extends the [inaudible] to API by adding the Catalog plugin. The Catalog plug-in API allows users to reject their own [inaudible] and take over the [inaudible]
69,data operations from Spark. This can give end users a more seamless experience to assist
69,"external tables. Now, end users [inaudible] reject [inaudible] and manipulate the tables"
69,"[inaudible], where before, end users have to reject each table. For example, let’s say"
69,you have rejected a MySQL connector [inaudible] named MySQL. You can use SELECT to get data
69,from existing MySQL table. We can also INSERT into a MySQL table with Spark’s [inaudible].
69,"You can also create an outer tables in MySQL with Spark, which was just not possible before,"
69,"because before, we don’t have the Catalog plug-in. Now this example will be available"
69,in Spark 3.1 when we finish [inaudible].
69,When to use Data Source V2 API?
69,Some people may have a question. Now Spark has both – it has a V1 and a V2
69,"APIs – which one should I use? In general, we want everyone to move to V2 [inaudible]."
69,But the V2 API is not ready yet as we need more feedback to polish the API. Here are
69,"some tips about when to pick the V2 API. So if you want [inaudible], the catalogue function"
69,"it is, so it has to be the V2 because V1 API doesn’t have this ability. If you want to"
69,"support both versions streaming in your data source, then you should use V2 because in"
69,"V1, the streaming and the [inaudible] are different APIs which makes it harder to reuse"
69,"the code. And if you are sensitive to the scan performance, then you can try the V2"
69,"API because it allows you to report the data provisioning to [inaudible] in Spark, and"
69,also it allows you to implement [inaudible] reader for better performance.
69,If you don’t
69,"care about this stuff and just want to [inaudible] source once and you change it, please use"
69,the V1 as V2 is not very stable.
69,Extensibility and Ecosystem
69,"The ecosystem also evolves very fast. In this release, Spark"
69,can be better integrated into the ecosystem by supporting the newer version of these common
69,"components like Java 11, Hadoop 3, Hadoop 3 [inaudible], and Hadoop 2.3 [inaudible]."
69,I want to mention some breaking changes here.
69,"Starting from this release, we’re only building Spark with Scala 2.12, so Scala 2.11 is no longer [inaudible]. And we deprecated Python 2 too because it is end of life. In the download image, we put a build of Spark with different"
69,"[inaudible] and Hadoop combinations. By default, it will be Hadoop 2.7 and it would have 2.3"
69,[exclusion?]. There are another two [companies?] of previews available. One is Hadoop 2.7 and
69,"Hadoop 1.2 execution, which is for people who can’t upgrade their end forms. The other"
69,"is Hadoop 3.2 and Hadoop 2.3 execution, which is for people who want to try Hadoop"
69,3. We also extend the support for different Hadoop and Hive versions from 0.12 to 3.1.
69,Documentation Improvements
69,Documentation improvements is the last existing news I want to share
69,with everyone. How to read on a standard the web UI is a common question to many new Spark
69,users. This is especially true for Spark SQL users and Spark streaming users. They are
69,"using the [inaudible]. They usually don’t know what it is, and what our jobs [inaudible]."
69,"Also, the [inaudible] are using many queries and matrix names, which are not very clear"
69,"to many users. Starting from this release, we add a new section for [inaudible] reading"
69,the web UI. It includes the [inaudible] job page and [inaudible] and also SQL streaming
69,"[inaudible]. This is just a start. We will continue to enhance it, then SQL reference."
69,"Finally, this release already has a SQL reference for Spark SQL. Spark SQL"
69,"is the most popular and important component in Spark. However, we did not have our own"
69,SQL reference to define the SQL [semantic?] and detailed behaviors. Let me quickly go
69,over the major chapters in SQL reference. So we have a page to explain the ANSI components
69,"of Spark. So as I mentioned before, we have SQL compatibility, but to avoid [correcting?]"
69,"the [effecting?] queries, we make it optional. So you can only enable the ANSI compatibility"
69,by enabling this flag.
69,"You also have a page to explain the detailed semantic of each [inaudible],"
69,so you can know what it means and what’s the behavior of them. You also have a page to
69,explain the data and partner strings used for formatting and parsing functions [inaudible].
69,There’s also a page to give the document for each function in Spark. We also have a page
69,"to explain the syntax, how to define the table or function [inaudible]. Also, there’s a page"
69,to explain the syntax and the semantics of each [inaudible] in Spark SQL.
69,"Also, there’s"
69,a page to explain the null semantic. The null is a very special value in Spark SQL and other
69,ecosystems. So there must be a page to either explain what’s the meaning of null in the
69,"null queries. Also, we have a page to explain the syntax for all the commands, like DDL"
69,"and DML commands, and also insert is also included in the document. In fact, SELECT has so many features, so we want to have a page to explain all of them. Yeah, there are"
69,also a page for other special commands like SHOW TABLES.
69,"Finally, [inaudible]. This is another critical enhancements in Spark"
69,"3.0 document. In this release, all the components have [inaudible] guides. When you upgrade"
69,"your Spark version, you can read them carefully, and, in fact, [inaudible]. You might be wondering"
69,why it is much longer than the previous version. It’s because we try to document all the important
69,"looking changes you want to hear. If you upgrade into some errors, that’s another wordy document"
69,"or slightly confusing error message, please open a ticket, and we will try and fix it"
69,in subsequent releases.
69,"Now, that Spark is almost 10 years old now. The Spark community"
69,is very serious about making change. And we try our best to avoid [inaudible] changing.
69,"If you upgrade to Spark 3.0 at this time, you may see explicit error messages about"
69,changing. So the error message also provides config names for you to either go back to existing behavior or go with the new behavior.
69,"this talk, we talked about many exciting features and improvements in"
69,"Spark 3.0. Due to the lack of time, there are still many other nice features not being"
69,covered by this talk. Please download Spark 3.0 and try yourself.
69,You can also try the
69,[inaudible] Databricks [inaudible] 10.0 beta. All the new features are already available.
69,"The Community Edition is for free. Without the contributions by the whole community,"
69,it is impossible to deliver such a successful release. It’s thanks to all the Spark committers
69,"all over the world. Thank you. Thank you, everyone."
69,Watch more Spark + AI sessions here
69,Try Databricks for free
69,« back
69,About Xiao Li
69,Databricks
69,"Xiao Li is an engineering manager, Apache Spark Committer and PMC member at Databricks. His main interests are on Spark SQL, data replication and data integration. Previously, he was an IBM master inventor and an expert on asynchronous database replication and consistency verification. He received his Ph.D. from University of Florida in 2011."
69,About Wenchen Fan
69,Databricks
69,"Wenchen Fan is a software engineer at Databricks, working on Spark Core and Spark SQL. He mainly focuses on the Apache Spark open source community, leading the discussion and reviews of many features/fixes in Spark. He is a Spark committer and a Spark PMC member."
69,Video Archive
69,Terms of Use
69,Privacy Policy
69,Event Policy
69,Looking for a talk from a past event? Check the Video Archive
69,Organized by Databricks
69,"If you have questions, or would like information on sponsoring a Spark + AI Summit, please contact organizers@spark-summit.org."
69,"Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation."
69,The Apache Software Foundation has no affiliation with and does not endorse the materials provided at this event.
70,ApexSQL September 2020 Newsletter
70,ApexSQL September 2020 Newsletter
70,Where did the summer go?
70,"ApexSQL has been chugging along during the whole summer when, in general, many software companies typically tend to slow down a bit. Our engineering teams have been hard at work"
70,"bringing great new releases for the majority of our tools, and we'd like to highlight a few of them below:"
70,What's fresh
70,The releases of new products and technologies i.e. Visual Studio Code and MySQL is cracking.
70,MySQL data compare
70,"We got a brand new MySQL data comparison and synchronization tool, ApexSQL Data Diff for MySQL."
70,"This tool can detect data differences between two databases and resolve them without any errors. Moreover, it allows users to:"
70,Compare data between two MySQL databases
70,Customize object mapping
70,Select specific objects for comparison
70,"Export comparison results to HTML, Excel, or CSV"
70,Automate and schedule data synchronization
70,Create data synchronization scripts
70,"Besides MySQL, it also supports Azure Database, Amazon RDS for MySQL and MariaDB. The list of"
70,"features is rather long, and therefore we encourage you to see the product update alert for the full list of features. Furthermore, here are some key resources to"
70,help you get started with ApexSQL Data Diff for MySQL:
70,Getting started - a collection of resources to begin using ApexSQL Data Diff for MySQL quickly
70,Product page - learn more about what the MySQL data compare tool has to offer
70,Database Power Tools for VS Code
70,"The new big thing in ApexSQL Database Power Tools for VS Code 2020 R6 version is the browsable code completion list for inserting MySQL and MariaDB keywords, database, schema, object, parameter, and variable names in the SQL statements AKA IntelliSense. This feature speeds up coding by inserting keywords, schemas, objects, etc. while typing queries."
70,"For more details on the new release, please see the latest product update alert"
70,What's cooking?
70,ApexSQL Audit 2020 R4
70,We are pleased to announce significant enhancements coming to the next minor version of our SQL Server audit and compliance solution
70,"that will include, among other usability and quality improvements, features like:"
70,Extended session settings - Customizable session settings to manage auditing data collection
70,Additional report header options - Extracted reports including more options to formalize the audit documentation
70,ApexSQL Manage 2020 R5
70,The upcoming version of the SQL Server instance management tool from the ApexSQL DBA family will have a new set of Security
70,rules to extend the range of health check analysis.
70,This new set of rules will provide DBAs with the possibility to perform high-level analysis to
70,test the security of SQL Server instance(s):
70,SQL Server files not on NTFS drives
70,Dynamic data masking
70,Operating system up to date
70,Database Master Keys Encrypted by Password
70,SQL browser service not enabled
70,Symmetric keys encrypted by certificate
70,Symmetric keys not created on system databases
70,CLR should be disabled
70,Common Criteria Compliance
70,Max number of concurrent sessions
70,Allow Updates to System Tables is disabled
70,Default Trace enabled
70,Error log files maximum number
70,Asymmetric Key Size
70,Full-Text search should not be installed
70,Hide SQL Instance
70,Replication component installed
70,Transparent data encryption enabled for databases
70,Server does not contain orphaned users
70,"By executing the above set of rules, the SQL Server is examined and searched for any potential security issues and a comprehensive report is displayed to the DBA."
70,Quest Database Training Days Fall Series
70,"Join the interactive performance tuning sessions delivered by industry experts, Brent Ozar, Janis Griffin, and Pinal Dave."
70,"The next great opportunity for free SQL Server training from Oracle ACE Director Janis Griffin on the subject ""Top Five MySQL Query Tuning Tips"" is on September 29."
70,"Attend this event if you want to learn how to simplify MySQL query tuning. See how to monitor and review the execution plan. Then, look at the table, column, and index information to"
70,"understand selectivity to find the driving table. Finally, find performance inhibitors. With her tips, you'll perform MySQL query tuning quickly and easily!"
70,"Use the checkboxes to select the events you wish to attend, then complete the form to register on this link."
70,DBA tools discounted at 20%
70,"Now through September, get 20% off our already affordable tools that help you meet SQL Server"
70,"audit, compliance, and instance management; read SQL transaction logs, and recover data. Need"
70,them all? We even have our bundled tools discounted.
70,Put them in your shopping cart and buy it now:
70,ApexSQL Audit: Meet SQL Server audit and compliance requirements for all instances across your enterprise.
70,"Buy now! USD $1,359.20 $1699"
70,"ApexSQL Manage: Manage your SQL Server inventory, perform health checks, documentation, and comparison."
70,Buy now! USD $399.20 $499
70,"ApexSQL Log: Read the SQL Server transaction logs to audit, replicate, and/or rollback changes."
70,"Buy now! USD $1599.20 $1,999"
70,"ApexSQL Recover: Recover damaged, deleted, dropped, or lost data."
70,"Buy now! USD $1,599.20 $1,999"
70,"[Best Value] ApexSQL DBA Suite: All 9 ApexSQL DBA tools, combined 65% savings, free support, and upgrades for a year, more!"
70,"Buy now! USD $1,759.20 $2,199"
70,"Offers expire on September 30, 2020."
70,Other news and fresh content
70,Here is some content posted since the last newsletter from the Solution center...
70,Executing a TempDB health check analysis of SQL Server instances - learn how to perform TempDB health checks for SQL Server instances to increase overall instance performance
70,"Executing a health check analysis of the SQL Server instance Security category - this article covers part 1 of SQL instance security health check rules. The insight is given into the first set of rules by using the 3rd party solution, ApexSQL Manage"
70,Top things you need in a SQL Server coding productivity tool - this article is a walkthrough of what every SQL Server coding productivity tool should have with examples of one such SQL complete add-in for SSMS and VS
70,What's new
70,Videos
70,Blog
70,Solution Center
70,Knowledgebase
70,SQL Shack
70,What's next
70,What's new
70,Here are our most recent releases:
70,ApexSQL Database Power Tools for VS Code 2020 R6
70,"This release of ApexSQL Database Power Tools for VS Code 2020 R6 comes with: Browsable code completion list for inserting MySQL and MariaDB keywords, database, schema, object, parameter, and variable names in the SQL statements. Usability and quality improved with 10 bugs fixed"
70,ApexSQL Data Diff for MySQL 2020
70,"This release of ApexSQL Data Diff for MySQL 2020 comes with: MySQL data compare for databases. MySQL Server 5.6 or higher support. MariaDB Server 10.1 and higher support. Azure Database for MySQL support. Azure Database for MariaDB support. Amazon RDS for MySQL support. Amazon RDS for MariaDB support. Comparison options: Ignore case in table/view names, Ignore case in text data types, Ignore underscores in table/view names, Ignore new objects, Ignore leading and trailing spaces, Treat empty strings and nulls as equal. Synchronization options: Script Use for database. Object types to compare: Tables and views. Object mapping. Object filter. Export comparison results into 3 output types: HTML, CSV and Excel. Command line interface. GUI themes. High DPI and 4K resolution support. Visual Language Dictionary for Visual Studio for iconography and other imagery concepts applied. Application telemetry collects anonymous data on the use and performance of applications and application components. ApexSQL Updater configures advanced updating settings of all installed ApexSQL products"
70,ApexSQL Model 2020
70,This release of ApexSQL Model 2020 comes with: Application telemetry now collects anonymous data on the use and performance of applications and application components. New ApexSQL Updater allows users to configure advanced updating settings of all installed ApexSQL products. Usability and quality improved with 1 bug fixed
70,ApexSQL DevOps toolkit 2020 R6
70,This release of ApexSQL DevOps toolkit 2020 R6 comes with: TeamCity plug-in: Support for TeamCity 2020.01.
70,ApexSQL
70,DevOps toolkit 2020 R5
70,This release of ApexSQL DevOps toolkit 2020 R5 comes with: Octopus deploy plug-in: Improved quality. Usability and quality improved with 2 bugs fixed
70,ApexSQL Analayze 2020
70,This release of ApexSQL Analyze 2020 comes with: 	Application telemetry now collects anonymous data on the use and performance of applications and application components. New ApexSQL Updater allows users to configure advanced updating settings of all installed ApexSQL products. Usability and quality improved with 1 bug fixed
70,See What's new for descriptions of all of our recent releases
70,Videos
70,Take a look at our most recent videos:
70,An introduction to ApexSQL Diff for MySQL
70,An introduction to ApexSQL Mask
70,Automated database lifecycle using ApexSQL continuous integration tools - Source control
70,Automated database lifecycle using ApexSQL continuous integration tools - Development
70,An introduction to ApexSQL Manage
70,Blog
70,Here are our most recent blog
70,posts:
70,ApexSQL Database Power Tools for VS Code 2020 R6 - Product update alert
70,ApexSQL Script 2020 R2 - Product update alert
70,ApexSQL Data Diff for MySQL 2020 - Product update alert
70,ApexSQL Model 2020 - Product update alert
70,What's new in ApexSQL Database Power Tools for VS Code 2020 R6
70,ApexSQL DevOps toolkit 2020 R6 - Product update alert
70,ApexSQL DevOps toolkit 2020 R5 - Product update alert
70,ApexSQL Data Diff for MySQL 2020 - Screen shot tour
70,ApexSQL Diff 2019 R3 - Product update alert
70,What's new in ApexSQL: ApexSQL Data Diff for MySQL
70,ApexSQL Refactor 2020 R4 - Product update alert
70,ApexSQL Analyze 2020 - Product update alert
70,Solution center
70,Our Solution center is a content site with in-depth articles focusing on tough nuts that DBAs and developers are challenged with
70,Here are our most recent articles:
70,MySQL search: Searching for data in tables
70,How to export MySQL data to HTML
70,Top things you need in a SQL Server coding productivity tool
70,Executing a health check analysis of the SQL Server instance Security category
70,Executing a TempDB health check analysis of SQL Server instances
70,Knowledgebase
70,Take a look at some tricks and how-to articles for ApexSQL tools at our knowledgebase
70,blog:
70,Exporting and importing database masks that will be used to mask SQL Server data
70,Automatic import of SQL Server instances
70,Perform health check analysis on SQL Server instance databases
70,How to manually unlink a database from SQL source control
70,Mask SQL Server data with the Use original generator
70,SQL Shack
70,Here is a list of all SQL Shack articles published since our last newsletter:
70,Explore Manual Snapshots in AWS RDS SQL Server
70,Suspend and Resume Data Movement in SQL Server Always On Availability Groups
70,Configure SQL Server Reporting Services databases in SQL Server Always On Availability Groups
70,Data Disaster Recovery with Log Shipping
70,Refresh SQL Server Always On Availability Group databases using DBATools PowerShell
70,How to prepare for the Exam DP-201: Designing an Azure Data Solution
70,Learn MySQL: Delete and Update Statements
70,Impact of dropping a login in the active directory tied to SQL Server Always On Availability Groups
70,Backup SQL databases on the AWS S3 bucket using Windows PowerShell
70,How to prepare for the Exam DP-200: Implementing an Azure Data Solution
70,Backup compression in TDE enabled databases in SQL Server Always On Availability Groups
70,Configuring ODBC drivers for Azure Database for MySQL
70,How to parse JSON in SQL Server
70,Manticore search: a continuation of the Sphinx search engine
70,How to prepare for the Exam DP-300: Administering Relational Databases on Microsoft Azure
70,Exporting SSRS reports to multiple worksheets in Excel
70,Exploring AG dashboards for monitoring SQL Server Always On Availability Groups
70,Automatic Page Repair in SQL Server Always On Availability Groups
70,How to prepare for the Exam AZ-900: Microsoft Azure Fundamentals
70,Getting started with Azure SQL Database using Azure CLI
70,Restore an existing availability group database participating in SQL Server Always On Availability Groups
70,How to create an AWS SageMaker Instance
70,Deploy MSDTC for distributed transactions in SQL Server Always On Availability Groups
70,Explore Cross-database MSDTC for distributed transactions in SQL Server Always On Availability Groups
70,SQL interview questions
70,An introduction to SSIS Data Lineage concepts
70,Understanding Data Lineage in ETL
70,Saving AWS Redshift costs with elastic resize
70,Getting started with Azure Automation
70,Learn SQL: SQL-Related Jobs
70,Configuring SQL Server replication for distribution databases in SQL Server Always On Availability Groups
70,Data Reduction Technique: Principal Component Analysis in Azure Machine Learning
70,Configure SQL Server replication for a database in SQL Server Always On Availability Groups
70,Saving AWS Redshift costs with scheduled pause and resume actions
70,Enhancing Customer Experiences with Subscriptions in SSRS
70,Transparent Data Encryption for SQL Server Always On Availability Groups
70,Working with AWS Neptune Graph Databases
70,Deep dive into IT Cloud Automation using PowerShell
70,Don't fear SQL Server performance tuning
70,Importing data from JSON files and Power BI Rest APIs into Power BI
70,Monitor and failover a Distributed SQL Server Always On Availability Group
70,How to backup SQL databases to an FTP server using the SSIS FTP Task
70,Deploy a distributed SQL Server Always On Availability Group
70,Test-driven database hotfix development (TDHD) with SQL unit test based framework (tSQLt)
70,Exploring databases in Python using Pandas
70,Getting started with AWS RDS Aurora DB Clusters
70,Migrating SQL workloads to Microsoft Azure: Planning the jump
70,T-SQL scripts to copy or remove files from a directory in SQL Server 2019
70,Importing Performance Monitor data into SQL databases
70,An overview of distributed SQL Server Always On Availability Groups
70,Uploading SQL data into Azure Blob Storage using SSIS
70,Introduction to SQLAlchemy in Pandas Dataframe
70,What's next
70,We've got a full development pipeline. Here is what will be coming out soon:
70,ApexSQL Audit 2020 R4
70,The upcoming release of ApexSQL Audit will include features such as: Extended session settings. Additional report header options. Usability and quality will be improved
70,ApexSQL Doc 2020
70,The upcoming release of ApexSQL Doc will include features such as: Application telemetry now collects anonymous data on the use and performance of applications and application components. New ApexSQL Updater allows users to configure advanced updating settings of all installed ApexSQL products. Server Analysis Services compatibility level for Tabular models 1500 support. Usability and quality will be improved
70,ApexSQL Manage 2020 R5
70,The upcoming release of ApexSQL Manage will include features such as: New set of Security rules are added to extend the range of health check analysis. Usability and quality will be improved
70,ApexSQL Database Power Tools for VS Code 2020 R7
70,The upcoming release of ApexSQL Database Power Tools for VS Code will include features such as: Format MySQL and MariaDB scripts using predefined profiles: Compact and Extended. Safely rename database objects in MySQL and MariaDB. Usability and quality will be improved
70,ApexSQL Mask 2020 R3
70,The upcoming release of ApexSQL Mask will include features such as: Masking data without data loss using dynamic data masks. Usability and quality will be improved
70,Stay tuned for more releases and new products on our
70,What's next page
70,"Quest Software | 4 Polaris Way | Aliso Viejo, CA 92656 USA"
70,Phone: 1-800-306-9329
70,Email:
70,questsalessupport@quest.com
71,PostgreSQL Performance Tuning Tips - Ubiq BI
71,Toggle navigation
71,Build dashboards & reports in minutes
71,Features
71,Pricing
71,Blog
71,What is Ubiq
71,Free Trial
71,PostgreSQL Performance Tuning Tips
71,"October 20, 2020October 20, 2020"
71,Ubiq
71,"PostgreSQL performance tuning helps in database maintenance and update. It allows you to speed up your database and optimize PostgreSQL performance. Otherwise, your databases and queries will slow down over time and affect application performance. Here are the top 5 PostgreSQL performance tuning tips to help you optimize your databases and tables."
71,Best PostgreSQL Performance Tuning Tips
71,Here are some simple PostgreSQL performance tuning tips to help you improve database performance.
71,1. Using ANALYZE
71,"When we run a SQL query in PostgreSQL, it creates a query plan after parsing your query string, and based on certain database metrics and statistics that it collects based on all queries that it has run so far. These metrics need to be updated periodically, to ensure that PostgreSQL creates query execution plan based on latest information and data."
71,"ANALYZE command allows PostgreSQL to update these statistics based on latest table schema, indexes and other information. This improves query speed and performance. So every time you update table or schema, or add/update index, make sure that to run ANALYZE command."
71,2. Using EXPLAIN ANALYZE
71,"EXPLAIN command explains how the PostgreSQL query planner will execute your SQL query, which joins it will use, how it will extract data, and estimated rows of information in result."
71,When used with ANALYZE command it even provides the amount of time each of these query operations will take. It will also tell you which operations will be done in-memory. This is very useful in identifying performance bottlenecks and optimization opportunities.
71,3. Using Slow Query Log
71,"PostgreSQL even provides the ability to log slow running queries. By logging long running queries into log file, you can easily identify which queries take most of your server’s time."
71,Here are the detailed steps to enable slow query log in PostgreSQL.
71,4. Using Indexing
71,"Indexes make it easy for PostgreSQL to do lookups which are useful for WHERE conditions and JOINS. Otherwise, each of these conditions will lead to a full table lookup, which is time consuming."
71,"PostgreSQL supports various types of indexes such as B-Tree (default), Hash, GiST, SP-GiST, and GIN. Here are the detailed steps to create PostgreSQL index."
71,5. Increase maximum connections
71,"By default, PostgreSQL supports a maximum of 100 concurrent connections. This is stored in max_connections server variable. You can increase this number to support more concurrent connections and keep users from waiting. However, each connection consumes memory, so don’t increase it, unless required."
71,Some more performance tips
71,You must also consider regularly updating your PostgreSQL to the latest version. Each update is faster than its predecessor and contains important performance updates.
71,"Similarly, if possible, run your database and application on different servers. Many times, application bugs consume a lot of memory and slow down the memory available to run database queries."
71,"Hopefully, the above performance tuning tips will help you improve PostgreSQL speed and performance."
71,"About AuthorAbout UbiqUbiq is a powerful dashboard & reporting platform. Build dashboards, charts & reports for your business in minutes. Try it for free!Related posts:How to Get First Row Per Group in PostgreSQLHow to Calculate Median in PostgreSQLHow to Calculate Moving Average in Redshift"
71,PostgreSQL
71,postgresql performance tuning.
71,permalink.
71,Post navigation
71,How To Enable MySQL Slow Query Log in MySQLHow To Enable MySQL Query Cache
71,"About UsUbiq is a business intelligence & reporting software. Build business dashboards, charts & reports in minutes. Get insights from data quickly. Try it for free!"
71,Data Reporting
71,MySQL Reporting
71,PostgreSQL Reporting
71,Online Reporting
71,Web Reporting
71,Redshift BI Reporting
71,SQL Reporting
71,Business Intelligence
71,BI Solution
71,BI Reporting Software
71,MySQL BI Reporting Tools
71,Self Service BI
71,SaaS BI
71,Data Visualization
71,Data Visualization Tools
71,Data Analysis Tools
71,Visual Analytics
71,MySQL Charts
71,MySQL Graph Generator
71,MySQL Report Builder
71,Online Report Generator
71,Redshift Data Visualization
71,Dashboards
71,Dashboard Builder
71,Dashboard Reporting Software
71,Dashboard Creator
71,KPI Dashboard Software
71,Quicklinks
71,Contact Us
71,Docs
71,Jobs
71,BI Blog
71,Database Blog
71,Tech Blog
71,Resources
71,Security
71,Privacy
71,T&C
71,Sitemap
71,dazzling					Theme by Colorlib Powered by WordPress
72,MariaDB vs MySQL: [2021] Everything You Need to Know
72,Programming
72,Data Science
72,DevOps
72,Design
72,Sign Up
72,Submit a tutorial
72,Sign Up / Sign In
72,Programming
72,Data Science
72,DevOps
72,Design
72,MariaDB and MySQL
72,MariaDB Tutorials
72,Related Tutorials
72,MariaDB
72,MongoDB
72,MySQL
72,SQL
72,SQLite
72,Recommended Learning
72,Getting Started with MariaDB (amazon.com)
72,Crash Course on MySQL (sysadmincasts.com)
72,The Ultimate MySQL Bootcamp: Go from SQL Beginner to Expert (udemy.com)
72,View More
72,MariaDB vs MySQL: [2021] Everything You Need to Know
72,Posted in
72,"MariaDB,"
72,MySQL
72,Aman Goel
72,"Last Updated 08 Jan, 2021"
72,Share:
72,2 Comments
72,Table of Contents
72,MariaDB vs MySQL
72,MariaDB vs MySQL Performance Comparison
72,Database Views
72,ColumnStore
72,Better Performance in Flash Storage
72,Segmented Key Cache
72,Virtual Columns
72,Parallel Execution of Queries
72,Thread Pooling
72,Storage Engines
72,Compatibility
72,Open Source vs Proprietary Database
72,Conclusion
72,"MySQL is one of the most widely used databases across the world. It is free and is open-source as well. Developed in C/C++, MySQL is one of the most popular database choices."
72,"The database was started by a Swedish company “MySQL AB” in 1995. MySQL AB was later acquired by Sun Microsystems in 2008. Later, Sun Microsystems was acquired by Oracle in 2010. Since then, MySQL is maintained and managed by Oracle."
72,"During the acquisition of Sun Microsystems by Oracle, some of the senior engineers who were working on the development of MySQL felt that there is a conflict of interest between MySQL and Oracle’s commercial database - Oracle Database Server. As a result, these engineers created a fork of MySQL code base and started their own organization. This is how MariaDB was born."
72,"As of today, both databases are highly popular and are extensively used by the developer community. MySQL is ranked #2 among the relational databases and #2 overall (#1 being Oracle database). On the contrary, MariaDB is slightly behind - #9 among the relational databases and #14 overall."
72,MariaDB vs MySQL
72,"In this blog post, we will try to compare some of the features of both of these databases to see which one is the best for usage in 2021."
72,MariaDB vs MySQL Performance Comparison
72,"MariaDB has several optimizations that tend to improve the performance as compared to MySQL. In fact, that was exactly the vision in mind when MariaDB was started by Michael Widenius, the original founder of both MySQL as well as MariaDB."
72,Database Views
72,"As an example, there is a huge performance optimization with respect to database “views”. “Views” are essentially virtual database tables which can be queried like regular tables of the database. In MySQL, when you query a view, all of the tables that are connected to the view are queried, irrespective of the fact that the query may not require some of the views. This has been optimized in MariaDB where only those tables are queried that are required by the query."
72,ColumnStore
72,"As another example, MariaDB provides yet another powerful performance improvement in the form of “ColumnStore” which is a distributed data architecture that allows scaling MariaDB greatly. It can scale linearly to store petabytes of data across various servers in a database cluster."
72,Better Performance in Flash Storage
72,MariaDB also provides MyRocks storage engine that adds the RocksDB database to it. RocksDB is a database that has been designed for better performance in flash storage by providing a higher level of data compression.
72,Segmented Key Cache
72,"MariaDB introduces another performance improvement in the form of Segmented Key Cache. In a typical cache, various threads compete to take a lock over the cached entry. These locks are called as mutexes. When multiple threads are competing for a mutex, only one of them is able to get it while others have to wait for the lock to get freed before performing the operation. This leads to execution delays in these threads slowing down the database performance. In case of Segmented Key Cache, the thread need not lock the entire page, but it can lock only the particular segment to which the page belongs. This helps multiple threads to work in parallel thereby increasing the parallelism in the application leading to better performance of the database."
72,Virtual Columns
72,"An interesting feature that MariaDB supports is that of virtual columns. These columns are capable of performing the calculations at the database level. This is extremely useful when many applications are accessing the same column and so, there is no need to write the calculation in each application - the database can do that for you. This feature isn’t available in MySQL."
72,Parallel Execution of Queries
72,"One of the recent versions of MariaDB - 10.0 allows for parallel execution of several queries. The idea is that some queries from the Master can be replicated in the slave and can, therefore, be executed in parallel. This parallelism in query execution certainly provides MariaDB an edge over MySQL."
72,Thread Pooling
72,"MariaDB also introduces a new concept called “Thread Pooling”. Previously, when multiple connections to a database were needed, for each connection, a thread was opened leading to a “one thread per connection” based architecture. With “Thread Pooling”, there will be a pool of open threads which a new connection can pick up and query the database. This way, a new thread need not be opened for every new connection request leading to faster query results. This feature is available in the Enterprise edition of MySQL but is unfortunately unavailable in the Community edition."
72,Storage Engines
72,"MariaDB provides several powerful storage engines out-of-the-box which are not available in MySQL. For example, XtraDB, Aria, etc. To set up these storage engines for MySQL, you need to install them manually which may not be the most convenient thing."
72,Compatibility
72,"MariaDB team is making sure that MariaDB can seamlessly replace MySQL in the existing applications. In fact, for each version of MySQL, they release the same version number of MariaDB to indicate that MariaDB is generally compatible with the corresponding MySQL version. This opens up the possibility of switching to MariaDB seamlessly without any modifications in the application code-base."
72,Open Source vs Proprietary Database
72,"MySQL is a large project and is managed by one of the largest organizations in the world - Oracle. This has its pros and cons. One of the biggest con is that releasing new features in organizations that big takes a lot of time. On the other hand, MariaDB is fully open sourced and they are quite fast in accepting outside contributions and releasing as new features and enhancements. This is yet another point that must be kept in mind while deciding between MySQL and MariaDB."
72,Conclusion
72,"MariaDB is undoubtedly quite powerful and provides many features that are extremely useful and are not supported in MySQL. Such features indeed make MariaDB a lucrative choice to be used as the primary backend database. Generally speaking, organizations that have already purchased licenses for Oracle need not invest in MariaDB. However, those who are starting afresh and want to decide on which database to use, undoubtedly MariaDB is a better choice."
72,"Which database you opt for, Hackr.io has programming community-recommended tutorials for both:"
72,MySQL Tutorials and Courses
72,MariaDB Tutorials and Courses
72,People are also reading:
72,PostgreSQL vs MySQL
72,Python vs Java
72,CSS vs CSS2
72,Python vs PHP
72,HTML vs HTML5
72,Django vs Laravel
72,Kotlin vs Java
72,Back-end development MariaDB MySql Database
72,Share:
72,"Aman Goel Entrepreneur, Coder, Speed-cuber, Blogger, fan of Air crash investigation! Aman Goel is a Computer Science Graduate from IIT Bombay. Fascinated by the world of technology he went on to build his own start-up - AllinCall Research and Solutions to build the next generation of Artificial Intelligence, Machine Learning and Natural Language Processing based solutions to power businesses. View all posts by the Author"
72,Related Posts
72,MySQL Cheat Sheet: Download PDF for Quick Reference
72,Read More
72,MySQL Create Database Statement
72,Read More
72,MongoDB vs MySQL
72,Read More
72,Leave a comment
72,Email address*
72,Your email will not be published
72,Name*
72,Comment*
72,Submit
72,Cancel
72,Benjamin Morel
72,"Virtual columns have been supported in MySQL since version 5.7, in 2015."
72,Reply
72,Max
72,mariaDB / raspberry Pi B+ / npm NodeRed
72,"my question, where the Databases/Tables are physically stored?"
72,İ have installed a usb SSD on this İ wants
72,have all the data
72,but İ could not find the procedure to define the path
72,The OS is raspian-stretch
72,Thank you very much for your help
72,kind regards
72,Reply
72,WebTechLabs
72,the database storage location is set in my.cnf
72,Reply
72,Related Tutorials
72,MariaDB
72,MongoDB
72,MySQL
72,SQL
72,SQLite
72,Recommended Learning
72,Getting Started with MariaDB (www.amazon.com)
72,Crash Course on MySQL (sysadmincasts.com)
72,The Ultimate MySQL Bootcamp: Go from SQL Beginner to Expert (www.udemy.com)
72,View More
72,Welcome Back
72,Continue with:
72,Facebook
72,Github
72,Forgot Password
72,Login
72,Don't have an account? Sign Up
72,"Welcome to Hackr.io Signup to submit and upvote tutorials, follow topics, and more."
72,Continue with:
72,Facebook
72,Github
72,Minimum 6 characters
72,Create Account
72,Already have an account? Login
72,Forgot Password Password reset link will be sent to your email.
72,Send Password
72,Didn’t recieve the password reset link? Resend
72,Loading...
72,Blog
72,Roadmaps
72,About Us Programming Tips Help & FAQ
72,We Feedback
72,"Disclosure: This page may contain affliate links, meaning when you click the links and make a purchase, we receive a commission."
73,Oracle Video Hub
73,Skip to content
73,My MediaMy PlaylistsMy History
73,Login
73,Add New
73,Media Upload
73,My MediaMy PlaylistsMy History
73,Login
73,Home
73,Cloud Applications
73,Analytics
73,Customer Experience
73,Commerce
73,CPQ
73,CX Marketing
73,CX Service
73,Intelligent Advisor
73,Data Cloud
73,CX Sales
73,Subscription Management
73,Enterprise Resource Planning
73,EPM
73,Human Capital Management
73,Marketplace
73,Supply Chain and Manufacturing
73,Industry Applications
73,Communications Global Business Unit
73,Construction and Engineering
73,English
73,German
73,Spanish
73,Financial Services
73,Food and Beverage
73,Health Sciences
73,Clinical One English
73,Clinical One Mandarin
73,InForm
73,Safety One Intake
73,User Management Tool
73,Hospitality
73,Public Sector
73,Retail
73,Spanish Retail Video Gallery
73,Utilities
73,On-Premises Applications
73,JD Edwards
73,Siebel CRM
73,Oracle NetSuite
73,Services
73,SuiteSupport
73,Partners
73,Why Oracle
73,Onboard
73,Get Enabled
73,Go-to-Market
73,Manage & Grow
73,Product Help
73,Advanced Customer Services
73,Cloud Applications Tutorial Videos
73,Cloud Customer Connect Community
73,Cloud Digital Learnings
73,Cloud Readiness
73,Consulting
73,EMEA Consulting
73,ERP
73,HCM
73,NA Consulting (Applications)
73,NA Consulting (Tech)
73,Oracle Cloud Method Platform (OCMP)
73,Oracle Unified Method (OUM)
73,Partner Cloud Enablement
73,Project Management
73,My Oracle Support Portal
73,On-Premises Applications Tutorial Videos
73,Premier Support
73,Channels
73,Home
73,Cloud Applications
73,Analytics
73,Customer Experience
73,Commerce
73,CPQ
73,CX Marketing
73,CX Service
73,Intelligent Advisor
73,Data Cloud
73,CX Sales
73,Subscription Management
73,Enterprise Resource Planning
73,EPM
73,Human Capital Management
73,Marketplace
73,Supply Chain and Manufacturing
73,Industry Applications
73,Communications Global Business Unit
73,Construction and Engineering
73,English
73,German
73,Spanish
73,Financial Services
73,Food and Beverage
73,Health Sciences
73,Clinical One English
73,Clinical One Mandarin
73,InForm
73,Safety One Intake
73,User Management Tool
73,Hospitality
73,Public Sector
73,Retail
73,Spanish Retail Video Gallery
73,Utilities
73,On-Premises Applications
73,JD Edwards
73,Siebel CRM
73,Oracle NetSuite
73,Services
73,SuiteSupport
73,Partners
73,Why Oracle
73,Onboard
73,Get Enabled
73,Go-to-Market
73,Manage & Grow
73,Product Help
73,Advanced Customer Services
73,Cloud Applications Tutorial Videos
73,Cloud Customer Connect Community
73,Cloud Digital Learnings
73,Cloud Readiness
73,Consulting
73,EMEA Consulting
73,ERP
73,HCM
73,NA Consulting (Applications)
73,NA Consulting (Tech)
73,Oracle Cloud Method Platform (OCMP)
73,Oracle Unified Method (OUM)
73,Partner Cloud Enablement
73,Project Management
73,My Oracle Support Portal
73,On-Premises Applications Tutorial Videos
73,Premier Support
73,Channels
73,"Search for tag: ""heatwave"""
73,24 Media
73,Sort by Creation Date - Descending
73,Alphabetically - A to ZAlphabetically - Z to ACreation Date - AscendingCreation Date - DescendingUpdate Date - AscendingUpdate Date - DescendingViewsPlaysLikes
73,View All Media
73,All MediaVideoQuizAudioImageWebcasting Events
73,44:29
73,400x Query Acceleration for MySQL with HeatWave
73,400x Query Acceleration for MySQL with HeatWave
73,400x Query Acceleration for MySQL with HeatWave
73,"HeatWave is a new, in-memory query accelerator for MySQL Database"
73,Service available in Oracle Cloud. HeatWave can accelerate performance
73,"on large multi-TB datasets, and scale across 1,000s cores.…"
73,mysqlmysql heatwave
73,From
73,Catherine Carlson
73,"on March 29th, 2021"
73,1 likes
73,| 42
73,42 plays
73,Retention Time
73,5 Years
73,11:14
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 |…"
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 | Session 6 - Q&A & FAQs"
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 | Session 6 - Q&A & FAQs"
73,"MySQL Cloud Day 2021 | ANZ, ASEAN & HK - Get a deep dive and interactive experience on how the world's #1 open source database in Oracle Cloud, can help you with your cloud and database…"
73,mysqlmysql database servicemysql heatwavemysql cloud serviceoracle cloudoracle cloud infrastructuredatabaseopensource
73,From
73,Elizabeth Lee
73,"on March 26th, 2021"
73,0 likes
73,| 5
73,5 plays
73,Retention Time
73,5 Years
73,27:33
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 |…"
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 | Session 5: Upgrading to MySQL 8.0 on OCI"
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 | Session 5: Upgrading to MySQL 8.0 on OCI"
73,"MySQL Cloud Day 2021 | ANZ, ASEAN & HK - Get a deep dive and interactive experience on how the world's #1 open source database in Oracle Cloud, can help you with your cloud and database…"
73,mysqlmysql database servicemysql heatwavemysql cloud serviceoracle cloudoracle cloud infrastructuredatabaseopensource
73,From
73,Elizabeth Lee
73,"on March 26th, 2021"
73,0 likes
73,| 7
73,7 plays
73,Retention Time
73,5 Years
73,32:44
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 |…"
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 | Session 4: Migration from Amazon RDS to MySQL Database Service"
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 | Session 4: Migration from Amazon RDS to MySQL Database Service"
73,"MySQL Cloud Day 2021 | ANZ, ASEAN & HK - Get a deep dive and interactive experience on how the world's #1 open source database in Oracle Cloud, can help you with your cloud and database…"
73,mysqlmysql database servicemysql heatwavemysql cloud serviceoracle cloudoracle cloud infrastructuredatabaseopensource
73,From
73,Elizabeth Lee
73,"on March 26th, 2021"
73,0 likes
73,| 8
73,8 plays
73,Retention Time
73,5 Years
73,27:38
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 |…"
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 | Session 3: Developing cloud-native application with MySQL Database Service"
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 | Session 3: Developing cloud-native application with MySQL Database Service"
73,"MySQL Cloud Day 2021 | ANZ, ASEAN & HK - Get a deep dive and interactive experience on how the world's #1 open source database in Oracle Cloud, can help you with your cloud and database…"
73,mysqlmysql database servicemysql heatwavemysql cloud serviceoracle cloudoracle cloud infrastructuredatabaseopensource
73,From
73,Elizabeth Lee
73,"on March 26th, 2021"
73,0 likes
73,| 18
73,18 plays
73,Retention Time
73,5 Years
73,29:38
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 |…"
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 | Keynote: MySQL Database Service and HeatWave"
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 | Keynote: MySQL Database Service and HeatWave"
73,"MySQL Cloud Day 2021 | ANZ, ASEAN & HK - Get a deep dive and interactive experience on how the world's #1 open source database in Oracle Cloud, can help you with your cloud and database…"
73,mysqlmysql database servicemysql heatwavemysql cloud serviceoracle cloudoracle cloud infrastructuredatabase#opensource
73,From
73,Elizabeth Lee
73,"on March 26th, 2021"
73,0 likes
73,| 20
73,20 plays
73,Retention Time
73,5 Years
73,22:41
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 |…"
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 | Session 1 - MySQL Performance Tuning Tips & Tricks"
73,"MySQL Cloud Day ANZ, ASEAN & HK 2021 | Session 1 - MySQL Performance Tuning Tips & Tricks"
73,"MySQL Cloud Day 2021 | ANZ, ASEAN & HK - Get a deep dive and interactive experience on how the world's #1 open source database in Oracle Cloud, can help you with your cloud and database…"
73,mysqlmysql database servicemysql heatwavemysql cloud serviceoracle cloudoracle cloud infrastructuredatabaseopensource
73,From
73,Elizabeth Lee
73,"on March 26th, 2021"
73,0 likes
73,| 52
73,52 plays
73,Retention Time
73,5 Years
73,27:03
73,分析クエリのパフォーマンスを400倍向上!! HeatWaveのご紹介
73,分析クエリのパフォーマンスを400倍向上!! HeatWaveのご紹介
73,分析クエリのパフォーマンスを400倍向上!! HeatWaveのご紹介
73,2021年3月に開催した「ゲーム業界におけるMySQL Vol.2　〜スクウェア・エニックスの検証結果に学ぶ、MySQL HeatWaveを用いたクエリ爆速術〜」での「分析クエリのパフォーマンスを400倍向上!! HeatWaveのご紹介」の講演の模様です。ぜひご活用ください。 資料ダウンロードはこちら 分析クエリのパフォーマンスを400倍向上!! HeatWaveのご紹介
73,#mysqlmysqlmysql database servicemysql heatwaveheatwave
73,From
73,Tomomi Kisaichi
73,"on March 8th, 2021"
73,1 likes
73,| 5
73,5 plays
73,Retention Time
73,5 Years
73,01:02:18
73,MySQL 按需灵活部署 | 如何从RDS或者其他云以及本地数据库迁移到MDS云服务
73,MySQL 按需灵活部署 | 如何从RDS或者其他云以及本地数据库迁移到MDS云服务
73,MySQL 按需灵活部署 | 如何从RDS或者其他云以及本地数据库迁移到MDS云服务
73,"本期我们将研讨的主题是 --- 如何从Amazon RDS或其他云数据库服务以及本地数据库迁移到高度安全的OCI Gen2云平台中的MySQL数据库服务。 了解有关数据库迁移的快速步骤和最佳实践 探索从MySQL团队获得最新修补程序，新功能和支持的主要好处 详解与Amazon RDS相比，MDS如何为您节省多达70％的TCO Speaker | Ivan Ma, MySQL解决方案工程师,…"
73,mysqlmysql database servicemysql heatwavemysql cloud serviceoracle clouddatabaseopensource
73,From
73,Elizabeth Lee
73,"on March 8th, 2021"
73,0 likes
73,| 1
73,1 plays
73,Retention Time
73,5 Years
73,20:02
73,MySQL Cloud Day India 2021 | Q&A Session…
73,MySQL Cloud Day India 2021 | Q&A Session & FAQs
73,MySQL Cloud Day India 2021 | Q&A Session & FAQs
73,"MySQL Cloud Day 2021 | India - Get a deep dive and interactive experience on how the world's #1 open source database in Oracle Cloud, can help you with your cloud and database solutions."
73,We…
73,mysqlmysql database servicemysql heatwavemysql cloud serviceoracle cloudoracle cloud infrastructuredatabaseopen source
73,From
73,Elizabeth Lee
73,"on March 8th, 2021"
73,0 likes
73,| 3
73,3 plays
73,Retention Time
73,5 Years
73,31:19
73,MySQL Cloud Day India 2021 | Session 5: MySQL HA…
73,MySQL Cloud Day India 2021 | Session 5: MySQL HA with InnoDB Cluster
73,MySQL Cloud Day India 2021 | Session 5: MySQL HA with InnoDB Cluster
73,"MySQL Cloud Day 2021 | India - Get a deep dive and interactive experience on how the world's #1 open source database in Oracle Cloud, can help you with your cloud and database solutions."
73,We…
73,mysqlmysql database servicemysql heatwavemysql cloud serviceoracle cloudoracle cloud infrastructuredatabaseopensource
73,From
73,Elizabeth Lee
73,"on March 8th, 2021"
73,0 likes
73,| 13
73,13 plays
73,Retention Time
73,5 Years
73,24:43
73,MySQL Cloud Day India On-demand | Session 4:…
73,MySQL Cloud Day India On-demand | Session 4: Migration from Amazon RDS to MySQL Database Service
73,MySQL Cloud Day India On-demand | Session 4: Migration from Amazon RDS to MySQL Database Service
73,"MySQL Cloud Day 2021 | India - Get a deep dive and interactive experience on how the world's #1 open source database in Oracle Cloud, can help you with your cloud and database solutions."
73,We…
73,mysqlmysql database servicemysql heatwavemysql cloud serviceoracle cloudoracle cloud infrastructuredatabaseopensource
73,From
73,Elizabeth Lee
73,"on March 8th, 2021"
73,0 likes
73,| 9
73,9 plays
73,Retention Time
73,5 Years
73,31:44
73,MySQL Cloud Day India 2021 | Session 3: Oracle…
73,MySQL Cloud Day India 2021 | Session 3: Oracle Analytics Cloud with MySQL Database Service
73,MySQL Cloud Day India 2021 | Session 3: Oracle Analytics Cloud with MySQL Database Service
73,"MySQL Cloud Day 2021 | India - Get a deep dive and interactive experience on how the world's #1 open source database in Oracle Cloud, can help you with your cloud and database solutions."
73,We…
73,mysqlmysql database servicemysql heatwavemysql cloud serviceoracle cloudoracle cloud infrastructuredatabaseopensource
73,From
73,Elizabeth Lee
73,"on March 8th, 2021"
73,0 likes
73,| 22
73,22 plays
73,Retention Time
73,5 Years
73,36:15
73,MySQLCloud Day India 2021 | Keynote: MySQL…
73,MySQLCloud Day India 2021 | Keynote: MySQL Database Service and HeatWave
73,MySQLCloud Day India 2021 | Keynote: MySQL Database Service and HeatWave
73,"MySQL Cloud Day 2021 | India - Get a deep dive and interactive experience on how the world's #1 open source database in Oracle Cloud, can help you with your cloud and database solutions."
73,We…
73,mysqlmysql database servicemysql heatwavemysql cloud serviceoracle cloudoracle cloud infrastructuredatabaseopensource
73,From
73,Elizabeth Lee
73,"on March 8th, 2021"
73,0 likes
73,| 16
73,16 plays
73,Retention Time
73,5 Years
73,22:00
73,MySQL Cloud Day India 2021 | Session 1: MySQL…
73,MySQL Cloud Day India 2021 | Session 1: MySQL Performance tuning: Tips & Tricks
73,MySQL Cloud Day India 2021 | Session 1: MySQL Performance tuning: Tips & Tricks
73,"MySQL Cloud Day 2021 | India - Get a deep dive and interactive experience on how the world's #1 open source database in Oracle Cloud, can help you with your cloud and database solutions."
73,We…
73,mysqlmysql database servicemysql heatwavemysql cloud serviceoracle cloudoracle cloud infrastructuredatabaseopensource
73,From
73,Elizabeth Lee
73,"on March 8th, 2021"
73,1 likes
73,| 70
73,70 plays
73,Retention Time
73,5 Years
73,56:01
73,HeatWaveのご紹介 - MySQLの更新処理と分析処理を一つのデータベースで実現！…
73,HeatWaveのご紹介 - MySQLの更新処理と分析処理を一つのデータベースで実現！ (2020年12月23日)
73,HeatWaveのご紹介 - MySQLの更新処理と分析処理を一つのデータベースで実現！ (2020年12月23日)
73,2020年12月23日に開催した「MySQLの更新処理と分析処理を一つのデータベースで実現！新機能『HeatWave』のご紹介」での「MySQL Database ServiceのHeatWaveによるデータベースの新しい可能性」の講演の模様です。ぜひご活用ください。
73,資料ダウンロードはこちら MySQL Database ServiceのHeatWaveによるデータベースの新しい可能性
73,mysqlmysql database service#mysqlmysql cloud servicemysql heatwave
73,From
73,Tomomi Kisaichi
73,"on February 21st, 2021"
73,0 likes
73,| 12
73,12 plays
73,Retention Time
73,5 Years
73,Oracle Integrated Applications & Platform Services
74,How to Optimize SQL with EXPLAIN – Looker Help Center
74,User Guide
74,Getting Started
74,Help Center
74,Documentation
74,Community
74,Training
74,Certification
74,User Guide
74,Getting Started
74,Help Center
74,Documentation
74,Community
74,Training
74,Certification
74,Sign in
74,Looker Help Center
74,Exploring Data
74,How To
74,Articles in this section
74,How to Plot Dimensions on a Y-Axis
74,How to Forecast in Looker with Table Calculations
74,Getting the First Record with a Non-Null Value in a Pivot Column or Pivot Row
74,How to Find the Top Performers Over Time with Table Calculations
74,Interactive Map Visualizations (3.34+)
74,Best Practice: Optimize Query Performance
74,Rolling Average Using offset_list in Table Calculations
74,How to Sort by One or More Columns
74,Creating Reference Lines for Charts through the Visualization Editor
74,Looker-Built Google Sheets Import Script
74,See more
74,How to Optimize SQL with EXPLAIN
74,Maxie Corbin
74,"June 01, 2020 20:58"
74,Updated
74,Follow
74,How EXPLAIN Helps Optimize Queries
74,"This article discusses the EXPLAIN command, which is not supported by all database dialects. Be sure to check the Feature Support section in the Looker Database Configuration Instructions specific to your database to confirm whether this function is supported by your database dialect."
74,"Very slow SQL queries are sometimes a fact of life for a database. The database could be running as fast as it can while those queries are transforming a lot of data, or the database could be performing certain tasks that are hard for SQL query planners to accomplish. However, sometimes it's possible to optimize these queries by looking at the steps involved in completing the query, and using this information to redesign the query to be faster. In many SQL dialects, this can be achieved by using the EXPLAIN command."
74,"This article gives a very brief introduction to and an example of how to interpret the results of the EXPLAIN command in SQL Runner. Since every database dialect has a slightly different implementation of EXPLAIN, different dialects often return different information in different formats. Make sure to reference the documentation for your database to find out how to interpret query plans for your dialect."
74,Example
74,Here is an example of using the results from an EXPLAIN to optimize a query in MySQL. Suppose we have a persistent derived table that we are using to generate company facts for each company in the database. The LookML and SQL used to generate the derived table looks like this:
74,Note: permalink here is used as an ID for each company or person. It's only unique in companies.
74,view: company_facts {
74,derived_table: {
74,"persist_for: ""24 hours"""
74,# This derived table is built in MySQL
74,sql: SELECT
74,"companies.permalink AS permalink,"
74,"companies.name AS name,"
74,"acquisition_by.acquisition_count AS acquisition_count,"
74,"acquired.acquired AS acquired,"
74,"acquired.price_amount AS acquisition_price,"
74,"COUNT(DISTINCT investments.investor_permalink) AS investor_count,"
74,"COUNT(DISTINCT investments.funding_id) AS lifetime_funding_rounds,"
74,"SUM(funding.raised_amount) AS lifetime_funding_raised,"
74,"-- List of all investors, whether they were a person or a company"
74,GROUP_CONCAT(
74,(CASE
74,WHEN investor_people.investor_name IS NOT NULL THEN investor_people.investor_name
74,WHEN investor_people.investor_name IS NULL THEN investor_companies.name
74,"END) SEPARATOR "", "") AS investor_name"
74,FROM companies
74,LEFT JOIN funding
74,ON companies.permalink = funding.permalink
74,LEFT JOIN investments
74,ON funding.id = investments.funding_id
74,LEFT JOIN (
74,SELECT
74,"people.permalink AS permalink,"
74,"CONCAT(people.first_name, ' ', people.last_name) AS investor_name"
74,FROM people) AS investor_people
74,ON investments.investor_permalink = investor_people.permalink
74,LEFT JOIN companies AS investor_companies
74,ON investments.investor_permalink = investor_companies.permalink
74,-- Calculate the number of acquisitions made by each company
74,-- Join into main query
74,LEFT JOIN (
74,SELECT
74,"acquisitions.acquired_by_permalink,"
74,"acquisitions.price_amount,"
74,COUNT(*) AS acquisition_count
74,FROM acquisitions
74,GROUP BY acquisitions.acquired_by_permalink) AS acquisition_by
74,ON companies.permalink = acquisition_by.acquired_by_permalink
74,-- Calculate the number of times a company was acquired (should only ever be equal to or less than 1)
74,-- Join into main query
74,LEFT JOIN (
74,SELECT
74,"acquisitions.acquired_permalink,"
74,"acquisitions.price_amount,"
74,COUNT(*) AS acquired
74,FROM acquisitions
74,GROUP BY acquisitions.acquired_permalink) AS acquired
74,ON companies.permalink = acquired.acquired_permalink
74,GROUP BY companies.permalink
74,"We have generated a model for this table, joined it on companies, and tested it out in an Explore. What we expected to be a short-running query, never finished, even after waiting 30 minutes."
74,-- use existing company_facts in crunchtrain_scratch.LR$DB1LLHE8FN9VSAQYNZMIC_company_facts
74,SELECT
74,"companies.name AS `companies.name`,"
74,"company_facts.acquisition_count AS `company_facts.lifetime_acquisitions`,"
74,"company_facts.lifetime_funding_raised AS `company_facts.lifetime_funding_raised`,"
74,company_facts.lifetime_funding_rounds AS `company_facts.lifetime_funding_rounds`
74,FROM companies
74,LEFT JOIN crunchtrain_scratch.LR$DB1LLHE8FN9VSAQYNZMIC_company_facts AS company_facts ON companies.permalink = company_facts.permalink
74,"GROUP BY 1,2,3,4"
74,ORDER BY companies.name
74,LIMIT 500
74,"No one wants to have to wait that long for a query, so let's try to improve this."
74,We'll run an EXPLAIN on this query and figure out what is going on.
74,Looker provides a very convenient Explain in SQL Runner button under the SQL tab in the Explore.
74,"When we click that button, the query is loaded into SQL Runner inside an EXPLAIN function. Once we click on Run in the SQL Runner window, Looker displays a short query plan with some key pieces of information:"
74,The Important Bits
74,"The type column tells us what kind of table scan was done on that step. The key difference in the results is between ALL and index. This tell us that the database is using an index scan to find the rows we care about in companies, but it's doing a full table scan figuring out what we need out of company_facts, our PDT."
74,"The rows column tells us how many rows were generated in this step and passed along to the next step as an intermediary result. The key thing to look for here is ""throw away"" rows, which are rows that were generated in one step and then ignored in the next step."
74,"The Extra column tells us a little bit about other special conditions that might be at work here. In this case, we see that the table scan on companies used an index and was sorted, but the scan on company_facts did not. We also see that a nested loop was used to join this table to companies."
74,"From this information, I've surmised that I'm missing an index on my derived table, which might be why this query is running so slowly. Joining without an index is forcing the query to loop over all the possible combinations companies.permalink and company_facts.permalink."
74,"We can see this because the query planner told us it was using a nested loop in the Extra column. Since each step in the query planner takes up 158,772 and 189,108 rows respectively, that means that the database has to compare 158,772 * 189,108 = 30,025,055,376 possible matches. No wonder it never finished!"
74,How to Fix It
74,"Luckily, this is a pretty easy problem to get past. We just need to add an index to my derived table. I can do that like this:"
74,view: company_facts {
74,derived_table: {
74,"indexes: [""permalink""]"
74,"persist_for: ""1 minute"""
74,sql: SELECT
74,etc...
74,"Once I do that, I can refresh my Explore, rebuild the derived table, and run it again:"
74,Wow. That's a huge jump in efficiency! What changed?
74,"As expected, the company_facts step in the query plan has changed and no longer says ALL under type, which means we've eliminated the full table scan. But the most important change is in the rows column, where we see that the value for company_facts has dropped from 189,108 to 1. This means, for each value of permalink, only 158,772 * 1 = 158,772 comparisons need to be performed, which is over 189,108 fewer comparisons total."
74,"Since this is the only really major operation in this query, just adding an index to my PDT made this query faster by a factor of about 189,000."
74,Other Resources
74,There are great resources that can guide you through interpreting and using EXPLAIN results. These general resources about using EXPLAIN for SQL are typically dialect-specific.
74,Here are a few that are very helpful:
74,Oracle's
74,Database Performance Tuning Guide
74,Severalnines' Using Explain to improve SQL Queries (focuses on MySQL)
74,Redshift's EXPLAIN documentation
74,"The great and very general guide on Use the Index, Luke"
74,"Also, consider reading this related Help Center article, Best Practice: Optimize Looker Performance."
74,Facebook
74,Twitter
74,LinkedIn
74,Was this article helpful?
74,1 out of 1 found this helpful
74,Have more questions? Submit a request
74,Return to top
74,Related articles
74,Identifying and Building PDTs for Performance Optimization
74,More powerful data drilling
74,Custom Drilling Using HTML and Link
74,A Simple Explanation of Symmetric Aggregates or 'Why On Earth Does My SQL Look Like That?'
74,Aggregate Awareness using _in_query
74,"Looker Data Sciences, Inc.Privacy | Terms | Cookies"
75,How to optimize a MySQL database for performance - Quora
75,Please enable Javascript and refresh the page to continue
77,Improve database performance with connection pooling - Stack Overflow Blog
77,Help your team find the answers they need to get work done. Stack Overflow for Teams.
77,What is Teams?
77,"Essays, opinions, and advice on the act of computer programming from Stack Overflow."
77,Search for:
77,Latest
77,Newsletter
77,Podcast
77,Company
77,code-for-a-living
77,"October 14, 2020"
77,Improve database performance with connection pooling
77,"We tend to rely on caching solutions to improve database performance. Caching frequently-accessed queries in memory or via a database can optimize write/read performance and reduce network latency, especially for heavy-workload applications, such as gaming services and Q&A portals. But you can further improve performance by pooling users’ connections to a database. Client users need…"
77,Michael Aboagye
77,"We tend to rely on caching solutions to improve database performance. Caching frequently-accessed queries in memory or via a database can optimize write/read performance and reduce network latency, especially for heavy-workload applications, such as gaming services and Q&A portals. But you can further improve performance by pooling users’ connections to a database."
77,"Client users need to create a connection to a web service before they can perform CRUD operations. Most web services are backed by relational database servers such as Postgres or MySQL. With PostgreSQL, each new connection can take up to 1.3MB in memory. In a production environment where we expect to receive thousands or millions of concurrent connections to the backend service, this can quickly exceed your memory resources (or if you have a scalable cloud, it can get very expensive very quickly)."
77,"Because each time a client attempts to access a backend service, it requires OS resources to create, maintain, and close connections to the datastore. This creates a large amount of overhead causing database performance to deteriorate."
77,"Consumers of your service expect fast response times. If that performance deteriorates, it can lead to poor user experiences, revenue losses, and even unscheduled downtime. If you expose your backend service as an API, repeated slowdowns and failures could cause cascading problems and lose you customers."
77,"Instead of opening and closing connections for every request, connection pooling uses a cache of database connections that can be reused when future requests to the database are required. It lets your database scale effectively as the data stored there and the number of clients accessing it grow. Traffic is never constant, so pooling can better manage traffic peaks without causing outages. Your production database shouldn’t be your bottleneck."
77,"In this article, we will explore how we can use connection pooling middleware like pgpool and pgbouncer to reduce overhead and network latency. For illustration purposes, I will use pgpool-II and pgbouncer to explain concepts of connection pooling and compare which one is more effective in pooling connections because some connection poolers can even affect database performance."
77,We will look at how to use pgbench to benchmark Postgres databases since it is the standard tool provided by PostgreSQL.
77,"Different hardware provides different benchmarking results based on the plan you set. For the  tests below, I’m using these specifications."
77,Specs of my test machine:
77,Linode Server: Ubuntu 16 – 64 bit ( Virtual Machine)  Postgres version 9.5Memory: 2GBDatabase size: 800MBStorage: 2GB
77,Also it is important to isolate the Postgres database server from other frameworks like logstash shipper and other servers for collecting performance metrics because most of these components consume more memory and will affect the test results.
77,Creating a pooled connection
77,"Connecting to a backend service is an expensive operation, as it consists of the following steps:"
77,Open a connection to the database using the database driver.Open a TCP socket for CRUD operations Perform CRUD operations over the socket.  Close the connection.Close the socket.
77,"In a production environment where we expect thousands of concurrent open and close connections from clients, doing the above steps for every single connection can cause the database to perform poorly."
77,"We can resolve this problem by pooling connections from clients. Instead of creating a new connection with every request, connection poolers reuse some existing connections. Thus there is no need to perform multiple expensive full database trips by opening and closing connections to backend service. It prevents the overhead of creating a new connection to the database every time there is a request for a database connection with the same properties (i.e name, database, protocol version)."
77,"Pooling middleware like pgbouncer comes with a pool manager. Usually, the connection pool manager maintains a pool of open database connections. You can not pool connections without a pool manager."
77,A pool contains two types of connections:
77,Active connection: In use by the application.Idle connection:  Available for use by the application.
77,"When a new request to access data from the backend service comes in, the pool manager checks if the pool contains any unused connection and returns one if available. If all the connections in the pool are active, then a new connection is created and added to the pool by the pool manager. When the pool reaches its maximum size, all new connections are queued until a connection in the pool becomes available."
77,"Although most databases do not have an in-built connection pooling system, there are middleware solutions that we can use to pool connections from clients."
77,"For a PostgreSQL database server, both pgbouncer and pgpool-II can serve as a pooling interface between a web service and a Postgres database. Both utilities use the same logic to pool connections from clients."
77,"pgpool-II offers more features beyond connection pooling, such as replication, load balancing, and parallel query features."
77,How do you add connection pooling? Is it as simple as installing the utilities?
77,Two ways to integrate a connection pooler
77,There are two ways of implementing connection pooling for PostgreSQL application:
77,As an external service or middleware such as pgbouncer
77,Connection poolers such as pgbouncer and pgpool-II can be used to pool connections from clients to a PostgreSQL database. The connection pooler sits in between the application and the database server. Pgbouncer or pgpool-II can be configured in a way to relay requests from the application to the database server.
77,Client-side libraries such as c3p0
77,There exist libraries such as c3p0 which extend database driver functionality to include connection pooling support.
77,"However, the best way to implement connection pooling for applications is to make use of an external service or middleware since it is easier to set up and manage. In addition external middleware like pgpool2 provides other features such as load balancing apart from pooling connections."
77,"Now let’s take a deeper look at what happens when a backend service connects to a Postgres database, both with and without pooling."
77,Scaling database performance without connection pooling
77,"We do not need a connection pooler to connect to a backend service. We can connect to a Postgres database directly. To examine how long it takes to execute concurrent connections to a database without a connection pooler, we will use pgbench to benchmark connections to the Postgres database."
77,"Pgbench is based on TPC-B. TPC-B measures throughput in terms of how many transactions per second a system can perform. Pgbench executes five SELECT, INSERT, and UPDATE commands per transaction."
77,"Based on TPC-B-like transactions, pgbench runs the same sequence of SQL commands repeatedly in multiple concurrent database sessions and calculates the average transaction rate."
77,"Before we run pgbench, we need to initialize it with the following command to create the pgbench_history, pgbench_branches, pgbench_tellers, and pgbench_accounts tables. Pgbench uses the following tables to run transactions for benchmarking."
77,pgbench  -i  -s 50  database_name
77,"Afterward, I executed the command below to test the database with 150 clients"
77,pgbench  -c 10  -j 2  -t  10000  database_name
77,"As you see, in our initial baseline test, I instructed pgbench to execute with ten different client sessions. Each client session will execute 10,000 transactions."
77,"From these results, it seems our initial baseline test is 486 transactions per second."
77,"Let’s see how we can make use of connection poolers like pgbouncer and pgpool to increase transaction throughput and avoid a ‘Sorry!, too many clients already’ error."
77,Scaling database performance with pgbouncer
77,Let’s look at how we can use pgbouncer to increase transaction throughput.
77,"Pgbouncer can be installed on almost all Linux distributions. You can check here how to set up pgbouncer. Alternatively, you can install pgbouncer using package managers like apt-get or yum."
77,"If you find it difficult to authenticate clients with pgbouncer, you can check GitHub on how to do so."
77,Pgbouncer comes with three types of pooling:
77,"Session pooling: One of the connections in the pool is assigned to a client until the timeout is reached.  Transaction pooling: Similar to session polling, it gets a connection from the pool. It keeps it until the transaction is done. If the same client wants to run another transaction, it has to wait until it gets another transaction assigned to it. Statement pooling: Connection is returned to the pool as soon as the first query is completed."
77,"We will make use of the transaction pooling mode. Inside the pgbouncer.ini file, I modified the following parameter:"
77,max_client_conn = 100
77,The max_client_conn parameter defines how many client connections to pgbouncer (instead of Postgres) are allowed.
77,default_pool_size = 25
77,The default_pool_size parameter defines how many server connections to allow per user/database pair.
77,reserve_pool_size = 5
77,The reserve_pool_size parameter defines how many additional connections are allowed to the pool.
77,As in the previous test I executed pgbench with ten different client sessions. Each client executes 1000 transactions as shown below.
77,pgbench  -c 10  -p -j 2  -t 1000 database_name
77,"As you see, transaction throughput increased from 486 transactions per second to 566 transactions per second. With the help of pgbouncer, transaction throughput improved by approximately 60%."
77,Now let’s see how we can increase transaction throughput with pgpool-II since it comes with connection pooling features.
77,"Unlike pgbouncer, pgpool-II offers features beyond connection pooling. The documentation provides detailed information about pgpool-II features and how to set it up from source or via a package manager"
77,I changed the following parameters in the pgpool.conf file to make it route clients connections from pgpool2 to Postgres database server.
77,connection_cache  = on
77,listen_addresses  = ‘postgres_database_name’’
77,port  = 5432
77,Setting the connection_cache parameter to on activates pgpool2 pooling capability.
77,"Like the previous test, pgbench executed ten different client sessions. Each client executes 1000 transactions to the Postgres database server. Thus we expect a total of 10,000 transactions from all clients."
77,gbench  -p 9999  -c  10  -C  -t 1000  postgres_database
77,"In the same way we increased transaction throughput with pgbouncer, it seems pgpool2 also increased transaction throughput by 75% as compared to the initial test."
77,Pgbouncer implements connection pooling ‘out of the box’ without the need to fine-tune parameters while pgpool2 allows you to fine-tune parameters to enhance connection pooling.
77,Choosing a connection pooler: pgpool-II or pgbouncer?
77,"There are several factors to consider when choosing a connection pooler to use. Although pgbouncer and pgpool-II are great solutions for connection pooling, each tool has its strengths and weaknesses."
77,Memory/resource consumption
77,"If you are interested in a lightweight connection pooler for your backend service, then pgbouncer is the right tool for you. Unlike pgpool-II, which by default allows 32 child processes to be forked, pgbouncer uses only one process. Thus pgbouncer consumes less memory than pgpool2."
77,Streaming Replication
77,"Apart from pooling connections, you can also manage your Postgres cluster with streaming replication using pgpool-II.  Streaming replication copies data from a primary node to a secondary node. Pgpool-II supports Postgres streaming replication, while pgbouncer does not. It is the best way to achieve high availability and prevent data loss."
77,Centralized password management
77,"In a production environment where you expect many clients/applications to connect to the database through a connection pooler concurrently, it is necessary to use a centralized password management system to manage clients’ credentials."
77,You can make use of auth_query in pgbouncer to load clients’ credentials from the database instead of storing clients’ credentials in a userlist.txt file and comparing credentials from the connection string against the userlist.txt file.
77,Load balancing and high availability
77,"Finally, if you want to add load balancing and high availability to your pooled connections, then pgpool2 is the right tool to use. pgpool2 supports Postgres high availability through the in-built watchdog processes. This pgpool2 sub-process monitors the health of pgpool2 nodes participating in the watchdog cluster as well as coordinating between multiple pgpool2 nodes."
77,Conclusion
77,"Database performance can be improved beyond connection pooling. Replication, load balancing, and in-memory caching can contribute to efficient database performance."
77,"If a web service is designed to make a lot of read and write queries to a database, then you have multiple instances of a Postgres database in place to take care of write queries from clients through a load balancer such as pgpool-II while in-memory caching can be used to optimize read queries."
77,"Despite the pgpool-II ability to function as a loader balancer and connection pooler, pgbouncer is the preferred middleware solution for connection pooling because it is easy to set up, not too difficult to manage, and primarily serves as a connection pooler without any other functions."
77,"Tags: connection pooling, databases, pgbouncer, postgreSQL"
77,"The Stack Overflow Podcast is a weekly conversation about working in software development, learning to code, and the art and culture of computer programming."
77,Related
77,newsletter
77,"October 23, 2020"
77,The Overflow #44: Machine learning in production
77,"Welcome to ISSUE #44 of the Overflow! This newsletter is by developers, for developers, written and curated by the Stack Overflow team and Cassidy Williams at Netlify. This week, get in the fast lane and start pooling your database connections, make a CPU out of electronic components drawn by hand on paper, and learn to toggle multiple property…"
77,Medi Madelen Gwosdz
77,Content Strategist
77,code-for-a-living
77,"January 14, 2021"
77,Have the tables turned on NoSQL?
77,"NoSQL was the next big thing in system architecture in 2011, but overall interest in it has plateaued recently. What is NoSQL, what does it have to do with modern development, and is it worth implementing in your project?"
77,John Biggs and Ryan Donovan
77,code-for-a-living
77,"March 3, 2021"
77,Best practices can slow your application down
77,"In order to get the most performant site possible when building the codebase for our public Stack Overflow site, we didn’t always follow best practices."
77,Roberta Arcoverde and Ryan Donovan
77,code-for-a-living
77,"February 24, 2021"
77,What I wish I had known about single page applications
77,"Single page apps are all the rage today, but they don't always operate the same as traditional web pages."
77,Michael Pratt
77,11 Comments
77,Iwouldliketonotprovidemyname says:
77,14 Oct 20 at 11:20
77,"As you see, transaction throughput increased from 486 transactions per second to 566 transactions per second. With the help of pgbouncer, transaction throughput improved by approximately 60%."
77,That 60% is a bit huge. The increase of 80 TPS is more like 16% of the initial 486 TPS. The 75% increase for pgpool-II is also a bit large.
77,Reply
77,Tien Do says:
77,21 Oct 20 at 6:35
77,"Yeah, how is it 60% and 75%?"
77,Reply
77,CanadianLuke says:
77,22 Oct 20 at 6:55
77,A week with no reply to simple math… Not looking good…
77,Reply
77,Travis says:
77,14 Oct 20 at 3:49
77,"It is certainly an interesting technique, although there are a whole host of unintended consequences associated with this approach that would be well to mention. Pooling transactions to memory prior to pushing them to the database has very serious implications with regards to data integrity, as one main example. This approach also will require a very large amount of server memory to be used; in instances where memory is shared across multiple nodes, this can be problematic if there is a node failure. So, while it may be the case that more transactions per second occur, the risk seems to outweigh any gains."
77,Reply
77,Michael Aboagye says:
77,19 Oct 20 at 7:23
77,"@Travis, please I referred to pooling connection in this article."
77,"But do you know pgpool supports postgres stream replication? In addition, the presence of transaction log ensures data integrity is maintained."
77,Even without in the absence
77,"of pgpool cluster, postgres supports replication concepts such as synchronous and asynchronous replication to prevent data loss."
77,Reply
77,Galletto says:
77,22 Oct 20 at 9:54
77,"Maybe I’m missing something entirely, but I thought connection pooling has been the default in .NET for many years…"
77,Correct?
77,Reply
77,Nilesh says:
77,16 Oct 20 at 11:19
77,@Travis — The article talks about pooling connections but mentions no such thing as pooling transactions in memory .
77,I understand the static memory footprint that postgres has on the server would increase but I can’t see how this would lead to data integrity issues. Am I missing anything ?
77,Reply
77,Michael Aboagye says:
77,19 Oct 20 at 7:24
77,"Thanks for your comments, Nilesh."
77,Reply
77,Jeff Dafoe says:
77,22 Oct 20 at 5:21
77,"One thing to be aware of with connection pooling, particularly under PG, is that a reused pool connection may not be in the same initial state as a brand new connection from the backend. Session variables persist across shared connections, data may not be cleared from temp tables, and some types of errors are not cleared until the backend is recycled. It’s important that the code that is establishing the connection be written with this in mind, it must perform initialization that would not be necessary if the backend were fresh and it should also test the connection to make sure it can actually be queried from."
77,Reply
77,Matthew E says:
77,26 Oct 20 at 3:23
77,"right on, Nilesh and Dafoe."
77,Others:
77,#include
77,Reply
77,Emmanuel Casas says:
77,21 Jan 21 at 5:48
77,"Im concern about the fact PGBENCH is based on http://www.tpc.org/tpcb/ which is obsolete, maybe im missing something here, any thoughts about this guys ? Is there a better way to measure the TPS in postgresql ?"
77,Reply
77,Leave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment Name *
77,Email *
77,Website
77,"Save my name, email, and website in this browser for the next time I comment."
77,This site uses Akismet to reduce spam. Learn how your comment data is processed.
77,© 2021 All Rights Reserved.
77,Proudly powered by Wordpress
77,Stack Overflow
77,Questions
77,Jobs
77,Developer Jobs Directory
77,Salary Calculator
77,Products
77,Teams
77,Talent
77,Advertising
77,Enterprise
77,Company
77,About
77,Press
77,Work Here
77,Legal
77,Privacy Policy
77,Contact Us
77,Channels
77,Podcast
77,Newsletter
77,Facebook
77,Twitter
77,LinkedIn
77,Instagram
78,The Ultimate WordPress Speed Optimization Guide
78,Skip to primary navigation Skip to main content Skip to footerWPJohnnyWordPress Performance Guides and ReviewsStart a WordPress site
78,Hosting
78,Themes
78,Plugins
78,Blogging
78,Marketing
78,search
78,The Ultimate WordPress Speed Optimization Guide
78,"WordPress hosting Apr 22, 2020 by Johnny 46 Comments"
78,130+ performance tips to overcome your WordPress speed addiction.
78,"Are you a speed addict?Are you worried about poor SEO rankings?Do you want instant loads for your website, EVEN ON MOBILE?!Do you want to save money on server costs?Do you want to bedazzle your clients? Or are you just some DIY-er looking for new tactics?"
78,"Whatever the reason for your addiction, you’ve come to the right place. I’m not going to judge you. Or call you OCD about speed. I’m gonna show you the pro tactics that I cleverly thought up in a time lonnnngggggg before WordPress speed-up was a trend. (I’ll also warn you about dumb ones that only waste time.)"
78,I promise this is the toughest speed-up guide you’ve ever read.
78,Edits:
78,"AUG 15, 2017 – started writing. Gave up because it was too long.APR 22, 2020 – finished during Corona downtime.JUN 3, 2020 – more tips and clarifications.JUN 24, 2020 – added network firewall, cache prebuild, and form lazy load."
78,Table of contents:
78,My journey through WordPress speed addiction
78,Webhosting optimizationTheme optimizationPlugin optimizationImage optimizationFont optimizationCaching optimizationLocal asset optimizationExternal asset optimizationSecurity optimizationBad optimization (tactics)
78,My journey through WordPress speed addiction
78,"My addiction for faster load times began in 2007, with Joomla (not WordPress)."
78,"I had a few successful sites growing out of control. Many plugins installed, ads, hundreds of comments on many posts (some had thousands). Slow frontend and backend. It was a pain to update as I’m possibly the most impatient person in the world. I couldn’t stand it…being a boxer, I could perceive even 100ms difference in load times."
78,So what did I do?
78,I moved to WordPress around 2008. Switched themes like 20 times. Then switched my webhosting 3 times. Then got a VPS plan. Then I got 5 more VPS because the first one sucked. Then I taught myself how to manage servers because I got tired of admins not being available when my site went down.
78,"When I finally got my site stable, then came the theme rebuild. I rebuilt my theme from scratch several times. Also enlisted some developers and coders with much more experience than myself. Then I argued with some of them because I didn’t trust their logic. Sometimes only one of us was right, other times we were both wrong."
78,"Load times improved but my addiction only got worse. The faster things got, the more used to it I became. The moment I broke the mythical 1-sec barrier, it only felt like I reached the bottom of a new mountain to climb. Now I wanted a sub-500ms site!"
78,"Every developer working with me argued the same thing,"
78,Why do you want it faster? Your site is already fast!
78,"I knew my site was already faster than others. What I couldn’t understand was why a custom-coded site still needed even half a second to load. Anybody working with me needs to understand they’re dealing with an addict, not a hobbyist. Anyway, I found a developer friend who was every bit as willing to explore with me. He went the code route. I went the server route."
78,"He rewrote our WP_Query from scratch, completely refactored all our CSS. Rebuilt all our custom work to be mobile-first. And when I say from scratch…I mean from scratch. All the CSS/JS optimization you see nowadays, but done by hand. He integrated many of our plugins directly into the theme or rewrote them entirely."
78,"In the meanwhile, I was screwing with servers. I didn’t like servers but I was the more Linux-capable one between us two. So I tested various stacks. Screwed around with configurations. Turning things on and off. Tried different security systems. Lucky for me, the server world was already quite evolved in performance-tuning. With the help of a few senior admins, I was ‘up to speed’ within 1-2 years."
78,We were way ahead of the times.
78,"I doubled back to WordPress this time. Now as a speed architect alongside my developer friend. We got even more aggressive, inventing many tactics that are either obsolete today or more conveniently-implemented via plugins. It’s amazing how much we accomplished in a time when WordPress and web servers weren’t so user-friendly. Anything we wanted to do, we had to do it manually or invent it ourselves."
78,"Today, it seems all WordPress users want more speed. It’s actually a trend now. Hahaha. It’s not only Johnny “the speed addict” harassing developers. There’s speed tests. And performance plugins like caching, combining, minification, image compression, etc. It seems even the average user is trying to remove bloat and provide the best user experience possible."
78,Speed optimization is more important than ever now…
78,"While we do have faster internet, websites are more bloated than ever. Themes and plugins keep getting fancier and loaded with more features. The majority of internet visitors are browsing from mobile (with less bandwidth and processing power). Google now considers your speed as a ranking signal. Facebook ads don’t convert as well when your site is slow (users just click “BACK” and keep scrolling). Faster websites also make more money! No surprise, right?"
78,"Faster websites make more money, rank better, and improve overall user experience!"
78,Are you a WordPress speed addict (with no life just like me)?
78,"If yes, then you’ve found my black book of speed-up secrets. If you wanted just simple tips, please choose an easier speed optimization guide to follow. This guide will make you break up with your developer and pick fights with your server admin. I want you to learn, but also have fun. I’ve been doing it for many years and come from a much harder time when not so many tools were available."
78,"Everything I share here is EXACTLY what I recommend to all clients. Even the $20k ones. Even the developers and agencies who consulted with me to train them and their team. They all get exactly what I put below. Instead of just bullshit generic advice, I’ve listed many sneaky ones as well. Some of them are extremely technical. If you don’t understand my brief explanation for it…you are probably not at the level to attempt it! Follow what you can but hire if you need. Test carefully."
78,"I’ve gotten many fancy sites – nice graphics, over 1mb size, 100+ requests, to load instantly and well under 500ms. I’m sure you can, too. I will give you almost every WordPress speed-up trick that I know! 😉"
78,Let’s do it!
78,SPEED OPTIMIZATION RULE #1: Always optimize for HUMAN VISITORS! (not speed tests)
78,Optimize for humans!
78,"Why Google Pagespeed, Pingdom, and GTmetrix scores don’t matterHow to optimize for Google Pagespeed, Pingdom, and GTmetrixKeep in mind that mobile is slower, making speed differences more noticeable."
78,SKILL:
78,BEGINNER – can Google and follow instructions.INTERMEDIATE – working as WordPress contractor.ADVANCED – programmer or server-admin.
78,IMPACT:
78,LOW – maybe 100-200ms difference. Possibly unnoticeable.MEDIUM – around 500ms difference.HIGH – 1 second difference or more.
78,1. Webhosting optimization
78,"Your webhosting speed determines how fast it can process code, and how many visitors it can handle. Compare your website to a car. To make a car go faster, you either A) get a stronger engine and/or B) lighten the weight. For websites, the web-server is the “engine” and the code is the”weight”."
78,"The goal is to improve our web-server “engine” while decreasing code “weight”, ok?"
78,Changing your webhosting is one of the easiest ways to improve speed. Those of you on cheap $5/month shared webhosting will benefit the most from moving to a managed hosting service or even your own VPS. The difference will be night and day without any site changes. Moving from managed hosting to an optimized VPS or dedicated “bare metal” server will be another night-and-day jump.
78,"The difference isn’t only speed but also a matter of cost (savings). A fast server can handle more visitors than a slow one. If your server can handle double the traffic, theoretically the bill can be twice as cheap. Not a big deal for a small site but what about a huge ecommerce site with a $1k/month server bill? 50% cost reduction sounds mighty attractive!"
78,"1. Choose nearby datacenter location (BEG, LOW-MED)"
78,"Obviously, you should pick a server location that’s closest to your visitors. Ideally, you don’t want your DNS ping time more than 100ms from the server to your visitor’s computer. There are many implications depending on your needs."
78,"Local businesses should get a server as close to their visitors as possible. Keep it within 100ms or less, within 50ms is better. Check ping times with WonderNetwork.The USA is about 80ms from coast to coast. Canada and Mexico are close enough as well.All of Western Europe is only 40-50ms, very close. Asia is within 80ms between most countries.India/Pakistan, Australia/NZ, Africa are somewhat isolated. Local businesses there need a local datacenter. Even Singapore to Australia is borderline “far” by DNS standards (~150ms).South America can be unreliable infrastructure. For that reason, many companies in Central/South America still use US-based datacenters like in California, Texas, or Florida (Miami). Luckily, Google Cloud opened up a datacenter in Sao Paolo, Brazil. Reliable but pricey. If you have worldwide traffic (including Asia/Pacific) and no particular core region, I like USA west coast as perfect location for fast traffic to Europe and Asia.If you have only USA & Europe traffic and no particular core region, I like USA east coast for fast traffic to Europe."
78,It’s also good to have a webhosting company on the same timezone as your core audience. That way they can (quickly) support or troubleshoot issues when most of your visitors are awake.
78,Those of you thinking a CDN can make up for far server location (that’s not necessarily true!)
78,Those of you hunting for dedicated nodes…the best is TIER-4 datacenter with four 9’s (99.9999% uptime guarantee). But good luck getting those guaranteed!
78,Uptime calculator (99.9% uptime means 43min downtime per month)Nearest.host – cool site showing nearby server companies.
78,"2. Choose the right website hosting service (BEG, HIGH)"
78,"Shared hosting ($5-30/month) – fine for small sites and low traffic up to 100k hits/month. No access to server configurations. Safe choice is SiteGround, Kinsta, or WP Engine.VPS/cloud hosting ($30-300/month) – great for medium sites and traffic up to 30 million hits/month. Safe choice is Cloudways or Gridpane, or try a fully managed VPS service.Dedicated (bare metal) server ($200/month & up) – great for large sites with TONS of traffic. Safe choice is LiquidWeb.Read my webhosting review of many companies.I’m also biased for my own webhosting service JohnnyVPS."
78,Buy the best that you can comfortably afford. A small website doesn’t need much power but it’s still noticeable when you get a better server and appreciated more than you think. Think of a new phone that opens apps just a fraction of a second quicker. You really can feel the difference and it improves user experience tremendously.
78,"Shared webhosting is usually slow because they stuff hundreds of customers/websites onto the same server (maximize profits). This increases slowdowns, unexpected crashes or server restarts, security attacks, and your email IP getting marked as spam.Shared hosting environments are also slow because they load many scripts/modules to maximize compatibility for as many users as possible. And without dedicated resources, your visitors end up waiting in line while the server is busy handling other websites first.VPS/Dedicated servers are faster because there’s more resources available per account and your resources are serving only your websites. You have more control over your environment, can configure it for your needs. VPS/dedicated can be costly or difficult to manage for regular users. There are cloud-panel services to help manage it and also fully-managed services where they take care of everything for you.Those unable to handle technical responsibilities of VPS can go for “premium shared hosting” like Kinsta or WP Engine. They don’t crowd the server as much"
78,but the performance (while better than regular shared hosting) will be still be far behind a VPS.
78,"3. Choose a high performance web server (INT-ADV, HIGH)"
78,"Use any web server software but Apache. The best is NGINX or LiteSpeed, or highly-optimized Apache (rare to find). The higher your traffic, the more noticeable the difference."
78,"NGINX shines at simple sites. Just set it and go. Not much settings to optimize. But once you have a complicated site, NGINX is a mixed bag. Some NGINX features aren’t easy to configure. If you have a server-admin to fine-tune, it’s great but many people don’t.LiteSpeed has more easy-accessible features than NGINX. Like when you need some things cached but not others, or dealing with server-level redirects via htaccess. LiteSpeed also has a WordPress cache plugin which NGINX doesn’t. That’s a HUGE advantage. (I personally prefer LiteSpeed.)OpenLiteSpeed is the free community version of LiteSpeed. It’s a great alternative for those wanting the free price of NGINX but the powerful LiteSpeed cache plugin.Some webhosts have the Apache+NGINX hybrid stack. I feel those are outdated now and makes for unnecessarily slower/heavier stack.If using Apache, MPM events is best (compared to worker or prefork).Keep your webserver updated. Later versions can speed up certain protocols and processes noticeably."
78,"4. Web server configuration (ADV, MED-HIGH)"
78,"Most web servers come with safe/functional configurations right off the bat. Adequate for the average small site with little traffic. It’s when you get more traffic and more security attacks, or have more demanding apps that fine-tuning the configurations makes a big difference."
78,"Timeout – 30 to 60 seconds is a safe start. You can increase up to 600 or beyond if needed for long processes (import, export, backups). Keep in mind that allows poorly-coded processes or hack exploits to run out your server resources.# of child processes allowed – depends on the server environment. Default should be fine.Concurrent connections allowed – anywhere from 1-20k. Higher is not necessarily better!Keep alive – on, off, or LiteSpeed’s “smart keep-alive”. I think “on” is safer. If you have LiteSpeed, the smart keep-alive is awesome!Keep alive timeout – 3-5 seconds is a safe start. Increase if needed."
78,"How many threads, body/buffer size, workers, clients, etc….all that you can look up online. It depends on your server size and use scenario. Jump on forums and ask around or have a sys-admin configure for you. Keep in mind different admins have their own ways of configuring."
78,The most important distinction for me is to decide whether this server should be set aggressive or conservative:
78,AGGRESSIVE configuration – gives every site as much resources as possible. Good for low-tenant or dedicated servers.CONSERVATIVE configuration – gives every site as little resources as possible. Good for high-tenant or shared servers.
78,"5. Disable unused services (INT, HIGH)"
78,"Many servers are automatically set up with all features running to make things easy for you. But they’re just like brand new computers with pre-installed software. Get rid of the ones you don’t use. Even if they don’t use much memory, they can still be bombarded by hackers and that eats resources."
78,"DNS – disable if you’re using external DNS service. (Cloudflare, DNSME, etc.)Email – disable if you’re using 3rd-party email. (G-Suite, MXroute, etc.)FTP/SFTP – disable if not using.Memcache/Redis – disable if you don’t use it.Other services – Varnish, Elastipress, etc."
78,"If you want to be OCD, scan your system for all listening ports and services."
78,"6. Remove unused server modules (ADV, LOW)"
78,Want to be even more OCD? Disable every single module not used by the server. Some of them are junk unused server stuff; others are unused Linux distro stuff. Old school Apache-compatible stacks or unoptimized control panels tend to have many unused modules enabled by default (while also not enabling ones you might need).
78,"Read documentation and check online before blindly removing or replacing them. The danger is you disable things you need (or worse, one that improves performance). You should make a list of disabled services/modules to reference later or give to a contractor when troubleshooting."
78,"7. Use the latest PHP version (INT-ADV, HIGH)"
78,The PHP version alone makes a HUGE difference.
78,"Use the latest PHP version possible! (Easily-configured from your webhosting control panel.)For example, PHP 7.0 is 3 times faster than PHP 5.6. Even PHP 7.3 is 10% faster than PHP 7.2. At the time of this writing, PHP 7.4 is available.Be wary of any webhosts still using old PHP!"
78,"Keeping your website PHP version updated is not only for speed but also security. The only issue is some themes or plugins may not be compatible with the latest PHP version. You’ll know because your site doesn’t work right, or looks weird. So test carefully and keep themes/plugins updated, which helps them stay compatible with the latest PHP."
78,"8. Recommended php.ini configurations (INT, MED):"
78,"Most of you (on shared hosting) won’t even have access to these settings or know how to set them. But nonetheless, here are my recommendations."
78,"max_execution_time – lower (30-60 sec) is better to prevent resources hogs from lagging out the server. But you may need higher execution times for long processes like imports, exports, backups.max_input_time – lower (60 sec) is better. Increase only if you’re trying to import something that takes forever.max_input_vars – set to “1000”, unless some plugins need higher.memory_limit – try “256M” to be safe. Raise if you have heavy plugins. I like to set lower so I’m notified immediately when there are memory hogs. The “error_log” will tell you if you need more.zlib.output_compression – may or may not help. I leave it off."
78,"9. Use an updated MySQL fork version (INT-ADV, LOW)"
78,"Most people only know of MySQL, which is now acquired/owned by Oracle. There’s also MariaDB (a fork of MySQL by its original creators) and Percona, and also others."
78,"MySQL 8 is much better than MySQL 5.7.But it’s better if you can use MariaDB over MySQL. Community-friendly and better performance than vanilla MySQL 5.7.Use the latest MariaDB version that you can. Whatever you do, just don’t use MySQL 5.7."
78,"What about Percona? What about the other 3rd-party MySQL-compatible forks? For most sites, it makes little difference if any. Don’t forget to backup your database before changing or upgrading MySQL."
78,MySQL vs MariaDB vs Percona
78,"10. Convert MySQL tables from MyISAM to InnoDB (BEG, LOW):"
78,"Make sure your tables are set to InnoDB instead of MyISAM.InnoDB is newer and regarded as being better overall (faster, safer).MyISAM can be faster in some scenarios (when mostly read-only).You can convert manually in phpMyAdmin or use a plugin (Servebolt Optimizer or LiteSpeed Cache). Can delete the plugin afterwards if you don’t need it."
78,"11. Tuning MySQL configurations (ADV, LOW):"
78,"Usually not required (or noticeably-beneficial) for the average site but can help tremendously for large sites with high traffic and varying query lengths. You can run MySQLTuner for general recommendations or ask around the sys-admin community to see what everyone else uses.Buffer size, packet size, cache, connections, cache, stack, etc…are all among the general things to tune.Simple Linode guide."
78,"When trying out random configurations online or copying somebody else’s, please make sure their environment is similar to yours. The main distinctions are:"
78,"server size, resources availablehow many clients/sites on serverhow many end users on serverhow much traffichow big are the siteswhat kind of read/write behavior"
78,It’s important to know whether their settings are for efficiency (high-tenant webhost) or aggressive performance (low-tenant webserver).
78,"12. Server full-page caching (ADV, HIGH)"
78,Full-page caching can help speed-up any website. But caching directly from the server is much more powerful and resource-efficient than PHP/application-level caching done through a plugin.
78,"Some Apache or NGINX servers use Varnish – ugghhh, outdated. Don’t use Varnish proxy. Just upgrade to pure-LiteSpeed or pure-NGINX stack.LiteSpeed servers can use LiteSpeed cache – powerful, many features, and comes with a handy WordPress cache plugin (called “LiteSpeed Cache”).NGINX servers can use FastCGI – great, super fast! While there’s no official NGINX cache plugin for WordPress, there are various “NGINX helper” plugins to facilitate basic cache functions (like purging)."
78,"To be safe, you should disable caching on pages with forms, carts, or checkouts. Private pages (for logged-in users) CAN be cached but don’t mess with that unless you have that much private traffic and ready to spend hours configuring private cache."
78,"You can only enable server-level caching if you own or have access to the server. Otherwise, your webhost decides what caching options you have."
78,Shared hosting usually allows all caching plugins.Managed hosting usually limits you to only their proprietary one.
78,"13. Memory object-caching (ADV, LOW-HIGH)"
78,"Object-caching caches only the database queries instead of the entire page html. This technically makes it “slower” than full-page caching (since you’re not caching the entire page) but useful for speeding up dynamic pages or private pages (logged-in users, admin backend) that can’t be static-cached."
78,"Any site with lots of constantly-refreshed data on frontend, or lots of numbers and reports on backend…would stand to benefit from object caching. Mostly-static sites or low-traffic sites would not benefit from object-caching at all; don’t use it on them…it can make them slower!"
78,"Redis is the gold standard in object caching now. It’s superior to the older memcache in almost every way.Memcache is only used in rare situations where Redis doesn’t work or is slower.If your data doesn’t change much, you can set longer object caching times (e.g. 60 mins and up). Longer times means fewer database queries.Otherwise, stick with the default 5-10 mins to be safe…unless you don’t mind users seeing stale data.Object caching can be managed by WordPress plugins. Most ideal if you have one cache plugin to manage both full-page caching and object caching."
78,You can get ~25% faster object caching by using a Unix Socket
78,"UNIX sockets are run from a lower-level layer on the OSI networking model and therefore faster than standard TCP sockets.Redis and Memcache UNIX socket configuration guides for CentOS.Redis and Memcache UNIX socket configuration guides for Ubuntu.Note: with UNIX socket enabled, only one server user account (and presumably all sites by that user) can use object caching. So you can’t use this if you plan to have multiple server users deploying object cache."
78,Some background on memory-caching…
78,"Memory-caching is the gold standard in caching, because cache runs faster from memory than than from disk. The issue is we have limited amounts of memory (most of it already used for applications) so we can’t store the entire site cache in there. It matters less nowadays anyway since server disks are so much faster now (thanks to SSD technology)."
78,"Sure memory is more abundant now too but then again, applications are bigger. You may have heard of others loading their entire site into memory…some using the cache route, others by mounting their WordPress directory into memory. It works great if your site is small enough but for most people: your memory is only big enough to store database queries, anything else you want to cache will be stored on your disk."
78,"14. Use the latest HTTP protocol (BEG, HIGH)"
78,HTTP/2 loads browser requests so much faster than HTTP/1 (thanks to parallelization). It feels like 3 times faster to me.
78,"You should be using HTTP/2 or even HTTP/3 (recently released). Avoid older web servers still on the archaic HTTP/1.Check your site for HTTP/2 and HTTP/3.Using HTTP/2 requires HTTPS/SSL. If your site isn’t in HTTPS, do it now!"
78,"15. Content encoding (INT, HIGH)"
78,"GZIP is so 2016. Every web-server should have BROTLI compression nowadays. It’s superior to GZIP (produces smaller files AND in less time). But shockingly, BROTLI still isn’t available on all web-servers."
78,"If using BROTLI – set static compression to 4.If using GZIP – set dynamic compression to 1, static compression to 6."
78,"You can push static compression levels higher if your CPU is strong (or low-usage server) and/or your static content is cached for a long time (long expiry times). If you’re using a CDN or Cloudflare, make sure you enable BROTLI compression there as well."
78,"16. Control-panel (INT, HIGH)"
78,"This issue matters only for VPS users. Control panels used to be critiqued for the initial weight they put on the server. That’s because control panels were originally designed for large dedicated servers, but have since been optimized for smaller VPS. While it’s true that having no control panel is lighter than having one, it makes day-to-day tasks harder. Their footprint is now negligible considering how useful panels can be."
78,The best performing control panel is one that fits your needs.
78,Allows you to pick the web server of your choice – NGINX or LiteSpeed.Easily configure redirects at server level – instead of slower PHP-level redirection plugin.Easily configure granular caching rules – choosing what and what not to cache.Easy to manage – for you or your sys-admin.Can cage users – preventing resource-hogs on high-tenant servers.Secure against hacks – as hacking attempts eat many resources.Easy to use – for you or your clients.
78,"17. Use external DNS service (INT, LOW)"
78,Lower DNS latency (small benefit)Easy to update DNS (convenience)
78,Will using an external DNS like Cloudflare or DNS Made Easy make a world of difference in terms of webhosting speed? I think it improves lookup time but not so noticeable unless your previous DNS server was a piece of junk by cheap webhost.
78,"The main benefit for me is how quickly I can redirect things. Suppose you get hacked and need to redirect through a security proxy. Or maybe you’re switching certain aspects of your site to another server. In moments like this, having a DNS service is so convenient. You can switch things over with very little downtime, and even switch them back quickly if there’s an issue."
78,"DNS services may seem like an extra hassle to setup, but once in place they allow you to integrate new services and mitigate performance issues so much faster."
78,"18. Run WP-cron from your server (BEG, MED)"
78,"Many WordPress tasks need a trigger to function. Such as sending out system emails, run backups, release scheduled posts. By default, WordPress uses a function called WP-Cron (conveniently located at yourdomain.com/wp-cron.php). It works by checking (and executing) for any pending tasks any time someone visits the website. It’s great for small sites, but terrible if you have tons of traffic (triggering many unnecessary cron-checks). Also an obvious DDOS vulnerability."
78,The sensible thing is to disable WP-cron and use a real cron job whether from your server or an external cron service:
78,Disable WP-Cron and use real CRON JOB – WPJohnnyThe nightmare that is wp-cron.php – The cPanel GuyHow to Disable WP-Cron (wp-cron.php) for Faster Performance – KinstaEasyCron (3rd-party service)
78,Some cron jobs visit the website directly. Others go through the Linux directory. Use whichever one works. I think a 5-minute interval is good.
78,"19. Caging users & resource limits (ADV, MED-HIGH)"
78,"Do you have many sites or clients on the same machine? Are you unable to police them all effectively? If you’re losing control of your tenants, it’s probably time to limit their resources. I’ll start you off with a few tactics below. You look up the rest."
78,"Cloudlinux & CageFS – limit CPU and memory usageServer-wide bans on cache-prebuilding, broken-link checkers, and other resource hogs.Decrease php execution time.Global configs blocking the usual offenders, bots, etc."
78,"Worst-case scenario, just split off some clients onto another server. Split them up by size, space usage, traffic amount, whatever you want."
78,"20. Tracking down resource hogs (ADV, MED-HIGH)"
78,"We often run into slow server issues with no obvious clue of where to look. Today, it’s this client. Tomorrow, it’s that client. It seems any site can be the culprit on any given day. When you have so many clients, and none of them can afford switching plugins on and off, it is really really difficult to track down the problem."
78,Here are some ideas:
78,"Check server logs – are you being hacked? Are there excessive requests?Check server monitors – which users are hogging the CPU, memory, and bandwidth?Once you know which site it is…check WordPress error log. Run Query Monitor.Of course, it might not even happen all the time. You have to track down what users or processes were doing when the slowdown happened."
78,"Sometimes you’ll need more of a developer mentality. What plugin was updated last? Any new themes or plugins that were custom-coded? (Check for memory-exhaustive commands.) Yes, you can use applications monitors like New Relic but for me, it’s overkill. The trickiest problems are when it seems like every site is the problem. Or also when the server load is low but the sites are still slow. Good luck!"
78,2. Theme optimization
78,"Most slow sites that I see can attribute up to 50% of their slowness to the theme alone. Slow themes load way too much junk, offering every fancy effect for every user. Imagine a truck with every tool in the trunk, vs loading only the tools you use. Sure…many themes advertise themselves as being “lightweight” yet still have way more processing than necessary. The worst is when their excessive CSS and JS conflict with other plugins (ecommerce, multi-lingual, sliders, caching, etc)."
78,"Ideally you’d have a theme that focuses on the design and nothing else. It loads styles ASAP so content can quickly paint down the browser. No stupid libraries for non-essential items, or endless function checks for user-selected options."
78,"And you really don’t want a theme handling heavy functions better left to plugins. Sure, these themes with endless “extra features” seem like a convenient 1-stop shop…until you realize they don’t do as good a job as specialized plugins AND cause conflicts with other plugins."
78,"21. Use a well-coded WordPress theme or framework (BEG, HIGH)"
78,"The lightest and best-coded WordPress theme is the one custom-coded specifically for your use. It has only the features you need and nothing else. No extra CSS/JS loaded for unused options. If coding from scratch, just be careful how you make database queries."
78,"I like Genesis as the base framework for a custom-coded theme.I like GeneratePress as the base framework for DIY (non-coder) theme.Oxygen Builder is a good visual builder, but meant for developers than DIY.Underscores, Beans, or Roots…are also ok if you know what you’re doing.For custom layouts that need to be edited easily, use or make a Gutenberg block. For all else, make it static. 😉"
78,"Not everyone can afford a (quality) custom-coded theme. If you’re buying an off-the-shelf theme, I recommend the following:"
78,"GeneratePress – using prebuilt styles.Artisan Themes – beautiful styles.Stay away from Theme Forest, Envato, Code Canyon.You can also read my theme review page."
78,Thinking of any themes I didn’t mention? Check out their support channels and Facebook group. What’s their level of support? What type of users are in their community? (Beginner or pro?)
78,"22. Tips when custom-coded themes (ADV, HIGH)"
78,"Mobile-first design – cleaner code parsing and puts the redraw effort on stronger desktop processors, rather than forcing weaker mobile CPU’s to repaint.Use WP_Query wisely – it’s powerful, but don’t abuse your database/memory.Hard-code selectively – I like to hardcode menus, headers, footers, and as much of the homepage as possible. I let you decide what should be easy to change or not.Clean CSS – write it from scratch at the beginning. (I think frameworks are too bloated.) Grid vs flexbox, Sass or not, choose wisely. Refactor it all over time!No JS – if you can help it, don’t have any JS. (If needed for mobile menu and mobile search, do it cleanly!)Choose between CPT or custom Gutenberg blocks.Building functions in theme – avoids unnecessary micro-plugins."
78,"23. Clean up your existing theme (BEG, MED)"
78,"Already have a theme and can’t change it? No worries, we’ll look at settings and optimize what we can."
78,"Disable loading animations/spinners – improves perceived load.Disable lazy loading of images – improves perceived load.Disable unused features – effects, smooth scrolling, Google maps, etc.Combine custom CSS – disable all custom CSS enqueued by plugins, and combine into theme custom CSS area.CSS/JS optimizations – if your theme has any CSS/JS minification or combination options, enable them!"
78,"If your theme still runs slow, I think it’s easier to just get another theme and make it look the same. You can also recode the theme from scratch and copy the same look as well."
78,"24. JS reduction (ADV, MED)"
78,"I love reducing JS use as much as possible to make themes leaner. My common belief is that CSS is for styling, JS is for effects/functions. And effects/functions are almost never essential for page load. Probably the only essential JS are the ones used for mobile menu, search box, or ATF sliders. All other JS are probably non-essential IMO (animations, tab-boxes, pop-ups) unless they’re loading on every page."
78,"Convert JS effects to CSS (if possible) – this is not only lighter, but can decrease conflicts with plugins.Remove JS dependency from mobile menus – try building it in CSS. Or even better, just avoid hamburger menus.Disable jquery migrate – if you don’t need it.Combine all JS hacks into one global JS – better than having many little JS. (Only reason for splitting is if they’re loading conditionally.)Decide whether you can build all effects with jQuery – or dequeue jquery and make your own mini-library.All non-critical JS (if you have any) should be stuck in the footer!"
78,"25. Pagebuilder removal (INT-ADV, HIGH)"
78,"Quite often, many sites (lazily) use a bloated pagebuilder to produce custom page layouts. But there are 2 better ways:"
78,Replace pagebuilder using Gutenberg plugins.Hard-code the design and/or create your own Gutenberg blocks!
78,I promise you the effort is worth it. Just do it and you’ll be so glad later! Getting rid of pagebuilders also reduces your number of DOM elements (less work for browsers to process).
78,3. Plugin optimization
78,"Plugins can cause up to 50-80% of your site’s slowness. Lean plugins load only the features you use AND make as few and as efficient database queries as possible. Bloated plugins do lots of unnecessary processing, make slow queries, and load CSS and JS even on pages not using them. The worst plugins add on endless features over the years and never rewrite from scratch, spitting out PHP errors, hogging memory, also creating security vulnerabilities."
78,Even better is to know what plugins you really need! Just know that plugins usually do either one of two things:
78,"Give your site a new function (using php/database), and or Give your site a new aesthetic (using CSS/JS effect)."
78,"Do you really need those functions or visual effects? Are there plugins that can do the same with less code/bloat? And for the plugins you do have, don’t enable every feature if you can help it. Load only what is essential to actually creating conversions."
78,"26. Use only essential (well-coded) plugins (BEG, HIGH)"
78,"There are many guides out there trying to teach what is a “good plugin” or not. Unfortunately, you will only know with time and experience. Many plugins advertise similar things and it isn’t easy for the average user to know which plugins are coded well or not."
78,"Consumer grade plugins – many features and styling options. Care more about ease-of-use and having more selling-points, than speed or code quality.Developer grade plugins – do essential features really well, include helpful developer functions/filters, and not much built-in styling.Many small specialized plugins can be leaner than one bloated kitchen sink plugin with many features you don’t use.One big plugin can also be leaner than many small ones with overlapping functions."
78,"Unfortunately, many people think reading the WordPress repository reviews is enough to know if a plugin is good or not but it’s sometimes misleading. High ratings there might only mean that it’s popular…doesn’t necessarily tell you if it’s popular among respected developers."
78,"When in doubt, use the plugin with the better reputation among other developers. Ask a developer for their opinion! The pros ask each other all the time as well. We don’t have time to free trial or read reviews. We ask our friends to get quick info."
78,"27. Avoid bloated plugins (BEG, HIGH)"
78,I don’t hate all big plugins with many features. My problem is with bad coding and unnecessary functions. These plugins are on my shitlist for being unnecessarily heavy and slowing sites down. I can assure you there are much better alternatives:
78,"Pagebuilders – WPBakery, DIVI, AVADA, Elementor, Beaver Builder. All of them are heavy! Use Gutenberg blocks instead.Slider Revolution (RevSlider) – use Metaslider (lightest) or Smart Slider 3 (full-featured) instead.UberMenu – use Max Mega Menu instead if you really need. Or custom-code it. Or go with plain simple menu.Thrive Comments or Thrive Leads – anything made by Thrive. I don’t like Thrive Architect either.Jetpack – eww. Avoid unless you absolutely need it (like for WooCommerce). You can also try Toolbelt which was written to be a cleaner (less spammy) version of Jetpack.ExactMetrics – use GAinWP instead (it’s a fork of the original GADWP)Image lightbox effects – try WP Featherlight (yes, it still works).Image galleries – try Meow Gallery instead of Envira, NextGEN, FooGallery, etc…or even just default Gutenberg gallery with WP Featherlight."
78,Usual plugins suspected of bloat:
78,"Media plugins – anything with lots of fancy image/gallery displays.Anything with visual effects – animations or pop-ups, all require JS load.Conditional load logic – anything that lets you decide which things and widgets to show on which pages.Font plugins – they often add tons of autoloads.Lots of options – any plugin with lots of options, or where you enter lots of info."
78,"How to check for bloat? Check website frontend with developer tools and see what CSS/JS it loads. Then check Query Monitor for slow queries on frontend, and the database to see if it adds lots of autoloads. Should also to check error logs for php errors and also their changelog to see how often it’s updated."
78,"28. Avoid unnecessary plugins (BEG-ADV, MED)"
78,"These are plugins you don’t really need. There are many other ways to get the same result that don’t require any plugin at all. Yes, I understand some of these plugins will be easier to replace than others."
78,"Add custom CSS – can just put into your theme settings or use the Customizer > Additional CSS.Browser cache or expiry times – just use your cache plugin.Font plugins – please do that from your theme.Header & footer code injector – can put the code directly into your theme files. Ask the theme developer if you need help.HTTPS/SSL plugins – you don’t need them at all! You can set HTTPS manually.Gallery plugins – much cleaner to use Gutenberg blocks and WP Featherlight or my forked WP Featherlight Disabled.Performance plugins – simple ones disabling emojis/embed, or doing simple htaccess edits. The only performance plugin you need is a cache plugin.Redirection plugins – avoid these if you can. Don’t use the redirect functions in your SEO plugin either. If your server has .htaccess, use that to do redirects! If you need to log 404’s, just check your Google Search Console.Security plugins – many of them only add stuff to your htaccess, which you can do manually.Unused plugins – deactivate and or delete!Widget conditional load – instead of using widget load plugins, you can try creating a new page template in your theme with its own widget position."
78,"29. Prevent plugins from loading globally (INT, MED-HIGH)"
78,"Many plugins load their CSS styles and JS scripts on every page, even when they’re not being used! They do that to make sure it always works but the downside is that it slows down your site. Some plugins have an option in the settings to disable global load and/or configure which pages to load on. Others can only be disabled with a filter snippet placed in your functions.php file."
78,Examples of plugins loading globally:
78,"Contact Form 7 – loads CSS/JS on every page even when they don’t have a form. Sure, there are optimization hacks for CF7 but maybe you can try newer ones like Caldera or WP Fluent Forms.Form security plugins – some captcha or forms security plugins may also run unnecessarily on pages that don’t have forms.Pagebuilders – extremely guilty! Many load crap on pages not using them, and also for features not being used!Slider plugins – disable “load globally” option in settings.WooCommerce – if you don’t need it on every page (like non-shopping pages), there are functions.php snippets you can use. Some people disable the ajax cart fragments function, too."
78,Beware of any plugin that creates its own custom post type or content layout/styling on the frontend. Also of any plugin that creates or parses its own shortcodes.
78,"If you don’t know which plugins are loading unnecessary, it’s easy:"
78,"Open up your browser Developer Tools and click on the “Network” tab. Then click on the CSS or JS tabs and browse around on your site.You can also load up Query Monitor and browse around on frontend to see if the plugin is making unnecessary queries.Oh and btw, just because you removed a plugin’s CSS/JS doesn’t mean you stopped it from loading. It still could be running php and DB queries in the background."
78,"30. Dequeue plugin CSS & JS (ADV, HIGH)"
78,Many plugins load their own CSS and JS which could be easily dequeued/disabled from the plugin and combined into your theme custom CSS/JS (if even needed at all).
78,"Easy plugins – there’s a box to uncheck for “load CSS styles” from plugin settings. If you see JS options, you can disable but check if your plugin still works.Harder plugins – you can only dequeue the CSS/JS if the plugin has a filter for developers to put into theme functions.php file."
78,"After disabling the CSS/JS, you can copy them (if needed) to your theme custom CSS."
78,"31. Clean up wp_option autoloads (ADV, HIGH)"
78,"Many plugins may appear to be “lean” but in fact eat up chunks of memory on every page load! They do this by abusing the WP autoload function, which loads data on every page and is intended for only critical data. But some plugin developers are jerks, hogging all the memory for themselves like your annoying neighbor hogging all the laundry machines. They do it because their bloated plugin won’t load up quick enough otherwise!"
78,You’d be surprised how many unnecessary autoloads are in your database and how big they can get. Let’s find the biggest database memory offenders by cleaning out the autoloads.
78,"Keep your autoloads below 1mb (good), below 500kb (best).Many old themes/plugins leave crap in database, even when deleted!If the name isn’t obvious what it is, try opening it up and look through the data.When in doubt, search the database entry in Google with its name in quotation marks. (e.g. “cherry_fonts_css”) You can also search the database entry name or plugin name on PluginTests.You’d be surprised which plugins have lots of autoloads. Anything with lots of settings or data saved is suspect, but even small plugins can create giant autoloads.Backup the database before deleting entries!"
78,"Careful about autoloads for existing plugins. Check with the developer first before deleting/disabling autoload for active plugins. If you want to try anyway (even though I don’t recommend it), you can edit the autoload option to “NO” and then delete after a month if you don’t have any issues. Better idea is to get rid of the plugin altogether."
78,"32. Give feedback to plugin developers (BEG, HIGH)"
78,"Quite often, you can complain to the developer directly and they might fix performance issues right away. My advice is to tell them how much you love the plugin and just wished they could fix such-and-such so you wouldn’t have to use a competitor plugin. This is much better than a 1-star review of “THIS PLUGIN SUCKS!”"
78,"33. Custom-code your own plugins (ADV, HIGH)"
78,"Sometimes, the only way is to write some plugins yourself. You could also take an existing plugin and chop down from its code, cutting out the stuff you don’t need. If it’s simple enough, you can also enqueue it into your theme."
78,"34. Don’t use redirect plugins (INT, MED)"
78,"Try not to do redirects from a plugin (whether redirect plugin or SEO plugin). It’s a bad idea speedwise since php-level redirects are slower than server-level redirects. Mind you, it slows down ALL traffic, not only the redirected ones."
78,"To run redirects from your server, use htaccess (if you have Apache or LiteSpeed) and the proper rewrite rules/directives. If you have NGINX, it’s most likely near impossible unless you have access to server-config and know how to do NGINX configs. If you’re lucky, some NGINX webhosts will have a redirect function in their control panel.Most redirect plugins can export to htaccess rules.If you need to monitor 404’s, use Google Search Console.If you absolutely need a redirect plugin, use Safe Redirect Manager (not Redirection or YOAST redirect function)."
78,"This is also a good chance to clean out your redirects. Get rid of duplicates or redirects for links with no traffic. More often than not, I see hundreds of rules that could been more efficiently redirected with just a few clever rewrite directives. If you see many 404’s for Apple touch icons, create them!"
78,"Maybe some of you are concerned about the old saying that Apache is slow because of htaccess (and that you shouldn’t use it if you can help it). Yes, there’s truth to that. But htaccess is still much faster than php-redirects. Also, if you’re on LiteSpeed there’s no worry at all since LiteSpeed caches htaccess so it has zero impact."
78,"35. Choosing highly-respected plugins (INT, HIGH)"
78,"Aside from choosing well-coded plugins with essential functionality, is to choose plugins with a good reputation. Sure you can look up their site and read the sales copy but it’s better to ask around. Ask experienced developers what they use. Why they like certain plugins."
78,"The best plugins typically have the best features (lots of updates), best support, best compatibility with other themes/plugins, best UI (easy-to-use), easy to work with (easily-customizable for developers), scale well (don’t slow down big sites)."
78,When I look for a plugin…
78,"I check the development community. I check the WordPress repository reviews. I check their development company to see what else they’ve built. Respected development companies love showing their success with other themes or plugins, and the people behind them!Logical business model. One-time fee for small plugins, monthly fee for big plugins, lifetime plan for new plugins, ongoing cost for updates/support."
78,"The worst plugins aren’t known by their developer company. They’re on ThemeForest, Code Canyon, Envato, or other crappy stores. They showcase the product more so than the company behind it (of course, they don’t show past failures). And their pricing is too cheap. Cheaply-priced plugins will get abandoned soon over time and won’t be updated often. Look at their marketing…if it’s aimed at newbie users, it probably isn’t the best one."
78,4. Image optimization
78,"Images are actually very simple assets, not much complex processing required from web servers. Where they hold back your page load is in their delivery across networks and rendering in the user’s browser. Make them too big and they take longer to send and eat up the browser’s memory. More than anything, images affect your website’s perceived load time."
78,Below are my strategies to serving images that not only load fast but keep your website looking just as great as you designed it. Please read my MONSTER Guide to WordPress Image Optimization. It explains in fuller detail than some abbreviated explanations in this section.
78,"36. Choose proper image format (BEG, MED)"
78,"JPEG – produces smaller files for photos or images with many colors.PNG (aka “png-8”) – produces smaller files for images with few colors, sharp lines and contrast, and can also do transparency. PNG 24 – higher quality version of PNG suitable for photos, but still larger file-size than JPEG, so not recommended unless you need transparency.WEBP – combines the best of JPEG and PNG, allowing great photo quality and transparency, and better compression than JPEG (smaller filesize). Can be used in place of JPEG or PNG 24, but should have fallbacks just in case! (FWIW, I don’t personally use WebP yet.)GIF – most common in low-quality meme animations. Useful when you want to share a video, but don’t need sharpness or sound. GIF is basically an image slideshow format masquerading as low-quality video format. Rarely used in a professional setting like a website, where you typically want high-definition images and videos. Just know that GIF’s can be optimized, too."
78,"37. Choose proper image dimensions (BEG, MED)"
78,Don’t use a bigger image than you need!
78,"Resize images to fit the exact area size. (e.g. using 400px-width image for a 400px-width area.)To make important or larger images appear better on retina screens, use an image double the area size (e.g. 800px-width image for 400px-width area.)Less important or smaller images can be sized exactly as the area size (no need to be retina-compatible).I believe most people don’t use retina or ultra-HD monitors in their full resolution. Many will scale at 150%, which means you can (probably) get away with only 1.5x image sizes.Don’t upload raw photos directly from your camera. Resize, crop, and edit them first! Or use an image compression plugin to automatically optimize uploaded photos.A big waste would be loading a 2000px width image into a 300px width space. It wastes server storage, internet bandwidth, and browser processing."
78,"38. Set proper media sizes in theme and plugins (ADV, MED-HIGH)"
78,"Luckily, most themes set the right image sizes already since they control the layouts. The issue is more when you have frameworks or plugins using generic media sizes because they don’t know what you’ll end up using. Or you created a new layout that needs a specific media size not already set by your theme."
78,"Finish your theme design.Then go to WordPress Settings > Media, and set the correct sizes used throughout your site.If you have WooCommerce…check Appearance > Customize > WooCommerce > Product Images.If your theme or plugins create their own image sizes, check their settings and image sizes as well. (Some are hardcoded in theme files.)If any media sizes were adjusted or created, please regenerate all your thumbnails."
78,"What you don’t want is to have image sizes that don’t match your layouts. For example: your site has only 400px & 800px image sizes, but some image areas are 500px size…it will wastefully use the 800px one! The solution in this case is either to adjust the closest media size to 500px or create a whole new media size. (Ideally, we want to have as few media sizes as possible to save server space.)"
78,"39. Using responsive (adaptive) images for smaller devices (INT, MED)"
78,Make sure your theme is mobile-adaptive (most themes are) and shows smaller images for smaller devices.
78,"This feature is default with WordPress and most well-coded themes, and can also be enabled by adaptive image plugins. Obviously, it’s not good to load desktop-size images for mobile devices! The problem is that some themes don’t enable this feature properly. Either they load only the desktop-size version for all devices, or even more stupid…they load BOTH (desktop & mobile) sizes for all devices."
78,"If your site uses large background images, you better make sure this feature is working correctly in mobile!"
78,"40. Image compression (BEG-INT, MED)"
78,"Manual JPEG compression – use an image editor (e.g. Photoshop) to decide the exact quality output you want. 60% for jpeg is safe. Can test a few percentage points up or down. Good for large or important images.Manual PNG compress – use image editor and adjust the number of colors.Automatic compression – use a plugin to do it. My favorite image compression plugins (in order) are ShortPixel, LiteSpeed Cache, WP Compress. You can use these for less important images or non-tech-savvy users."
78,Another quick way to manually compress images: just upload to ShortPixel’s compression tool (free).
78,"41. Convert unnecessary transparent PNG to JPEG (INT, MED)"
78,I often see large transparent PNG images sitting on a plain white background. Eating up tons of space when they could easily be half the size and even look better as JPEG.
78,"If your transparent PNG sits on a solid background, you can easily convert that image to JPEG and match the color of the background.You’ll save many bytes and won’t notice any difference!If your image is simple, a solid background PNG might still be smaller than transparent PNG."
78,"42. Redraw images or icons in CSS (ADV, LOW)"
78,"Some themes or plugins unnecessarily use image files for very simple graphics (lines, blocks, icons) that could be easily drawn using only CSS.You can check by inspecting visual decorations around your site theme.Redraw them in CSS and you’ll save a few unnecessary HTTP requests.Here’s a fun collection of 512 pure-CSS icons (by CSS Icon), some even animate!"
78,Want to be clever? You can mix images and CSS together. Using CSS to draw shapes in your images or color filters. There are so many clever uses that save space and look better.
78,"Gradients – been a part of CSS for a while, especially during web 2.0 era.Shadows – use CSS instead of putting edge shadows in your images!B&W filter – great for reusing an existing color image.Color filter – reverse of above."
78,"43. Icon fonts vs SVG’s (ADV, MED)"
78,"There are many debates in the developer world about whether icons are better as SVG or icon fonts. Developers are free to choose because they know which fits their use and workflow. Between you and I, here are my guidelines:"
78,"Simple icons (arrows, search glass, hamburger menu) can be drawn with CSS. Complex icons (social media icons, carts, notepads, etc) need either SVG or icon font.If you only have 2-3 icons, SVG will be less weight (and can even be inlined).If you have 10 or more icons, use a custom icon font service (Fontastic, Fontello, IcoMoon). Or create your own custom font.Self-made icon font (my favorite) and locally-loaded is fastest because even a 2kb small icon-font file can load 10-20 icons, which is all you really need.The hassle with self-made icon fonts is that they take more time to update if you want to add/edit an icon later.The worst icon fonts are the 3rd-party ones loading externally! For example, Font Awesome is 30-200kb and loads many unused icons.If using SVG, you can optimize them with Jake Archibald’s SVGOMG. See what you can disable or decrease in quality without affecting appearance.Please don’t inline SVG’s unless it’s really small and only used in one place on the page."
78,"Most people don’t have the skills to redraw in CSS or create their own icon font. All I ask is that you either use SVG’s or follow one of those custom font services. Whatever you do, please don’t load FontAwesome as that’s a good 40-75kb and costs you 100-150ms load time. Of course, many theme developers load FA as that’s the easiest/laziest way to put many icons on a theme."
78,"44. Remove unused media sizes (ADV, LOW)"
78,Does having a ton of image files really slow down your site? Probably not. Does it slow down backend site functions? Maybe. Can it affect your site speed during longer backup times? Yes!Can it make your webhosting more expensive? Yes.
78,"More than anything, it makes an unnecessary mess out of your website. Making it bigger and bulkier than necessary. Let’s clean this up. Go to your theme and plugin settings and uncheck/disable any media sizes you don’t need. Some of this work may require a developer to thoroughly check all your page templates, edit functions.php and manually dequeue unused media sizes."
78,Having a hard time finding all the media sizes that your site is using? A handy trick for me is to install one of those image compression plugins. They’ll list all the sizes in one handy place. 😉
78,"Get rid of media sizes you don’t use.Can also remove sizes that are too similar. (Keeping the bigger one.) Once you’ve deleted unused media sizes, you’ll have to delete the unused images…but be careful!There are plugins that say they can do the job. Please research and test carefully. I’m honestly unsure of which ones delete only unused media sizes and which ones delete entire unused images.Be very wary of those newspaper/magazine themes that have many image sizes!"
78,"45. Using CSS sprites (ADV, LOW)"
78,"Css sprites is an old tactic that actually still works for some use cases. A long time ago, some developers used to combine many tiny images into one image file and then call only parts of it within CSS. It was a clever way of reducing HTTP requests. Nowadays this tactic is isn’t necessary since HTTP/2 allows you to have many more HTTP requests and most small images can be drawn in CSS, icon fonts and SVG’s. (Design coloring is much flatter as well nowadays, instead of the overly color-esque WEB 1.0 period.)"
78,You can try CSS sprites if you have many small images that have multiple colors (and/or can’t be drawn in icon fonts or SVG’s).
78,"46. Video compression (BEG-INT, MED-HIGH)"
78,Compress videos as much as possible.Output in webm or mp4 format. (I like mp4.)
78,"Any video that’s auto-played on page load should absolutely be compressed. If you’re a pro-editor and know how to render the perfect balance between small file-size and good quality, do your thing. For everybody else, just do my easy cheat!"
78,"Upload the video to Vimeo, Youtube, Facebook, etc.Then re-download their compressed version."
78,"These services put out gajillions of videos everyday, I think they got the best algorithms! Youtube’s compression is the best balance of small size and good quality. If you want slightly higher quality, try Vimeo. If you want smaller size, try Facebook."
78,"More important than compressing the video, is to decide what you need from it:"
78,Do you even need the sound? (Remove if not.)Do you even need it in color?Is the video just a backdrop for text? (Can lower quality even more.)Does it even have to be that big?
78,Do you even need a video? Does it even get watched? Maybe nice big images are better and cleaner to use!
78,5. Font optimization
78,"You would think fonts are so small in filesize that they don’t affect performance, but they totally do. Users perceive your website load by your content and the font is what allows your text to load. Load the fonts efficiently and users see instant content."
78,"Do it inefficiently, and users see a lot of blank space (aka FOIT, flash-of-invisible-text)…or worst, ugly unstyled text followed by a screen flash before the font loads (aka FOUT, flash-of-unstyled-text). It’s a really jarring experience. Looks like crap and makes your site appear slow (both to users and speed tests)."
78,Fighting FOIT and FOUT Together – CSS-TRICKS
78,"What makes fonts so slow nowadays is due to the abundance of free fancy fonts (Google fonts). Back in the days, you had to pay for fancy fonts so you were much more judicious about their selection. Now even a free plugin can load hundreds of Google fonts. This makes it tougher to police where fonts are loaded from and for which text."
78,"Fonts make a visible speed impact no matter how big or small your site. And actually, one of my favorite optimization tactics for speeding up already-lightweight sites. There are many factors to consider when loading fonts, all of them affecting your content load time noticeably!"
78,"47. System font vs locally-loaded vs webfont (BEG-ADV, MED)"
78,"System-fonts – faster than locally-loaded fonts or webfonts.Locally-loaded fonts – faster than webfonts, and can be CDN/browser-cached.Webfonts – slowest, but can load equally fast if already cached."
78,"System-fonts (already built into the OS) will load faster than webfonts from 3rd-party server (Google webfont, Adobe, Typekit, Typography.com, etc). Just know that some system fonts (like Arial, Helvetica) are available on both Windows & Mac, whereas other system fonts are not and must fallback to their similar equivalent (like Segoe UI for Windows, and System UI for OS X)."
78,"I use system fonts for 2 of my sites. The rest of them, like most sites today, need a nice webfont to look polished. Luckily, webfonts keep improving their load time in various ways. They’re constantly getting lighter and removing unnecessary bytes."
78,"Webfonts are also cached in browsers so if you’re using a popular webfont, there’s a good chance your users won’t have to re-download it (since they already got it from another site). Also, I imagine popular Google webfonts will soon be natively stored in Chrome browser (if they aren’t already). But then again, mobile browsers clear their cache often. But then again, there’s also DNS prefetch which helps webfonts."
78,"Locally-loaded fonts (meaning “locally” from your server) is somewhat seen as an outdated way of enqueuing fonts. Web designers used to buy fancy fonts from 3rd-party sites, download the whole font family package, then carefully choose and enqueue in the site. It was a tedious process and didn’t let end users (easily) switch fonts in and out. But since webfonts are kinda bloated in their deployment, locally-loading fonts is still popular among speed-conscious developers."
78,You can locally-load any webfont by copying it to your sever and requesting it locally instead of from a 3rd-party webfont service.This saves a few requests and decreases wait time (with minor and probably unnoticeable drawbacks).Load Google fonts locally – google-webfonts-helper by Mario RanftlThere are also a handy plugin like OMGF to host Google fonts locally.And tools like Jaime Caballero’s localFont.
78,"I personally use only system fonts, or locally-loaded webfonts."
78,"48. Use fewer fonts (BEG, MED)"
78,"Using just 1 font is fastest.If using 2 or 3 fonts, make sure you disable unused styles!"
78,"Having fewer fonts will help, obviously. It’s great if you can get away with just one font. It’s the cleanest look IMO and you can still create different styles by playing with size, weight, letter-spacing, letter case, color, etc. But if you need, you can add a second font used for headline only. Some really professionally designed sites will even have a 3rd font for subtitles, H3 header, quotes, prices, or other emphasized text areas."
78,"Some fonts are heavier than others. Sometimes, even one font can be heavier than 2 others.Don’t forget to disable the font load (enqueue) from your site. It’s not enough to simply stop using it.If using more than 2 weights of the same font, you can try using variable fonts. They load the same amount of bytes no matter how many weights you have. Really cool new font technology."
78,"49. Don’t enqueue unused font weights, styles, character sets (INT, MED)"
78,"Many people don’t notice the unused fonts styles loaded on their site! I’ve seen many sites loading all the font weights (100, 200, 300, 400, 500, 600, 700) and font styles (regular, italic), and character sets (extended latin, greek, cyrillic, etc)….when in fact they only need font weights 400 (normal) and 700 (bold), and only the basic latin character set."
78,Check your site for all unused font styles.Remove their requests from theme and plugin settings.Also remove from header and stylesheets (if manually installed).
78,"If you don’t know which ones you’re actually using, make a list. Link to the FOUNT font finder on your browser bookmark bar and identify every font used on your site. Test on different page layouts and click on all the content bits. Title, nav, headers, content, text in footer and widgets. Don’t forget to check hidden elements like carousel, pop-ups, tabbed or collapsed content. Are you feeling overwhelmed? That only goes to show that you’re loading more than you need!"
78,"Body text usually needs regular (400), italics (400), and bold (700)Titles and headings can re-use the regular 400 or 700, but some people want super-skinny (300) or super-heavy (900).Most sites never use italics for titles/headings, so if you’re loading 900 for them you probably won’t need italics 900."
78,"Anyway, most of this is common sense. Those of you manually enqueuing fonts are already aware and making sensible choices. It’s the newbies loading fonts through pagebuilders or plugins that are likely having unnecessary weights, styles, and character sets."
78,"50. Font enqueue method (ADV, MED)"
78,"This is IMO an art and massive area of debate within the development world. There are many ways to enqueue fonts (especially webfonts), each with their own pros and cons. I’ll leave some general guidelines below:"
78,"Fonts should load before the content – I don’t care about FOIT or render-blocking, it’s better UX to load text in its proper font from the get-go.Font should be (carefully) loaded by the theme, not a plugin. If your plugins have font/typography options, choose “inherit”."
78,Webfonts load faster when loaded-locally and with long cache expiry times. But that speed difference isn’t as noticeable if you’re using a common (lightweight) webfont.
78,Varvy breakdown on various webfont loading methods.
78,"The biggest problem with how webfonts are enqueued are due to developers trying to make them user-friendly. If you’re manually enqueuing just one webfont and carefully inserting it into the theme, there’s usually no issue. It’s when developers try to have endless font-choice dropdowns for newbie users that it gets messy and more stuff is loaded than needed."
78,Don’t get caught up in reading crazy-detailed guides like this or this. They’ll spin your head around with all kinds of preload and fallback chatter. My bottom line is this…load the font before the text. Don’t let the text load first as that creates lots of FOUT chaos for bloated WordPress sites.
78,"51. Automated font subsetting (ADV, MED)"
78,"Font subsetting is an ingenious weight-reduction tactic of loading only the font characters actually used in your content. There are many use cases where you don’t need all the characters in a font. For some people, getting rid of unused characters made their font file-sizes 1040x times smaller!"
78,"Only basic latin characters – if you don’t need accent marks, Egyptian symbols and such.Only uppercase – title fonts, headings.Only numbers – for specific styled numbers?Only currency symbols – if you use this font only for price menu?Only certain characters – logos, headlinesOnly common characters – all the ones used in 99% of websites."
78,"Some font subsetting is easily-allowed by webfont services. Others, you can only do it manually (but there are helpful tools). Below are some tools and guides."
78,"Slash Page Load Times With CSS Font Subsetting (the new code) – easy guide for subsetting both webfonts and locally-loaded fonts.Webfont Generator (Font Squirrel) – awesome tool with many options. Pick “Expert”, scroll down and then Basic or Custom subsetting. I love this one.Font Subsetter (Everything Fonts) – simpler tool to upload and subset any font you like. They have helpful basic options. You can also pick “extra characters” at the bottom.Save page weight with web font subsetting (Benjamin Black) – font subsetting from server CLI using fonttools, glyphhanger, and pyftsubset."
78,"Usually when you do this level of font optimization, your font files get so small that it only makes sense to have them locally-loaded. There’s no point in making multiple calls to Google’s font server only to get 1/3rd of a font. But hey, that’s your call to make. I think if you’re manually optimizing to this point, just go all the way."
78,"This “locally-loaded subsetted-webfont” method gives me nice fonts AND lightweight ultra-fast load.Yes, I lose out on the benefit of them already being cached from other websites or easily updated via font-hosting service.But likewise, they are much lighter and can also be cached in CDN and browser at my specified intervals. (Also appears faster in page tests as well.) Overall, benefits outweigh any drawbacks.I recommend using only WOFF2 as it’s the most compressed/lightweight and suits all modern browsers. Sometimes, font services load WOFF which is bigger but compatible with really old browsers (not important to me)."
78,"Font-subsetting can also open up creative design flexibility. Like mixing letters of one font with numbers or signs of another. Or maybe you just want a really unique “&” sign or unique letter because that’s your customized logo letter. In case you’re already doing that, at least now you know how it’s done without loading entire fonts."
78,The Joy of Subsets: Crossbreeding Web Fonts – sitepoint
78,"52. Manual font subsetting (ADV, MED)"
78,Some really obsessive speed addicts (like me) will manually edit fonts to remove ALL unnecessary characters and glyphs. And I mean like… ALLLLLL! This tactic is extremely tedious and will certainly earn you the “I have no life” award from me.
78,"The only tool of choice here is none other than the legendary FontForge, used by professional designers to create and edit new fonts. It looks like a giant photo library of every character in your font. Allows you to manually adjust each one’s shape and spacing with adjacent characters."
78,2 ways to work with FontForge:
78,"Reduction – load a font and then delete unwanted characters. (Probably smarter to start with an already subsetted font.)Build from scratch – open existing font and new font side-by-side, and copy over. This route makes sense if you only need a few characters."
78,"Yes, you’ve got to be crazy to tweak this far but it really works to make your fonts super lightweight. I actually find it to be so much fun (despite the work). It’s one of the few tactics that can make already lightweight sites even more lightweight."
78,Other thoughts regarding font optimization and subsetting:
78,Web font optimization – NerdWalletWeb Font Anti-Pattern: Aggressive subsetting – bram stein
78,"53. Load all Google webfonts in single request (MED, LOW)"
78,"How to Load Multiple Google Fonts in One URL Request – ShellcreeperYes, this matters even if you only use one Google font family."
78,"When requesting webfonts, put it all into one request instead of multiple. If anything, this is a practice of loading all webfonts in one place! Do not have some fonts loaded by theme, others by plugin, and such and such."
78,"54. Optimizing Font Awesome (ADV, MED)"
78,"I don’t like FA but maybe you like it or don’t want to re-style a theme already built for FA. Here’s how you can make it lighter, improving speed AND page scores!"
78,"Optimize Font Awesome to ridiculously low size of 10kb! – by WEBJEDALazy? Just load it locally.Have more time? Get rid of unused icons.If you’re thinking it’s easier to just rebuild a new iconfont or use SVG’s, it might be…only issue is replacing all the CSS calls to FA."
78,"I swear it’s easy to load FA locally. Just download it from the site (can even match the same version you have already). Upload the FA folder to your website directory, then go into your theme or wherever FA is loaded from and change the request to load from your site instead of the FA domain."
78,"55. Preload fonts (ADV, LOW)"
78,Are you feeling like your content isn’t coming up super instantly? You can preload fonts in your header instead of waiting for the CSS to load. Which also makes font preloading useful for folks with giant (or combined) CSS files.
78,How to load web fonts to avoid performance issues – freeCodeCamp
78,"56. Avoid duplicate font calls (BEG, MED)"
78,"Total rookie mistake and one I see happening all the time. The theme calls a webfont (let’s say Google’s Montserrat), the pagebuilder calls the exact same one, and the pop-up plugin calls it one more time. So now, we got 3 calls and 30 requests to Google’s webfont servers to download the same damn font multiple times. How stupid!"
78,"Try to load fonts only from one place, preferably your theme.All other font-settings places (like plugins) should use the “inherit” option instead of picking a font.Watch out for plugins that let you select fonts (pagebuilder, slider, pop-up, mega-menu)."
78,The common mistake is that users think they have to re-select their font in every website setting. Don’t do that!
78,6. Caching optimization
78,"Caching is best defined as saving the processed requests so they can be served faster when requested again. It’s used in every aspect of computer technology since forever, and absolutely essential for speeding up your page load."
78,"There are many kinds of caching (across different layers, protocols, and services) and many ways to configure them. Set them up right and you’ll have a fast site that reduces server load and saves money. Set them up wrong and you have a site that’s sometimes-fast and or has broken design and functions."
78,"If you’ve ever heard of people complaining about cache, it’s a combination of them not knowing how to configure for their use case and/or using a caching solution configured for server-efficiency more so than speed."
78,"57. Choosing the right server for caching (BEG-ADV, HIGH)"
78,"Full-page caching (aka “static caching” or just “caching”) means to prebuild the entire page, making it static like HTML so it’s ready to serve immediately when the page is visited. There is no wait for any database queries or PHP processing…the page simply appears instantly. Like a burger that was made before you order it!"
78,"Back in the days, caching was only done by the web server (therefore called “server caching”) and utilized to keep server load down. It was a tactic to help busy web servers handle lots of traffic. Somebody later realized it could help speed up today’s bloated sites so now, caching is also used from a speed point-of-view and deployed even on small webservers with little traffic."
78,"Developers even created software-level caching mechanisms to recreate the same benefit for users without server access, or on servers without “server-caching” modules enabled. Of course, they’re not as fast as true server-caching but still massively impactful and in some cases even more useful because of having extra caching features."
78,"The most important distinction for regular users is to know their caching limitations with each webhost or web-server. In terms of webserver…Apache, LiteSpeed, and NGINX all have different server-caching available."
78,"In terms of webhosting…just know that some webhosts allow you the freedom to use their server-caching or any 3rd-party plugin of your choice. Other webhosts force you to use their proprietary caching solution. Of course, they will sell it as being high-tech and “best performance” but usually it’s a stripped-down caching solution that’s built for resource-efficiency rather than aggressive performance. If you try to use an outside plugin, it simply won’t work or the webhosting company will automatically disable it."
78,"Caching is usually faster and more resource-efficient from the server rather than through software-level PHP caching.If using LiteSpeed server, you can use built-in LiteSpeed server caching.If using NGINX server, you can use built-in FastCGI server caching.If using Apache (which you shouldn’t), you can use Varnish proxy or even NGINX proxy.All web-servers can use any software-level php caching (via cache plugin). But some plugins are designed more for Apache/LiteSpeed servers, others for NGINX servers.Careful of certain webhosts (especially “managed webhosts”) who force you to use their caching solutions and won’t let you use other cache plugins."
78,Just know that the webserver or webhosting company you choose will affect your caching experience and what caching plugins you can use.
78,"58. Choosing the right cache plugins (BEG-ADV, HIGH)"
78,"I’ve tried over 50 different cache plugins. The best ones have useful features, easy-to-use, and don’t break your site. If you manage only a few sites, you might prefer aggressive plugins with many optimizations and even Facebook groups where people share tricks. If you manage dozens of clients, you’ll prefer a safer plugin with less features and less chance of issues."
78,"Best all-around cache plugins (allowed on any webserver) are Swift Performance or WP Rocket. Swift more aggressive, Rocket more stable.There are other good cache plugins as well (WP Fastest Cache, Breeze, WP Performance, Comet Cache, Borlabs).W3TC sucks and very difficult to configure. Don’t use unless you’re a pro.If using LiteSpeed server, you can use LiteSpeed Cache plugin. My favorite.If using NGINX server, you can stick with NGINX helper plugins to keep it simple or a full-featured cache plugin like (Swift or WP Rocket).Do not combine multiple cache plugins!I don’t like combining cache plugins with other performance plugins either, but it can work if configured carefully (and not overlapping functions)."
78,Deciding which cache plugin to use depends on your use case.
78,"Small site with 400 pages and little traffic? – Swift or WP Rocket, with precaching enabled.Medium site with 400-1k pages, and medium traffic? – Can go with cache plugin Swift/Rocket or server caching LiteSpeed/NGINX. Both have pros and cons.Big site with 1k or more pages and lots of traffic? – I like custom-configured LiteSpeed Cache or native-NGINX cache and no precaching necessary since you have so much traffic."
78,"59. Cache plugin configuration (INT-ADV, HIGH)"
78,"Cache plugins are more complicated than ever to configure now since they do so many more things than just caching. This section covers specifically only caching-related settings. Regarding other options you see in cache plugin settings, my recommendation will be in other parts of this guide."
78,"Rewrites vs PHP – most cache plugins only give you the PHP option. If you have the option to use the rewrite method, try that as it’s faster. If it causes problems, then switch to php method.Cache TTL – this means how long the cache should last for. You should set this about as long as your content update intervals. If you update every day, then set 24 hours. If you almost never update, can make it 1 month.Private cache or logged-in users/pages – don’t use unless you really have that many logged-in users and you know how to prevent cache from mixing up content between users.Separate mobile cache – don’t use unless you have AMP or a specific design or content that only shows up on mobile. Just because you have a mobile-responsive site doesn’t mean you need this.Caching 404 pages – yes if you have many users hitting them. Better if you redirect all of them to actual pages.Dynamic cache – great for caching urls with queries or filters, like searches or ecommerce product filters.Exclude – absolutely critical for excluding pages that shouldn’t be cached. I typically exclude checkout, logged-in user pages, or pages with forms.Browser cache – yes, enable it.Heartbeat control – I usually disable for all pages except posts. Or if you need it on, you can increase its interval to every 120 seconds."
78,"Not every caching setup or cache plugin will allow you to configure all those options. Don’t freak out if you don’t see them. In fact, there are so many possible settings…plese refer to my cache configuration guides below:"
78,"Swift Performance cache setup guideWP Rocket cache setup guideLiteSpeed cache setup guideFor all other cache plugins, I think you can follow my guides above to get a general idea."
78,"60. Configuring cache-prebuild (INT, MED-HIGH)"
78,"One of the most overlooked aspects of caching for me is the cache-prebuild process. Some plugins call it “cache-preload”, others call it “cache-warming” or “cache-crawler”. The cache-prebuild function is a mechanism that pre-caches pages so they load quickly when visited. Back in the old days, caching was only used on high traffic sites and so no cache-preload was necessary. The cache was built by the first visitor and all subsequent visitors benefitted from an already-built cache. This was fine since you had thousands of visits and only the very first person was unlucky to see a “slower” page."
78,"But in today’s caching era, letting the “first person warm your cache” is a terrible idea if you have very little traffic. On a not-so-busy site with no cache prebuild, it might seem like all your visitors hit a cold (uncached) page. FYI: users hitting cold cache will see a slower page load than without any caching at all!"
78,"So anyway, we have cache-prebuilding now. There are very few servers that have or allow a server-wide cache prebuild function. They’re probably afraid of users abusing it. It’s in the same logic that many webhosts won’t allow you to use those broken-link checker plugins (because it eats up precious resources)."
78,"LiteSpeed servers have a built-in cache crawl function. Few webhosts allow it on shared servers though. You’ll probably only get access if you have it on your own VPS.Anybody not on LiteSpeed….well, there are many server-level cache warming mechanism and scripts out there but not quite so user-friendly. You can search them out if you insist. It’s much easier if you’re on LiteSpeed, though.The best server caching script I found is OCP. Read my OCP guide.The only option (for most people) is to use a cache plugin that has cache prebuild functions built-in like Swift Performance and now WP Rocket (which copied that idea from Swift).Prebuilding improves the caching experience on your site by a whole lot. Enable it if you can!"
78,"There are a few instances where you SHOULD NOT use cache-prebuild. One is if you have many visitors, it’s useless since there’s already live traffic warming the cache. The other is when you have too many pages."
78,"I would say 1k pages is the general threshold. If you have over 1k pages and you update your site often, I recommend NOT pre-caching since your server will use many resources and never even finish the job. If your cache gets purged before it ever finishes, it’s stuck in a perpetual cycle of constantly prebuilding cache that’s never used."
78,"61. Object caching configuration (BEG-INT, MED-HIGH)"
78,Object caching is useful for dynamic sites (constantly-refreshed data and/or can’t be cached) or any sites with lots of calculated numbers/reports in admin backend. Object caching saves the database calls in RAM so they don’t have to be looked up every time they’re queried.
78,"Because RAM is precious (less abundant than hard drive space and necessary for running programs), object caching is not usually allowed on many shared hosting plans. Also it has to be enabled through the server…so it’s only possible if you have your own server or on a hosting plan that allows object caching."
78,"Object caching must be enabled from the server.Object caching can be managed through the webhosting control panel or WordPress plugin. (Plugin is better.)It’s best if you have a cache plugin that manages both page caching and object caching (like LiteSpeed Cache). This way, they purge cache at the same time.You can use an object cache expiry time of 5 to 10 minutes to be safe. But if your content isn’t update often, you can go higher like up to 30-60 minutes."
78,"I recommend not to use object caching if you have just a static site. In some cases, it’s even slower than just plain page-caching. Enabling it “just in case” does not help! Of course, you can test and see for yourself."
78,Top reasons to use object cache:
78,Slow admin backend.Slow database-heavy functions.Don’t know if your pages are suffering from slow query? Check with Query Monitor.
78,"62. Browser cache aka “htaccess expires headers” (BEG, LOW)"
78,"This is that common tactic where people paste a bunch of lines into their htaccess so static files are browser-cached for a long time (1 week, 1 month, or even a year) so users don’t have re-download them again. Maybe it was a big deal before…well it isn’t anymore."
78,"Many devices (like mobile) have limited space and delete their browser cache when they want, not when you want.Browser cache only helps for returning visitors, and many sites don’t have much repeat traffic anyway. (Besides, it’s the new visitors that we care more about improving user experience.)If you want to use browser cache, it’s more conveniently done through a cache plugin…rather than copy-pasting long commands into your htaccess file."
78,"63. Ignore query strings on page caching (BEG, MED)"
78,Query strings are (the extra text at the end of urls) used to send information to the server. Examples of query strings below and what they do:
78,"search – show different search results. (e.g. search?=wordpress+tips)ref – lets the site track who referred the user and save info within conversion tracking software. (e.g. ref?=affiliateID)fbclid – lets the site know the user came from Facebook. (e.g. fbclid=123ABC)(product) filter – show different products based on name, price, size or other product specifications. (e.g. filter_size?=large)"
78,"As you can see, some of these query strings actually alter the page content whereas other strings show the same page content regardless of the query string. The idea here is to exclude the query strings (that don’t change content) from caching so your cache mechanism doesn’t store those hits as a different page. This would save your server from unnecessary work recaching the same page under a different url string AND serve the page faster to visitors from existing cache."
78,"This makes a big difference for visitors coming in from query string urls such as Facebook ads, affiliate links, email newsletter links (with conversion-tracking)."
78,Here’s how you IGNORE Query Strings when Caching.
78,"64. Private caching for logged-in users (ADV, MED)"
78,Private caching is rarely practiced as most sites don’t need it (they don’t have many logged-in users). Caching public pages is easy since everyone sees the same page. Caching private pages is hard because of having to cache how each user sees the page differently AND (preferably) not wasting so much space resaving mostly similar pages over and over in cache.
78,"Some private pages are easy – same content for all logged-in users, but something else for users not logged in.Some private pages are harder – shows mostly same content but also custom data depending on the user (like their name/username).Other private pages are tricky – shows completely different content to each user."
78,"The easiest way to deal with private pages is to use object caching instead of page caching. To be more aggressive, you can carefully enable private caching…BUT make sure users can’t see each others info and the public can’t see private content."
78,"There are certain mechanisms out there like LiteSpeed’s ESI feature where you can show different content and widgets depending on the users…which is nice. But ultimately at this level, you need to either hire a pro or spend lots of time to configure something that not only works but also doesn’t eat up so much resources (memory and space). Private caching is tough!"
78,"65. HTML-caching at the edge (INT, MED)"
78,This is a relatively new technology for WordPress. There are several plugins and services out there that can cache your pages at the edge and use CDN mirrors to serve them to clients around the world. There are 3 main benefits:
78,"Caching done on their server, not yours – reduces your server load, helps slow webhosting, decrease server costs, allow caching even if server doesn’t have it. Basically…allows you fast speeds even if your webhosting/server sucks!HTML delivered from CDN – traditionally, CDN’s only transfer static assets like images and CSS/JS. With this new edge-caching for HTML, they can serve the HTML from the nearby pop server as well…theoretically improving page load for visitors far from your origin server.Further DDOS protection – since less of your server is used, less of your server is exposed to DDOS attacks."
78,Notable edge-caching plugins and services available:
78,QUIC.cloud – from the makers of LiteSpeed webservers. Their Q.C service allows any website to benefit from LiteSpeed quickness even if they don’t have LiteSpeed servers.Shifter – sold as a “static site generator for WordPress”. Seems to be a very different user experience from the usual WordPress workflow.WP Cloudflare Super Page Cache – allows you to cache everything with the free Cloudflare plan. Something previous plugins have claimed to do but fail in one way or another.I believe there are some more but I can’t find them now.
78,"Do you need these services, or are they beneficial if you already have a fast server? I think not so much. I don’t use any of them as I’m happy with my server caching already. But these can be great options for those on weaker servers. Keep an eye on this segment as I’m sure it will evolve quickly."
78,7. Local asset optimization
78,"Assets are any static files loading from your server. CSS stylesheets, JS scripts, images, fonts, videos. They need to be processed quickly because they often ARE the content OR they directly affect how the content is shown. Simply put, your content cannot load until your assets are loaded!"
78,"Now the trick is in how we load the assets. Some should load first, others should load later. Stuff that’s visual and near the top of the page should be prioritized. What we don’t want is less-important assets to block the critical ones. How else can we optimize the way they load?"
78,"66. Reduce asset requests. (BEG-ADV, HIGH)"
78,The fastest request is the one not made! And the most obvious way to reduce asset requests (and overall HTML) is not to call them in the first place. Many people spend so much time trying to compress and combine junk that shouldn’t have even been there in the first place! Please reconsider everything you have on your page.
78,"Get rid of stuff you don’t need. (Plugins, images, fonts, etc.)Get rid of visual effects you don’t need. (Animations, mouseovers, etc.)Disable unnecessary plugin CSS/JS.Consider moving things to other pages. Don’t put your entire site on the homepage.The less stuff you have loaded, the fewer assets needed."
78,And for the assets you still have? We have more tricks…
78,"67. Remove unused WordPress core JS (BEG, LOW)"
78,WordPress by default includes a couple javascripts that might not be used. You can disable them with some simple snippets in your functions.php file.
78,"wp-embed.js – conveniently embed Youtube videos or unfurl links in the editor and content. If you literally never embed videos and all your content is just text and images, you can dequeue it.wp-emoji-release.js – little JS that turns punctuation marks into emojis. You can disable this since modern browsers already support emojis natively.jquery-migrate.min.js – supports older WordPress themes and plugins still running on the old jquery library. Sites using updated themes/plugins can safely disable this.jquery.js – largish JS library used by many themes/plugins. It’s convenient since themes/plugins won’t load extra JS if they can use the default jquery JS. The issue is some themes/plugins DON’T use it and they load their own JS anyway. Disable if your theme/plugins don’t use it. If you don’t know, then disable and check everything carefully…or consult the documentation for every theme/plugin that you installed."
78,I’m sure you’ve seen some performance plugins that offer to disable these for you. I suggest not using them as they sometimes create their own extra load. I prefer disabling these manually in functions.php OR using the built-in functions in my caching plugin.
78,"68. Use native CSS/JS optimizations from your theme & plugins (BEG, MED)"
78,"If your theme or plugin has built-in CSS/JS optimization features, use them as they are safer and likely won’t cause any issues."
78,"Theme or pagebuilders allowing CSS/JS merge and minification – use it.Plugins allowing different CSS generation options – always pick “external CSS file”, as it’s faster and safer than putting them “inline” or in “database”. Only use inline if it’s super tiny CSS (like 3 lines)."
78,"69. Combine CSS/JS for small sites only! (BEG, LOW)"
78,"There is a huge debate whether you should combine and minify your CSS/JS, aka “concatenation”. I believe it’s better (in general) if you don’t combine CSS/JS . With that said, smaller sites can benefit from combining since most of the load time for tiny CSS/JS will come from the DNS lookup rather than the code processing."
78,"As for larger sites or larger CSS files, I definitely recommend not combining them. You can read the link above for my explanation why. Basically, it’s because HTTP/2 makes concatenation unnecessary and also that not all CSS/JS should load in one file."
78,"If you plan to combine CSS/JS, it’s best IMO to do it only through your theme CSS options. (Of course, it only combines theme-related CSS and not plugins.)"
78,"70. Combining CSS/JS for big sites (ADV, MED)"
78,"I already said why not to combine CSS/JS, and I still stand by that. But I know some people are going to insist on it anyway because they care more about page scores or what other people on the internet said…fine, do it like this:"
78,"EASY – use the CSS/JS combine options in your cache plugin. Test your site. If some styling or functions break, then exclude the problematic ones.ADVANCED – use a specialized CSS/JS combination plugin (Autoptimize is the best) alongside a cache plugin. This method requires more configuration but gives you more granular control over the optimizations and takes load off your caching mechanism. Can actually be nice since content changes won’t purge your CSS/JS.NOTE: don’t use the combination features in your cache plugin if you’re doing it with another plugin. Duh!NOTE #2: if your theme/pagebuilder has combine options as well…yes, you can use them all together. But combine the pagebuilder and plugins first and then do it overall from your cache plugin."
78,"What to do when styling or functions break (which will happen for 95% of you). Disable the optimizations and re-enable one at a time – test each change. Do one with just CSS, then another with just JS. Sometimes, it’s only one CSS or JS that’s causing the problem. Find out which one it is and exclude it from combining:"
78,"Isolation Method #1 – with combine CSS/JS on, open your site in Chrome > Developer Tools > Network (tab), and reload the page. Click the little red error circle to see which CSS/JS are missing. Exclude them and see if things work.Isolation Method #2 – disable combine (and also caching), and scan your site in Pingdom. Sort the waterfall items by file-type (neatly displaying all CSS/JS). Now re-enable combine but manually exclude whichever CSS/JS you think is causing the issue. HINT: whatever’s breaking is probably related to the problem. Did a certain plugin or theme function stop working? Try disabling those CSS/JS. Yes, it takes a lot of trial and error. It could be anywhere; maybe a plugin, maybe your theme.TIP: sick of trying to isolate the issue? How about combining only the CSS/JS for the single most bloated extension? For most people, this is either the theme or pagebuilder. So combine all the assets for one thing and exclude everything else. It’s an easy way to get most of the benefit you’re looking for without the combine conflict issues."
78,What if you’re still having issues? What if your site seems fast but has a small FOIT or FOUT issue? What if your site feels even slower? This is why I told you not to do it!
78,"71. Manually combine custom CSS (INT-ADV, LOW)"
78,"Another clever way of reducing unnecessary CSS requests is to disable all custom CSS enqueued by plugins and manually adding them to your theme custom CSS area.Yes…if you’re combining all CSS into your theme custom CSS, you probably won’t need those “Add custom CSS” plugins anymore. Get rid of them."
78,"If you’re lucky, some plugins have an easy checkbox to disable their CSS styling. Others you can only dequeue the CSS with filter code snippet placed into your functions.php. For plugins that don’t allow either, you can manually edit out the CSS call from the plugin (but you’ll have to re-do these hacks if you update the plugin)."
78,"72. Remove unused CSS styles (ADV, MED)"
78,You can lighten your CSS by removing all unused styles. Simply go into your theme or plugin CSS and delete anything not used. It’s common to have themes and plugins with redundant/unnecessary styling for the same elements. Probably helps to make screenshots of your site page templates in desktop and mobile before you edit.
78,Unused sections.Unused blocks or elements.Unused buttons or icons.Unused tables.Unused typography.
78,"One thing to note is that if you’re removing from your theme or plugins default CSS, they might come back if you update your theme/plugin later."
78,Wishing for a methodical process?
78,"The Tale of Removing Unused CSS from WordPress (WP Speed Matters) – a great guide written by another respected WP speed addict. He also mentions all the tools I would have listed.Check all site templates on frontend. See which scripts they load. See which parts are obviously unused. Usually, there are many design elements of the theme/plugin that you never use. Start commenting styles out and then fully delete after a month if your site has no issues. Or make a backup and delete on the spot, up to you."
78,"73. Remove unused JS scripts (ADV, MED)"
78,"Good luck trying to do this. It’s damn near impossible unless this JS was custom-written by you. The reason why many sites load unused JS is because it’s loading JS for all the features available in your themes/plugins (yes, even the ones you don’t use)."
78,"So sure, you can try to remove them manually but that means your theme or plugins might break if you change their settings or options later. There really isn’t any way except to do a code-refactor and how the heck are you going to do that? That’s the theme or plugin developer’s job."
78,"What we can do is perhaps diagnose your site and see how much unused JS there is. And with that, you can find where the bloat is in your theme and plugins and which ones to replace."
78,Find Unused JavaScript And CSS Code With The Coverage Tab In Chrome DevTools – Google Tools for Developers
78,"74. Manually refactor CSS/JS (ADV, LOW-MED)"
78,"If you got the skills, you can take it a step further by rewriting your entire CSS from scratch. Carefully cleaning and reorganizing the styles to make it easier to read and lighter to parse. In moments like this, you can even rethink your design to make things simpler."
78,"75. Javascript async and defer (ADV, MED)"
78,"There’s a lot of talk behind JS async/defer tactics, but they aren’t always used for good reason. And it doesn’t help when annoying page tests make generic recommendations for users who don’t know how JS works."
78,Not all javascript should be async or deferred!Any JS used for ATF items should be left alone.Any JS not used for critical elements should be deferred if you don’t need their functionality immediately.JS async can be used as long as you test first.
78,The problem with speed tests is that they try to improve page scores at all costs. They don’t know what your JS is specifically used for so they make silly blanket suggestions that might actually hurt your page load!
78,Deferring JS will delay the visible effects or functions reliant on them!
78,"AJAX (autofill) search functions – defer is ok.Conversion tracking – do you want to start tracking sooner or later? (I like later.)Chatbox – for sales/support. Deferring makes sense.Content-rendering – do you have special pages or content relying on JS? If that specific content is important or near the top of page, don’t defer.Link plugins – do you have specific JS for rendering affiliate links? I think defer is ok.Pop-ups – defer is better.Security functions – any JS that applies captcha or prevents right-clicking, or any other security. I think deferring is ok.Sliders or image animation – terrible idea if they’re at the top of your site! They’ll load slower if deferred! (Probably a good rule never to defer visible elements near the top of your page.)"
78,"What’s even funnier for me is that you could just manually “defer” things yourself by putting the code in the footer instead of the header. Things like Google Analytics can be placed later and it will load later (after all HTML has been parsed). So if you really think about it, the only reason people use JS async/defer is to optimize the load for any JS automatically placed in the header. And if it’s placed there, don’t you think it probably means it should be loaded earlier???"
78,"So like I said, be careful what you async/defer! If you wanna play with it, test carefully. But in general, I prefer to leave them untouched to be safe. There’s no point in scoring higher on a page test but your certain site elements or functions loads slower. JS makes little impact on well-coded sites."
78,"76. How to minify HTML, CSS, JS (BEG, LOW)"
78,Code minification decreases file sizes by removing all spaces and unnecessary code comments. This results in fewer bytes sent through the internet without any loss of function. It’s a win-win all around!
78,"Small sites don’t have much to benefit from minification.Larger sites will benefit more (especially for slower mobile visitors), but even then minification doesn’t help much once your CSS/JS is browser-cached anyway!"
78,My only issue is with how this code is minified.
78,"Through a cache plugin – ok for small sites.Through specific CSS/JS optimization plugin (Autoptimize) – better option for large sites.Through a CDN – best and easiest method, IMO."
78,"Most people do it through a plugin, either cache plugin or one of those “performance plugins” (Autoptimize) that specialize in combining and minifying your CSS/JS. The cache plugin is most convenient and works fine for all sites. But if your site is big with many pages, and/or lots of traffic…you’ll get better results with a dedicated CSS/JS optimization plugin. They have more feature, more control, and best of all…they save you a lot of server load when you purge cache (since your cache plugin doesn’t have to rebuild CSS/JS when page cache is rebuilt)."
78,"The extra load on cache prebuild is bad if you have a big site (and minifying with cache plugin). Every time cache purges, it completely rebuilds all pages and CSS/JS. It’s fast on a 10-20 page site but awful if you have many pages and different page layouts.The extra load can also be bad if you have low traffic (and no cache prebuild). Since uncached visits will take longer to load (thanks to the extra task of minification)."
78,The easiest way to minify is enable it from your CDN (so it processes on their severs) instead of slowing down your web server.
78,I only enable minification from Cloudflare. I don’t use it from any plugins.Enabling it is easy. Just check the boxes.
78,"77. Lazy loading images (INT, LOW)"
78,"I hate lazyload as a general tactic. It makes your site appear faster to page tests, but loads images slower for human visitors. This is terrible UX! If you’re going to use it, you have to know which things should be lazy loaded and which things should not."
78,"It is SAFE to lazy load images below the fold but NOT if your site has many fast-scroll users (like shopping sites).It is NOT RECOMMENDED to lazy load any images at the top of the site (header, banner) obviously since that makes them appear slower to visitors.It is NOT NECESSARY to lazy load images if you don’t have many or if they’re small."
78,"In optimizing many sites every week, I can assure you that 99% of them would look faster without lazy load. It sucks that most sites can’t control which images lazy load or not. Currently, it’s either an ON or OFF setting in cache plugins. No granular access. Native lazy load is available in recent browsers through attributes to specify “lazy” (on) or “eager” (off) or “auto” (browser determines). But if you want to make it easy on yourself, I suggest not lazy-loading."
78,"So lazy loading doesn’t speed up page load? – nope! It delays it. Your site only gets higher page scores because those don’t trigger your images to load like real visitors will.But doesn’t lazy loading decrease bandwidth use? – yes but that’s not much benefit unless your images are huge or you have that many of them.Doesn’t lazy loading help by loading fewer things? – it doesn’t load fewer. It loads the same but delays them. If your user doesn’t scroll, it helps. If your user scrolls, it hurts."
78,"78. Conditional CSS/JS asset loaders (ADV, LOW-MED)"
78,"There are several plugins out there (usually called “asset optimizers”, “asset organizers”, or “plugin organizers”) that can decide which pages will allow which CSS/JS to load. Some organize by plugin, disabling PLUGIN activation on the pages you choose. Others organize by CSS/JS, disabling CSS/JS load on the pages you choose."
78,"They are a lot of work to configure and cause problems for your site if you’re not careful. You also have to be careful which ones you choose. Some are written well. Others are terrible because their plugin adds its own load overhead, which defeats the purpose! If you insist on messing with these, research them for yourself and good luck."
78,"Don’t try to remove every little thing off every little page. Focus on the heavier plugins and CSS/JS.Focus on your slower pages and more important pages (like Home or Cart/Checkout).I like WP Gonzalez and Asset CleanUp. Generally, I recommend you not to use any of these plugins and to just pick lean plugins in the first place. I don’t use these as I like to optimize manually."
78,"79. Deploying a CDN aka “content delivery network” (INT, MED)"
78,Do you need a CDN? If your traffic is local—NO!Try Cloudflare for easiest free CDN. (Great for beginners and many sites.)Want better performance than Cloudflare? Try BunnyCDN (still low cost).Need CDN for large files? Try S3 and Cloudfront!Want a more aggressive “push CDN”? Look it up!Some CDNs cover certain geographical areas better than others.
78,"Network latency time is a common area of slowdown in delivering static assets (images, css, js). Faraway visitors have to wait 1-3 secs longer to receive requested assets. But if you use a CDN service, the files load faster since they’ll come from a mirror server closer to the visitor. That’s all a CDN service really is, a company with multiple mirror servers all over the world. How they differ is in their pricing, features, and performance around the world."
78,"If you don’t know what you’re doing, just get Cloudflare and be done with it. It’s free and works well enough. If you want to venture down the rabbit hole of CDN’s. Just know that there are 2 kinds of CDN’s:"
78,"Pull CDN – the dominant method since it’s less hassle for users. The first visitor loads the CDN (experiencing delayed page load) but all subsequent visitors get faster speeds. Sites with low traffic may feel it’s not worth it, since CDN cache expires before next visitor arrives.Push CDN – higher performance since files are manually “pushed” to the CDN’s mirror servers by you or your CDN management plugin. With push CDN’s, no visitors will ever experience a “slow CDN pull”. Downside is it’s more management to push files and things can break or look incorrect if visitors reach a site CDN missing updated files."
78,Traditional CDN’s vs Cloudflare
78,"Technically, Cloudflare is not a CDN but has CDN-like features. It’s actually a DNS service. You enable it by pointing your domain (from your registrar) to Cloudflare’s nameservers. Then manage your DNS functions from Cloudflare, pointing it back to your webhost and enabling Cloudflare’s performance/security features.Traditional CDN’s work by supplying you with a domain zone (like “123yourdomain.maxcdn.com) where your assets are mirrored. Then you configure your site to load all static assets from this zone. You can also mask that CDN domain behind yours (like “cdn.yourdomain.com”).Both traditional CDN’s and Cloudflare can be managed by a CDN plugin on your site. You can also manage them from your cache plugin (most convenient). This way, your cache plugin purges both server cache and CDN cache at the same time."
78,"80. Cloudflare optimization (INT, LOW-MED)"
78,There are many little optimizations you can do with Cloudflare aside from just enabling a CDN. There are also some settings you shouldn’t touch (because they potentially break your site or hurt performance). Please follow my guide:
78,Cloudflare settings guide (best performance)
78,"Yes, I think you should use Cloudflare even if you don’t want their performance/security features. You can use them only for DNS functions. Their proxy is easily disabled from the DNS tab; just turn all the orange clouds to grey."
78,"81. Hosting videos locally vs externally (INT, HIGH)"
78,"Don’t host videos from your server (especially if you need them to load fast) OR you have high traffic or large videos.CDN can help serve locally-loaded videos.Video hosts (Youtube, Vimeo) can make external-hosting cheaper.S3 or Vimeo are great for hosting private videos."
78,"If your video is auto-played on page load, you need the fastest load possible. I don’t recommend loading them from your server since that can quickly run up bandwidth limits and slow down your server."
78,"CDN – less technical work as your videos still sit on the server. CDN will serve the video quickly worldwide, but can get expensive if you have big videos and/or lots of traffic.Free video hosting (Youtube, Vimeo) – low cost, integrates with video community, but loads slow external scripts. You can get around the external scripts using iframe-lazyload but that won’t help for auto-play videos.Paid external hosting (S3+Cloudfront, Vimeo business) – costs money but gives you more control over how your videos look and also allows you to make them private, only viewable from your sites. (Good for members-only content.) Some services like Wistia, are really expensive but come with conversion-tracking features and other junk, hahaha."
78,Hosting videos elsewhere is always a great idea. The only issue is making sure they load fast enough and don’t run up your bills. Storing them in S3 is handy if you like having control and flexibility over the videos.
78,"S3 is great for integrating with other software, like ecommerce or download management, but requires more technical work and can costs money. S3 is good for video downloads and streaming, but can be slow for overseas users. Therefore, you should always integrate S3 with Cloudfront CDN!S3 and Cloudfront are both Amazon AWS, making them easy to integrate."
78,"Vimeo paid plans are absolutely awesome; I love them, too. Easy to generate featured images (using plugins), easy to put into your content (it just embeds). Can control which domains it shows on. Comes with nice video reports, counting views, other user tracking."
78,"My last note is that if this video is on your home page right at the top…it’s probably worth testing different methods to see which one loads it the fastest. Maybe from your server, maybe S3 + Cloudfront, maybe Youtube/Vimeo…who knows, test it! All other non-essential videos, their slowdown is more because of the video player than the actual video itself. And for those, I have more tactics. 🙂"
78,"82. Hosting large files (INT, MED)"
78,"Just like with videos, you should not host large files on your server at all. Even if they’re seldom accessed or downloaded. Just one person downloading a 1gb file from your server is enough to slow down your regular web-traffic. Also, it’s more sensible not to have giant files on the server when doing site backups (waste of space)."
78,"Use S3 to store the files.Combine it with Amazon’s Cloudfront CDN feature if you need faster download speeds to visitors around the world.Large files for me is anything above 50mb. If you have just one 200mb that rarely gets downloaded, ok fine. If you have a 1gb file or many 200mb files…probably not.Some servers are stronger than others, right?"
78,"83. Managing a large media library (INT, MED)"
78,Some sites have such a massive media library (tons of images) that it no longer fits on the web-server. This is a performance issue because the web-servers disk is usually the fastest and most convenient place to retrieve website files. Putting your images anywhere else is likely to be slower retrieval.
78,"You have a few choices in moments like this, each with pros & cons:"
78,"Upgrade the server plan – easy way to get more space and better performance without changing your site much. But costs more and still might not be enough space.Offload to S3 – using the fantastic WP Offload Media plugin. It allows Cloudfront CDN integration, too! I love this method but some people might not like the cost.Add block storage – theoretically fast enough for application-use and also affordable ($1/month per 10gb) but I feel like it’s messy and not all that flexible. Sure you can mount it to just some directories or even the entire wp-uploads directory.ORRR you can just delete unused media sizes if you want to quickly shrink your library. I’ve seen sites go from 100k to 10k images with no design changes!"
78,"Yes, there are alternative plugins that do what WP Offload Media does, but none of them with the same respect as its development team (Delicious Brains)."
78,8. External asset optimization
78,"Optimizing external assets is very difficult if not impossible. These are all the files that load from somewhere other than your web server (thus “external assets”). This can be webfonts (Google fonts) or font icons (FontAwesome), images, JS scripts for Google Analytics, JS for conversion tracking (Facebook Pixel, Hotjar), JS script for affiliate (Amazon), JS library, JS script for Youtube (or Vimeo), embeds like Facebook box or Twitter, JS for social media share counts, and on and on and on."
78,"The hardest part about this is that their servers aren’t always the fastest (no duh, a free service is not gonna dedicate all server resources to you) and their files aren’t always the lightest. You may have seen complaints from page tests about them not being browser cached for long enough, or being too big, etc. Whatever the case may be, you have little control over files that aren’t loaded on your server!"
78,So what can we do?
78,Load them locally.Load them early (aka “prefetch”).Load them late (aka “defer”).
78,"84. Audit all external requests (INT, MED)"
78,"Do you even know what’s loading externally from other domains? It’s funny but I see many clients unaware of all the external calls made from their site. To check: open up your Developer Tools, click on “Sources” tab and load all your site pages."
78,Common (unexpected) external requests:
78,"Absolute urls – to your own page. Technically not an external request but try to use relative urls. If you’re doing for outdated SEO tactics, stop it.Stock theme images – default images linked from the demo site (even when they exist on your site).Old conversion scripts (Hotjar, Pixel) – that you stopped using but forgot to remove from your theme files.Unnecessary crap – any images, CSS, JS scripts or libraries, loading for things you don’t even use."
78,"85. Load simple assets locally (INT, MED)"
78,"Anything simple enough for you to load locally, do it. Images, CSS, JS…copy them to your site."
78,"86. Load webfonts locally (BEG-ADV, HIGH)"
78,"As I’ve already explained, you can load webfonts locally to reduce their speed impact."
78,"Best if you can do it manually (and also to subset out unnecessary stuff).You can also use handy plugin like OMGF | Host Google Fonts Locally.Don’t use FontAwesome! (use other ways to show icons)Btw, Swift Performance cache plugin has a cool Critical Font option to optimize for Font Awesome."
78,"87. Load Google Analytics locally (BEG, MED)"
78,There are tons of hacks out there. Some of them do excessive stuff like cutting down the JS file to the bare minimum or loading the GA script from public CDN (jsDelivr). Other methods are tedious hacks liking copying the GA script to your server and running cron commands to keep it updated. I think that’s fine for totally small sites without any conversion tracking. But otherwise…keep it simple and use CAOS plugin (my favorite).
78,"CAOS | Host Google Analytics Locally (my favorite and easiest method)How to Load Google Analytics 10x Faster in WordPress (WP Speed Matters) – fun guide fellow WP speed addict, Gijo Varghese.Flying Analytics plugin – built by Gijo to locally-load GA. Has 3 useful settings depending on your GA use."
78,"88. Lazy loading videos (INT, HIGH)"
78,You can use the “lazy load iframe” feature in your cache plugin.Do NOT lazy load any videos at the top of your homepage!
78,"If you’re embedding videos through Youtube, Vimeo or other iframe, there are plugins that not only lazy load it but also have extra features:"
78,"Load only on user interaction – mouse move, scroll, or mobile touch. Great idea!Lazy load sensitivity – loads the iframe once the screen gets close. Great idea!Pseudo image – loads an IMAGE of the video player with fake play button, and only loads the real video if clicked. Very clever! (Usually only compatible with Youtube, some can do Vimeo as well.)(Many of these can be found in Swift Performance.)"
78,"Even if you don’t have all those fancy features in your cache plugin, just being able to lazy load iframes alone is a huge difference. Another way you can manually hack things is to show a fake image of the player which then opens up a lightbox modal with the video inside. Anyway, I’m sure more native browser lazy load is soon to come."
78,"Regarding best plugins to do the pseduo video image effect, I suggest you test on your own. Try search the WP repo for “video lazy load“.If there’s any concern with lazy loading, I do wonder if it affects your SEO since crawlers don’t see the video loaded. Who knows, right?"
78,"89. Google Maps optimizations (BEG, HIGH)"
78,Don’t be one of those newbies loading the Google Maps box right on your site! It slows your site down by a lot!
78,"Put an image of the map instead, then link to Google Maps from that image or from some text under it.Or use “lazy load iframes” option from cache plugin to delay the Google Map load."
78,"90. Social media integrations; Facebook, Twitter, Instagram (BEG, HIGH)"
78,"Have you ever wanted to show the Facebook like box right on your page? Just don’t! It lags like hell. Will cost you a solid 1 second. If you want, put it on a specific page but not your home page or global widget!Same goes for Twitter box. What about an Instagram stream? For whatever reason, those have been less laggy for me. I like the Social Feed Gallery by QuadLayers.What about Facebook Pixel? Sorry guys, I don’t have any methods yet."
78,"Unless these boxes are your main page content….trust me, you don’t need it. It’s not worth the tremendous slowdown they cause, especially on mobile devices."
78,Still insist on having a social feed on your site? Maybe this Social Feed Cloudflare app can help.
78,"91. Comments and Gravatar (INT, LOW-MED)"
78,"Comments are a problems most people don’t have…until their blogs get popular and there’s hundreds or thousands of comments. In fact, even 100 comments on one post is enough to chip away at your page speed and start making your site less fun to visit. We have a few concerns to look at and different ways of tackling them."
78,Which comment system to use? And how they deal with 3rd-party assets.
78,"The built-in WordPress comments system is fast and free but loads all at once. This can be annoying if you don’t want all 500 comments (and their Gravatar) loading on one page. It’s not only a speed issue but a design issue. Of course, there are work-arounds."
78,"You can limit how many comments are loaded. Some people paginate comments. On my busiest site, I put a button to [LOAD COMMENTS] on mobile, so users aren’t intimidated by a giant scrolling page on their phone. You can also use alternative avatar systems, or cache your Gravatars (via cache plugin or specific Gravatar cache plugin). Or not use any comment avatar at all!I personally prefer native WordPress comment system."
78,"Disqus is a 3rd-party commenting system that’s no longer free, but has lots of engagement features and looks nice (styled cleanly, and only loads recent comments). The big con aside from the price is that it loads 3rd-party assets on all pages even if they don’t have any comments."
78,The Facebook comment is also a great option and FREE. It’s fantastic for getting tons of engagement and doesn’t show all comments at once (like Disqus). It also racks up your Facebook share count number with each comment. I only don’t like it because of the external loads.
78,"Please, no Thrive Comments! If you’re attracted to Disqus you might consider Commento but beware (you can’t migrate there natively from WordPress, yet…you have to pass through Disqus first)."
78,"92. Deferring chatboxes (BEG, MED)"
78,"Many of you have slow page loads because of your (sales/support) chatboxes. These aren’t related to your page design and shouldn’t get in the way. Luckily, they’re easy to deal with."
78,Manually put the code in the footer.Defer JS – most cache plugins have this.Lazy load JS – Swift cache has this. It’s fantastic.
78,"93. Speeding up email forms (INT, MED)"
78,"If you’ve got newsletter forms on your site to integrate with email marketing services (Mailchimp, MailerLite, etc), you’ll notice many of them load annoying 3rd-party scripts. Some of these JS are for form security (preventing spam registrations), others are for other functions (like conversions)."
78,Here are my tactics for dealing with them:
78,"Don’t load email forms globally – don’t put them on every page! I also hate them from a UX point of view (annoying users). You can try just a “SIGN UP” button or link that redirects them to the newsletter page. But yeah, that might affect your conversions and take them off the page.Use a form that doesn’t need external JS – some email services can do it.Load the form locally – but risky if it stops working one day!Always manual embed – instead of automated through a plugin."
78,"Ultimately, the service you pick will determine your options. Some have JS-free options. Others require JS no matter what."
78,"94. DNS Prefetch (INT, LOW)"
78,Enable DNS prefetch by putting prefetch tags in your header.Or do it through your cache plugin.But please don’t install a performance plugin just for DNS prefetch.
78,"A great tactic for speeding up external asset calls is to speed up their DNS wait times. What slows down external assets is usually the network latency time to send a request to the 3rd-party server. The file itself may be very small but the network time is what delays it. By putting a DNS prefetch to external domains, your server makes the HTTP request earlier during the site load so the asset loads quicker when it’s needed."
78,Common URLs to prefetch:
78,Google fontsFacebook PixelOther social media networksChat scripts?Conversion scripts?
78,Don’t know which URLs to prefetch?
78,"Run a Pingdom speed test – and look at the domains listed.Or to get a full list – open up your browser’s Developer Tools > Sources (tab).You should browse several pages of your site to make sure you get all the common ones.Only prefetch the root domain/subdomain, not the entire URL.Also not necessary to preload all domains that you see.Be careful of ad-related domains that change on each page load. (You shouldn’t prefetch these.)"
78,"95. DNS preconnect (INT, LOW)"
78,"“Preconnect” is the lesser known brother to “prefetch”. Arghhh, here’s comes an explanation:"
78,"preload – preloads assets for the current page.prefetch – preloads only the DNS call for the next page.preconnect – preloads the DNS call, TLS negotiation and TCP handshake…for the next page."
78,"Preloading shouldn’t be used as it can eat a lot of resources and doesn’t make sense for WordPress unless you know exactly why some assets need to be loaded first (and can’t be prioritized in other ways). Prefetch aka “DNS prefetch” is a safe low-resource way to preload calls, and heavily supported by browsers. It conveniently establishes the DNS connection so assets coming from external domains will be downloaded quicker when requested."
78,"Preconnect is like prefetch but does a little more (it establishes the TLS/TCP connection as well as the DNS) but isn’t as popular for several reasons. One is because it wasn’t supported by as many browsers. Another is because there’s a simultaneous connection limit in most browsers. Ultimately, you have to decide what’s most important to preconnect, and the rest can be"
78,prefetched.
78,"Limit preconnect only to priority connections.Some sites do preconnect before prefetch. Others do opposite but also prefetch the preconnect-ed ones. Some also put prefetch in the same tag as the preconnects.I probably wouldn’t preconnect more than 4 domains. Prefetch, you can do as many as you want.I personally don’t bother with preconnect. Only some very big sites do. The preconnect and dns-prefetch resource hints – Web Platform NewsDNS-Prefetch and Preconnect – One, or Both? Fallback? – Stack OverflowUsing Preload and Prefetch in Your HTML to Load Assets – Alligator.io"
78,"96. Cloudflare apps (INT, MED)"
78,"It’s relatively new and I haven’t played much with it. But it looks extremely promising! The new Cloudflare Apps allows developers to create applications and services that integrate at the EDGE-level (aka “DNS”) rather than at the SOFTWARE-level (aka WordPress, php). This opens up a wide range of possibilities because:"
78,"The applications/scripts are loaded from Cloudflare servers! (Making them faster.)It’s less work for developers. Their app can be applied to any website (not limited to any CMS, or WordPress)."
78,Check it out yourself! https://www.cloudflare.com/apps/
78,"See which apps you use are on there.Tawk.to, Facebook (chat/like/comments), social feeds, chat boxes, forms, support, media, etc.There are so many!!!"
78,"Really incredible stuff as they don’t load anything from your server! No plugins to install or configure. Just enable from your Cloudflare account. Amazing! (This could completely change the game not only for how future applications and services are integrated with websites, but make slow websites a thing of the past.)"
78,9. Security optimization
78,Hacking and cyber attacks can cause massive server performance problems if not outright interruptions. Many people have no idea how often servers get attacked because they never see the logs. I can assure you every server will get attacked several thousand times (sometimes in one hour) every month. Your site might even be attacked right now but you just don’t know it.
78,"Securing against these attacks requires a delicate balance. You don’t want a server that lets hackers freely bombard your ports and resources. But you also don’t want a server that’s too secure and excessively auditing all traffic that it slows your users or worse (it blocks legitimate users). Of the recommendations below, follow the ones you have access to."
78,Servers hosting only you and/or few tenants:
78,Easier to secure because there are fewer sites to invite hacking and you can safely disable many ports without affecting your sites.Harder to secure if you’re managing it yourself and lack experience.
78,Servers hosting many tenants:
78,"Easier to secure if there’s admins already monitoring the server. Many common attacks already secured against.Harder to secure if many services need to be open for different client use cases. Therefore, more sites and open services to invite hackers."
78,You should read my guide on How to HARDEN a Linux web server.
78,"97. Shutdown unnecessary server services (ADV, HIGH)"
78,"You can think of unused services as unused phones or email accounts. They sit around eating up resources (MEMORY) and take up your time with unwanted connections (SPAM, HACKERS). Whatever you’re not using, disable it from your server!"
78,"DNS – disable if using external DNS server. (Cloudflare, DNSME, etc.)Email – disable if using 3rd-party email. (G-Suite, MXroute, etc.)FTP/SFTP – disable if not using.Other proxies – like Varnish."
78,"Many of these services are enabled by default with your server stack or control panel. You can read their documentation to get a list. For services that need to be running, you can limit their exposure to bad traffic using firewalls."
78,"98. Server firewall configuration (ADV, HIGH)"
78,Most default firewall configurations are set too lax to avoid causing issues. You should jump in there and block off as much as possible. Some example logic below:
78,"Ports used by specific people (SSH, FTP) – only you or a few others. Do an IP whitelist and block the rest. Ports used by certain country (POP3, IMAP, FTP) – if some services are only used from within one country, you can block all other countries. Be careful though as someone traveling will lose access!Ports attacked only by certain country – if you have many attacks coming from certain countries or regions, you can ban by country or entire IP ranges."
78,There are many server firewalls out there. Each with their own pros and cons and recommended for different uses cases. You can read up online how others use and configure them. It’s easiest to start with the default one that comes with your stack.
78,"99. Server brute force protection (ADV, HIGH)"
78,Brute-force protection is like a smart firewall. It leaves services and ports open but automatically bans the obvious offenders.
78,"It automatically bans anyone putting in the wrong authentication, or using blacklisted generic user-names, etc."
78,They’re easy to set and very powerful. Just be careful that they don’t block legitimate users/traffic. You can see what brute-force or DDOS protection came with your server and enable it. Maybe don’t set it so strict if you have many users on this server.
78,"100. Brute-force protection on wp-login.php (BEG, HIGH)"
78,"The WordPress admin login page is often bombarded by bots trying random user-names and passwords to get through. While they might not get in, their constant attempts eat up lots of resources. There are several ways to prevent them, each with their pros and cons."
78,"Server-level brute force protection – easy and efficient but can lockout legitimate users on busy servers with sites using Cloudflare. Problem is brute-force lockouts block by IP and visitors coming in through Cloudflare all share the same (proxy) IP. Sure, you can configure to pass true client IP through Cloudflare headers but this slows down page load!Application-level brute force protection – many WordPress security plugins can do this. They secure the login page by banning users with bad credentials.Some plugins hide the login page – moving it to a different URL. Just make sure the standard login URL is either blocked or cached to prevent visits on it from using resources.Other plugins protect the login form by putting a captcha and banning certain robots, crawlers, devices. This can work well but might annoy or false-flag legitimate users."
78,Only server I know with native brute-force protection on wp-login.php is LiteSpeed. All other servers (Apache & NGINX) will have to enable it with either a security plugin or http auth.
78,"101. HTTP authentication (BEG, MED)"
78,Do you have specific pages being bombarded and no convenient way of blocking access to them? HTTP AUTH is a quick-and-dirty way of locking out all users. Only problem is it’s a slight hassle for legitimate users. Most guides show you how to protect the wp-admin directory but you can protect other frequently-visited ones as well.
78,Setup HTTP AUTH on Apache/LiteSpeedSetup HTTP AUTH on NGINXHandy HTTP AUTH passwords generator.
78,"102. Disable XML-RPC protocol (BEG, MED)"
78,"The XML-RPC protocol allows external apps (like mobile apps), to log into your WordPress and edit content or view WooCommerce sales. Unfortunately, it’s often exploited by hackers and bots brute-forcing their way into your site."
78,"If you don’t use it, disabling XML-RPC prevents server slowdowns caused by the thousands of XML-RPC hack requests.If you need to leave it on, you can whitelist your IP’s (and also for Jetpack, if you use it)."
78,"103. Security plugin configuration (BEG-INT, MED)"
78,"If you don’t have access to your server, you can use security plugins. Yes, security is more efficiently run at the server level (closer to raw computing power) than at the application level (slower PHP processing)…but sometimes, it’s hard to set global security rules when you have many clients/sites and each one needs something different."
78,"Nonetheless a software-level security plugin like WordFence is still a useful option to block attacks that the server doesn’t, and/or prevent hacked sites from doing more damage."
78,"My favorite WordPress security plugin is WordFence.Most important feature of security plugins IMO is malware scanning. Scan manually or schedule during low-traffic hours. This feature doesn’t necessarily improve website speed, it detects system exploits and prevents them from using up resources (hosting spam sites or attacking other servers).The firewall features on security plugins probably aren’t needed if you have a server firewall already. Firewalls activated at the PHP level slow all incoming requests."
78,"The performance problem with security plugins is due to A) over-aggressively filtering all incoming traffic, and B) scanning too often. Both eat up many resources especially on large sites with many pages and visitors. I suggest not using software firewall, and also to set your malware scans to a slower speed."
78,"104. DNS edge-level security configuration (BEG, MED)"
78,Remember how I said that security is more efficiently done at the server level than at the application level? Well doing it at the edge level (DNS-level) can be even more efficient than at your server level since it’s using someone else’s servers. There are some performance implications between dealing with security at the edge VS on your server. You can decide what works best for your use case.
78,"Dealing with security on your server can be more convenient since you have more control. You can optimize for your specific use. Only downside is it uses your server resources and also that you need admin skills.Dealing with security via another server (like DNS proxy, Cloudflare) or security service (Sucuri) saves your server precious resources but might add slight load delay issues since visitors are passing through an extra proxy before reaching your web-server."
78,"The weaker your server and server-admin skills, the more likely a security service is more efficient at blocking DDOS requests. Then again, for a smaller site you might not have so much security problems. Whatever you do, don’t try to put overly-aggressive DDOS security at both levels (DNS & server). This can cause false-positives where legit visitors are blocked because all visitors (good and bad) share the same IP when coming through a proxy."
78,"Most people don’t have to worry about DDOS attacks, ok? Most lower-level DDOS attacks are easily handled by your server.The highest-level DDOS attacks are the ones that overwhelm servers (even with good security) but they cost money and concentrated effort from hackers. Unless someone is specifically targeting you, you don’t have to worry about them.Easiest way to deal with high-level DDOS attacks is to immediately sign up with a dedicated security company like Sucuri (when it happens).I don’t recommend paying for fancy security services that you mostly won’t need."
78,105. HTTPS and HTTPS redirect
78,"(BEG, LOW)"
78,"You should absolutely be using HTTPS. (It’s the only way to get the benefits of HTTP/2 protocol.)Put 301 HTTPS redirects on your server so visitors are quickly redirected to the proper HTTPS protocol and correct domain version of your site (with or without “www”). Without these server redirects, WordPress can still do it but it takes a little longer."
78,"Also, don’t forget to make sure all your internal urls are using HTTPS (follow step 3). Don’t rely on SSL plugins (unnecessary) or WordPress (slow) to redirect you. Set the redirects from the server!"
78,"Bonus tip: if using Cloudflare, set a page rule to do your HTTPS 301 redirects from there as well. (Even faster than from local server!)"
78,"106. Web Application Firewalls, aka “WAF” (INT, MED)"
78,"WAF are enabled through your server security stack.The easiest way to block bots without any performance loss is manually through htaccess or global server config. But this can be too much manual work when managing many sites. Therefore it’s safer to have WAF for them.If you’re using ModSecurity, check out these ModSecurity performance tips from Trustwave and Packt.You may also enable a high-performance WAF through Cloudflare security rules, or 3rd-party paid security services…like Sucuri."
78,"Most people don’t know the difference between network firewall and web application firewall (WAF). Network firewall is for opening and closing ports. WAF is for blocking bots through open ports. You have to be careful with WAF security because it can slow down your server since it checks all incoming traffic (regardless of good or bad). Due to performance reasons, I mostly avoid WAF if I can but my style of server management might not be feasible for others."
78,10. Bad optimization (tactics)
78,"All the unnecessary, illogical or outdated optimization advice passed around on the internet. Listed here (along with my thoughts) in case you were curious. Some sites are slow because of their users!"
78,Most of these tactics are bad because:
78,"They don’t fix the root problem.They don’t increase speed. Some make it worse.They are outdated/irrelevant for today’s technology.Can break your website design or functions.Increase your server load.Make nearly unnoticeable benefit if any at all.Only work in limited scenarios.Only give you better page scores, but make the user experience worse.Their benefits don’t outweigh the disadvantages."
78,Bad webhosting optimizations:
78,107. Horizontal-scaling (adding more servers).
78,"Scaling will never be faster than running all services locally (off one machine). Chaining servers together means your data has more proxies to jump through.Horizontal-scaling is meant for preventing high-traffic from slowing down your base speed. It can’t speed up a low-traffic environment.The only scaling that increases baseline performance is vertical scaling i.e. “upgrading server resources” like CPU, memory, disks, etc."
78,108. Buying an expensive server.
78,"This is a bandaid way of fixing a slow setup. The extra money you spend on hosting could easily pay for better recoding.If your site has little traffic yet needs a $300/month server to be tolerable, just fix the code!Expensive servers are noticeably faster for dynamic load only (admin pages, checkouts). If your site is cached effectively, it makes no difference."
78,109. Optimizing for speed tests and page scores.
78,"Did you know that optimizing for high page scores can sometimes give you a slower site? (It’s true!)Why Google Pagespeed, Pingdom, and GTmetrix scores don’t matterHow to optimize for Google Pagespeed, Pingdom, and GTmetrix (for the people who insist)"
78,110. Deploying caging tactics for low-tenant servers.
78,Many people read random guides and tactics (like installing CloudLinux/CageFS) for servers and think it actually helps them.Most of those guides were written for high-tenant busy dedicated servers. And not relevant to this new age of affordable VPS with only a few sites.Caging tactics will slow down your sites because they limit resources.
78,Bad theme optimizations:
78,111. Using overly minimal themes.
78,"If your theme is so empty that you have to load bloated pagebuilders and extra plugins to get the desired look, you might be defeating the purpose of a lightweight theme in the first place! Maybe consider a nice child theme or prebuilt site design for Genesis or GeneratePress.Maybe consider Artisan Themes."
78,112. Switching to static CMS (instead of WordPress).
78,Dumb move. (WordPress vs Static CMS)Static CMS are fast because they’re simpler.They can’t do all the fancy designs and functions that WordPress can.Switching to static CMS actually requires more development skills!
78,Bad plugin optimizations:
78,113. Installing multiple performance plugins.
78,"The only one you need is a cache plugin.Do not install multiple performance plugins! (Browser cache, disable wp-embeds, CSS combine, lazy load, etc.)Don’t install “booster” plugins for other plugins, either. They’re likely using autoloads (memory) to speed it up…but that only speeds up the specific plugin at the expense of your entire site load."
78,114. Using (conditional load) plugin organizers.
78,"If you have to tell plugins not to load on pages where they aren’t used, it’s probably not a good plugin in the first place.It’s best if you just remove or replace those plugins."
78,Bad image optimizations:
78,115. Lazy loading images.
78,"Yes, I’m aware my belief goes against what many people think.I don’t care, I hate lazy load.Sure, it gets better page scores but slows down the user experience.Sites always look better with it off!"
78,116. Over-compressing images.
78,"Don’t over-do it to the point that they look ugly.Maybe make them smaller?Or if you lower their quality, darken them and put text on top so it’s less noticeable."
78,117. Excessive inline SVG’s.
78,"It’s fine if you have like 1 or 2. Any more than that and you’re just making your HTML bigger.SVG’s are almost always not critical items.AND they’re usually loaded on every page. Let them be separated requests from the HTML so they can be browser cached.If you really want lighter weight, create a custom icon font!"
78,118. Leaving image compression backups on the server.
78,"If using image compression plugins, delete their backups off your server to keep it light.If you want to keep the originals, download them to your computer."
78,119. Media cloud hosting service.
78,"Did some fancy company convince you to offload your images to S3? And use their fancy CDN service to “load your images faster?”I can assure you it’s a dumb idea. Image assets will most likely load faster from your origin server than through a slow external storage server like S3.S3 is basically only a last-ditch effort if you run out of space on your server and don’t want to pay for an expensive server (with fast disks) elsewhere. Let me repeat, S3 is slow storage.And it’s because that S3 is slow that those media-cloud plugins and services feel the need to add a CDN to it. Don’t let them fool you. Loading images from a slow external storage through a CDN proxy is going to be slower than loading off your origin server.If you can, load from your local server. And if needed, use a CDN with your local server.The only time this external storage & CDN method is ever useful is if you have heavier video files or so many images, or you want to save money, or you have so much traffic that your CDN is always warm. I would also add that if you’re going the external storage route, you should use a more proactive push-CDN than a pull-CDN."
78,Bad caching optimizations.
78,120. Enabling every feature on cache plugins.
78,"Don’t be the idiot breaking their site with cache plugins.Don’t enable separate mobile caching or AMP if you don’t have it!Don’t cache private or logged-in users unless you have to.Don’t cache WooCommerce cart sessions (it’s almost never needed).If you don’t understand a feature, check the documentation or ask for help.Use only what you need. Less is more!"
78,121. Enabling object caching when you don’t need it.
78,"It’s only meant for dynamic pages.If all pages are static, using object caching can slow it down.Unnecessary object caching can also eat up memory, improperly configured object caching can serve old data."
78,122. Browser caching for too long.
78,"Browser caching is usually intended for static assets that rarely change (images, CSS, JS).Safe settings can be 1-7 days so they’re not downloaded again if the same browsers revisits within that time period.Aggressive settings can be up to 30 days or even 1 year.The problem is if you change the images or CSS within that time period, some users will see the old version."
78,123. Preloading pages.
78,"Clever tactic of preloading pages before you click on them! (Then they appear instantly when clicked.)Some will preload all available links. Others guess or preload only what you hover your mouse over.There are many technical implications regarding preloading…such as extra server requests (for items not even clicked), screwing with conversion/affiliate tracking, accidental self-DDOS, can even break page design or functions because of CSS/JS loaded at unexpected times.My biggest detraction is that they try to speed up sites by using extra server load on a server that wasn’t fast enough in the first place. Sure, it can work on a slow server or bloated site if it doesn’t get much traffic. Anyway, be careful when you use it!Quicklink vs Instant.page vs Flying Pages – Why I built Flying Pages (WP Speed Matters) – great guide comparing preload plugins by Gijo Varghese."
78,124. Static WordPress plugins/services.
78,"I think these are more complicated (manual) ways of using cache.But can be useful for the power user who knows what he’s doing.For everyone else, you’re likely to have more troubles from using them.Want to try? See for yourself…WP2Static and Shifter."
78,Bad asset load optimizations.
78,125. Combining CSS/JS assets.
78,"I hate combining CSS/JS.Can break site designs, delay initial response, increase server load, or even slow down your sites. The benefits aren’t even noticeable (except for silly page tests).The benefits are better on big bloated sites, but so are the drawbacks. Just test with it on vs off. (And test with your eyes, not via page tests.)"
78,126. Generate Critical CSS.
78,"Unless your site actually needs it AND you’re carefully generating the critical CSS, there won’t be much benefit.Often slows down your site load, break styling, or create FOIT/FOUT issues.The only effective way to generate Critical CSS is to do it manually. But most people don’t have that skill and do it via automated plugin (which often includes unnecessary styles and misses necessary ones).Totally unnecessary for already lean sites.My full rant against critical CSS."
78,Looking for tools that can generate critical CSS?
78,Try Paul Kinlan’s bookmarklet for Google Chrome.Google “critical CSS generator”.
78,What’s the problem if you don’t generate critical CSS perfectly?
78,"A) your site doesn’t look right, suffers FOUC for a little bit and then finally loads everything.B) doesn’t look right periodC) looks right but still loads too many unnecessary styles.D) worst case scenario, you get some combo of multiple above."
78,127. Inlining CSS.
78,"Mostly dumb idea.Intended only for page-specific CSS, like simple 1-liners that don’t need to be an extra HTTP request.Not intended for outputting entire CSS into the HTML. This adds extra load to each page that could have been globally cached.Will actually slow down subsequent visits since that CSS has to be re-parsed again and probably not even cached.This tactic is only for ultra-light pages with nearly zero styling. It’s terrible for speeding up bloated pages."
78,128. Removing query strings for CSS/JS assts.
78,"Maybe some page test told you to do this.But then what happens when you do it? CSS changes take forever to show on browsers that already saw your site (since they have the old version cached).Good job, genius. Now you’re stuck with cache-busting issues.This tactic is already outdated since most browsers and services (like CDN) can effectively cache assets with query strings.If you still insist on it, at least wait until you’re absolutely done with the site design. Or else design changes will take longer to show."
78,129. Defer CSS or JS.
78,"Another potentially stupid idea. (Usually done by users trying to avoid “render-blocking” warnings.)It can delay critical CSS/JS used at the top of your page (like for sliders or other content effects).If you’re deferring and delaying CSS/JS load for critical items, your page may get higher page scores but appear slower for visitors.FYI: some assets need to be render-blocking for a reason. It’s because they control how the content looks. For example…do you want your carousel images to load BEFORE the carousel? Or do you want your text to load before the font? NO!…because it’ll look ugly or all jangled up!"
78,130. Cache-all using Cloudflare.
78,"Many have tried, many have failed.Seems like an incredible trick until you realize some function broke. I don’t recommend even trying this unless you have a simple site (that probably won’t need it anyway).Still want to try? Caching WordPress Pages using Cloudflare Page Rules (WP Speed Matters) or the WP Cloudflare Super Page Cache plugin."
78,131. Bad Cloudflare settings.
78,"Rocket Loader – don’t enable this! It breaks stuff.SSL/TLS on Full (strict) = don’t use this option as it makes your SSL handshake take longer. Just use the default option! It is fine and everything will look normal with the padlock.HSTS – leave this off! It will prevent browsers from displaying your site if you ever have SSL problems.Cache entire page with page rules – already mentioned above.Just follow my Cloudflare settings guide, ok?"
78,Bad external asset optimizations.
78,132. Using CDN when you have a local business.
78,Completely unnecessary! Doesn’t help speed at all!PS: Cloudflare can be used without its CDN functions. You can use it for DNS-only.
78,133. Using Google AMP.
78,"The main benefit of AMP in my opinion, is better syndication and SEO through Google’s search engine results.But I think it helps only for blog and news type of sites.AMP is not a good solution for speed. It adds more complexity to your site and often breaks functions and design. Worst of all, it doesn’t exactly help your speed and makes your site (and server) work harder to cache things."
78,Bad security optimizations.
78,134. Excessive security deployed.
78,Aggressive firewall filtering – slows website visits.Security proxy (Sucuri) – slows visits.Brute-force security too strict – blocks legitimate users.
78,135. Mixing server security with Cloudflare security.
78,"Can block legitimate users since all users coming from Cloudflare share the same IP. And if hackers trigger an IP ban, many legit users will be banned!Be careful when using Cloudflare proxy with server security.Not usually an issue unless you’re on your own server. To clarify: the issue is usually for logged-in users, regular public visitors usually won’t have any problems."
78,135. HTTP Strict Transport Security aka “HSTS”.
78,"Dumb idea! Very little benefit but god forbid you have any random SSL issues (SSL didn’t renew or missing after migration), your site will not load.It’s a giant risk and always causes nightmares eventually.Don’t you ever turn this on! I scream at all clients who do this."
78,What’s the secret to speeding up WordPress sites?
78,"For me, it use to be finding endless tactics and places to optimize. And it worked. I was able to drop from 4 seconds to 1 second. Then 1 second to 500ms. Then 500ms to 380ms, then painstakingly chip off every 10-20ms. Major gains at this point were only 50ms."
78,…but this isn’t how the real world works.
78,Ain’t nobody got that kind of time. Ain’t nobody want to spend that kind of money.
78,"Speed optimization for the real world requires intuition. You have to be able to look at a site, smell it, and know right away what it needs. So you don’t waste a bunch of hours on tactics that make no difference."
78,The problem with most speed optimization tactics and speed-up services:
78,"They use a general formula that doesn’t work for every site.They do things the easy way, not the best way.They don’t account for the user’s workflow and content intervals.They don’t account for the type of webhosting (and it’s limitations).They don’t account for the user experience.They almost never do any manual optimizations."
78,They only strive for A+ page scores and lower total load times…which aren’t anywhere near as important as TTFB and content paint times.
78,"Anyway, I hope this guide helps you. Or at the very least, helps you find people who can care for your site as much as you do. Speed optimization is a masterful art combining the skillsets of web developer, server admin, UI/UX designer, and website owner who’s actually owned a high-traffic website before."
78,Want to learn more?
78,Check out my WordPress Speed Optimization Courses
78,Share this post:Share on FacebookShare on TwitterShare on LinkedInShare on WhatsAppShare on EmailShare on SMS
78,"Read all my posts on WordPress hosting Popular RecommendedAbout JohnnyRight on the edge of WordPress development! 10+ years of WordPress design, development, hosting, speed optimization, product advisor, marketing, monetization. I do all that."
78,More WordPress Guides
78,Video-Selfie Camera Angle (and how it affects your marketing “voice)
78,Expected conversion rate for membership sign-ups?
78,Helpful links for new copywriters
78,GeneratePress vs Genesis – WordPress Theme Review
78,How Do You Stay Motivated?
78,How to Make Subtitles for Your Videos
78,Reader Interactions46 Comments
78,Richie
78,"April 23, 2020 at 4:22 am"
78,Excellent stuff (again!).
78,Thanks Johnny.
78,"A quick question, if I may?"
78,It relates to PHP.
78,One of the first things that I did when switching to Krystal hosting was to update the PHP version to 7.4.
78,All well and good.
78,"However, whilst looking at cPanel I noticed a ‘Server information’ link and took a look."
78,This says that the PHP version is 5.6.40.
78,Is this something different?
78,(Sorry – I’m a beginner but trying to understand)
78,Reply
78,Johnny
78,"April 23, 2020 at 11:45 am"
78,"Hi Richie, I know Krystal to be a solid host. You should ask them about it. It might be showing a different PHP version for various reasons…such as cPanel loading on different PHP than your actual site. Or them using a different PHP package that for whatever reason doesn’t register with Apache."
78,You can check using phpinfo file or one of those Server Info plugins.
78,Reply
78,Richie
78,"April 24, 2020 at 7:50 am"
78,"Hi Johnny,"
78,"I asked and, as always, got an instant reply (I really do like Krystal)."
78,Apparently that is the default version of PHP for the server – it has no relevance to the actual version being used.
78,"In terms of helping to speed up a site, by not loading things that aren’t required for the page or post in question, have you ever used anything like the Asset Cleanup plugin?"
78,Would you recommend that approach?
78,Reply
78,Johnny
78,"April 24, 2020 at 9:31 am"
78,Holy crap. They outta default everyone on version 7.x but I understand they probably have so many legacy clients. At least they can default new clients to 7.
78,"I’ve used many asset organizer plugins before. I don’t recommend them as your overall tactic. It’s a big mess to manage over time. Use it only on the biggest offenders and the most critical pages (like home or checkout). And if you think about it, better off just to remove your biggest offenders entirely."
78,Reply
78,sat.paolo
78,"April 24, 2020 at 2:52 am"
78,I thank you for this precious work and congratulations on your site …. it’s perfect
78,Reply
78,Johnny
78,"April 24, 2020 at 2:13 pm"
78,"Thanks, Paolo. I’m glad it helps you!"
78,Reply
78,Jesse
78,"April 24, 2020 at 5:36 am"
78,Or check the server area under site health in WordPress
78,Reply
78,FABRIZIO DI LELLO
78,"April 24, 2020 at 8:19 am"
78,Thank’s for your job. Excellent article.
78,What do you think about themes developed by MyThemeShop?
78,I found the tips in this guide very helpful.
78,Reply
78,Johnny
78,"April 24, 2020 at 9:38 am"
78,"Thank you, Fabrizio. I don’t like MyThemeShop in general. They’ve asked me to do a review but I hesitate to cover mass-produced theme shops like that. I haven’t audited their code yet but from the screenshots, I don’t like the UI. And from just browsing through their site, I 100% don’t like the design. They’re just ripping off common layouts from other places. Their recent Schema theme for example looks like a ripoff of GeneratePress default child theme (which was also copied by Astra IMO)."
78,Reply
78,Damon
78,"May 25, 2020 at 7:58 am"
78,"Hey Johnny. I’ve learned so much from your blog, it’s crazy. Thanks for this gold mine above!"
78,Quick question about forms. You mention that you don’t cache pages with forms.
78,"Does that mean if my client has an email opt-in form embedded in their footer I shouldn’t cache their site at all? I see that you use one on your site that appears to be cached (as well as comment forms), so I’m a bit confused."
78,Could you shed any light into when or what kind of forms I shouldn’t be caching?
78,Thanks again 🙂
78,Reply
78,Johnny
78,"May 25, 2020 at 12:06 pm"
78,"Hey Damon, thanks for the comments. Yes…forms are a major PITA for caching. Not because of the actual form HTML code and CSS styles, but the JS that comes along with it for blocking spam registrations. I’ll leave some thoughts below and you figure out how to deal with them:"
78,"– Forms CAN be cached and mostly still function but they often only work for one entry. Once one person has signed up, every other sign-up won’t work AND/OR may see the previous person’s email. Yes, there’s all kinds of hole-punching cache tactics but even those at best only dynamically load the form, they can’t dynamically load the JS and the sign-up still won’t go through."
78,– It’s most ideal (safest) if you can put all newsletter forms on a separate page and then exclude that page from caching.
78,"– If you need to have a form on every page, it’s preferred to use a pop-up…as that somewhat sort of negates the form being cached. I don’t know the exact particulars right now but it seems only the pop-up mechanism is cached but not the JS trigger upon sign-up."
78,"– Loading newsletter forms in the footer globally is definitely not ideal. Now I feel backed into a corner because I did that myself, hahaha. It’s a long story. I hate email newsletter, I hate sending and receiving them. This world has too much email already and also, I hate pop-ups and I hate begging for people’s info. But when I went without any newsletter whatsoever…I kept getting so many annoying comments and emails begging me to put a newsletter. So I put it to save myself from extra emails. And I deal with its load on my site. I’ll probably locally-load that external JS call and see how it works out. Most likely it’ll be faster but I’ll have to keep updating that call every now and then. Or can also use the HELL plugin. Aghhhhhh….so much to think about."
78,Reply
78,Daan van den Bergh
78,"June 7, 2020 at 12:55 pm"
78,"Hi Johnny,"
78,Wow! Great read!
78,And thanks for mentioning CAOS and OMGF!
78,How’s that post coming along? 😉
78,Reply
78,Johnny
78,"June 7, 2020 at 1:05 pm"
78,"It’s all done! Just waiting for an open day to post. Probably within the next 2 weeks. Great to have you on my site, Daan. 🙂"
78,Reply
78,Henrico Ellis
78,"June 14, 2020 at 10:23 am"
78,Hi Johnny great tips I applied them and my blog is lightning fast. I also use the WP Rocket plugin getting really good results.
78,Reply
78,Johnny
78,"August 6, 2020 at 1:05 am"
78,Awesome! I’m really glad to hear.
78,Reply
78,Marian
78,"June 30, 2020 at 12:12 am"
78,"Hey Johnny, this is a truly great list of tips and I felt the need to bookmark the article. I applied a bunch of the tips and I saw some cool results. I need to find it for sure next time I have to optimize a WordPress website."
78,Reply
78,Johnny
78,"August 6, 2020 at 1:05 am"
78,"That’s great news, Marian. I’m glad it helped you."
78,Reply
78,Chelo
78,"September 18, 2020 at 1:38 pm"
78,"Hello Johnny, I fall in love with your content. Why?"
78,"1- I love the fact that this blog is your unique voice, your honesty, and willing to help us is notable."
78,"2- You always explain things with examples from daily life, making things very simple to understand."
78,"3- The consistency between what you say and what you do, according to what I see in this blog, is inspiring to me and encouraged me to write these words here."
78,Thank you very much.
78,Reply
78,Johnny
78,"September 18, 2020 at 8:20 pm"
78,"Thank you, Chelo! This is heartwarming to hear and exactly what I strive to do here. I share my honest findings to help people."
78,Reply
78,Trevor
78,"September 30, 2020 at 8:31 pm"
78,"Hey Johnny,"
78,I see you are using the MailerLite Plugin but above you said to hard code it in.
78,I’m using the plugin too and feel like its hella slow.
78,Should I be using the JavaScript code?
78,HTML Embed code?
78,Or just keep using the Plugin and suck it up.
78,Reply
78,Johnny
78,"September 30, 2020 at 9:17 pm"
78,"I’m not using the plugin. Mine is hardcoded in using whatever code they gave. Depending on how you show your newsletter form, there may be some tactics for you."
78,Reply
78,Trevor
78,"October 1, 2020 at 7:58 am"
78,Cool.
78,"They give me a Javascript, HTML, and Pop up on Click."
78,http://prntscr.com/urby3h
78,Which do you use and which is lighter?
78,I’m in the process of getting rid of Divi.
78,I’m following your lead going to get rid of page builders.
78,Reply
78,Johnny
78,"October 1, 2020 at 10:09 am"
78,"I would put the JS in the footer, HTML goes on the page where you need the form. Congrats on getting rid of pagebuilders…please see my Gutenberg guides for that (I just released a new one today, too)."
78,Reply
78,Joy Chetry
78,"December 16, 2020 at 5:18 am"
78,"Good job Johnny,"
78,"Pheww that was a long read no wonder it took so long for you to write. Thanks again, I will come back to this time and again."
78,Reply
78,Johnny
78,"December 16, 2020 at 11:23 am"
78,"Congrats, Joy! I’m glad you like it."
78,Reply
78,Christoph
78,"March 26, 2021 at 9:45 am"
78,Hey Johnny the more often i read this the more i learn!
78,What about my suggestion what i did on client site.
78,There where a lot of Plugins what load extra css for just very few things.
78,I use the chrome plugin used css. Select the Div and the chrome addon gives me the css what is used by the plugin.
78,I was wondering … just a few lines css and the plugin load often too much unused css.
78,So i dequeued the Plugin css and copy the few lines CSS into the theme css.
78,Till now this works pretty good for me.
78,What u think about this tactic?
78,Reply
78,Johnny
78,"March 26, 2021 at 9:53 am"
78,"Yes, that is one of the best manual ways to do it. Dequeue from the plugin if possible and manually add to your theme CSS. The only thing better than that is to completely rewrite (refactor) your CSS."
78,Reply
78,Christoph
78,"March 26, 2021 at 10:03 am"
78,From this point refactor is easy cause u have used css here.
78,Maybe if someone look for the same i use this chrome addon:
78,https://chrome.google.com/webstore/detail/css-used/cdopjfddjlonogibjahpnmjpoangjfff
78,U can copy only the styles what comes from the selected DIV block and differentiated in plugins.
78,Reply
78,Jack
78,"April 11, 2021 at 12:36 am"
78,"I was just wondering why it got me 2 days to fully read this, I just checked from tool and its more than 25,000 words!"
78,"amazing man, literally amazing."
78,Never saw such a detailed and to the point guide to optimize page speed for wordpress sites.
78,Reply
78,Leave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment Name *
78,Email *
78,Website
78,"Save my name, email, and website in this browser for the next time I comment."
78,Email me when someone replies to my comment
78,FooterMore links
78,Gadget reviews
78,Try my free WPJ plugins
78,Join the WPJ FB Group
78,WPJ YouTube & newsletter
78,Become a WPJ Affiliate
78,Popular Reviews
78,Best WordPress Hosting
78,Best WordPress Themes
78,Best WordPress Plugins
78,Best WordPress Cache Plugins
78,Services
78,Speed optimization
78,Speed optimization courses
78,WordPress hosting
78,Hire me or other experts
78,Client login
78,About Johnny
78,"10+ years of WordPress design, development, hosting, speed optimization, marketing. Contact me."
78,Enter your email
78,Copyright 2021 |
78,WordPress guides by Johnny NguyenClick to Copy
81,Azure SQL Database administration Tips and Hints Exam (DP-300) – DB Cloud TECH
81,Skip to content
81,Search for:
81,DB Cloud TECH
81,Menu
81,About Me
81,Article Series
81,EVENT AND GALLERY
81,Community Services
81,Consulting Services
81,Health Checks
81,Remote DBA
81,SQL Training
81,Contact US
81,"November 1, 2020 Mustafa EL-Masry Azure Database, Exam Preparation"
81,Azure SQL Database administration Tips and Hints Exam (DP-300)
81,"Finally, I got my certification Azure Database administrator Associate for Exam (DP-300) after two times failure, during the journey of study I watched many courses, videos, and articles, and this post of today is for spreading what I have from the knowledge and what I learned during the journey, and I do two things during my study published around 70 articles in Azure technologies and prepared one document to contain many pieces of information for Azure SQL Database administration"
81,"So, in this post, you will find all of the resources that you can start your study from it and the document I created it"
81,Azure Database Administrator Associate
81,Azure SQL Resources
81,Azure SQL for beginners: https://aka.ms/azuresql4beginners or from here https://aka.ms/azuresql4beginnersch9Azure SQL Bootcamp:  https://aka.ms/azuresqlbootcampAzure SQL Workshop: https://aka.ms/sqlworkshopsAzure SQL Workshop Slides: https://aka.ms/azuresqlworkshopslidesAzure SQL fundamentals:  https://aka.ms/azuresqlfundamentalsmData Exposed: https://channel9.msdn.com/Shows/Data-Exposed/My Azure SQL Articles: https://lnkd.in/edn6nyY/#AzureSQLDP-300 Test Practices: https://www.examtopics.com/exams/microsoft/dp-300/view/Microsoft Learning Path: https://docs.microsoft.com/en-us/learn/certifications/exams/dp-300Pluralsight Guide: https://www.pluralsight.com/guides/cloud-certifications:-azure-database-administrator-associateDP-300 Exam Preparation: https://www.sqlshack.com/how-to-prepare-for-the-exam-dp-300-administering-relational-databases-on-microsoft-azure/DP-300 Study Guide: https://ravikirans.com/dp-300-azure-exam-study-guide/Udemy Course: https://www.udemy.com/course/professional-azure-sql-database-administration/
81,Azure Database Administration Documentation Exam (DB-300) V1
81,"In this document, you will find much information related to the below 30 topics:"
81,"SQL Server on Azure VMMariaDB, MySQL, and PostgreSQL on AzureAzure SQL Database ArchitectureAzure SQL Database ConnectivityIntroduction to Azure SQLAzure SQL Database TypesScaling out Azure SQL DatabaseAzure SQL DatabaseService Tier and Purchase ModelAzure SQL Database Elastic PoolAzure SQL Database Elastic Pool Compute + StorageAzure SQL Database MI Managed InstanceDifferences between SQL Server On-Premises and Azure SQL Managed instanceAzure Managed instance Service TierAzure SQL Managed instance SecurityAzure SQL MI ReferencesDifferences between Azure SQL Database and SQL Server and Non-Supported FeaturesAzure Database MigrationAzure SQL backupAzure SQL RestoreAzure SQL SecuritySQL Azure HA High AvailabilityAzure SQL PerformanceQuery performance insightAzure SQL AnalyticsAzure SQL Performance Recommendation ServicesAzure SQL Automatic Tuning ServicesAzure SQL Elastic Database JobsAzure SQL Database Data SyncAzure DB-300 Exam QA"
81,azure-database-administration-documentation-exam-dp-300-v1-1Download
81,Microsoft Certifications and Awards in 2020 Year
81,Check this Link to review the other Azure Technical Documents
81,Keep Following
81,Cloud Tech Website blog survey
81,IF you found this blog is helpful and sharing useful content please take few second to do rate the website blog from here
81,Share this:Click to share on Facebook (Opens in new window)Click to share on LinkedIn (Opens in new window)Click to email this to a friend (Opens in new window)Click to share on Twitter (Opens in new window)Click to share on WhatsApp (Opens in new window)Like this:Like Loading...
81,Related
81,"Tagged #AzureCertifiction, Azure, AzureDatabase, AzureSQL"
81,Published by Mustafa EL-Masry
81,"I am Microsoft database consultant working as a Database administrator for more than +10 Years I have very good knowledge about Database Migration, Consolidation, Performance Tuning, Automation Using T-SQL, and PowerShell and so many other tasks I do it in multiple customers here in KSA and as of now, I am working in Bank Albilad managing the core banking system that is hosted in SQL Server Database 8 TB. Also, I am Microsoft certified 2008 and 2016 in SQL Server (2x MCTS, 2x MCTIP, MCSA, MCSE) and I am Microsoft Certified Trainer (MCT) also I am azure Certified (AZ-900, AZ-103) also I was awarded by Microsoft Azure Heroes 3 times as (Azure Content hero, Azure Community hero and Azure Mentor) For more information check my page"
81,https://mostafaelmasry.com/about-me/
81,View all posts by Mustafa EL-Masry
81,Leave a Reply Cancel reply
81,Enter your comment here...
81,Fill in your details below or click an icon to log in:
81,Email (required) (Address never made public)
81,Name (required)
81,Website
81,You are commenting using your WordPress.com account.
81,( Log Out /
81,Change )
81,You are commenting using your Google account.
81,( Log Out /
81,Change )
81,You are commenting using your Twitter account.
81,( Log Out /
81,Change )
81,You are commenting using your Facebook account.
81,( Log Out /
81,Change )
81,Cancel
81,Connecting to %s
81,Notify me of new comments via email. Notify me of new posts via email.
81,This site uses Akismet to reduce spam. Learn how your comment data is processed.
81,Post navigation
81,Solve SQL Server Resource Database CrashFundamental Information for Azure Open Source Databases
81,Database Cloud Tech Survey I will be so happy if you take few second to survey the Database Cloud Tech Website blog From Here
81,Check My Latest Azure Articles IF you are interseting to know more abut Azure services check my Latest Microsoft Azure Articles and Posts
81,Search
81,Search for:
81,Top Posts & Pages
81,How to Find the Last Inserted Record in SQL Server
81,The subscription is not registered to Microsoft.insights resource provider
81,Copy Files to or from Azure Storage using Azcopy
81,How to Grant Show Plan Privilege
81,Distributed Tables in Azure Synapse SQL
81,How to hide SQL databases from users using SSMS
81,Azure Cosmos DB High Availability and Disaster Recovery Architecture
81,Error 22022 SQL Server Agent not running
81,Monitoring and Tracking SQL Server Blocking & Deadlocks process
81,Moving Database Files on SQL Server Always On
81,Follow Us
81,LinkedIn
81,Twitter
81,Facebook
81,YouTube
81,Follow me on TwitterMy TweetsFollow Blog via Email
81,Enter your email address to follow this blog and receive notifications of new posts by email.
81,"Join 1,069 other followers"
81,Email Address:
81,Follow
81,Follow DB Cloud TECH on WordPress.com
81,Blog Stats
81,"386,934 hits"
81,"it TutorialQuick Methods to Repair Corrupt Excel File Easily March 26, 2021Many times, users have come across corrupt Excel files that couldn’t easily retrieve information. These files can be extremely important for your business and give you a hard time collecting the same data again. Here are the most prominent ways to repair the corrupt Excel files by yourself, using only free tools that already exist … The post Quick Methods to […] Rajan SinghORA-27086: unable to lock file – already in use | ORA-31641 ORA-39000 March 11, 2021I got ” ORA-27086: unable to lock file – already in use | ORA-31641 ORA-39000  ” error in Oracle database during the expdp running.   ORA-27086: unable to lock file – already in use   Details of error are as follows. Connected to: Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production ORA-39001: invalid argument value … The post ORA-27086: un […] Mehmet Salih DeveciOGG-00664"
81,"OCI Error ORA-26804: Apply “OGG$R” is disabled March 11, 2021I got OGG-00664 OCI Error ORA-26804: Apply “OGG$R” is disabled on Goldengate while running Replicat.   OGG-00664 OCI Error ORA-26804: Apply “OGG$R” is disabled Details of error are as follows.   ThreadBacktrace : [20] elements : [/ggateb01/goldengate/product/GG19cFor18cDB/libgglog.so(CMessageContext::AddThreadContext())] : [/ggateb01/goldengate/product/GG19c […] Mehmet Salih DeveciInstall multiple MySQL instances on a Linux server -use a separate MySQL configuration file March 1, 2021curl -L -O https://dev.mysql.com/get/Downloads/MySQL-8.0/mysql-8.0.23-el7-x86_64.tar [root]# rpm -qa|grep mysql [root]# rpm -qa |grep mariadb-libs yum remove mariadb-libs -y tar -xvf mysql-8.0.23-el7-x86_64.tar [root]# groupadd mysql [root]# useradd -r -g mysql -s /bin/false mysql [root]# cd /usr/local/ [root local]# tar xzvf /root/mysql-8.0.23-el7-x86_64.ta […] Mughees AhmedDATA WAREHOUSE – CHANGE DATA CAPTURE (CDC) February 27, 2021CDC (Change Data Capture) It can be called the process of defining and capturing changes made in the database. It is also referred to as a design pattern to identify and track changes in this type of database. Unlike traditional methods, it finds and updates changes in data instantaneously instead of batch processing. Differences between … The post DATA WARE […] Onur CirkinCertification LogoThis slideshow requires JavaScript.Founder ContactSaudi Arabia - Riyadh+966543990968dbconsultant@mostafaelmasry.comFounderCategoriesCategories"
81,Select Category
81,#Azure  (69)
81,Azure AD  (1)
81,Azure Data lake  (1)
81,Azure Data Studio  (2)
81,Azure Database  (16)
81,Azure Event  (1)
81,Azure Issues  (2)
81,Azure Monitoring  (2)
81,Azure Replication  (3)
81,Azure Storage services  (11)
81,Azure Synapse  (1)
81,Azure Tips  (3)
81,Azure VM  (4)
81,AzurePowerShell  (2)
81,Big Data  (1)
81,Cosmos DB  (6)
81,Exam Preparation  (3)
81,Open Source DB  (1)
81,Arabic Posts  (8)
81,Azure Storage  (4)
81,Event  (1)
81,MSDN  (2)
81,Performance  (5)
81,Performance MSSQL  (29)
81,Dedloack  (1)
81,Index  (10)
81,Query Store  (1)
81,SQL Server 2008  (181)
81,backups  (5)
81,Configuration  (11)
81,Database mail  (3)
81,Encryption Tips  (5)
81,Errors  (12)
81,General topics  (76)
81,Log Shipping  (3)
81,problems  (4)
81,Replication  (7)
81,SQL Server 2008 Features  (11)
81,Mirroring SQL Server 2008  (10)
81,SQl server Administration  (48)
81,DB corruption  (2)
81,SQL Server Index  (2)
81,Transact-SQL  (2)
81,SQL Server 2012  (79)
81,Administration  (23)
81,backups  (7)
81,Configuration  (7)
81,Database Alert  (2)
81,New Feature  (6)
81,New Function  (4)
81,Script  (26)
81,SQL Server 2014  (18)
81,SQL Server 2016  (17)
81,SQL Server 2017 on Linux  (1)
81,SQL Server 2019  (1)
81,Uncategorized  (3)
81,Windows Server 2008 R2  (7)
81,HA Clustering  (2)
81,Introducation  (2)
81,Network  (1)
81,Archives Archives
81,Select Month
81,November 2020
81,October 2020
81,September 2020
81,August 2020
81,July 2020
81,June 2020
81,May 2020
81,April 2020
81,March 2020
81,August 2019
81,February 2019
81,January 2019
81,November 2018
81,October 2018
81,September 2018
81,January 2018
81,December 2017
81,November 2017
81,October 2017
81,May 2017
81,March 2017
81,February 2017
81,January 2017
81,November 2016
81,October 2016
81,August 2016
81,July 2016
81,June 2016
81,May 2016
81,April 2016
81,February 2016
81,January 2016
81,December 2015
81,November 2015
81,October 2015
81,September 2015
81,August 2015
81,June 2015
81,May 2015
81,April 2015
81,March 2015
81,February 2015
81,January 2015
81,December 2014
81,November 2014
81,August 2014
81,May 2014
81,March 2014
81,September 2013
81,August 2013
81,July 2013
81,June 2013
81,May 2013
81,April 2013
81,March 2013
81,February 2013
81,January 2013
81,December 2012
81,October 2012
81,August 2012
81,April 2012
81,March 2012
81,February 2012
81,January 2012
81,December 2011
81,November 2011
81,October 2011
81,September 2011
81,August 2011
81,Recent posts
81,Monitoring and Tracking SQL Server Blocking & Deadlocks process
81,"November 8, 2020"
81,Fundamental Information for Azure Open Source Databases
81,"November 3, 2020"
81,Azure SQL Database administration Tips and Hints Exam (DP-300)
81,"November 1, 2020"
81,Solve SQL Server Resource Database Crash
81,"October 20, 2020"
81,Troubleshooting Performance issues Like Microsoft Engineers Part 4
81,"September 10, 2020"
81,Authors
81,Ayman Elnory
81,Andrew Jackson
81,Mirza Husain
81,Mustafa EL-Masry
81,Mohit
81,Blog at WordPress.com.
81,Add your thoughts here... (optional)
81,Post to
81,Cancel
81,Email (Required)
81,Name (Required)
81,Website
81,Loading Comments...
81,Comment
81,Send to Email Address
81,Your Name
81,Your Email Address
81,Cancel
81,Post was not sent - check your email addresses!
81,"Email check failed, please try again"
81,"Sorry, your blog cannot share posts by email."
81,<span>%d</span> bloggers like this:
83,SolarWinds Database Performance Analyzer • Adeptec
83,About
83,Company
83,Partners
83,Partner Program
83,Partnerships
83,Submit A Referral
83,Process
83,Careers
83,Job Postings
83,Events
83,Services
83,Technical Consulting
83,Prepaid Consulting
83,Technical Training
83,Training Courses
83,Premier Support
83,Maintenance Renewals
83,All Services
83,Solutions
83,Application Management
83,AppOptics
83,Papertrail
83,Pingdom
83,Server & Application Monitor
83,Web Performance Monitor
83,Database Management
83,Database Performance Analyzer
83,DPA for Oracle MySQL
83,DPA for Oracle Database
83,DPA for IBM DB2
83,Database Performance Monitor
83,Enterprise Management
83,Network Automation Manager
83,Network Operations Manager
83,Enterprise Operations Console
83,Orion High Availability
83,Orion Scalability Engine
83,IT Infrastructure
83,Server Backup
83,Server Configuration Monitor
83,Storage Resource Monitor
83,Virtualization Manager
83,IT Security
83,Access Rights Manager
83,Patch Manager
83,Security Event Manager
83,Serv-U FTP Server
83,Serv-U MFT Server
83,IT Service Management
83,Dameware Mini Remote Control
83,Dameware Remote Support
83,Service Desk
83,Web Help Desk
83,Network Management
83,Engineer’s Toolset
83,IP Address Manager
83,Kiwi CatTools
83,Kiwi Syslog Server
83,Network Configuration Manager
83,Network Performance Monitor
83,Network Topology Mapper
83,NetFlow Traffic Analyzer
83,User Device Tracker
83,VoIP & Network Quality Manager
83,Client Portal
83,My Account
83,Schedule
83,Logout
83,Resources
83,Case Studies
83,Data Sheets
83,Downloads
83,Infographics
83,White Papers
83,Webinars
83,Contact
83,About
83,Company
83,Partners
83,Partner Program
83,Partnerships
83,Submit A Referral
83,Process
83,Careers
83,Job Postings
83,Events
83,Services
83,Technical Consulting
83,Prepaid Consulting
83,Technical Training
83,Training Courses
83,Premier Support
83,Maintenance Renewals
83,All Services
83,Solutions
83,Application Management
83,AppOptics
83,Papertrail
83,Pingdom
83,Server & Application Monitor
83,Web Performance Monitor
83,Database Management
83,Database Performance Analyzer
83,DPA for Oracle MySQL
83,DPA for Oracle Database
83,DPA for IBM DB2
83,Database Performance Monitor
83,Enterprise Management
83,Network Automation Manager
83,Network Operations Manager
83,Enterprise Operations Console
83,Orion High Availability
83,Orion Scalability Engine
83,IT Infrastructure
83,Server Backup
83,Server Configuration Monitor
83,Storage Resource Monitor
83,Virtualization Manager
83,IT Security
83,Access Rights Manager
83,Patch Manager
83,Security Event Manager
83,Serv-U FTP Server
83,Serv-U MFT Server
83,IT Service Management
83,Dameware Mini Remote Control
83,Dameware Remote Support
83,Service Desk
83,Web Help Desk
83,Network Management
83,Engineer’s Toolset
83,IP Address Manager
83,Kiwi CatTools
83,Kiwi Syslog Server
83,Network Configuration Manager
83,Network Performance Monitor
83,Network Topology Mapper
83,NetFlow Traffic Analyzer
83,User Device Tracker
83,VoIP & Network Quality Manager
83,Client Portal
83,My Account
83,Schedule
83,Logout
83,Resources
83,Case Studies
83,Data Sheets
83,Downloads
83,Infographics
83,White Papers
83,Webinars
83,Contact
83,+1 206 486 1916
83,Mon - Fri
83,8:00 AM - 5:00 PM PST
83,SolarWinds Database Performance AnalyzerSolution OverviewSolarWinds Database Performance Analyzer
83,"Database and SQL query performance monitoring, analysis, and tuning"
83,Interactive Demo: Database Performance Analyzer for Oracle
83,At A Glance
83,"SolarWinds® Database Performance Analyzer (formerly Confio Ignite) is a comprehensive database performance monitoring and analysis solution for DBAs, IT managers, and application developers. Database Performance Analyzer eliminates performance bottlenecks, improves application service, and reduces overall cost of database operations."
83,Features Overview
83,Expert database and SQL query tuning advisors
83,Identify database problems in real-time
83,True root cause analysis
83,Cross-vendor database support from a single interface
83,Low overhead on monitored databases
83,Blocking and deadlock analysis
83,Key Features
83,Expert database and SQL query tuning advisors
83,Expert tuning advisors.
83,SQL query tuning advice for all skill levels helps both novices and experts quickly determine how to tune.
83,Identify database problems in real-time
83,"Monitors every active session, every second."
83,A granular polling engine details exactly what’s happening in your database without loading on monitored instances.
83,True root cause analysis
83,Multi-dimensional analysis isolates root cause of complex issues.
83,"Correlate wait times, queries, users, files, plans, objects, storage, and time to pinpoint problems."
83,Cross-vendor database support from a single interface
83,Monitor all your databases from a single installation.
83,"Monitor Oracle, SQL Server, MySQL, DB2, and ASE on-premises, virtualized, and in the cloud."
83,Low overhead on monitored databases
83,"Scalable, agentless architecture."
83,"Whether you are monitoring one or hundreds, DPA Central consolidates performance metrics from thousands of instances."
83,Blocking and deadlock analysis
83,Integration with Orion® Platform.
83,View database performance in the context of nodes and applications by integrating with Server & Application Monitor.
83,Is SolarWinds
83,right for your
83,organization?
83,Talk with us today
83,for a free assessment.
83,1 (206) 486-1916
83,M-F: 8am-5pm PST
83,Resources
83,Case Studies
83,Data Sheets
83,Infographics
83,White Papers
83,GSI Commerce Internet Retail
83,"GSI Commerce, a leading provider of e-commerce solutions for retailers, branded manufacturers, entertainment and professional sports organizations, employs Database Performance Analyzer to ensure its Oracle database instances can handle peak loads in high-volume online shopping applications."
83,Liquid Digital Media Performance
83,Liquid Digital Media uses Database Performance Analyzer to meet its service commitments to its music download customers by quickly identifying and resolving bottlenecks in its Oracle database instances.
83,Fortune 200 Bank Builds Database Performance into Application Development
83,"Learn how one of the largest U.S. banks, ranked at 165 on the Fortune 165 list, uses Database Performance Analyzer to gain actionable insights about the performance of its database instances-and the direct impact to its customers-without requiring privileged access to secure production databases or servers"
83,ProLogis Service Response Acceleration
83,"ProLogis, the world’s largest owner, manager and developer of distribution facilities, improved the overall performance of its application 25% by using Database Performance Analyzer to proactively monitor performance in real time, and reduce time to respond and resolve support calls."
83,Indiana University Speeds Performance Tuning and Diagnostics
83,"With more than 110,000 students, 19,600 employees, eight campuses and 200 research centers, Indiana University uses Database Performance Analyzer to monitor and quickly resolve performance issues on its enterprise-critical Oracle and SQL Server databases running on VMware."
83,Financial Organization Boosts Performance with Oracle Databases
83,"This U.S. based, Fortune 100 financial services organization with diversified operations in property, casualty and life insurance markets, uses Database Performance Analyzer to boost performance, reduce costs, and improve service of its Oracle databases."
83,Case: Norway’s largest auto glass company sees clearly with the help of SolarWinds
83,"Andr. L. Riis AS is the largest supplier and fitter of windscreens in Norway. Erik Olne, Andr. L. Riis AS CIO joined the company three years ago and began an intensive renewal program to modernize the company’s IT infrastructure. As the rebuilding phase came to an end, Olne and his team turned their attention to network performance and began looking for a software solution that would help to analyze and maintain their newly built infrastructure."
83,Datasheet: Database Performance Analyzer for Oracle
83,"SolarWinds® Database Performance Analyzer for Oracle is a comprehensive database performance monitoring and analysis solution for DBAs, IT managers, and application developers. Database Performance Analyzer eliminates performance bottlenecks, improves application service, and reduces overall cost of Oracle operations"
83,Datasheet: Database Performance Analyzer for SQL Server
83,"SolarWinds® Database Performance Analyzer for SQL Server is a comprehensive database performance monitoring and analysis solution for DBAs, IT managers, and application developers. Database Performance Analyzer eliminates performance bottlenecks, improves application service, and reduces overall cost of SQL Server operations"
83,Datasheet: Database Performance Analyzer for IBM DB2
83,"SolarWinds® Database Performance Analyzer for DB2 LUW is a comprehensive database performance monitoring and analysis solution for DBAs, IT managers, and application developers. Database Performance Analyzer eliminates performance bottlenecks, improves application service, and reduces overall cost of DB2 LUW database operations."
83,Datasheet: Database Performance Analyzer for Sybase
83,"SolarWinds® Database Performance Analyzer for Sybase ASE is a comprehensive database performance monitoring and analysis solution for DBAs, IT managers, and application developers. Database Performance Analyzer eliminates performance bottlenecks, improves application service, and reduces overall cost of Sybase ASE operations."
83,Datasheet: Database Performance Analyzer (formerly Confio Ignite)
83,"The SolarWinds® family of database performance solutions improve the productivity and efficiency of IT organizations worldwide. By resolving customer problems faster, speeding development cycles, and squeezing more performance out of expensive database systems running on physical or virtual servers, SolarWinds Database Performance Analyzer (formerly Confio Ignite) makes DBA and development teams more valuable to the IT organization"
83,Datasheet: Database Performance Analyzer for VM Option
83,"SolarWinds® Database Performance Analyzer – VM Option is a performance monitoring tool for database administrators running databases on VMware, helping them achieve high performance and reliability."
83,Datasheet: Database Performance Analyzer for MySQL
83,"SolarWinds® Database Performance Analyzer (formerly Confio® Ignite) for MySQL® is a comprehensive database performance monitoring and analysis solution for DBAs, IT managers, and application developers.Database Performance Analyzer eliminates performance bottlenecks, improves application service, and reduces the overall costs of database operation"
83,MySQL Performance Tuning – 12 Step Guide Find and fix query performance faster with SolarWinds Database Performance Analyzer
83,Oracle Query Performance Tuning: A 12-Step Program
83,"Unlike trying to speed Oracle performance by through system-level server tuning, query tuning can have immediate and powerful results, offering as much as 10x increase in performance! Learn the 12 key steps to successful query tuning in this infographic."
83,8 Tips for Faster SQL Server Performance: Without the Expense of Over-provisioning
83,"Rapidly declining costs in memory and storage can make it seem attractive to tackle database performance issues by provisioning new hardware. But throwing hardware at slow performance doesn’t always make the database faster. Learn what you can do to assess the true cause of slow performance, and 8 things that will speed performance without new hardware"
83,How Will DBAs Handle Big Data?
83,"What’s hype and what’s fact in the Big Data landscape for the DBA? While the hype is that Big Data will replace relational database management systems (RDBMSs), the fact is that RDBMS is a fundamental technology in most Big Data projects and will continue to be a significant investment. This infographic clarifies the challenges DBAs face in increasingly complex data environments and underscores the need for exceptional and proactive RDBMS performance."
83,SQL Server Query Performance Tuning: A 12-Step Program
83,"Unlike trying to speed SQL Server performance by through system-level server tuning, query tuning can have immediate and powerful results, offering as much as 10x increase in performance! Learn the 12 key steps to successful query tuning in this infographic."
83,eBook: DBA Survivor Guide: Your First 100 Days
83,This Survivor’s Guide will help you determine how to get started and help you plan for your first 100 days!
83,SolarWinds Database Performance Analyzer or Oracle Enterprise Manager?
83,"The DBA says the answer is both! The combination of OEM and DPA provides the kind of help DBAs need to better understand database performance issues and save time, money, and frustration."
83,SQL Server 2014 Install Guide (by Tom LaRock)
83,This paper discusses best practices and tips for installing and configuring SQL Server 2014 for optimal performance.
83,SQL Query Tuning for SQL Server
83,"Authored by Dean Richards, Senior DBA, the SQL tuning methods presented in this detailed paper follow a simple, repeatable, four-step process. Several tuning case studies help illustrate how the method works in real applications. This methodology is a valuable time saver because it will help you get it right the first time."
83,Response Time Analysis – A Pragmatic Approach for Tuning and Optimizing Oracle Database Performance
83,"Response time analysis is an approach that enables DBAs to measure the time spent on query execution and, therefore, measure the performance impact on end users. In this paper, Senior DBA Dean Richard describes response time analysis in detail and provides an overview of the tools available for effective response time analysis within an Oracle database environment"
83,5 Risks for Databases on VMware
83,Virtualization makes it much more difficult for DBAs to see the interaction between the database and the underlying server resources. Here are five scenarios that merit attention for DBAs with VMware.
83,Monitoring Database Performance on VMware
83,"Databases are one of the last frontiers for virtualizing into a VMware environment. They can be very resource intensive, so many companies have been hesitant to move them into a virtual environment. Most of the hesitation has come from poor experiences during initial tests of VMware. Many of our customers are successfully running databases inside VMware and the key for them was monitoring the correct metrics that can help show where the bottlenecks are. This paper will review the VMware metrics that helped many companies have a far better virtualization experience."
83,Checklist for SQL Server® 2014 Upgrade: Ensuring the Best Outcome for Your Upgrade
83,"This whitepaper will help you create a list of pre- and post-installation tasks to complete when migrating your database server to SQL Server 2014. Preparing and following a checklist is likely to help you avoid most, if not all, potential upgrade issues"
83,Checklist for SQL Server 2012 Upgrade: Top 11 Tips
83,It can be a daunting task to put together everything you need in a pre-upgrade checklist. This paper compiles the top eleven items that you should include in any checklist for migrating your database server to SQL Server 2012. Including these items is likely to help you avoid many potential upgrade issues.
83,SQL Server Query Tuning: A 12-Step Program
83,"In this white paper, Tom LaRock, Technical Evangelist and SQL Server MVP, demystifies query tuning by providing a rigorous 12-step process that database professionals at any level can use to systematically assess and adjust query performance, starting from the basics and moving to more advanced query tuning techniques like indexing. When you apply this process from start to finish, you will improve query performance in a measurable way, and you will know that you have optimized the query as much as is possible."
83,Oracle Exadata Performance – Yes. You still need to tune.
83,"One of the selling points of Exadata is that you just install it, drop all your indexes and it will increase your database performance significantly all within about a week. Although part of this statement is correct, if you only follow these steps, not only do you open the door for specific queries to run slower at the start, but there is a good chance even some of the queries that are running ok now will exponentially get worse."
83,DevOps Essentials for DBAs and Developers
83,"Many CIOs have embraced the need to reposition the IT organization as a strategic component of delivering world-class products and services. For CIOs in SaaS or Web-enabled businesses, an important element in this effort is the transformation from reactive to proactive application performance monitoring and management. This is accomplished by building performance into development from the start and breaking down the silos that separate teams, from architects to developers to QA to DBAs to systems administrators."
83,Oracle RAC on VMware with DRS and HA
83,"Oracle’s evolving support and licensing policies for Oracle RAC on VMware means you can realize the benefits of virtualization– reducing IT costs while increasing the efficiency, utilization, and flexibility of your existing assets—without risk."
83,Oracle RAC Tuning Tips: There is More to Know
83,"There are differences between performance tuning on a RAC database versus a single instance. In this paper, Kathy Gibbs, Sr. Oracle DBA, will explore the RAC-only waits as well as some tuning suggestions for code to help your RAC environment run more smoothly."
83,SQL Query Tuning for Oracle – Getting It Right the First Time
83,"Authored by Dean Richards, Senior DBA, the SQL tuning methods presented in this paper are derived from years of SQL tuning and Oracle database performance consulting. The methodology follows a simple, repeatable, four-step process and is a valuable time saver because you’ll get it right the first time."
83,Improve SQL Server Performance Management with Extended Events: A Guide to Getting Started
83,"Microsoft SQL Server Extended Events, a replacement for Trace, can provide you with a useful and very customizable framework for managing very technical, low-level event information about a SQL Server database and its instances. In this white paper, Janis Griffin, Senior DBA at Confio, explores how Extended Events are different than Trace, how to best use them as a deep-dive, complementary tool to a lightweight continuous performance monitoring solution such as Confio Ignite, and guidelines for getting started with them."
83,Response Time Analysis: A Pragmatic Approach for Tuning and Optimizing Database Performance
83,"Response time analysis is an approach that enables DBAs to measure the time spent on query execution and, therefore, measure the performance impact on end users. In this paper, you’ll learn about response time analysis in detail and get an overview of the tools available for effective response time analysis within an Oracle, SQL Server, SAP ASE or DB2 database environment."
83,Five Concerns for Production Databases in the Cloud
83,"This paper discusses five key concerns that every company should consider when weighing the costs, benefits, and risks of migrating their production database servers to the Cloud, including platform capabilities, data security, total costs (including sometimes hidden costs), recovery options and ability to proactively manage database performance in the Cloud."
83,A Comparison of Oracle Performance on Physical and VMware Servers
83,"To evaluate the difference in Oracle operation between physical and VMware servers, a direct head-to-head comparison of transaction performance was conducted using both scenarios at separate times on the same physical server. By illustrating how a DBA can visualize the response time, query throughput, and server resources utilized in both scenarios, this paper demonstrates how a DBA team can effectively monitor Oracle on VMware to ensure continued performance."
83,Case Studies
83,GSI Commerce Internet Retail
83,"GSI Commerce, a leading provider of e-commerce solutions for retailers, branded manufacturers, entertainment and professional sports organizations, employs Database Performance Analyzer to ensure its Oracle database instances can handle peak loads in high-volume online shopping applications."
83,Liquid Digital Media Performance
83,Liquid Digital Media uses Database Performance Analyzer to meet its service commitments to its music download customers by quickly identifying and resolving bottlenecks in its Oracle database instances.
83,Fortune 200 Bank Builds Database Performance into Application Development
83,"Learn how one of the largest U.S. banks, ranked at 165 on the Fortune 165 list, uses Database Performance Analyzer to gain actionable insights about the performance of its database instances-and the direct impact to its customers-without requiring privileged access to secure production databases or servers"
83,ProLogis Service Response Acceleration
83,"ProLogis, the world’s largest owner, manager and developer of distribution facilities, improved the overall performance of its application 25% by using Database Performance Analyzer to proactively monitor performance in real time, and reduce time to respond and resolve support calls."
83,Indiana University Speeds Performance Tuning and Diagnostics
83,"With more than 110,000 students, 19,600 employees, eight campuses and 200 research centers, Indiana University uses Database Performance Analyzer to monitor and quickly resolve performance issues on its enterprise-critical Oracle and SQL Server databases running on VMware."
83,Financial Organization Boosts Performance with Oracle Databases
83,"This U.S. based, Fortune 100 financial services organization with diversified operations in property, casualty and life insurance markets, uses Database Performance Analyzer to boost performance, reduce costs, and improve service of its Oracle databases."
83,Case: Norway’s largest auto glass company sees clearly with the help of SolarWinds
83,"Andr. L. Riis AS is the largest supplier and fitter of windscreens in Norway. Erik Olne, Andr. L. Riis AS CIO joined the company three years ago and began an intensive renewal program to modernize the company’s IT infrastructure. As the rebuilding phase came to an end, Olne and his team turned their attention to network performance and began looking for a software solution that would help to analyze and maintain their newly built infrastructure."
83,Data Sheets
83,Datasheet: Database Performance Analyzer for Oracle
83,"SolarWinds® Database Performance Analyzer for Oracle is a comprehensive database performance monitoring and analysis solution for DBAs, IT managers, and application developers. Database Performance Analyzer eliminates performance bottlenecks, improves application service, and reduces overall cost of Oracle operations"
83,Datasheet: Database Performance Analyzer for SQL Server
83,"SolarWinds® Database Performance Analyzer for SQL Server is a comprehensive database performance monitoring and analysis solution for DBAs, IT managers, and application developers. Database Performance Analyzer eliminates performance bottlenecks, improves application service, and reduces overall cost of SQL Server operations"
83,Datasheet: Database Performance Analyzer for IBM DB2
83,"SolarWinds® Database Performance Analyzer for DB2 LUW is a comprehensive database performance monitoring and analysis solution for DBAs, IT managers, and application developers. Database Performance Analyzer eliminates performance bottlenecks, improves application service, and reduces overall cost of DB2 LUW database operations."
83,Datasheet: Database Performance Analyzer for Sybase
83,"SolarWinds® Database Performance Analyzer for Sybase ASE is a comprehensive database performance monitoring and analysis solution for DBAs, IT managers, and application developers. Database Performance Analyzer eliminates performance bottlenecks, improves application service, and reduces overall cost of Sybase ASE operations."
83,Datasheet: Database Performance Analyzer (formerly Confio Ignite)
83,"The SolarWinds® family of database performance solutions improve the productivity and efficiency of IT organizations worldwide. By resolving customer problems faster, speeding development cycles, and squeezing more performance out of expensive database systems running on physical or virtual servers, SolarWinds Database Performance Analyzer (formerly Confio Ignite) makes DBA and development teams more valuable to the IT organization"
83,Datasheet: Database Performance Analyzer for VM Option
83,"SolarWinds® Database Performance Analyzer – VM Option is a performance monitoring tool for database administrators running databases on VMware, helping them achieve high performance and reliability."
83,Datasheet: Database Performance Analyzer for MySQL
83,"SolarWinds® Database Performance Analyzer (formerly Confio® Ignite) for MySQL® is a comprehensive database performance monitoring and analysis solution for DBAs, IT managers, and application developers.Database Performance Analyzer eliminates performance bottlenecks, improves application service, and reduces the overall costs of database operation"
83,Infographics
83,MySQL Performance Tuning – 12 Step Guide Find and fix query performance faster with SolarWinds Database Performance Analyzer
83,Oracle Query Performance Tuning: A 12-Step Program
83,"Unlike trying to speed Oracle performance by through system-level server tuning, query tuning can have immediate and powerful results, offering as much as 10x increase in performance! Learn the 12 key steps to successful query tuning in this infographic."
83,8 Tips for Faster SQL Server Performance: Without the Expense of Over-provisioning
83,"Rapidly declining costs in memory and storage can make it seem attractive to tackle database performance issues by provisioning new hardware. But throwing hardware at slow performance doesn’t always make the database faster. Learn what you can do to assess the true cause of slow performance, and 8 things that will speed performance without new hardware"
83,How Will DBAs Handle Big Data?
83,"What’s hype and what’s fact in the Big Data landscape for the DBA? While the hype is that Big Data will replace relational database management systems (RDBMSs), the fact is that RDBMS is a fundamental technology in most Big Data projects and will continue to be a significant investment. This infographic clarifies the challenges DBAs face in increasingly complex data environments and underscores the need for exceptional and proactive RDBMS performance."
83,SQL Server Query Performance Tuning: A 12-Step Program
83,"Unlike trying to speed SQL Server performance by through system-level server tuning, query tuning can have immediate and powerful results, offering as much as 10x increase in performance! Learn the 12 key steps to successful query tuning in this infographic."
83,White Papers
83,eBook: DBA Survivor Guide: Your First 100 Days
83,This Survivor’s Guide will help you determine how to get started and help you plan for your first 100 days!
83,SolarWinds Database Performance Analyzer or Oracle Enterprise Manager?
83,"The DBA says the answer is both! The combination of OEM and DPA provides the kind of help DBAs need to better understand database performance issues and save time, money, and frustration."
83,SQL Server 2014 Install Guide (by Tom LaRock)
83,This paper discusses best practices and tips for installing and configuring SQL Server 2014 for optimal performance.
83,SQL Query Tuning for SQL Server
83,"Authored by Dean Richards, Senior DBA, the SQL tuning methods presented in this detailed paper follow a simple, repeatable, four-step process. Several tuning case studies help illustrate how the method works in real applications. This methodology is a valuable time saver because it will help you get it right the first time."
83,Response Time Analysis – A Pragmatic Approach for Tuning and Optimizing Oracle Database Performance
83,"Response time analysis is an approach that enables DBAs to measure the time spent on query execution and, therefore, measure the performance impact on end users. In this paper, Senior DBA Dean Richard describes response time analysis in detail and provides an overview of the tools available for effective response time analysis within an Oracle database environment"
83,5 Risks for Databases on VMware
83,Virtualization makes it much more difficult for DBAs to see the interaction between the database and the underlying server resources. Here are five scenarios that merit attention for DBAs with VMware.
83,Monitoring Database Performance on VMware
83,"Databases are one of the last frontiers for virtualizing into a VMware environment. They can be very resource intensive, so many companies have been hesitant to move them into a virtual environment. Most of the hesitation has come from poor experiences during initial tests of VMware. Many of our customers are successfully running databases inside VMware and the key for them was monitoring the correct metrics that can help show where the bottlenecks are. This paper will review the VMware metrics that helped many companies have a far better virtualization experience."
83,Checklist for SQL Server® 2014 Upgrade: Ensuring the Best Outcome for Your Upgrade
83,"This whitepaper will help you create a list of pre- and post-installation tasks to complete when migrating your database server to SQL Server 2014. Preparing and following a checklist is likely to help you avoid most, if not all, potential upgrade issues"
83,Checklist for SQL Server 2012 Upgrade: Top 11 Tips
83,It can be a daunting task to put together everything you need in a pre-upgrade checklist. This paper compiles the top eleven items that you should include in any checklist for migrating your database server to SQL Server 2012. Including these items is likely to help you avoid many potential upgrade issues.
83,SQL Server Query Tuning: A 12-Step Program
83,"In this white paper, Tom LaRock, Technical Evangelist and SQL Server MVP, demystifies query tuning by providing a rigorous 12-step process that database professionals at any level can use to systematically assess and adjust query performance, starting from the basics and moving to more advanced query tuning techniques like indexing. When you apply this process from start to finish, you will improve query performance in a measurable way, and you will know that you have optimized the query as much as is possible."
83,Oracle Exadata Performance – Yes. You still need to tune.
83,"One of the selling points of Exadata is that you just install it, drop all your indexes and it will increase your database performance significantly all within about a week. Although part of this statement is correct, if you only follow these steps, not only do you open the door for specific queries to run slower at the start, but there is a good chance even some of the queries that are running ok now will exponentially get worse."
83,DevOps Essentials for DBAs and Developers
83,"Many CIOs have embraced the need to reposition the IT organization as a strategic component of delivering world-class products and services. For CIOs in SaaS or Web-enabled businesses, an important element in this effort is the transformation from reactive to proactive application performance monitoring and management. This is accomplished by building performance into development from the start and breaking down the silos that separate teams, from architects to developers to QA to DBAs to systems administrators."
83,Oracle RAC on VMware with DRS and HA
83,"Oracle’s evolving support and licensing policies for Oracle RAC on VMware means you can realize the benefits of virtualization– reducing IT costs while increasing the efficiency, utilization, and flexibility of your existing assets—without risk."
83,Oracle RAC Tuning Tips: There is More to Know
83,"There are differences between performance tuning on a RAC database versus a single instance. In this paper, Kathy Gibbs, Sr. Oracle DBA, will explore the RAC-only waits as well as some tuning suggestions for code to help your RAC environment run more smoothly."
83,SQL Query Tuning for Oracle – Getting It Right the First Time
83,"Authored by Dean Richards, Senior DBA, the SQL tuning methods presented in this paper are derived from years of SQL tuning and Oracle database performance consulting. The methodology follows a simple, repeatable, four-step process and is a valuable time saver because you’ll get it right the first time."
83,Improve SQL Server Performance Management with Extended Events: A Guide to Getting Started
83,"Microsoft SQL Server Extended Events, a replacement for Trace, can provide you with a useful and very customizable framework for managing very technical, low-level event information about a SQL Server database and its instances. In this white paper, Janis Griffin, Senior DBA at Confio, explores how Extended Events are different than Trace, how to best use them as a deep-dive, complementary tool to a lightweight continuous performance monitoring solution such as Confio Ignite, and guidelines for getting started with them."
83,Response Time Analysis: A Pragmatic Approach for Tuning and Optimizing Database Performance
83,"Response time analysis is an approach that enables DBAs to measure the time spent on query execution and, therefore, measure the performance impact on end users. In this paper, you’ll learn about response time analysis in detail and get an overview of the tools available for effective response time analysis within an Oracle, SQL Server, SAP ASE or DB2 database environment."
83,Five Concerns for Production Databases in the Cloud
83,"This paper discusses five key concerns that every company should consider when weighing the costs, benefits, and risks of migrating their production database servers to the Cloud, including platform capabilities, data security, total costs (including sometimes hidden costs), recovery options and ability to proactively manage database performance in the Cloud."
83,A Comparison of Oracle Performance on Physical and VMware Servers
83,"To evaluate the difference in Oracle operation between physical and VMware servers, a direct head-to-head comparison of transaction performance was conducted using both scenarios at separate times on the same physical server. By illustrating how a DBA can visualize the response time, query throughput, and server resources utilized in both scenarios, this paper demonstrates how a DBA team can effectively monitor Oracle on VMware to ensure continued performance."
83,Last Update: 8/6/2020
83,Interested in SolarWinds Database Performance Analyzer?
83,Looking to purchase SolarWinds products or services?
83,Request Quote
83,Smart Solutions for IT Management
83,CompanyAbout
83,Careers
83,Legal
83,Events
83,Process
83,Partners
83,News
83,Contact
83,Latest News
83,SolarWinds Supply-Chain Attack / SUNBURST Malware
83,"December 14, 2020"
83,Search
83,"Adeptec, Inc. 2016-2020 © All Rights Reserved."
83,Privacy Policy
83,Terms & Conditions
83,Trademarks
84,VMware Knowledge Base
84,LoadingÃ—Sorry to interruptCSS ErrorRefresh
85,Performance - OpenSimulator
85,Views
85,Page
85,Discussion
85,View source
85,History
85,Performance
85,From OpenSimulator
85,"Jump to: navigation, search"
85,Home
85,Download
85,News
85,Support
85,Admin
85,Dev
85,Screen Shots
85,Grid List
85,Bugs
85,Languages:
85,English
85,Français
85,Contents
85,1 Introduction
85,2 Monitoring Performance
85,3 General Tips
85,3.1 3 Kinds of Ticks
85,3.2 MetricsCollectorTime
85,4 Viewer Performance Topics
85,5 Simulator Performance Topics
85,5.1 Hardware Requirements
85,5.1.1 CPU
85,5.1.2 Memory
85,5.1.3 Disk
85,5.1.4 Network
85,5.1.5 Examples
85,5.2 Database
85,5.3 Scripts
85,5.4 Configuration tweaks
85,5.4.1 Set DeleteScriptsOnStartup = false
85,5.4.2 Set AppDomainLoading = false
85,5.4.3 Increase MaxPoolThreads in [Startup]
85,5.4.4 Change async_call_method in [Startup]
85,6 Grid Performance Topics
85,6.1 Database
85,6.1.1 Assets
85,6.1.1.1 The problem
85,6.1.1.2 Possible solutions
85,6.1.2 Other databases
85,6.2 Services
85,6.2.1 The problem
85,6.2.2 Possible solutions
85,7 Performance studies and blog posts
85,7.1 Performance hints
85,7.1.1 Running Squid on your region server as a reverse proxy to the asset server
85,Introduction
85,OpenSimulator performance is a very complex issue.
85,"Performance can be affected by any number of things, including the number of prims on a region, number of regions, number of avatars, network quality between server and viewer, network quality between simulator and grid services, etc."
85,We can break down performance considerations into
85,"Network issues (bandwidth and latency between viewer and simulator, between simulators and between simulator and grid service)."
85,"Simulator issues (e.g. number of scene objects, number of textures, number of scripts)."
85,"Grid issues (chiefly scaling services such as asset, inventory, etc to serve more simulators and users)."
85,The biggest issues are probably network and simulator issues.
85,"To run a simulator you must have good bandwidth (to download textures), good latency (so that movements are seen and actions processed in a timely manner) and good quality (so that random packet drops don't result in missed actions or other problems)."
85,You must also be aware of your hardware's capabilities.
85,"The more scene objects, scripts and avatars you have, the more memory and CPU will be used."
85,Grid issues are less important until you reach larger grid sizes (e.g. over 60 simultaneous users).
85,Monitoring Performance
85,Gathering data to analyze performance issues is an evolving area.
85,We can split this into client side monitoring (e.g. statistics you can see from the statistics window on a viewer program) and server side performance analysis.
85,Server side performance analysis will involve OpenSimulator server monitoring and general system tools (e.g. top on Linux to monitor which processes are taking up CPU/memory and more general monitoring tools such as Zabbix).
85,General Tips
85,"Where at all possible, don't assume something is a performance bottleneck, measure it!"
85,"You might think your asset database is large, for example, but even large asset database seldom cause real issues."
85,Make as many objects phantom as possible.
85,"Phantom objects do not need to be tested for collisions with avatars and other objects, reducing physics frame time and increasing performance."
85,"Make as few objects subject to physics (e.g. falling under gravity, movable by other avatars) as possible."
85,Physics objects need a lot more collision testing than ordinary non-phantom objects.
85,It can be hard to perform measurement at the moment since not a lot of tools exist.
85,"However, one such is pCampbot which is bundled with OpenSimulator."
85,This can instantiate a number of simultaneous libomv clients on a simulator that can take certain actions such as clicking things and bouncing aroud.
85,Apects of it (e.g. appearance) are currently rather buggy and generate a lot of logging guff.
85,"If your avatars are twitching a lot or flying around uncontrollably, this is often a signal of dropped packets caused by network issues."
85,"For important packets, the simulator will retry the send 3 times but drop the packet after that."
85,"On the simulator console, the command ""show queues"" will indicate how many packets the simulator has to resend and the total number of sends."
85,"If the total resends is greater than 10% then this is a signal of network issues, at least between a particular viewer and the simulator."
85,"The problem may be at the user's end (e.g. a bad router being used over wifi, a badly performing ISP) which are difficult to do anything about!"
85,3 Kinds of Ticks
85,"If you measure times in C#, be aware that the word ""Tick"" is overloaded: there are 3 different values for a Tick, and it's important not to mix them."
85,"Stopwatch - varying resolutions, depending on the operating system. Stopwatch.Frequency contains the number of ticks per second. On Windows, this is about 300 ticks per millisecond."
85,"TimeSpan - 10,000 ticks = 1 millisecond"
85,Environment.TickCount - 1 tick = 1 millisecond
85,"It's recommended to use the Stopwatch class for timing, because it can measure sub-ms times. You can get the Stopwatch's value in ms by calling Stopwatch.ElapsedMilliseconds."
85,"If you want to add up times, e.g. to get the total time spent executing a script, then accumulate the the sum using Stopwatch Ticks: again, because this is the only high-resolution timer available. If you do this then you will have a 'long' variable that contains the number of ticks. When you want to convert this value into elapsed time, use code such as this:"
85,long numStopwatchTicks = xxxx;
85,TimeSpan elapsed = TimeSpan.FromMilliseconds((numStopwatchTicks * 1000) / Stopwatch.Frequency);
85,MetricsCollectorTime
85,"The class MetricsCollectorTime may be useful to you. It implements a sliding window that collects performance measurements. For example, its' used to calculate the total execution time over the past 30 seconds for each script. For example, instantiate it as follows:"
85,"MetricsCollectorTime executionTime = new MetricsCollectorTime(30000, 10);"
85,"This creates a sliding window that covers 30 seconds (30000 ms), and has 10 ""buckets"". The buckets determine the granularity of the window: in this case, it's 30/10 = 3 seconds."
85,"Whenever you get a timing sample, add it to the collector:"
85,Stopwatch timer = new Stopwatch();
85,// Measure something with 'timer'...
85,collector.AddSample(timer);
85,"Whenever you want to get the total value in the collection window, call:"
85,int elapsedMS = collector.GetSumTime().TotalMilliseconds;
85,"MetricsCollectorTime is actually a subclass of the generic MetricsCollector class. The generic class can use any data type as the Sample value. It uses a circular buffer for its buckets, so it's highly efficient."
85,Viewer Performance Topics
85,Performance issues can be tackled on the viewer side as well as on the OpenSimulator side.
85,This typically involves lowering viewer graphical settings (e.g. reducing viewer-side draw distance).
85,See http://community.secondlife.com/t5/English-Knowledge-Base/How-to-improve-Viewer-performance/ta-p/1316923 for more information.
85,Simulator Performance Topics
85,Hardware Requirements
85,"Unfortunately, this is a very difficult question in light of all the factors mentioned in the introduction."
85,"Apart from network, the most important"
85,CPU
85,We can say that OpenSimulator does not run well when it only has access to a single CPU core - you should regard a dual-core machine as the minimum.
85,"An extremely approximate rule of thumb is to have one core per regularly used region, with the minimum of two above."
85,So a 4 region simulator would need 4 cores.
85,"However, this assumes continuous use of those regions - one could probably get away with a higher core to region ratio if those other regions were much less used (or not used simultaneously)."
85,"On OpenSimulator 0.7.6, we have also observed that an idle region (one which has very few if no active scripts and no avatars on it or in neighbouring regions) requires approximately 2.5% of a CPU core."
85,The requirement before OpenSimulator 0.7.6 was much higher.
85,"We can also say, again extremely approximately, that each active avatar requires 8% CPU."
85,An active avatar is one that is moving around and the chief cause of this load is physics processing with the ODE physics engine plugin.
85,"Other physics engine plugins, such as Bulletsim, may require less CPU."
85,Continually running scripts (such as scripts on timers) will also generate continuous CPU load on a region.
85,"A few scripts of this type probably won't have much of an impact, but a larger number of scripts will start to consume CPU resources."
85,"Finally, physics objects (those which have their physics checkbox marked in the viewer and so are moved around by gravity and collisions) will also generate physics CPU load on the simulator if they are not at rest."
85,Memory
85,"As a rule of thumb, a region with lots of avatars, 15000 or more prims and 2000 scripts may require 1G of memory."
85,So a simulator with 4 such regions may require 4G.
85,"One could use less memory if not all regions will be occupied with avatars simultaneously, or where the are fewer scripts, for instance."
85,Disk
85,"At the simulator level, storage performance is not a big issue unless one has scripts which need extremely high performance in rezzing and derezzing objects."
85,"Even in this case, 7200 rpm 3.5"" desktop drives are generally sufficient - problems only start to arise with lower performing drives, such as those found in laptops."
85,"At the grid level, faster storage may be useful when handling large numbers of asset, inventory or other service requests."
85,Network
85,Download and upload bandwidth and latency are important.
85,The biggest use of upload bandwidth (from the server point of view) is to provide texture data to viewers.
85,"So a home network with poor bandwidth (e.g. 384 kbits up) will result in a poor experience for viewers, at least until they have received all texture data."
85,The requirement for upload bandwidth peaks when a viewer enters a region for the first time or after clearing their asset cache.
85,The amount of bandwidth required will vary heavily with the number of textures on the region.
85,An extremely approximate rule of thumb is to have 500 kbit per simultaneously logged in avatar if you know that all avatars will be downlaoding textures simultaneously.
85,The biggest use of download bandwidth (from the server point of view) is to receive the continuous UDP movement messages from connected avatars.
85,"However, in comparison to texture download, the bandwidth required is trivial - an approximate rule of thumb is 10K (80 kbit) per avatar."
85,"Latency is critical on both upload and download to the simulator, since any delay will affect avatar movement packets (download to server from viewer) and updates to the viewer about other object/avatar position changes (upload from server to viewer)."
85,"It's much harder to give advice here, though pings of greater than 350 milliseconds will start to degrade the user's experience on moving their avatar."
85,Examples
85,Below are some examples of hardware people use/have used.
85,"Please feel free to add to the list, or to add any reports to the performance studies and blog posts section."
85,"These are examples to help you in your selection, not recommendations."
85,Object Parts ~= # prim
85,"Sensors and Timers are generally more intensive then regular scripts, so please specify quantity of each."
85,Description
85,Operating System (please add Mono version if appropriate)
85,OpenSimulator version
85,RAM/AVG_USE_%
85,CPU
85,#/type of regions
85,# simultaneous avs
85,#scripts/timers/Sensors
85,Location
85,#objectparts
85,hosted Xen VPS
85,Ubuntu Intrepid (8.10)
85,Unknown
85,540MB/?
85,1x quad-core 2.5GHz Xeon (L5420)
85,1 region + 9 voids
85,generally 1-2
85,few
85,Knifejaw Atoll & surrounding on OSGrid
85,hosted Xen VPS
85,Ubuntu Jaunty (9.04)
85,Unknown
85,360MB/?
85,2x dual-core 2.0GHz Xeon (5130)
85,1 void
85,generally 1-2
85,none
85,Knifejaw Road on OSGrid
85,Dedicated Server from A+
85,Windows Server 2003
85,Unknown
85,1 Meg
85,1x single-core 2.8GHz Celeron
85,2regions per server
85,6 at once with no issues
85,"Waterfalls, texture anims, window texture switchers, lots of sound loops"
85,Pleasure Planet Welcome center & Region Pleasure Planet in OSGrid
85,20000 prims per region
85,"Amazon EC2 ""high-CPU medium instance"" (Xen VM)"
85,Windows Server 2003
85,Unknown
85,1.7GB
85,1x dual-core 2.3GHz (Intel E5345)
85,1 region with sailing race course
85,"7 avs, 4 in boats"
85,scripted start line
85,"Castle Rock, OSGrid"
85,Dedicated Server from simhost.com
85,SuSe 11.2 x64
85,Unknown
85,8gb / 50%
85,4x Core2Quad Q9300 2.6ghz
85,1 region (Wright Plaza) uses approx 4gb ram
85,20-25 users
85,Freebie Stores / Meeting Center / Video Theater
85,@osgrid.org Heavy Use Sim
85,17500 prims aprox 1500 scripts
85,Home machine
85,Windows XP SP3
85,0.7.0.1 (Diva r13558)
85,3GB / 15-40% incl. Opensim and MySQL
85,"4x Core2Quad Q6600 2.4 GHz. Use: generally, 0-10%"
85,11 regions
85,1-6 users
85,Many scripted objects (1934 scripts)
85,Condensation Land
85,"38,065 prims"
85,Home machine
85,Ubuntu Lucid 10.04 (32 bit pae)
85,0.7.0.1 (Diva r13558)
85,"160Mb no users, add 5Mb/user incl Opensim and MySQL"
85,"I7-920 (dual threaded quad core), 3.8Ghz, 6Gb RAM, 0-10% Load"
85,4 regions (Diva default config)
85,1-4 users (approx 20Kb/sec bandwidth/user)
85,Few scripted objects (<10)
85,Mars Simulation- Based on Erik Nauman's Open Blackboard
85,158 prims
85,Hosted Dedicated OVH
85,Suse 12.2
85,0.7.0.2 (D2)
85,"420Mb total, incl OS, Opensim and MySQL"
85,"i7 Quad 950 (Bloomfield) 3.07GHz, 8 Core, 24Gb RAM, 0-1% Avg Load"
85,16 regions (4x4 mega-region)
85,<6 users
85,vehicle scripted objects (<5)
85,Metaverse Sailing
85,<1000 prims
85,VPS
85,Debian Lenny 5 (mono 2.4.2.3)
85,OSgrid 0.7.1 (Dev) cd4d7a7: 2010-10-15
85,"655MB average out of 1722MB RAM, incl. MySQL"
85,Intel Quadcore 2.5 Ghz (1 core assigned to vps) average use: 40-45%
85,20 regions
85,<4 users
85,"about 20 different light scripts, but we're also experimenting with heavier HUD scripts (timers, lots of ll(Get/Set)PrimitiveParams and large lists) and custom IRC relay"
85,Phoenix Rising Isles on OsGrid
85,3727 prims
85,Database
85,"By default, OpenSimulator is configured to use the SQLite database."
85,This is very convenient for an out-of-the-box experience since it requires no configuration.
85,"However, it's not designed for heavy usage, so if you build very quickly or have more than a few people on your simulator then you will start to see performance issues."
85,"Therefore, we recommend that you switch to MySQL as soon as possible."
85,This will provide a much better experience but will take a little bit of work to set up.
85,"Unfortunately, tools for migrating OpenSimulator SQLite data to MySQL are currently limited."
85,Migration is also possible by saving OARs/IARs of your data from sqlite and loading them up once you've reconfigured to use MySQL.
85,There is also a database plugin for MSSQL but this is not well maintained between OpenSimulator releases.
85,"In standalone mode, both services and the simulator itself can use SQLite."
85,"In grid mode, SQLite is only supported for simulator data - the ROBUST instances must use a MySQL (or MSSQL) database."
85,"In general a single MySQL instance for the ROBUST services instance will serve small, medium and even large grids perfectly well - it's a configuration that's widely used for even quite large websites."
85,Scripts
85,See Scripts Performance.
85,Configuration tweaks
85,There are a couple of OpenSimulator configuration tweaks that you can do to significantly improve script loading performance in certain situations.
85,These tweaks can be made in your OpenSim.ini config file.
85,"These apply to current OpenSimulator development code (0.7.3-dev) and may also apply to 0.7.2, though certainly not any earlier."
85,Set DeleteScriptsOnStartup = false
85,[XEngine]
85,DeleteScriptsOnStartup = false
85,"This means that OpenSimulator will not delete all the existing compiled script DLLs on startup (don't worry, this setting doesn't touch the actual LSL scripts in your region)."
85,This will significantly improve startup performance.
85,"However, if you upgrade OpenSimulator in place (e.g. you regularly update your installation directly from development code) then you may occasionally see problems if code changes and your previously compiled DLLs can't find their old references."
85,"In this case, you can either set DeleteScriptsOnStartup = true for one restart in order to clean out and recompile script DLLs or you can manually delete the relevant bin/ScriptEngines/<region-uuid>/*.dll.* files, which will force OpenSimulator to recompile them."
85,You could also delete the entire bin/ScriptEngines/<region-uuid> directory but this would lose all persisted script state (which is kept in the <script-item-id>.state files).
85,Set AppDomainLoading = false
85,[XEngine]
85,AppDomainLoading = false
85,"setting this option to true has a high performance cost, because information cross between application domains is very slow, a few us operation can turn into a several ms one. It is a lot better to set it fo false and restart the region periodicly to avoid excessive memory usage issues, due to the fact that scritps are never removed from memory"
85,Yengine script engine does not have this option or issues.
85,Increase MaxPoolThreads in [Startup]
85,"At the present time, OpenSimulator uses up to several types of threads for its operations."
85,"Firstly, the default .net main threadpool is used by the .net framework methods, like IO etc, some opensim methods also use it, or can be told to."
85,"Secondly, a third-party threadpool package called SmartThreadPool is used"
85,"this is the default pool used, and it used by several parts of opensim (like Xengine)"
85,"The use of this pools allow for threads reuse, savign a lot of very costly thread creation and destruction operations"
85,SmartThreadPool has a option to set the maximum number of threads it can use
85,[Startup]
85,MaxPoolThreads = 300
85,"Note each threads does have costs both in resources, but also CPU on housekeeping. Allowing for too many will degradate performance, not improve it. The inflection point depends on the number of cpu cores, and the number of applications starving for those on the machine."
85,"A lot of bad threading usage of previus versions of opensim has been corrected, making more efficient usage of each thread. Currently a large number if still needed, to handle slow IO (like network) operations, this will need to be improved"
85,Change async_call_method in [Startup]
85,By default it set to use SmartThreadPool
85,"Alternatively, you can change to use the main .net threadpool or even to use a new thread"
85,".net main threadpool has improved a lot, but some do report situations where it self induces threads starvation, totally stopping working"
85,to use it:
85,[Startup]
85,async_call_method = UnsafeQueueUserWorkItems
85,"The use of a new thread is not recomended, because thread creation is a high cost operation, the reason why pools where introduced."
85,[Startup]
85,async_call_method = Thread
85,note that several components of opensim have hardcoded usage of a particular type of pool
85,Grid Performance Topics
85,Scaling a grid is a complex task and only for the very technically inclined.
85,It is also an area under active investigation.
85,The advice below will change considerably over time as OpenSimulator and its environment changes and we learn more about perfomrnace issues.
85,When would you start to meet grid scaling issues?
85,"As an extremely rough and really quite arbitrary and pessimistic rule of thumb, you will probably start to have to think about things when you exceed 50 regions containing a large number of prims or 50 simultaneous users with large inventories."
85,But this will very a tremendous amount depending on your situation.
85,If you have thousands of regions with very few prims that much more supportable than 50 regions with 45000 prims each.
85,You are likely to encounter issues in two areas - database and services.
85,Database
85,Assets
85,The problem
85,"Due to the architecture of distributed architecture of OpenSimulator, where regions are running on a number of different machines over a network, it's extremely hard to identify assets that are not in use and hence can be deleted."
85,"Equally, the fact that assets are immutable leads to continual growth in the asset database."
85,"In theory, one could identify unused assets if one could identify all references in simulators and in user inventory."
85,"However, this is extremely hard to do where machines are distributed over a network."
85,"If a grid only has a few simulators all running on machines that are controlled by the same entity running a grid, then it becomes a little more tractable but even then would almost certainly involve significant downtime."
85,"For stand-alone grids, or for environments where assets are not passed between grids (ie: giving a texture to a friend on another grid) Wizardry and Steamworks provides an experimental script that will do exactly that. It is currently available at the asset cleaner project page and published under an MIT license."
85,Asset deletion would be easier for a one simulator grid or a standalone.
85,"However, even code to implement asset deletion on standalones has not yet been implemented and would certainly require significant simulator downtime."
85,A promising area of research involves improving OpenSimulator's recording of asset access times (e.g. recording access periodically).
85,Then assets which aren't accessed for a long time (e.g. a year) could be deleted or moved to cold storage (e.g. DVD).
85,One is left with the problem of not deleting assets permanently cached by simulators but perhaps this could be solved by the simulators occasionally 'pinging' the asset service with notification of what assets they cache.
85,Another step to reduce asset database size is to eliminate duplicate assets by hashing.
85,There is an experimental development asset service for this.
85,Third party services such as [SRAS] also do this.
85,Possible solutions
85,"FSAssets is intended for larger grids where the size of the database is expected to exceed 50GB. This option will save the assets to the file system as opposed to the default service which stores assets as blobs in the database. This option also provides deduplication abilities, each asset is hashed when it is received for storage and if the asset already exists, the asset service will link to the existing file rather than store two copies. WARNING: this uses both a database and files on the file system. You need to backup BOTH"""
85,"If you run a grid for yourself or, if you run a grid where you do not give away your content, then the asset cleaner from Wizardry and Steamworks may be a good solution to track down stray assets and delete them from the database automatically. It is based on dumping OARs and IARs, as per the second option in this section, but after dumping them, it automatically performs the search for you and prompts you to delete several supported assets. Current development is going towards automatically exporting OARs and IARs from PHP so that the procedure is made seamless."
85,Save every region to an OAR and every user's inventory to an IAR.
85,We believe this is equivalent to finding all referenced assets and can be done manually.
85,"However, it's very laborious for installations with a large number of users and requires grid downtime."
85,"Tools could be written to improve this, particularly in systematically saving all user inventories to IARs."
85,"For backup purposes, make sure they do store the assets, using correct options."
85,Do nothing.
85,MySQL can store a very large amount of data before encountering issues - it's used for extremely large websites and other applications after all.
85,This assumes you have the disk space.
85,Use an external asset service such as [SRAS].
85,"SRAS, in particular, is a third party asset service that does deduplication, asset compression, and stores assets on disk rather than in a database."
85,It also has some nice features like preventing assets being served without deleting them.
85,"It's used by OSGrid, for instance."
85,"However, it does work in a different way from the bundled OpenSimulator asset service (e.g. backup of on-disk assets involves some extra steps compared to just backing up a database)."
85,It also requires a migration step that may take a considerable time if you have an existing asset collection.note FSassets looks a lot like SRAS done in c# and included in opensim
85,Other databases
85,The space required by assets far outweighs other data storage requirements so only asset data is generally an issue.
85,Services
85,The problem
85,The other problem is with handling the number of requests to services when the number of simulators and users grow.
85,"The asset service isn't generally a problem since simulators cache all assets used, though it can form a bottleneck on OAR upload."
85,"The biggest issue is generally caused by users, chiefly due to inventory access and perhaps update last user positions in the GridUser service (and database table)."
85,ROBUST uses an embedded [C# HttpServer].
85,"Performance comparisons to other Webservers (e.g. Apache) have not been carried out (?) but responses appears to be much, much slower."
85,As it has been discontinued it's also rather unlikely to have it's performance improved.
85,"In future, OpenSimulator may embed a different HTTP server but this is extremely unlikely in the short term."
85,Possible solutions
85,Split up services.
85,"By default, ROBUST runs every service in one process."
85,"However, because services are separate from each other, you could run some services (e.g. inventory in one ROBUST instance and other services (e.g. asset) in a different instance, even if they both point to the same database."
85,"Because the embedded C# webserver is slow and possibly not very concurrent, this can achieve significant performance improvements even if all ROBUST instances are running on the same machine."
85,See Configuration#Running multiple ROBUST service instances for more information on how to do this.
85,Instantiate extra ROBUST copies of problem services (e.g. inventory).
85,"Because services are stateless (akin to a webservice), you can load balance requests between multiple instances using a reverse proxy such as nginx."
85,"Again, because the embedded webserver is probably inefficient, you can achieve performance improvements by running multiple copies of services on the same machine."
85,"Use an external service based on a more efficient HTTP server, e.g. SRAS (asset service only).Note that FSAssets looks a lot like SRAS rewritten in C# and integrated in OpenSim and using its tiny http server."
85,Performance studies and blog posts
85,These provide some interesting data on the performance limitations of OpenSimulator at various points in time.
85,https://lists.berlios.de/pipermail/opensim-users/2010-August/005189.html - Some interesting information from Mr Blue.
85,Physical objects and max avatars are limited by single thread performance in OpenSimulator.
85,NHibernate Performance Testing — SQLite and MySQL performance tests with NHibernate.
85,LibSecondLife performance problems - Another old page from November 2007 detailing issues with libsecondlife (now called libopenmetaverse).
85,Performance hints
85,Here are some specific things you might be able to do to improve performance
85,Running Squid on your region server as a reverse proxy to the asset server
85,"Download and install the Squid Proxy (CentOS/RHEL/Fedora: ""dnf install squid"", Debian: ""apt install squid"")"
85,Create your squid.conf configuration file.
85,Change your asset_server configuration in your OpenSim.ini to point to http://localhost:3128/
85,Start everything up!
85,"Now assets will be cached in the squid cache on the region server, and will be served up much faster, especially on region restart."
85,"Retrieved from ""http://opensimulator.org/index.php?title=Performance&oldid=50528"""
85,Personal tools
85,Log in / create account
85,General
85,Main Page
85,News
85,For Administrators
85,Admin Home
85,download
85,Running
85,Configuration
85,Building
85,FAQ
85,Related Software
85,Support
85,Report a Bug
85,For Developers
85,Dev Home
85,Contributions Policy
85,Bug Tracking
85,For Creators
85,Content Creation
85,Scripting
85,For Grid Users
85,Connecting
85,Grid List
85,Screenshots
85,Related Links
85,Related Software
85,Black Duck
85,OSGrid User Forums
85,About This Wiki
85,Recent changes
85,Search
85,Tools
85,What links here
85,Related changes
85,Special pages
85,Printable version Permanent link
85,"This page was last modified on 16 January 2021, at 19:13."
85,"This page has been accessed 162,616 times."
85,Content is available under Attribution-Share Alike 2.5
85,unless otherwise noted.
85,Privacy policy
85,About OpenSimulator
85,Disclaimers
87,CREATE DATASOURCE TABLE - Spark 3.0.0 Documentation
87,3.0.0
87,Overview
87,Programming Guides
87,Quick Start
87,"RDDs, Accumulators, Broadcasts Vars"
87,"SQL, DataFrames, and Datasets"
87,Structured Streaming
87,Spark Streaming (DStreams)
87,MLlib (Machine Learning)
87,GraphX (Graph Processing)
87,SparkR (R on Spark)
87,API Docs
87,Scala
87,Java
87,Python
87,"SQL, Built-in Functions"
87,Deploying
87,Overview
87,Submitting Applications
87,Spark Standalone
87,Mesos
87,YARN
87,Kubernetes
87,More
87,Configuration
87,Monitoring
87,Tuning Guide
87,Job Scheduling
87,Security
87,Hardware Provisioning
87,Migration Guide
87,Building Spark
87,Contributing to Spark
87,Third Party Projects
87,Spark SQL Guide
87,Getting Started
87,Data Sources
87,Performance Tuning
87,Distributed SQL Engine
87,PySpark Usage Guide for Pandas with Apache Arrow
87,Migration Guide
87,SQL Reference
87,ANSI Compliance
87,Data Types
87,Datetime Pattern
87,Functions
87,Identifiers
87,Literals
87,Null Semantics
87,SQL Syntax
87,Data Definition Statements
87,ALTER DATABASE
87,ALTER TABLE
87,ALTER VIEW
87,CREATE DATABASE
87,CREATE FUNCTION
87,CREATE TABLE
87,CREATE VIEW
87,DROP DATABASE
87,DROP FUNCTION
87,DROP TABLE
87,DROP VIEW
87,TRUNCATE TABLE
87,REPAIR TABLE
87,USE DATABASE
87,Data Manipulation Statements
87,Data Retrieval(Queries)
87,Auxiliary Statements
87,CREATE DATASOURCE TABLE
87,Description
87,The CREATE TABLE statement defines a new table using a Data Source.
87,Syntax
87,CREATE TABLE [ IF NOT EXISTS ] table_identifier
87,"[ ( col_name1 col_type1 [ COMMENT col_comment1 ], ... ) ]"
87,USING data_source
87,"[ OPTIONS ( key1=val1, key2=val2, ... ) ]"
87,"[ PARTITIONED BY ( col_name1, col_name2, ... ) ]"
87,"[ CLUSTERED BY ( col_name3, col_name4, ... )"
87,"[ SORTED BY ( col_name [ ASC | DESC ], ... ) ]"
87,INTO num_buckets BUCKETS ]
87,[ LOCATION path ]
87,[ COMMENT table_comment ]
87,"[ TBLPROPERTIES ( key1=val1, key2=val2, ... ) ]"
87,[ AS select_statement ]
87,"Note that, the clauses between the USING clause and the AS SELECT clause can come in"
87,"as any order. For example, you can write COMMENT table_comment after TBLPROPERTIES."
87,Parameters
87,table_identifier
87,"Specifies a table name, which may be optionally qualified with a database name."
87,Syntax: [ database_name. ] table_name
87,USING data_source
87,"Data Source is the input format used to create the table. Data source can be CSV, TXT, ORC, JDBC, PARQUET, etc."
87,PARTITIONED BY
87,"Partitions are created on the table, based on the columns specified."
87,CLUSTERED BY
87,Partitions created on the table will be bucketed into fixed buckets based on the column specified for bucketing.
87,NOTE: Bucketing is an optimization technique that uses buckets (and bucketing columns) to determine data partitioning and avoid data shuffle.
87,SORTED BY
87,Determines the order in which the data is stored in buckets. Default is Ascending order.
87,LOCATION
87,"Path to the directory where table data is stored, which could be a path on distributed storage like HDFS, etc."
87,COMMENT
87,A string literal to describe the table.
87,TBLPROPERTIES
87,A list of key-value pairs that is used to tag the table definition.
87,AS select_statement
87,The table is populated using the data from the select statement.
87,Data Source Interaction
87,"A Data Source table acts like a pointer to the underlying data source. For example, you can create"
87,a table “foo” in Spark which points to a table “bar” in MySQL using JDBC Data Source. When you
87,"read/write table “foo”, you actually read/write table “bar”."
87,"In general CREATE TABLE is creating a “pointer”, and you need to make sure it points to something"
87,"existing. An exception is file source such as parquet, json. If you don’t specify the LOCATION,"
87,Spark will create a default table location for you.
87,"For CREATE TABLE AS SELECT, Spark will overwrite the underlying data source with the data of the"
87,"input query, to make sure the table gets created contains exactly the same data as the input query."
87,Examples
87,--Use data source
87,"CREATE TABLE student (id INT, name STRING, age INT) USING CSV;"
87,--Use data from another table
87,CREATE TABLE student_copy USING CSV
87,AS SELECT * FROM student;
87,"--Omit the USING clause, which uses the default data source (parquet by default)"
87,"CREATE TABLE student (id INT, name STRING, age INT);"
87,--Specify table comment and properties
87,"CREATE TABLE student (id INT, name STRING, age INT) USING CSV"
87,COMMENT 'this is a comment'
87,TBLPROPERTIES ('foo'='bar');
87,--Specify table comment and properties with different clauses order
87,"CREATE TABLE student (id INT, name STRING, age INT) USING CSV"
87,TBLPROPERTIES ('foo'='bar')
87,COMMENT 'this is a comment';
87,--Create partitioned and bucketed table
87,"CREATE TABLE student (id INT, name STRING, age INT)"
87,USING CSV
87,PARTITIONED BY (age)
87,CLUSTERED BY (Id) INTO 4 buckets;
87,Related Statements
87,CREATE TABLE USING HIVE FORMAT
87,CREATE TABLE LIKE
88,Best Practices for Performance | SAP HANA Journey
88,Skip to content
88,Hero Backgroud Elements 2
88,Menu
88,Data & Analytics Solutions in the Cloud
88,powered by SAP HANA
88,Products
88,DATABASE AND DATA MANAGEMENT
88,SAP HANA Cloud
88,Experience the only end-to-end data management and decision-making cloud solution designed for business and enterprise-grade experiences.
88,SAP Data Intelligence Cloud
88,image/svg+xml
88,"Integrate, orchestrate, and enrich disjointed data landscapes into actionable insights."
88,ANALYTICS
88,SAP Data Warehouse Cloud
88,"Manage your data storage, federation, and run powerful applications all within a single cloud solution."
88,SAP Analytics Cloud
88,Learn how cloud analytics technology enables data visualization and improved business intelligence.
88,Platform
88,SAP Business Technology Platform
88,"We are part of SAP Business Technology Platform. See the entire range of products that span from intelligent technologies, application development and integration, analytics, and database and data mangement."
88,Learn more about SAP Business Technology Platform
88,Learn More
88,Data Defined Episode 6: The New Era of Finance and Data is Here
88,Register Now
88,Access the Ultimate Guide to Enterprise Planning
88,View Guide
88,Learning
88,GET STARTED
88,Onboarding Guides
88,Get started with guides that take you through different scenarios and product features.
88,Data and Analytics Learning Center
88,Interactive virtual learning lessons to help you gain an end-to-end understanding.
88,DEEP DIVE
88,Learning Overview
88,Find curated resources to help you learn.
88,Learning Tracks
88,A resource list of selected learning material on different topics and workflows.
88,Webinars
88,Sign up for the latest webinars and view past recordings by our experts.
88,Featured Learning Articles
88,SAP HANA Cloud Onboarding Guide
88,SAP Data Warehouse Cloud Onboarding Guide
88,Press Start with SAP Analytics Cloud
88,Data and Analytics Learning Center
88,Register Now
88,Resources
88,ENGAGE
88,Blogs
88,Discover the latest insights and think pieces around data management and analytics.
88,Customer Stories
88,See how other businesses are becoming data-driven.
88,Community
88,Find answers to the most frequently asked questions about the our data and analytics cloud solutions.
88,Connect
88,Partners
88,Partners help accelerate businesses and are a part of SAP's global partner network.
88,Ottogi Elevates Data and Analytics with New Predictive Capabilities
88,Read Customer Story
88,"SAP HANA Cloud, Adaptive Server Enterprise and Data Lake for Financial Services"
88,Read Blog
88,LOG IN
88,SAP Data Warehouse Cloud
88,SAP Analytics Cloud Trial
88,LOG IN
88,SAP Data Warehouse Cloud
88,SAP Analytics Cloud Trial
88,Try for free
88,What would you like to learn about?
88,Submit
88,Reading Time: 90 Min
88,Share this article:
88,Twitter
88,Facebook
88,LinkedIn
88,Best Practices for Performance
88,Learning Article
88,SAP Analytics Cloud
88,"SAP Analytics Cloud enables you to perform sophisticated analysis on large volumes of intricate data. These complex scenarios can sometimes lead to less than ideal performance times for end-users. However, there are things you can do when designing models and stories that will help SAP Analytics Cloud run at optimal performance levels. In this post, we’ll share some best practices and tips to help keep things running smoothly."
88,Last updated February 2021
88,Table of Contents
88,General
88,Modeling and Data
88,Story Design
88,Explorer
88,Charts and Tables
88,Geospatial
88,Filters
88,Planning
88,Mobile
88,Story Maintenance
88,Story Performance Tips & Tricks Webinar Recording
88,Bonus: Advanced Formulas Best Practices documentation on Help Portal
88,General
88,"Always use Google Chrome, our recommended browser, to take advantage of the latest performance improvements in SAP Analytics Cloud. * new"
88,"Take advantage of improved performance with browser cashing of story and boardroom. This is particularly important for stories with multiple pages, charts or models. Cache is valid as long as there are no structural changes made in the story or boardroom. Note that this performance improvement is only available for Chrome users in a non-incognito mode. * new"
88,Modeling and Data
88,"In Model Preferences under Data and Performance, toggle on “Optimize Story Building Performance”. This prevents the automatic refresh of data during story design."
88,"Tip: When editing a story, designers can manually refresh data when desired using the data refresh prompt in the builder."
88,"Whenever possible, choose to show unbooked data in a chart. This means that the software has to spend less time differentiating between booked and unbooked data."
88,"Avoid specifying Exception Aggregations in the Model (image below) and instead, use the Restricted Measures or Calculation functionality in your stories."
88,Back to top
88,Story Design
88,Try designing your story using Responsive pages instead of Canvas or Grid pages. Responsive pages allow your story content to re-flow depending on the size of the screen it is being viewed on.
88,Tip: Responsive pages are the only page-type that can be viewed with the SAP Analytics Cloud Mobile App for iOS.
88,"Rather than designing a large story with many pages, try to limit your pages and create different stories for each use-case or audience."
88,"Tip: If you want to refer to a related story, you can add a hyperlink to a different page, story, or external website."
88,Try to keep the number of individual tiles on each page of your story limited to six or less
88,Limit the number of content-rich tiles on each page like maps or charts with a high volume of data points. Overloading your pages with dense information will make it harder for your viewers to consume and may slow load-time.
88,Tip: Use pages to break up your story by category or type of information. Put your most-viewed content on the first page to make it easily accessible.
88,When adding images to your pages ensure that the images are sized for web and are smaller than 1MB
88,"Tip: SVG vectors image files still look great at a small file size. If you can’t use an SVG image file, PNG image files perform better than JPG."
88,"To ensure performance when working with blended data,  avoid creating Linked Dimensions on Calculated Dimensions. Keeping the number of models linked in each story at a minimum will also improve overall speed and performance."
88,Back to top
88,Explorer
88,"If your model contains a lot of dimensions, choose particular dimensions to show in the explorer rather than showing them all. This cuts down on scrolling and makes it easier for you to find the dimensions you need."
88,"Tip: When using the explorer to create a visualization, try de-selecting the “Automatically Synchronize Visualization” button. By turning off this feature you can change your measure and dimension selections without auto-refreshing the visualization. When you’re ready, simply click the “Synchronize Visualization” button to update your visualization."
88,"Story designers can choose to enable the explorer on charts and tables while in view-mode. When configuring this option, make sure to choose relevant measures and dimensions for your viewers to explore."
88,Back to top
88,Charts and Tables
88,Lowering the number of individual data points makes the information in your stories easier to read and analyze.
88,"When adding tables to your story, keep in mind the goal of the table and ensure that viewers can easily navigate the information. To help ensure readability, try to limit your tables to a maximum of 500 rows and 60 columns. If you need to display more, you can edit the drill limitation. Keep in mind that the more cells you include, the harder SAP Analytics Cloud has to work to display them."
88,Tip: Apply chart filters to reduce the volume of information in charts with more than 1000+ data points and use table filters to keep a manageable amount of information visible in your tables.
88,For a Dimension with a large Hierarchy and an ALL node it is recommended to use the chart drill capability for best performance.
88,"For Tables with Large datasets, it is recommended to avoid Formatting Rules"
88,Back to top
88,Geospatial
88,"Location Clustering is automatically enabled when there are more than 5,000 points on a bubble layer. To even further optimize your bubble layer, switch on Location Clustering and choose 1,000 for the maximum number of display points."
88,Tip: If you are working with thousands of locations consider using the choropleth layer instead. This layer aggregates your data into shapes and you’re able to filter and drill-down on the shapes to get further insights into location data.
88,"To create a location-enabled model from a live HANA data connection, you must first prepare a calculation view with location dimensions. Location Data must be prepared through SAP HANA Studio. After creating a model based on the calculation view in SAP Analytics Cloud, you can add one or more location dimensions to map the Location Data imported from the live HANA system. For help, download instructions on Creating a Geo model from Live HANA Calculation View."
88,Back to top
88,Filters
88,Use filtering relevant for your target audience to limit the number of facts in your story to one million or less. The one million fact limit has been set purposefully to help you work with specific meaningful information rather than high-volumes.
88,"If you’re creating a story with many elements based on the same information, try adding story filter capabilities instead of individual filters for pages, charts, or tables."
88,"Note: The exception to this suggestion is for filtering on information from models with linked dimensions. In these cases, it is best to add a filter to the individual tile when filtering on dimensions not used for linking."
88,Tip: Page filters that are displayed as tokens (eg. Product below) take up less space in your story and are easier to work with than expanded page filters (eg. Location below) with many members.
88,"When setting up your filter, try to select specific members rather than selecting “All Members”. Filters with fewer members are easier to use during analysis and when viewing a story."
88,It is recommended to limit the number of Members in a Dimension used in Member Selectors and Filters to the minimum possible number of Members.
88,"Tip: For members with large hierarchies, you can limit the number of levels available in the filter to make things easier to navigate. It’s also helpful to design your story with the most used level displayed."
88,"Tip: When drilling down, you can jump several levels at once to avoid loading levels that you are not interested in."
88,"The Cascading Effect feature is enabled on pages with multiple filters by default. When the feature is enabled, selections made on a story or page filter will affect related filters in the same story or page. Toggling off the Cascading Effect feature reduces query volumes and can speed up filter performance."
88,When adding date and time range filters you’re able to choose between a dynamic or fixed range of time. Check the SAP Analytics Cloud Help documentation for a detailed explanation of each option and tips on how to customize these filters.
88,"For Tables with Large datasets, it is recommended to avoid Hidden Combinations and to use the Filter instead to only show the necessary Columns."
88,Tip: If you have a number of input controls on the page – leave them collapsed for improved performance! *new
88,"Loading the list of values and cascading effects for collapsed story, page or topic filters will be delayed until the user selects the filter. Therefore, greater priority is given to displaying data on the page found in charts, tables and expanded input controls. *new"
88,Back to top
88,Planning
88,"When working with planning data, end users are mostly concerned with the numbers. When designing stories with planning information, make sure to limit the number of descriptive columns in your tables. This way users of your story will be able to access the information they need as quickly as possible."
88,Before:
88,After:
88,Back to top
88,Mobile
88,"Viewing your stories with the SAP Analytics Cloud Mobile App for iOS is extremely convenient. However, a mobile device looks and acts a bit differently than your other screens. There are a few ways that you can help ensure your stories are useful when viewed on mobile devices."
88,Design your stories using Responsive pages. This is the only page-type that is accessible from the Mobile App for iOS.
88,Make sure you have some free space on your device. The Mobile App performs best on iOS devices with 2GB or more of free space.
88,"Limit the number of data points and values displayed on charts designed for mobile. This helps to ensure that viewers of your story are able to navigate the information, no matter what size of screen they’re working with."
88,"For even more information on the mobile app, check out the SAP Analytics Cloud Mobile App Learning Track."
88,Back to top
88,Story Maintenance
88,Remove any unused pages in your story to help streamline your information and improve navigation for your users
88,"When exiting your story, accept the prompt to remove models that are not in use. You can always add new models when editing your story if you need to."
88,"To avoid saving uncompleted work, SAP Analytics Cloud does not automatically save your stories.  So, remember to save changes that you’ve made when editing your stories before closing your browser."
88,"Tip: To take advantage of performance improvements in each new release, you’ll have to open your stories in edit-mode and click save. If you have an old story, take a few seconds to re-save it! The latest performance improvements will be applied to help you enjoy your story content. *new"
88,Back to top
88,Re-Watch the Story Performance Tips & Tricks Webinar
88,"We’ve been hard at work adding a number of features for speeding up your story performance. In this webinar, we’ll review recent performance enhancements, learn specific changes you can make to improve performance and end with a Q&A."
88,Watch the recording
88,Back to top
88,Best-Practice Guides
88,Related Learning
88,Creating Restricted Measures
88,SAP Analytics Cloud Mobile App
88,Story and Page Filters
88,Creating Calculated Dimensions
88,Creating Linked Dimensions
88,GET STARTED TODAY
88,Discover our data management and analytics offerings to expand on your data journey. See how you can start making trusted business decisions with your data today.
88,Try for free
88,image/svg+xml
88,SAP HANA Cloud
88,image/svg+xml
88,SAP Data Intelligence Cloud
88,SAP Data Warehouse Cloud
88,image/svg+xml
88,SAP Analytics Cloud
88,Resources
88,Blogs
88,Customer Stories
88,Partners
88,Content Packages
88,Quick Answers
88,SAP HANA Cloud
88,SAP Data Warehouse Cloud
88,SAP Analytics Cloud
88,Community
88,SAP HANA Cloud
88,SAP Data Warehouse Cloud
88,SAP Analytics Cloud
88,Learning
88,Learning Center
88,Learning Tracks
88,Webinars
88,Onboarding Guides
88,SAP HANA Cloud
88,SAP Data Warehouse Cloud
88,Solutions
88,SAP HANA On-premise Extension
88,Data Warehouse Cloud Solutions Overview
88,SAP Analytics Cloud: Finance and Supply Chain
88,SAP Analytics Cloud: Human Resources
88,Privacy Statement
88,Terms of Use
88,Legal Disclosure
88,Copyright
88,Trademark
88,Login
88,SAP Data Warehouse Cloud
88,SAP Analytics Cloud Trial
88,All Rights Reserved. 2021 SAP Data Warehouse Cloud
88,Group 26
88,Created with Sketch.
88,Exit Video
89,Apache Flink 1.12 Documentation: JDBC SQL Connector
89,v1.12
89,Home
89,Try Flink
89,Local Installation
89,Fraud Detection with the DataStream API
89,Real Time Reporting with the Table API
89,Flink Operations Playground
89,Learn Flink
89,Overview
89,Intro to the DataStream API
89,Data Pipelines & ETL
89,Streaming Analytics
89,Event-driven Applications
89,Fault Tolerance
89,Concepts
89,Overview
89,Stateful Stream Processing
89,Timely Stream Processing
89,Flink Architecture
89,Glossary
89,Application Development
89,DataStream API
89,Overview
89,Execution Mode (Batch/Streaming)
89,Event Time
89,Overview
89,Generating Watermarks
89,Builtin Watermark Generators
89,State & Fault Tolerance
89,Overview
89,Working with State
89,The Broadcast State Pattern
89,Checkpointing
89,Queryable State
89,State Backends
89,State Schema Evolution
89,Custom State Serialization
89,User-Defined Functions
89,Operators
89,Overview
89,Windows
89,Joining
89,Process Function
89,Async I/O
89,Data Sources
89,Side Outputs
89,Handling Application Parameters
89,Testing
89,Experimental Features
89,Scala API Extensions
89,Java Lambda Expressions
89,Project Configuration
89,DataSet API
89,Overview
89,Transformations
89,Iterations
89,Zipping Elements
89,Hadoop Compatibility
89,Local Execution
89,Cluster Execution
89,Batch Examples
89,Table API & SQL
89,Overview
89,Concepts & Common API
89,Streaming Concepts
89,Overview
89,Dynamic Tables
89,Time Attributes
89,Versioned Tables
89,Joins in Continuous Queries
89,Detecting Patterns
89,Query Configuration
89,Legacy Features
89,Data Types
89,Table API
89,SQL
89,Overview
89,Queries
89,CREATE Statements
89,DROP Statements
89,ALTER Statements
89,INSERT Statement
89,SQL Hints
89,DESCRIBE Statements
89,EXPLAIN Statements
89,USE Statements
89,SHOW Statements
89,Functions
89,Overview
89,System (Built-in) Functions
89,User-defined Functions
89,Modules
89,Catalogs
89,SQL Client
89,Configuration
89,Performance Tuning
89,Streaming Aggregation
89,User-defined Sources & Sinks
89,Python API
89,Overview
89,Installation
89,Table API Tutorial
89,DataStream API Tutorial
89,Table API User's Guide
89,Intro to the Python Table API
89,TableEnvironment
89,Operations
89,Data Types
89,System (Built-in) Functions
89,User Defined Functions
89,General User-defined Functions
89,Vectorized User-defined Functions
89,Conversions between PyFlink Table and Pandas DataFrame
89,Dependency Management
89,SQL
89,Catalogs
89,Metrics
89,Connectors
89,DataStream API User's Guide
89,Data Types
89,Operators
89,Dependency Management
89,Configuration
89,Environment Variables
89,FAQ
89,Data Types & Serialization
89,Overview
89,Custom Serializers
89,Managing Execution
89,Execution Configuration
89,Program Packaging
89,Parallel Execution
89,Execution Plans
89,Task Failure Recovery
89,API Migration Guides
89,Libraries
89,Event Processing (CEP)
89,State Processor API
89,Graphs: Gelly
89,Overview
89,Graph API
89,Iterative Graph Processing
89,Library Methods
89,Graph Algorithms
89,Graph Generators
89,Bipartite Graph
89,Connectors
89,DataStream Connectors
89,Overview
89,Fault Tolerance Guarantees
89,Kafka
89,Cassandra
89,Kinesis
89,Elasticsearch
89,File Sink
89,Streaming File Sink
89,RabbitMQ
89,NiFi
89,Google Cloud PubSub
89,Twitter
89,JDBC
89,Table & SQL Connectors
89,Overview
89,Formats
89,Overview
89,CSV
89,JSON
89,Confluent Avro
89,Avro
89,Debezium
89,Canal
89,Maxwell
89,Parquet
89,Orc
89,Raw
89,Kafka
89,Upsert Kafka
89,Kinesis
89,JDBC
89,Elasticsearch
89,FileSystem
89,HBase
89,DataGen
89,Print
89,BlackHole
89,Hive
89,Overview
89,Hive Catalog
89,Hive Dialect
89,Hive Read & Write
89,Hive Functions
89,Download
89,DataSet Connectors
89,Deployment
89,Overview
89,Resource Providers
89,Standalone
89,Overview
89,Docker
89,Kubernetes
89,Native Kubernetes
89,YARN
89,Mesos
89,Configuration
89,Memory Configuration
89,Set up Flink's Process Memory
89,Set up TaskManager Memory
89,Set up JobManager Memory
89,Memory tuning guide
89,Troubleshooting
89,Migration Guide
89,Command-Line Interface
89,File Systems
89,Overview
89,Common Configurations
89,Amazon S3
89,Aliyun OSS
89,Azure Blob Storage
89,Plugins
89,High Availability (HA)
89,Overview
89,ZooKeeper HA Services
89,Kubernetes HA Services
89,Metric Reporters
89,Security
89,SSL Setup
89,Kerberos
89,REPLs
89,Python REPL
89,Scala REPL
89,Advanced
89,External Resources
89,History Server
89,Logging
89,Operations
89,State & Fault Tolerance
89,Checkpoints
89,Savepoints
89,State Backends
89,Tuning Checkpoints and Large State
89,Metrics
89,REST API
89,Debugging
89,Debugging Windows & Event Time
89,Debugging Classloading
89,Application Profiling & Debugging
89,Monitoring
89,Monitoring Checkpointing
89,Monitoring Back Pressure
89,Upgrading Applications and Flink Versions
89,Production Readiness Checklist
89,Flink Development
89,Importing Flink into an IDE
89,Building Flink from Source
89,Internals
89,Jobs and Scheduling
89,Task Lifecycle
89,File Systems
89,Javadocs
89,Scaladocs
89,Pythondocs
89,Project Page
89,Pick Docs Version
89,v1.11
89,v1.10
89,v1.9
89,v1.8
89,v1.7
89,v1.6
89,v1.5
89,v1.4
89,v1.3
89,v1.2
89,v1.1
89,v1.0
89,中文版
89,Connectors
89,Table & SQL Connectors
89,JDBC
89,JDBC SQL Connector
89,Scan Source: Bounded
89,Lookup Source: Sync Mode
89,Sink: Batch
89,Sink: Streaming Append & Upsert Mode
89,Dependencies
89,How to create a JDBC table
89,Connector Options
89,Features
89,Key handling
89,Partitioned Scan
89,Lookup Cache
89,Idempotent Writes
89,Postgres Database as a Catalog
89,Data Type Mapping
89,The JDBC connector allows for reading data from and writing data into any relational databases with a JDBC driver. This document describes how to setup the JDBC connector to run SQL queries against relational databases.
89,"The JDBC sink operate in upsert mode for exchange UPDATE/DELETE messages with the external system if a primary key is defined on the DDL, otherwise, it operates in append mode and doesn’t support to consume UPDATE/DELETE messages."
89,Dependencies
89,In order to use the JDBC connector the following
89,dependencies are required for both projects using a build automation tool (such as Maven or SBT)
89,and SQL Client with SQL JAR bundles.
89,Maven dependency
89,SQL Client JAR
89,<dependency>
89,<groupId>org.apache.flink</groupId>
89,<artifactId>flink-connector-jdbc_2.11</artifactId>
89,<version>1.12.0</version>
89,</dependency>
89,Download
89,A driver dependency is also required to connect to a specified database. Here are drivers currently supported:
89,Driver
89,Group Id
89,Artifact Id
89,JAR
89,MySQL
89,mysql
89,mysql-connector-java
89,Download
89,PostgreSQL
89,org.postgresql
89,postgresql
89,Download
89,Derby
89,org.apache.derby
89,derby
89,Download
89,JDBC connector and drivers are not currently part of Flink’s binary distribution. See how to link with them for cluster execution here.
89,How to create a JDBC table
89,The JDBC table can be defined as following:
89,-- register a MySQL table 'users' in Flink SQL
89,CREATE TABLE MyUserTable (
89,"id BIGINT,"
89,"name STRING,"
89,"age INT,"
89,"status BOOLEAN,"
89,PRIMARY KEY (id) NOT ENFORCED
89,) WITH (
89,"'connector' = 'jdbc',"
89,"'url' = 'jdbc:mysql://localhost:3306/mydatabase',"
89,'table-name' = 'users'
89,"-- write data into the JDBC table from the other table ""T"""
89,INSERT INTO MyUserTable
89,"SELECT id, name, age, status FROM T;"
89,-- scan data from the JDBC table
89,"SELECT id, name, age, status FROM MyUserTable;"
89,-- temporal join the JDBC table as a dimension table
89,SELECT * FROM myTopic
89,LEFT JOIN MyUserTable FOR SYSTEM_TIME AS OF myTopic.proctime
89,ON myTopic.key = MyUserTable.id;
89,Connector Options
89,Option
89,Required
89,Default
89,Type
89,Description
89,connector
89,required
89,(none)
89,String
89,"Specify what connector to use, here should be 'jdbc'."
89,url
89,required
89,(none)
89,String
89,The JDBC database url.
89,table-name
89,required
89,(none)
89,String
89,The name of JDBC table to connect.
89,driver
89,optional
89,(none)
89,String
89,"The class name of the JDBC driver to use to connect to this URL, if not set, it will automatically be derived from the URL."
89,username
89,optional
89,(none)
89,String
89,The JDBC user name. 'username' and 'password' must both be specified if any of them is specified.
89,password
89,optional
89,(none)
89,String
89,The JDBC password.
89,scan.partition.column
89,optional
89,(none)
89,String
89,The column name used for partitioning the input. See the following Partitioned Scan section for more details.
89,scan.partition.num
89,optional
89,(none)
89,Integer
89,The number of partitions.
89,scan.partition.lower-bound
89,optional
89,(none)
89,Integer
89,The smallest value of the first partition.
89,scan.partition.upper-bound
89,optional
89,(none)
89,Integer
89,The largest value of the last partition.
89,scan.fetch-size
89,optional
89,Integer
89,"The number of rows that should be fetched from the database when reading per round trip. If the value specified is zero, then the hint is ignored."
89,scan.auto-commit
89,optional
89,true
89,Boolean
89,"Sets the auto-commit flag on the JDBC driver,"
89,"which determines whether each statement is committed in a transaction automatically. Some JDBC drivers, specifically"
89,"Postgres, may require this to be set to false in order to stream results."
89,lookup.cache.max-rows
89,optional
89,(none)
89,Integer
89,"The max number of rows of lookup cache, over this value, the oldest rows will be expired."
89,Lookup cache is disabled by default. See the following Lookup Cache section for more details.
89,lookup.cache.ttl
89,optional
89,(none)
89,Duration
89,"The max time to live for each rows in lookup cache, over this time, the oldest rows will be expired."
89,Lookup cache is disabled by default. See the following Lookup Cache section for more details.
89,lookup.max-retries
89,optional
89,Integer
89,The max retry times if lookup database failed.
89,sink.buffer-flush.max-rows
89,optional
89,100
89,Integer
89,The max size of buffered records before flush. Can be set to zero to disable it.
89,sink.buffer-flush.interval
89,optional
89,Duration
89,"The flush interval mills, over this time, asynchronous threads will flush data. Can be set to '0' to disable it. Note, 'sink.buffer-flush.max-rows' can be set to '0' with the flush interval set allowing for complete async processing of buffered actions."
89,sink.max-retries
89,optional
89,Integer
89,The max retry times if writing records to database failed.
89,Features
89,Key handling
89,"Flink uses the primary key that defined in DDL when writing data to external databases. The connector operate in upsert mode if the primary key was defined, otherwise, the connector operate in append mode."
89,"In upsert mode, Flink will insert a new row or update the existing row according to the primary key, Flink can ensure the idempotence in this way. To guarantee the output result is as expected, it’s recommended to define primary key for the table and make sure the primary key is one of the unique key sets or primary key of the underlying database table. In append mode, Flink will interpret all records as INSERT messages, the INSERT operation may fail if a primary key or unique constraint violation happens in the underlying database."
89,See CREATE TABLE DDL for more details about PRIMARY KEY syntax.
89,Partitioned Scan
89,"To accelerate reading data in parallel Source task instances, Flink provides partitioned scan feature for JDBC table."
89,All the following scan partition options must all be specified if any of them is specified. They describe how to partition the table when reading in parallel from multiple tasks.
89,"The scan.partition.column must be a numeric, date, or timestamp column from the table in question. Notice that scan.partition.lower-bound and scan.partition.upper-bound are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned."
89,scan.partition.column: The column name used for partitioning the input.
89,scan.partition.num: The number of partitions.
89,scan.partition.lower-bound: The smallest value of the first partition.
89,scan.partition.upper-bound: The largest value of the last partition.
89,Lookup Cache
89,"JDBC connector can be used in temporal join as a lookup source (aka. dimension table). Currently, only sync lookup mode is supported."
89,"By default, lookup cache is not enabled. You can enable it by setting both lookup.cache.max-rows and lookup.cache.ttl."
89,"The lookup cache is used to improve performance of temporal join the JDBC connector. By default, lookup cache is not enabled, so all the requests are sent to external database."
89,"When lookup cache is enabled, each process (i.e. TaskManager) will hold a cache. Flink will lookup the cache first, and only send requests to external database when cache missing, and update cache with the rows returned."
89,The oldest rows in cache will be expired when the cache hit to the max cached rows lookup.cache.max-rows or when the row exceeds the max time to live lookup.cache.ttl.
89,"The cached rows might not be the latest, users can tune lookup.cache.ttl to a smaller value to have a better fresh data, but this may increase the number of requests send to database. So this is a balance between throughput and correctness."
89,Idempotent Writes
89,"JDBC sink will use upsert semantics rather than plain INSERT statements if primary key is defined in DDL. Upsert semantics refer to atomically adding a new row or updating the existing row if there is a unique constraint violation in the underlying database, which provides idempotence."
89,"If there are failures, the Flink job will recover and re-process from last successful checkpoint, which can lead to re-processing messages during recovery. The upsert mode is highly recommended as it helps avoid constraint violations or duplicate data if records need to be re-processed."
89,"Aside from failure recovery, the source topic may also naturally contain multiple records over time with the same primary key, making upserts desirable."
89,"As there is no standard syntax for upsert, the following table describes the database-specific DML that is used."
89,Database
89,Upsert Grammar
89,MySQL
89,INSERT .. ON DUPLICATE KEY UPDATE ..
89,PostgreSQL
89,INSERT .. ON CONFLICT .. DO UPDATE SET ..
89,Postgres Database as a Catalog
89,The JdbcCatalog enables users to connect Flink to relational databases over JDBC protocol.
89,"Currently, PostgresCatalog is the only implementation of JDBC Catalog at the moment, PostgresCatalog only supports limited Catalog methods include:"
89,// The supported methods by Postgres Catalog.
89,PostgresCatalog.databaseExists(String databaseName)
89,PostgresCatalog.listDatabases()
89,PostgresCatalog.getDatabase(String databaseName)
89,PostgresCatalog.listTables(String databaseName)
89,PostgresCatalog.getTable(ObjectPath tablePath)
89,PostgresCatalog.tableExists(ObjectPath tablePath)
89,Other Catalog methods is unsupported now.
89,Usage of PostgresCatalog
89,Please refer to Dependencies section for how to setup a JDBC connector and Postgres driver.
89,Postgres catalog supports the following options:
89,"name: required, name of the catalog."
89,"default-database: required, default database to connect to."
89,"username: required, username of Postgres account."
89,"password: required, password of the account."
89,"base-url: required, should be of format ""jdbc:postgresql://<ip>:<port>"", and should not contain database name here."
89,CREATE CATALOG mypg WITH(
89,"'type' = 'jdbc',"
89,"'default-database' = '...',"
89,"'username' = '...',"
89,"'password' = '...',"
89,'base-url' = '...'
89,USE CATALOG mypg;
89,EnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build();
89,TableEnvironment tableEnv = TableEnvironment.create(settings);
89,String name
89,"= ""mypg"";"
89,"String defaultDatabase = ""mydb"";"
89,String username
89,"= ""..."";"
89,String password
89,"= ""..."";"
89,String baseUrl
89,"= ""..."""
89,"JdbcCatalog catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl);"
89,"tableEnv.registerCatalog(""mypg"", catalog);"
89,// set the JdbcCatalog as the current catalog of the session
89,"tableEnv.useCatalog(""mypg"");"
89,val settings = EnvironmentSettings.newInstance().inStreamingMode().build()
89,val tableEnv = TableEnvironment.create(settings)
89,val name
89,"= ""mypg"""
89,"val defaultDatabase = ""mydb"""
89,val username
89,"= ""..."""
89,val password
89,"= ""..."""
89,val baseUrl
89,"= ""..."""
89,"val catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl)"
89,"tableEnv.registerCatalog(""mypg"", catalog)"
89,// set the JdbcCatalog as the current catalog of the session
89,"tableEnv.useCatalog(""mypg"")"
89,from pyflink.table.catalog import JdbcCatalog
89,environment_settings = EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build()
89,t_env = StreamTableEnvironment.create(environment_settings=environment_settings)
89,"name = ""mypg"""
89,"default_database = ""mydb"""
89,"username = ""..."""
89,"password = ""..."""
89,"base_url = ""..."""
89,"catalog = JdbcCatalog(name, default_database, username, password, base_url)"
89,"t_env.register_catalog(""mypg"", catalog)"
89,# set the JdbcCatalog as the current catalog of the session
89,"t_env.use_catalog(""mypg"")"
89,execution:
89,planner: blink
89,...
89,current-catalog: mypg
89,# set the JdbcCatalog as the current catalog of the session
89,current-database: mydb
89,catalogs:
89,- name: mypg
89,type: jdbc
89,default-database: mydb
89,username: ...
89,password: ...
89,base-url: ...
89,PostgresSQL Metaspace Mapping
89,"PostgresSQL has an additional namespace as schema besides database. A Postgres instance can have multiple databases, each database can have multiple schemas with a default one named “public”, each schema can have multiple tables."
89,"In Flink, when querying tables registered by Postgres catalog, users can use either schema_name.table_name or just table_name. The schema_name is optional and defaults to “public”."
89,Therefor the metaspace mapping between Flink Catalog and Postgres is as following:
89,Flink Catalog Metaspace Structure
89,Postgres Metaspace Structure
89,catalog name (defined in Flink only)
89,N/A
89,database name
89,database name
89,table name
89,[schema_name.]table_name
89,"The full path of Postgres table in Flink should be ""<catalog>.<db>.`<schema.table>`"" if schema is specified, note the <schema.table> should be escaped."
89,Here are some examples to access Postgres tables:
89,"-- scan table 'test_table' of 'public' schema (i.e. the default schema), the schema name can be omitted"
89,SELECT * FROM mypg.mydb.test_table;
89,SELECT * FROM mydb.test_table;
89,SELECT * FROM test_table;
89,"-- scan table 'test_table2' of 'custom_schema' schema,"
89,-- the custom schema can not be omitted and must be escaped with table.
89,SELECT * FROM mypg.mydb.`custom_schema.test_table2`
89,SELECT * FROM mydb.`custom_schema.test_table2`;
89,SELECT * FROM `custom_schema.test_table2`;
89,Data Type Mapping
89,"Flink supports connect to several databases which uses dialect like MySQL, PostgresSQL, Derby. The Derby dialect usually used for testing purpose. The field data type mappings from relational databases data types to Flink SQL data types are listed in the following table, the mapping table can help define JDBC table in Flink easily."
89,MySQL type
89,PostgreSQL type
89,Flink SQL type
89,TINYINT
89,TINYINT
89,SMALLINT
89,TINYINT UNSIGNED
89,SMALLINT
89,INT2
89,SMALLSERIAL
89,SERIAL2
89,SMALLINT
89,INT
89,MEDIUMINT
89,SMALLINT UNSIGNED
89,INTEGER
89,SERIAL
89,INT
89,BIGINT
89,INT UNSIGNED
89,BIGINT
89,BIGSERIAL
89,BIGINT
89,BIGINT UNSIGNED
89,"DECIMAL(20, 0)"
89,BIGINT
89,BIGINT
89,BIGINT
89,FLOAT
89,REAL
89,FLOAT4
89,FLOAT
89,DOUBLE
89,DOUBLE PRECISION
89,FLOAT8
89,DOUBLE PRECISION
89,DOUBLE
89,"NUMERIC(p, s)"
89,"DECIMAL(p, s)"
89,"NUMERIC(p, s)"
89,"DECIMAL(p, s)"
89,"DECIMAL(p, s)"
89,BOOLEAN
89,TINYINT(1)
89,BOOLEAN
89,BOOLEAN
89,DATE
89,DATE
89,DATE
89,TIME [(p)]
89,TIME [(p)] [WITHOUT TIMEZONE]
89,TIME [(p)] [WITHOUT TIMEZONE]
89,DATETIME [(p)]
89,TIMESTAMP [(p)] [WITHOUT TIMEZONE]
89,TIMESTAMP [(p)] [WITHOUT TIMEZONE]
89,CHAR(n)
89,VARCHAR(n)
89,TEXT
89,CHAR(n)
89,CHARACTER(n)
89,VARCHAR(n)
89,CHARACTER VARYING(n)
89,TEXT
89,STRING
89,BINARY
89,VARBINARY
89,BLOB
89,BYTEA
89,BYTES
89,ARRAY
89,ARRAY
89,Back to top
89,Want to contribute translation?
90,Optimizing Query Performance - InterSystems SQL Optimization Guide - InterSystems IRIS for Health 2020.4
90,Skip to main content
90,Learning
90,Documentation
90,Community
90,Open Exchange
90,Global Masters
90,Certification
90,Search:
90,InterSystems IRIS for Health 2020.4 >
90,Data Model >
90,SQL >
90,InterSystems SQL Optimization Guide >
90,Optimizing Query Performance
90,Contents
90,Management Portal SQL Performance Tools
90,SQL Runtime Statistics
90,Using Indices
90,Query Execution Plans
90,Alternate Show Plans
90,Writing Query Optimization Plans to a File
90,Comment Options
90,Parallel Query Processing
90,Generate Report
90,Scroll to top
90,Optimizing Query Performance
90,"InterSystems SQL automatically uses a Query Optimizer to create a query plan that provides optimal query performance in most circumstances. This Optimizer improves query performance in many ways, including determining which indices to use, determining the order of evaluation of multiple AND conditions, determining the sequence of tables when performing multiple joins, and many other optimization operations. You can supply &OpenCurlyDoubleQuotehints&CloseCurlyDoubleQuote to this Optimizer in the FROM clause of the query. This chapter describes tools that you can use to evaluate a query plan and to modify how InterSystems SQL will optimize a specific query."
90,InterSystems IRIS® data platform supports the following tools for optimizing SQL queries:
90,"SQL Runtime Statistics to generate runtime performance statistics on query execution. (For more in-depth analysis of the performance of specific queries in a development environment, refer to the &OpenCurlyDoubleQuoteSQL Performance Analysis Toolkit&CloseCurlyDoubleQuote chapter of this manual.)"
90,"Index Analyzer to display various index analyzer reports for all queries in the current namespace. This shows how InterSystems SQL is going to execute the query, giving you an overall view of how indices are being used. This index analysis may indicate that you should add one or more indices to improve performance."
90,"Query Execution Plans to display the optimal (default) execution plan for an SQL query (query plan), and optionally display alternate query plans for that SQL query, with statistics. Tools to display query plans include the SQL EXPLAIN command, the $SYSTEM.SQL.Explain() method, and various Show Plan tools from the Management Portal and the SQL Shell. query plans and statistics are generated when the query is prepared; it is not necessary to execute the query."
90,"You can direct the Query Optimizer by using the following options, either by setting configuration defaults or by coding optimizer &OpenCurlyDoubleQuotehints&CloseCurlyDoubleQuote in the query code:"
90,"Index Optimization Options available FROM clause options governing all conditions, or %NOINDEX prefacing an individual condition."
90,Comment Options specified in the SQL code that cause the Optimizer to override a system-wide compile option for that query.
90,Parallel Query Processing available on a per-query or system-wide basis allows multi-processor systems to divide query execution amongst the processors.
90,The following SQL query performance tools are described in other chapters of this manual:
90,Cached Queries to enable Dynamic SQL queries to be rerun without the overhead of preparing the query each time it is executed.
90,SQL Statements to preserve the most-recently compiled Embedded SQL query. In the &OpenCurlyDoubleQuoteSQL Statements and Frozen Plans&CloseCurlyDoubleQuote chapter.
90,Frozen Plans to preserve a specific compile of an Embedded SQL query. This compile is used rather than a more recent compile. In the &OpenCurlyDoubleQuoteSQL Statements and Frozen Plans&CloseCurlyDoubleQuote chapter.
90,"The following tools are used to optimize table data, and thus can have a significant effect on all queries run against that table:"
90,Defining Indices can significantly speed access to data in specific indexed fields.
90,"ExtentSize, Selectivity, and BlockCount to specify table data estimates before populating the table with data; this metadata is used to optimize future queries."
90,Tune Table to analyze representative table data in a populated table; this generated metadata is used to optimize future queries.
90,"This chapter also describes how to Write Query Optimization Plans to a File, and how to generate an SQL Troubleshooting Report to submit to InterSystems WRC."
90,Management Portal SQL Performance ToolsCopy link to this section
90,The InterSystems IRIS Management Portal provides access to the following SQL performance tools. There are two ways to access these tools from the Management Portal System Explorer option:
90,"Select Tools, then select SQL Performance Tools."
90,"Select SQL, then select the Tools drop-down menu."
90,From either interface you can select one of the following SQL performance tools:
90,SQL Runtime Statistics to generate performance statistics on query execution.
90,"Index Analyzer to display various index analyzer reports for all queries in the current namespace. This shows how InterSystems SQL is going to execute the query, giving you an overall view of how indices are being used. This index analysis may indicate that you should add one or more indices to improve performance."
90,"Alternate Show Plans to display available alternate query plans for an SQL query, with statistics."
90,Generate Report to submit an SQL query performance report to InterSystems Worldwide Response Center (WRC) customer support. To use this reporting tool you must first get a WRC tracking number from the WRC.
90,Import Report allows you to view SQL query performance reports.
90,SQL Runtime StatisticsCopy link to this section
90,"You can use SQL Runtime Statistics to measure the performance of SQL queries running on your system. SQL Runtime Statistics measures the performance of SELECT, INSERT, UPDATE, and DELETE operations (collectively known as query operations). SQL runtime statistics (SQL Stats) are gathered when a query operation is Prepared. See Using the SQL Runtime Statistics Tool."
90,"Gathering of SQL runtime statistics is off by default. You must activate the gathering of statistics. It is highly recommended that you specify a timeout to end the gathering of statistics. After activate the gathering of statistics, you must recompile (Prepare) existing Dynamic SQL queries and recompile classes and routines that contain Embedded SQL."
90,"Performance statistics include the ModuleName, ModuleCount (the number of times a module is called), RowCount (number of rows returned), TimeSpent (execution performance in seconds), GlobalRefs (number of global references), LinesOfCode (number of lines executed), and the ReadLatency (the disk read access time, in milliseconds). For details, see Stats Values."
90,You can explicitly purge (clear) SQL Stats data. Purging a cached query deletes any related SQL Stats data. Dropping a table or view deletes any related SQL Stats data.
90,Note:
90,"A system task is automatically run once per hour in all namespaces to aggregate process-specific SQL query statistics into global statistics. Therefore, the global statistics may not reflect statistics gathered within the hour. You can use the Management Portal to monitor this hourly aggregation or to force it to occur immediately. To view when this task was last finished and next scheduled, select System Operation, Task Manager, Task Schedule and view the Update SQL query statistics task. You can click on the task name for task details. From the Task Details display you can use the Run button to force the task to be performed immediately."
90,Using the SQL Runtime Statistics ToolCopy link to this section
90,You can display performance statistics for SQL queries system-wide from the Management Portal using either of the following:
90,"Select System Explorer, select Tools, select SQL Performance Tools, then select SQL Runtime Statistics."
90,"Select System Explorer, select SQL, then from the Tools drop-down menu select SQL Runtime Statistics."
90,SettingsCopy link to this section
90,The Settings tab displays the current system-wide SQL Runtime Statistics setting and when this setting will expire.
90,the Change Settings button allows you to set the following statistics collection options:
90,"Collection Option: you can set the statistics collection option to 0, 1, 2, or 3."
90,"0 = turn off statistics code generation; 1 = turn on statistics code generation for all queries, but do not gather statistics; 2 = record statistics for just the outer loop of the query (gather statistics at the open and close of the MAIN module); 3 = record statistics for all module levels of the query."
90,"To go from 0 to 1: after changing the SQL Stats option, Routines and Classes that contain SQL will need to be compiled to perform statistics code generation. For xDBC and Dynamic SQL, you must purge cached queries to force code regeneration."
90,To go from 1 to 2: you simply change the SQL Stats option to begin gathering statistics. This allows you to enable SQL performance analysis on a running production environment with minimal disruption.
90,"To go from 1 to 3 (or 2 to 3): after changing the SQL Stats option, Routines and Classes that contain SQL will need to be compiled to record statistics for all module levels. For xDBC and Dynamic SQL, you must purge cached queries to force code regeneration. Option 3 is commonly only used on an identified poorly-performing query in a non-production environment."
90,"To go from 1, 2, or 3 to 0: to turn off statistics code generation you do not need to purge cached queries."
90,"Timeout Option: if the Collection Option is 2 or 3, you can specify a timeout by elapsed time (hours or minutes) or by a completion date and time. You can specify elapsed time in minutes or in hours and minutes; the tool converts a specified minutes value to hours and minutes (100 minutes = 1 hour, 40 minutes). The default is 50 minutes. The date and time option defaults to just before midnight (23:59) of the current day. It is highly recommended that you specify a timeout option."
90,"Reset Option: if the Collection Option is 2 or 3, you can specify the Collection Option to reset to when the Timeout value expires. The available options are 0 and 1."
90,Query TestCopy link to this section
90,"The Query Test tab allows you to input an SQL query text (or retrieve one from History) and then display the SQL Stats and Query Plan for that query. Query Test includes the SQL Stats for all module levels of the query, regardless of the Collection Option setting."
90,"Input an SQL query text, or retrieve one using the Show History button. You can clear the query text field by clicking the round ""X"" circle on the right hand side."
90,Use the Show Plan With SQL Stats button to execute.
90,"The Run Show Plan process in the background check box is unselected by default, which is the preferred setting for most queries. Select this check box only for long, slow-running queries. When this check box is selected, you will see a progress bar displayed with a ""Please wait..."" message. While a long query is being run, the Show Plan With SQL Stats and Show History buttons disappear and a View Process button is shown. Clicking View Process opens the Process Details page in a new tab. From the Process Details page, you can view the process, and may Suspend, Resume or Terminate the process. The status of the process should be reflected on the Show Plan page. When the process is finished, the Show Plan shows the result. The View Process button disappears and the Show Plan With SQL Stats and Show History buttons reappear."
90,The Statement Text displayed using Query Test includes comments and does not perform literal substitution.
90,View StatsCopy link to this section
90,The View Stats tab gives you an overall view of the runtime statistics that have been gathered on this system.
90,You can click on any one of the View Stats column headers to sort the query statistics. You can then click the SQL Statement text to view the detailed Query Statistics and the Query Plan for the selected query.
90,The Statement Text displayed using this tool includes comments and does not perform literal substitution. The Statement Text displayed by exportStatsSQL() and by Show Plan strips out comments and performs literal substitution.
90,Purge Stats ButtonCopy link to this section
90,"The Purge Stats button clears all of the accumulated statistics for all queries in the current namespace. It displays a message on the SQL Runtime Statistics page. If successful, a message indicates the number of stats purged. If there were no stats, the Nothing to purge message is displayed. If the purge was unsuccessful, an error message is displayed."
90,Runtime Statistics and Show PlanCopy link to this section
90,The SQL Runtime Statistics tool can be used to display the Show Plan for a query with runtime statistics.
90,"The Alternate Show Plans tool can be used to compare show plans with stats, displaying runtime statistics for a query. The Alternate Show Plans tool in its Show Plan Options displays estimated statistics for a query. If gathering runtime statistics is activated, its Compare Show Plans with Stats option displays actual runtime statistics; if runtime statistics are not active, this option displays estimate statistics."
90,Using IndicesCopy link to this section
90,Indexing provides a mechanism for optimizing queries by maintaining a sorted subset of commonly requested data. Determining which fields should be indexed requires some thought: too few or the wrong indices and key queries will run too slowly; too many indices can slow down INSERT and UPDATE performance (as the index values must be set or updated).
90,What to IndexCopy link to this section
90,"To determine if adding an index improves query performance, run the query from the Management Portal SQL interface and note in Performance the number of global references. Add the index and then rerun the query, noting the number of global references. A useful index should reduce the number of global references. You can prevent use of an index by using the %NOINDEX keyword as preface to a WHERE clause or ON clause condition."
90,"You should index fields (properties) that are specified in a JOIN. A LEFT OUTER JOIN starts with the left table, and then looks into the right table; therefore, you should index the field from the right table. In the following example, you should index T2.f2:"
90,FROM Table1 AS T1 LEFT OUTER JOIN Table2 AS T2 ON T1.f1 = T2.f2
90,Copy code to clipboard
90,An INNER JOIN should have indices on both ON clause fields.
90,"Run Show Plan and follow to the first map. If the first bullet item in the Query Plan is &OpenCurlyDoubleQuoteRead master map&CloseCurlyDoubleQuote, or the Query Plan calls a module whose first bullet item is &OpenCurlyDoubleQuoteRead master map&CloseCurlyDoubleQuote, the query first map is the master map rather than an index map. Because the master map reads the data itself, rather than an index to the data, this almost always indicates an inefficient Query Plan. Unless the table is relatively small, you should create an index so that when you rerun this query the Query Plan first map says &OpenCurlyDoubleQuoteRead index map.&CloseCurlyDoubleQuote"
90,You should index fields that are specified in a WHERE clause equal condition.
90,"You may wish to index fields that are specified in a WHERE clause range condition, and fields specified in GROUP BY and ORDER BY clauses."
90,"Under certain circumstances, an index based on a range condition could make a query slower. This can occur if the vast majority of the rows meet the specified range condition. For example, if the query clause WHERE Date < CURRENT_DATE is used with a database in which most of the records are from prior dates, indexing on Date may actually slow down the query. This is because the Query Optimizer assumes range conditions will return a relatively small number of rows, and optimizes for this situation. You can determine if this is occurring by prefacing the range condition with %NOINDEX and then run the query again."
90,"If you are performing a comparison using an indexed field, the field as specified in the comparison should have the same collation type as it has in the corresponding index. For example, the Name field in the WHERE clause of a SELECT or in the ON clause of a JOIN should have the same collation as the index defined for the Name field. If there is a mismatch between the field collation and the index collation, the index may be less effective or may not be used at all. For further details, refer to Index Collation in the &OpenCurlyDoubleQuoteDefining and Building Indices&CloseCurlyDoubleQuote chapter of this manual."
90,"For details on how to create an index and the available index types and options, refer to the CREATE INDEX command in the InterSystems SQL Reference, and the &OpenCurlyDoubleQuoteDefining and Building Indices&CloseCurlyDoubleQuote chapter of this manual."
90,Index Configuration OptionsCopy link to this section
90,The following system-wide configuration methods can be used to optimize use of indices in queries:
90,"To use the PRIMARY KEY as the IDKey index, set the $SYSTEM.SQL.Util.SetOption() method, as follows: SET status=$SYSTEM.SQL.Util.SetOption(""DDLPKeyNotIDKey"",0,.oldval). The default is 1."
90,"To use indices for SELECT DISTINCT queries set the $SYSTEM.SQL.Util.SetOption() method, as follows: SET status=$SYSTEM.SQL.Util.SetOption(""FastDistinct"",1,.oldval). The default is 1."
90,"For further details, refer to SQL and Object Settings Pages listed in System Administration Guide."
90,Index Usage AnalysisCopy link to this section
90,You can analyze index usage by SQL cached queries using either of the following:
90,The Management Portal Index Analyzer SQL performance tool.
90,"The %SYS.PTools.UtilSQLAnalysis methods indexUsage(), tableScans(), tempIndices(), joinIndices(), and outlierIndices(). Refer to indexUsage() Method for further details."
90,Index AnalyzerCopy link to this section
90,You can analyze index usage for SQL queries from the Management Portal using either of the following:
90,"Select System Explorer, select Tools, select SQL Performance Tools, then select Index Analyzer."
90,"Select System Explorer, select SQL, then from the Tools drop-down menu select Index Analyzer."
90,"The Index Analyzer provides an SQL Statement Count display for the current namespace, and five index analysis report options."
90,SQL Statement Count
90,"At the top of the SQL Index Analyzer there is an option to count all SQL statements in the namespace. Press the Gather SQL Statements button. The SQL Index Analyzer displays &OpenCurlyDoubleQuoteGathering SQL statements ....&CloseCurlyDoubleQuote while the count is in progress, then &OpenCurlyDoubleQuoteDone!&CloseCurlyDoubleQuote when the count is complete. SQL statements are counted in three categories: a Cached Query count, a Class Method count, and a Class Query count. These counts are for the entire current namespace, and are not affected by the Schema Selection option."
90,The corresponding method is getSQLStmts() in the %SYS.PTools.UtilSQLAnalysis class.
90,You can use the Purge Statements button to delete all gathered statements in the current namespace. This button invokes the clearSQLStatements() method.
90,Report Options
90,"You can either examine reports for the cached queries for a selected schema in the current namespace, or (by not selecting a schema) examine reports for all cached queries in the current namespace. You can skip or include system class queries, INSERT statements, and/or IDKEY indices in this analysis. The schema selection and skip option check boxes are user customized."
90,The index analysis report options are:
90,"Index Usage: This option takes all of the cached queries in the current namespace, generates a Show Plan for each and keeps a count of how many times each index is used by each query and the total usage for each index by all queries in the namespace. This can be used to reveal indices that are not being used so they can either be removed or modified to make them more useful. The result set is ordered from least used index to most used index."
90,"The corresponding method is indexUsage() in the %SYS.PTools.UtilSQLAnalysis class. To export analytic data generated by this method, use the exportIUAnalysis() method."
90,"Queries with Table Scans: This option identifies all queries in the current namespace that do table scans. Table scans should be avoided if possible. A table scan can’t always be avoided, but if a table has a large number of table scans, the indices defined for that table should be reviewed. Often the list of table scans and the list of temp indices will overlap; fixing one will remove the other. The result set lists the tables from largest Block Count to smallest Block Count. A Show Plan link is provided to display the Statement Text and Query Plan."
90,"The corresponding method is tableScans() in the %SYS.PTools.UtilSQLAnalysis class. To export analytic data generated by this method, use the exportTSAnalysis() method."
90,"Queries with Temp Indices: This option identifies all queries in the current namespace that build temporary indices to resolve the SQL. Sometimes the use of a temp index is helpful and improves performance, for example building a small index based on a range condition that InterSystems IRIS can then use to read the master map in order. Sometimes a temp index is simply a subset of a different index and might be very efficient. Other times a temporary index degrades performance, for example scanning the master map to build a temporary index on a property that has a condition. This situation indicates that a needed index is missing; you should add an index to the class that matches the temporary index. The result set lists the tables from largest Block Count to smallest Block Count. A Show Plan link is provided to display the Statement Text and Query Plan."
90,"The corresponding method is tempIndices() in the %SYS.PTools.UtilSQLAnalysis class. To export analytic data generated by this method, use the exportTIAnalysis() method."
90,"Queries with Missing JOIN Indices: This option examines all queries in the current namespace that have joins, and determines if there is an index defined to support that join. It ranks the indices available to support the joins from 0 (no index present) to 4 (index fully supports the join). Outer joins require an index in one direction. Inner joins require an index in both directions. By default, the result set only contains rows that have a JoinIndexFlag < 4. JoinIndexFlag=4 means there is an index that fully supports the join."
90,"The corresponding method is joinIndices() in the %SYS.PTools.UtilSQLAnalysis class, which provides descriptions of the JoinIndexFlag values. To export analytic data generated by this method, use the exportJIAnalysis() method. By default, exportJIAnalysis() does not list JoinIndexFlag=4 values, but they can optionally be listed."
90,"Queries with Outlier Indices: This option identifies all queries in the current namespace that have outliers, and determines if there is an index defined to support that outlier. It ranks the indices available to support the outlier from 0 (no index present) to 4 (index fully supports the outlier). By default, the result set only contains rows that have a OutlierIndexFlag < 4. OutlierIndexFlag=4 means there is an index that fully supports the outlier."
90,"The corresponding method is outlierIndices() in the %SYS.PTools.UtilSQLAnalysis class. To export analytic data generated by this method, use the exportOIAnalysis() method. By default, exportOIAnalysis() does not list OutlierIndexFlag=4 values, but they can optionally be listed."
90,"When you select one of these options, the system automatically performs the operation and displays the results. The first time you select an option or invoke the corresponding method, the system generates the results data; if you select that option or invoke that method again, InterSystems IRIS redisplays the same results. To generate new results data you must use the Gather SQL Statements button to reinitialize the Index Analyzer results tables. To generate new results data for the %SYS.PTools.UtilSQLAnalysis methods, you must invoke getSQLStmts() to reinitialize the Index Analyzer results tables. Changing the Skip all system classes and routines or Skip INSERT statements check box option also reinitializes the Index Analyzer results tables."
90,indexUsage() MethodCopy link to this section
90,The following example demonstrates the use of the indexUsage() method:
90,"DO ##class(%SYS.PTools.UtilSQLAnalysis).indexUsage(1,1)"
90,"SET utils = ""SELECT %EXACT(Type), Count(*) As QueryCount ""_"
90,"""FROM %SYS_PTools.UtilSQLStatements GROUP BY Type"""
90,"SET utilresults = ""SELECT SchemaName, Tablename, IndexName, UsageCount ""_"
90,"""FROM %SYS_PTools.UtilSQLAnalysisDB ORDER BY UsageCount"""
90,SET tStatement = ##class(%SQL.Statement).%New()
90,SET qStatus = tStatement.%Prepare(utils)
90,"IF qStatus'=1 {WRITE ""%Prepare failed:"" DO $System.Status.DisplayError(qStatus) QUIT}"
90,SET rset = tStatement.%Execute()
90,DO rset.%Display()
90,"WRITE !,""End of utilities data"",!!"
90,SET qStatus = tStatement.%Prepare(utilresults)
90,"IF qStatus'=1 {WRITE ""%Prepare failed:"" DO $System.Status.DisplayError(qStatus) QUIT}"
90,SET rset = tStatement.%Execute()
90,DO rset.%Display()
90,"WRITE !,""End of results data"""
90,Copy code to clipboard
90,"Note that because results are ordered by UsageCount, indices with UsageCount > 0 are listed at the end of the result set."
90,"Methods in this class can be invoked either from ObjectScript, or from the SQL CALL or SELECT command. The SQL naming convention is to specify the package name %SYS_PTools, then prefix &OpenCurlyDoubleQuotePT_&CloseCurlyDoubleQuote to the method name that begins with a lower-case letter. This is shown in the following examples:"
90,ObjectScript:
90,DO ##class(%SYS.PTools.UtilSQLAnalysis).indexUsage()
90,Copy code to clipboard
90,SQL:
90,CALL %SYS_PTools.PT_indexUsage()
90,Copy code to clipboard
90,SELECT %SYS_PTools.PT_indexUsage()
90,Copy code to clipboard
90,Index Optimization OptionsCopy link to this section
90,"By default, the InterSystems SQL query optimizer uses sophisticated and flexible algorithms to optimize the performance of complex queries involving multiple indices. In most cases, these defaults provide optimal performance. However, in infrequent cases, you may wish to give &OpenCurlyDoubleQuotehints&CloseCurlyDoubleQuote to the query optimizer by specifying optimize-option keywords."
90,The FROM clause supports the %ALLINDEX and %IGNOREINDEX optimize-option keywords. These optimize-option keywords govern all index use in the query. They are described in detail in the FROM clause reference page of the InterSystems SQL Reference.
90,"You can use the %NOINDEX condition-level hint to specify exceptions to the use of an index for a specific condition. The %NOINDEX hint is placed in front of each condition for which no index should be used. For example, WHERE %NOINDEX hiredate < ?. This is most commonly used when the overwhelming majority of the data is selected (or not selected) by the condition. With a less-than (<) or greater-than (>) condition, use of the %NOINDEX condition-level hint is often beneficial. With an equality condition, use of the %NOINDEX condition-level hint provides no benefit. With a join condition, %NOINDEX is supported for ON clause joins."
90,"The %NOINDEX keyword can be used to override indexing optimization established in the FROM clause. In the following example, the %ALLINDEX optimization keyword applies to all condition tests except the E.Age condition:"
90,"SELECT P.Name,P.Age,E.Name,E.Age"
90,FROM %ALLINDEX Sample.Person AS P LEFT OUTER JOIN Sample.Employee AS E
90,ON P.Name=E.Name
90,WHERE P.Age > 21 AND %NOINDEX E.Age < 65
90,Copy code to clipboard
90,Query Execution PlansCopy link to this section
90,"You can use the EXPLAIN or Show Plan tools to display an execution plan for SELECT, DECLARE, UPDATE, DELETE, TRUNCATE TABLE, and some INSERT operations. These are collectively known as query operations because they use a SELECT query as part of their execution. InterSystems IRIS generates an execution plan when a query operation is prepared; you do not have to actually execute the query to generate an execution plan."
90,"By default, these tools display what InterSystems IRIS considers to be the optimal query plan. For most queries there is more than one possible query plan. In addition to the query plan that InterSystems IRIS deems as optimal, you can also generate and display alternate query execution plans."
90,InterSystems IRIS provides the following query plan tools:
90,"The $SYSTEM.SQL.Explain() method can be used to generate and display an XML-formatted query plan and, optionally, alternate query plans."
90,"The SQL EXPLAIN command can be used to generate an XML-formatted query plan and, optionally, alternate query plans and SQL statistics. All generated query plans and statistics are included in a single result set field named Plan. Note that the EXPLAIN command can only be used with a SELECT query."
90,The Management Portal—>System Explorer—>SQL interface Show Plan button.
90,The Management Portal—>System Explorer—>Tools—>SQL Performance Tools.
90,"For generated %PARALLEL and Sharded queries, these tools display all of the applicable query plans."
90,Using the Explain() MethodCopy link to this section
90,"You can generate a query execution plan by running the $SYSTEM.SQL.Explain() method, as shown in the following example:"
90,SET mysql=2
90,"SET mysql(1)=""SELECT TOP 10 Name,DOB FROM Sample.Person """
90,"SET mysql(2)=""WHERE Name [ 'A' ORDER BY Age"""
90,"SET status=$SYSTEM.SQL.Explain(.mysql,{""all"":0,""quiet"":1,""stats"":0,""preparse"":0},,.plan)"
90,"IF status'=1 {WRITE ""Explain() failed:"" DO $System.Status.DisplayError(status) QUIT}"
90,ZWRITE plan
90,Copy code to clipboard
90,"Setting the ""all"":0 option generates the query plan that InterSystems IRIS deems optimal. Setting the ""all"":1 option generates the optimal query plan and alternate query plans. The default is ""all"":0."
90,The result is formatted as an array of subscripts representing an XML-formatted text.
90,"If you specify a single query plan (""all"":0), the plan variable in the above method call would have the following format:"
90,plan: contains the total number of subscripts in the result.
90,"plan(1): always contains the XML format tag ""<plan>"". The last subscript always contains the XML format tag ""</plan>""."
90,"plan(2): always contains the XML format tag ""<sql>"""
90,"plan(3): always contains the first line of the query text. If ""preparse"":0 (the default) the literal query text is returned and additional subscripts are used for each line of a multi-line query; in the above example, the query is two lines, so two subscripts are used (plan(3) & plan(4)). If ""preparse"":1 the normalized query text is returned as a single line: plan(3)."
90,"plan(n): always contains the XML format tag ""</sql>""; in the case of the above example, 3+mysql = plan(5)."
90,"plan(n+1): always contains the XML formatted query cost ""<cost value=""""407137""""/>""."
90,plan(n+2): always contains the first line of the query plan. This plan may be of any length and may contain <module> ... </module> tags as separate subscript lines enclosing the query plan for a generated execution module.
90,"If you specify ""all"":1 Explain() generates alternate query plans. The plan variables follow the same format, except that they use the first-level subscript to identify the query plan and the second-level subscripts for the lines of the query plan. Therefore, plan(1) contains the count of second-level subscripts in the first query plan result, plan(2) contains the count of second-level subscripts in the second query plan result, and so forth. In this format, plan(1,1) contains the XML format tag ""<plan>"" for the first query plan;"
90,"plan(2,1) contains the XML format tag ""<plan>"" for the second query plan, and so forth. The only difference is that alternate query plans contain a second-level zero subscript (plan(1,0) variable that contains cost and index information; this zero subscript is not counted in the first-level subscript (plan(1)) value."
90,"If you specify ""stats"":1, Explain() generates performance statistics for each query plan module. These statistics for each module are tagged using the <stats> ... </stats> tags and appear immediately after the query cost (""<cost value=""""407137""""/>"") and before the query plan text. If the query plan contains additional <module> tags, the <stats> for that generated module are listed immediately after the <module> tag and before that module’s query plan. For each module, the following items are returned:"
90,<ModuleName>: module name.
90,"<TimeSpent>: total execution time for the module, in seconds."
90,<GlobalRefs>: a count of global references.
90,<LinesOfCode>: a count of lines of code executed.
90,<DiskWait>: disk wait time in seconds.
90,<RowCount>: number of rows in result set.
90,<ModuleCount>: number of times this module was executed.
90,<Counter>: number of times this program was executed.
90,Using Show Plan from InterSystems SQL ToolsCopy link to this section
90,You can use Show Plan to display the execution plan for a query in any of the following ways:
90,"From the Management Portal SQL interface. Select System Explorer, then SQL. Select a namespace with the Switch option at the top of the page. (You can set the Management Portal default namespace for each user.) Write a query, then press the Show Plan button. (You can also invoke Show Plan from the Show History listing by clicking the plan option for a listed query.) See Executing SQL Statements in the &OpenCurlyDoubleQuoteUsing the Management Portal SQL Interface&CloseCurlyDoubleQuote chapter of this manual."
90,"From the Management Portal Tools interface. Select System Explorer, then Tools, then select SQL Performance Tools, then SQL Runtime Statistics:"
90,From the Query Test tab: Select a namespace with the Switch option at the top of the page. Write a query in the text box. Then press the Show Plan with SQL Stats button. This generates a Show Plan without executing the query.
90,"From the View Stats tab: Press the Show Plan button for one of the listed queries. The listed queries include both those written at Execute Query, and those written at Query Test."
90,From the SQL Shell you can use the SHOW PLAN and SHOW PLANALT Shell commands to display the execution plan for the most recently executed query.
90,"By running Show Plan against a cached query result set, using :i%Prop syntax for literal substitution values stored as properties:"
90,SET cqsql=2
90,"SET cqsql(1)=""SELECT TOP :i%PropTopNum Name,DOB FROM Sample.Person """
90,"SET cqsql(2)=""WHERE Name [ :i%PropPersonName ORDER BY Age"""
90,"DO ShowPlan^%apiSQL(.cqsql,0,"""",0,$LB(""Sample""),"""",1)"
90,Copy code to clipboard
90,"Show Plan by default returns values in Logical mode. However, when invoking Show Plan from the Management Portal or the SQL Shell, Show Plan uses Runtime mode."
90,Execution Plan: Statement Text and Query PlanCopy link to this section
90,"The Show Plan execution plan consists of two components, Statement Text and Query Plan:"
90,"Statement Text replicates the original query, with the following modifications: The Show Plan button from the Management Portal SQL interface displays the SQL statement with comments and line breaks removed. Whitespace is standardized. The Show Plan button display also performs literal substitution, replacing each literal with a ?, unless you have suppressed literal substitution by enclosing the literal value in double parentheses. These modifications are not performed when displaying a show plan using the Explain() method, or when displayed using the SQL Runtime Statistics or Alternate Show Plans tools."
90,Query Plan shows the plan that would be used to execute the query. A Query Plan can include the following:
90,"&OpenCurlyDoubleQuoteFrozen Plan&CloseCurlyDoubleQuote is the first line of Query Plan if the query plan has been frozen; otherwise, the first line is blank."
90,"&OpenCurlyDoubleQuoteRelative cost&CloseCurlyDoubleQuote is an integer value which is computed from many factors as an abstract number for comparing the efficiency of different execution plans for the same query. This calculation takes into account (among other factors) the complexity of the query, the presence of indices, and the size of the table(s). Relative cost is not useful for comparing two different queries. &OpenCurlyDoubleQuoteRelative cost not available&CloseCurlyDoubleQuote is returned by certain aggregate queries, such as COUNT(*) or MAX(%ID) without a WHERE clause."
90,"The Query Plan consists of a main module, and (when needed) one or more subcomponents. One or more module subcomponents may be shown, named alphabetically, starting with B: Module:B, Module:C, etc.), and listed in the order of execution (not necessarily alphabetically)."
90,"By default, a module performs processing and populates an internal temp-file (internal temporary table) with its results. You can force the query optimizer to create a query plan that does not generate internal temp-files by specifying /*#OPTIONS {""NoTempFile"":1} */, as described in Comment Options."
90,A named subquery module is shown for each subquery in the query. Subquery modules are named alphabetically.
90,"Subquery naming skips one or more letters before each named subquery. Thus, Module:B, Subquery:F or Module:D, Subquery:G. When the end of the alphabet is reached, additional subqueries are numbered, parsing Z=26 and using the same skip sequence. The following example is an every-third subquery naming sequence starting with Subquery:F: F, I, L, O, R, U, X, 27, 30, 33. The following example is an every-second subquery naming sequence starting with Subquery:G: G, I, K, M, O, Q, S, U, W, Y, 27, 29. If a subquery calls a module, the module is placed in alphabetical sequence after the subquery with no skip. Therefore, Subquery:H calls Module:I."
90,"&OpenCurlyDoubleQuoteRead master map&CloseCurlyDoubleQuote as the first bullet item in the main module indicates an inefficient Query Plan. The Query Plan begins execution with one of the following map type statements Read master map... (no available index), Read index map... (use available index), or Generate a stream of idkey values using the multi-index combination... (Multi Index, use multiple indices). Because the master map reads the data itself, rather than an index to the data, Read master map... almost always indicates an inefficient Query Plan. Unless the table is relatively small, you should define an index so that when you regenerate the Query Plan the first map says Read index map.... For information on interpreting a Query Plan, refer to &OpenCurlyDoubleQuoteInterpreting an SQL Query Plan.&CloseCurlyDoubleQuote"
90,Some operations create a Show Plan that indicates no Query Plan could be generated:
90,"Non-query INSERT: An INSERT... VALUES() command does not perform a query, and therefore does not generate a Query Plan."
90,"Query always FALSE: In a few cases, InterSystems IRIS can determine when preparing a query that a query condition will always be false, and thus cannot return data. The Show Plan informs you of this situation in the Query Plan component. For example, a query containing the condition WHERE %ID IS NULL or the condition WHERE Name %STARTSWITH('A') AND Name IS NULL cannot return data, and therefore InterSystems IRIS generates no execution plan. Rather than generating an execution plan, the Query Plan says &OpenCurlyDoubleQuoteOutput no rows&CloseCurlyDoubleQuote. If a query contains a subquery with one of these conditions, the subquery module of the Query Plan says &OpenCurlyDoubleQuoteSubquery result NULL, found no rows&CloseCurlyDoubleQuote. This condition check is limited to a few situations involving NULL, and is not intended to catch all self-contradictory query conditions."
90,"Invalid query: Show Plan displays an SQLCODE error message for most invalid queries. However, in a few cases, Show Plan displays as empty. For example, WHERE Name = $$$$$ or WHERE Name %STARTSWITH('A"") (note single-quote and double-quote). In these cases, Show Plan displays no Statement Text, and Query Plan says [No plan created for this statement]. This commonly occurs when quotation marks delimiting a literal are imbalanced. It also occurs when you specify two or more leading dollar signs without specifying the correct syntax for a user-defined (&OpenCurlyDoubleQuoteextrinsic&CloseCurlyDoubleQuote) function."
90,Alternate Show PlansCopy link to this section
90,You can display alternate execution plans for a query using the Management Portal or the Explain() method.
90,To display alternate execution plans for a query from the Management Portal using either of the following:
90,"Select System Explorer, select Tools, select SQL Performance Tools, then select Alternate Show Plans."
90,"Select System Explorer, select SQL, then from the Tools drop-down menu select Alternate Show Plans."
90,Using the Alternate Show Plans tool:
90,"Input an SQL query text, or retrieve one using the Show History button. You can clear the query text field by clicking the round ""X"" circle on the right hand side."
90,"Press the Show Plan Options button to display multiple alternate show plans. The Run ... in the background check box is unselected by default, which is the preferred setting for most queries. It is recommended that you select the Run ... in the background check box for large or complex queries. While a long query is being run in background a View Process button is shown. Clicking View Process opens the Process Details page in a new tab. From the Process Details page, you can view the process, and may Suspend, Resume or Terminate the process."
90,"Possible Plans are listed in ascending order by Cost, with the Map Type and Starting Map. You can select the Show Plan (no statistics) or Show Plan with Stats link for each plan for further details."
90,"From the list of possible plans, use the check boxes to select the plans that you wish to compare, then press the Compare Show Plans with Stats button to run them and display their SQL statistics."
90,"The Explain() method with the all qualifier shows all of the execution plans for a query. It first shows the plan the InterSystems IRIS considers optimal (lowest cost), then displays alternate plans. Alternate plans are listed in ascending order of cost."
90,"The following example displays the optimal execution plan, then lists alternate plans:"
90,DO $SYSTEM.SQL.SetSQLStatsFlagJob(3)
90,SET mysql=1
90,"SET mysql(1)=""SELECT TOP 4 Name,DOB FROM Sample.Person ORDER BY Age"""
90,"DO $SYSTEM.SQL.Explain(.mysql,{""all"":1},,.plan)"
90,ZWRITE plan
90,Copy code to clipboard
90,Also refer to the possiblePlans methods in the %SYS.PTools.StatsSQL class.
90,StatsCopy link to this section
90,"The Show Plans Options lists assigns each alternate show plan a Cost value, which enables you to make relative comparisons between the execution plans."
90,"The Alternate Show Plan details provides for each Query Plan a set of stats (statistics) for the Query Totals, and (where applicable) for each Query plan module. The stats for each module include Time (overall performance, in seconds), Global Refs (number of global references), Commands (number of lines executed), and Read Latency (disk wait, in milliseconds). The Query Totals stats also includes the number of Rows Returned."
90,Writing Query Optimization Plans to a FileCopy link to this section
90,The following utility lists the query optimization plan(s) for one or more queries to a text file.
90,"QOPlanner^%apiSQL(infile,outfile,eos,schemapath)"
90,infile
90,A file pathname to a text file containing a listing of cached queries. Specified as a quoted string.
90,outfile
90,"A file pathname where query optimization plans are to be listed. Specified as a quoted string. If the file does not exist, the system creates it. If the file already exists, InterSystems IRIS overwrites it."
90,eos
90,"Optional — The end-of-statement delimiter used to separate the individual cached queries in the infile listing. Specified as a quoted string. The default is &OpenCurlyDoubleQuoteGO&CloseCurlyDoubleQuote. If this eos string does not match the cached query separator, no outfile is generated."
90,schemapath
90,"Optional — A comma-separated list of schema names that specifies a schema search path for unqualified table names, view names, or stored procedure names. Can include DEFAULT_SCHEMA, the current system-wide default schema. If infile contains #Import directives, QOPlanner adds these #Import package/schema names to the end of schemapath."
90,"The following is an example of evoking this query optimization plans listing utility. This utility takes as input the file generated by the ExportSQL^%qarDDLExport() utility, as described in &OpenCurlyDoubleQuoteListing Cached Queries to a File&CloseCurlyDoubleQuote section of the &OpenCurlyDoubleQuoteCached Queries&CloseCurlyDoubleQuote chapter. You can either generate this query listing file, or write a query (or queries) to a text file."
90,"DO QOPlanner^%apiSQL(""C:\temp\test\qcache.txt"",""C:\temp\test\qoplans.txt"",""GO"")"
90,Copy code to clipboard
90,"When executed from the Terminal command line progress is displayed to the terminal screen, such as the following example:"
90,Importing SQL Statements from file: C:\temp\test\qcache.txt
90,Recording any errors to principal device and log file: C:\temp\test\qoplans.txt
90,SQL statement to process (number 1):
90,"SELECT TOP ? P . Name , E . Name FROM Sample . Person AS P ,"
90,Sample . Employee AS E ORDER BY E . Name
90,Generating query plan...Done
90,SQL statement to process (number 2):
90,"SELECT TOP ? P . Name , E . Name FROM %INORDER Sample . Person AS P"
90,NATURAL LEFT OUTER JOIN Sample . Employee AS E ORDER BY E . Name
90,Generating query plan...Done
90,Elapsed time: .16532 seconds
90,The created query optimization plans file contains entries such as the following:
90,<pln>
90,<sql>
90,"SELECT TOP ? P . Name , E . Name FROM Sample . Person AS P , Sample . Employee AS E ORDER BY E . Name"
90,</sql>
90,Read index map Sample.Employee.NameIDX.
90,Read index map Sample.Person.NameIDX.
90,</pln>
90,######
90,<pln>
90,<sql>
90,"SELECT TOP ? P . Name , E . Name FROM %INORDER Sample . Person AS P"
90,NATURAL LEFT OUTER JOIN Sample . Employee AS E ORDER BY E . Name
90,</sql>
90,Read master map Sample.Person.IDKEY.
90,Read extent bitmap Sample.Employee.$Employee.
90,Read master map Sample.Employee.IDKEY.
90,Update the temp-file.
90,Read the temp-file.
90,Read master map Sample.Employee.IDKEY.
90,Update the temp-file.
90,Read the temp-file.
90,</pln>
90,######
90,"You can use the query optimization plan text files to compare generated optimization plans using different variants of a query, or compare optimization plans between different versions of InterSystems IRIS."
90,"When exporting the SQL queries to the text file, a query that comes from a class method or class query will be preceded by the code line:"
90,#import <package name>
90,Copy code to clipboard
90,"This #Import statement tells the QOPlanner utility what default package/schema to use for the plan generation of the query. When exporting the SQL queries from a routine, any #import lines in the routine code prior to the SQL statement will also precede the SQL text in the export file. Queries exported to the text file from cached queries are assumed to contain fully qualified table references; if a table reference in a text file is not fully qualified, the QOPlanner utility uses the system-wide default schema that is defined on the system when QOPlanner is run."
90,Comment OptionsCopy link to this section
90,"You can specify one or more comment options to the Query Optimizer within a SELECT, INSERT, UPDATE, DELETE, or TRUNCATE TABLE command. A comment option specifies a option that the query optimizer uses during the compile of the SQL query. Often a comment option is used to override a system-wide configuration default for a specific query."
90,SyntaxCopy link to this section
90,"The syntax /*#OPTIONS */, with no space between the /* and the #, specifies a comment option. A comment option is not a comment; it specifies a value to the query optimizer. A comment option is specified using JSON syntax, commonly a key:value pair such as the following: /*#OPTIONS {""optionName"":value} */. More complex JSON syntax, such as nested values, is supported."
90,A comment option is not a comment; it may not contain any text other than JSON syntax. Including non-JSON text within the /* ... */ delimiters results in an SQLCODE -153 error. InterSystems SQL does not validate the contents of the JSON string.
90,"The #OPTIONS keyword must be specified in uppercase letters. No spaces should be used within the curly brace JSON syntax. If the SQL code is enclosed with quote marks, such as a Dynamic SQL statement, quote marks in the JSON syntax should be doubled. For example: myquery=""SELECT Name FROM Sample.MyTest /*#OPTIONS {""""optName"""":""""optValue""""} */""."
90,"You can specify a /*#OPTIONS */ comment option anywhere in SQL code where a comment can be specified. In displayed statement text, the comment options are always shown as comments at the end of the statement text."
90,"You can specify multiple /*#OPTIONS */ comment options in SQL code. They are shown in returned Statement Text in the order specified. If multiple comment options are specified for the same option, the last-specified option value is used."
90,The following comment options are documented:
90,"/*#OPTIONS {""BiasAsOutlier"":1} */"
90,"/*#OPTIONS {""DynamicSQLTypeList"":""10,1,11""}"
90,"/*#OPTIONS {""NoTempFile"":1} */"
90,DisplayCopy link to this section
90,"The /*#OPTIONS */ comment options display at the end of the SQL statement text, regardless of where they were specified in the SQL command. Some displayed /*#OPTIONS */ comment options are not specified in the SQL command, but are generated by the compiler pre-processor. For example /*#OPTIONS {""DynamicSQLTypeList"": ...} */."
90,"The /*#OPTIONS */ comment options display in the Show Plan Statement Text, in the Cached Query Query Text, and in the SQL Statement Statement Text."
90,A separate cached query is created for queries that differ only in the /*#OPTIONS */ comment options.
90,Parallel Query ProcessingCopy link to this section
90,"Parallel query hinting directs the system to perform parallel query processing when running on a multi-processor system. This can substantially improve performance of certain types of queries. The SQL optimizer determines whether a specific query could benefit from parallel processing, and performs parallel processing where appropriate. Specifying parallel query hinting does not force parallel processing of every query, only those that may benefit from parallel processing. If the system is not a multi-processor system, this option has no effect. To determine the number of processors on the current system use the %SYSTEM.Util.NumberOfCPUs() method."
90,You can specify parallel query processing in two ways:
90,"System-wide, by setting the auto parallel option."
90,"Per query, by specifying the %PARALLEL keyword in the FROM clause of an individual query."
90,"Parallel query processing is applied to SELECT queries. It is not applied to INSERT, UPDATE, or DELETE operations."
90,System-Wide Parallel Query ProcessingCopy link to this section
90,You can configure system-wide automatic parallel query processing using either of the following options:
90,"From the Management Portal choose System Administration, then Configuration, then SQL and Object Settings, then SQL. View or change the Execute queries in a single process check box. Note that the default for this check box is unselected, which mean that parallel processing is activated by default."
90,"Invoke the $SYSTEM.SQL.Util.SetOption() method, as follows: SET status=$SYSTEM.SQL.Util.SetOption(""AutoParallel"",1,.oldval). The default is 1 (automatic parallel processing activated). To determine the current setting, call $SYSTEM.SQL.CurrentSettings() which displays the Enable auto hinting for %PARALLEL option."
90,Note that changing this configuration setting purges all cached queries in all namespaces.
90,"When activated, automatic parallel query hinting directs the SQL optimizer to apply parallel processing to any query that may benefit from this type of processing. At IRIS 2019.1 and subsequent, auto parallel processing is activated by default. Users upgrading from IRIS 2018.1 to IRIS 2019.1 will need to explicitly activate auto parallel processing."
90,"One option the SQL optimizer uses to determine whether to perform parallel processing for a query is the auto parallel threshold. If system-wide auto parallel processing is activated (the default), you can use the $SYSTEM.SQL.Util.SetOption() method to set the optimization threshold for automatic parallel processing as an integer value, as follows: SET status=$SYSTEM.SQL.Util.SetOption(""AutoParallelThreshold"",n,.oldval). The higher the n threshold value is, the lower the chance that this feature will be applied to a query. This threshold is used in complex optimization calculations, but you can think about this value as the minimal number of tuples that must reside in the visited map. The default value is 3200. The minimum value is 0. To determine the current setting, call $SYSTEM.SQL.CurrentSettings() which displays the Threshold of auto hinting for %PARALLEL option."
90,"When automatic parallel processing is activated, a query executed in a sharded environment will always be executed with parallel processing, regardless of the parallel threshold value."
90,Parallel Query Processing for a Specific QueryCopy link to this section
90,"The optional %PARALLEL keyword is specified in the FROM clause of a query. It suggests that InterSystems IRIS perform parallel processing of the query, using multiple processors (if applicable). This can significantly improve performance of some queries that uses one or more COUNT, SUM, AVG, MAX, or MIN aggregate functions, and/or a GROUP BY clause, as well as many other types of queries. These are commonly queries that process a large quantity of data and return a small result set. For example, SELECT AVG(SaleAmt) FROM %PARALLEL User.AllSales GROUP BY Region would likely use parallel processing."
90,"A &OpenCurlyDoubleQuoteone row&CloseCurlyDoubleQuote query that specifies only aggregate functions, expressions, and subqueries performs parallel processing, with or without a GROUP BY clause. However, a &OpenCurlyDoubleQuotemulti-row&CloseCurlyDoubleQuote query that specifies both individual fields and one or more aggregate functions does not perform parallel processing unless it includes a GROUP BY clause. For example, SELECT Name,AVG(Age) FROM %PARALLEL Sample.Person does not perform parallel processing, but SELECT Name,AVG(Age) FROM %PARALLEL Sample.Person GROUP BY Home_State does perform parallel processing."
90,"If a query that specifies %PARALLEL is compiled in Runtime mode, all constants are interpreted as being in ODBC format."
90,Specifying %PARALLEL may degrade performance for some queries. Running a query with %PARALLEL on a system with multiple concurrent users may result in degraded overall performance.
90,"Parallel processing can be performed when querying a view. However, parallel processing is never performed on a query that specifies a %VID, even if the %PARALLEL keyword is explicitly specified."
90,"For further details, refer to the FROM clause in the InterSystems SQL Reference."
90,%PARALLEL in SubqueriesCopy link to this section
90,%PARALLEL is intended for SELECT queries and their subqueries. An INSERT command subquery cannot use %PARALLEL.
90,%PARALLEL is ignored when applied to a subquery that is correlated with an enclosing query. For example:
90,"SELECT name,age FROM Sample.Person AS p"
90,WHERE 30<(SELECT AVG(age) FROM %PARALLEL Sample.Employee where Name = p.Name)
90,Copy code to clipboard
90,"%PARALLEL is ignored when applied to a subquery that includes a complex predicate, or a predicate that optimizes to a complex predicate. Predicates that are considered complex include the FOR SOME and FOR SOME %ELEMENT predicates."
90,Parallel Query Processing IgnoredCopy link to this section
90,"Regardless of the auto parallel option setting or the presence of the %PARALLEL keyword in the FROM clause, some queries may use linear processing, not parallel processing. InterSystems IRIS makes the decision whether or not to use parallel processing for a query after optimizing that query, applying other query optimization options (if specified). InterSystems IRIS may determine that the optimized form of the query is not suitable for parallel processing, even if the user-specified form of the query would appear to benefit from parallel processing. You can determine if and how InterSystems IRIS has partitioned a query for parallel processing using Show Plan."
90,"In the following circumstances specifying %PARALLEL does not perform parallel processing. The query executes successfully and no error is issued, but parallelization is not performed:"
90,The query contains the FOR SOME predicate.
90,"The query contains both a TOP clause and an ORDER BY clause. This combination of clauses optimizes for fastest time-to-first-row which does not use parallel processing. Adding the FROM clause %NOTOPOPT optimize-option keyword optimizes for fastest retrieval of the complete result set. If the query does not contain an aggregate function, this combination of %PARALLEL and %NOTOPOPT performs parallel processing of the query."
90,"A query containing a LEFT OUTER JOIN or INNER JOIN in which the ON clause is not an equality condition. For example, FROM %PARALLEL Sample.Person p LEFT OUTER JOIN Sample.Employee e ON p.dob > e.dob. This occurs because SQL optimization transforms this type of join to a FULL OUTER JOIN. %PARALLEL is ignored for a FULL OUTER JOIN."
90,"The %PARALLEL and %INORDER optimizations cannot be used together; if both are specified, %PARALLEL is ignored."
90,The query references a view and returns a view ID (%VID).
90,COUNT(*) does not use parallel processing if the table has a BITMAPEXTENT index.
90,%PARALLEL is intended for tables using standard data storage definitions. Its use with customized storage formats may not be supported. %PARALLEL is not supported for GLOBAL TEMPORARY tables or tables with extended global reference storage.
90,"%PARALLEL is intended for a query that can access all rows of a table, a table defined with row-level security (ROWLEVELSECURITY) cannot perform parallel processing."
90,%PARALLEL is intended for use with data stored in the local database. It does not support global nodes mapped to a remote database.
90,Shared Memory ConsiderationsCopy link to this section
90,"For parallel processing, InterSystems IRIS supports multiple InterProcess Queues (IPQ). Each IPQ handles a single parallel query. It allows parallel work unit subprocesses to send rows of data back to the main process so the main process does not have to wait for a work unit to complete. This enables parallel queries to return their first row of data as quickly as possible, without waiting for the entire query to complete. It also improves performance of aggregate functions."
90,"Parallel query execution uses shared memory from the generic memory heap (gmheap). Users may need to increase gmheap size if they are using parallel SQL query execution. As a general rule, the memory requirement for each IPQ is 4 x 64k = 256k. InterSystems IRIS splits a parallel SQL query into the number of available CPU cores. Therefore, users need to allocate this much extra gmheap:"
90,<Number of concurrent parallel SQL requests> x <Number cores> x 256 = <required size increase (in kilobytes) of gmheap>
90,"Note that this formula is not 100% accurate, because a parallel query can spawn sub queries which are also parallel. Therefore it is prudent to allocate more extra gmheap than is specified by this formula."
90,Failing to allocate adequate gmheap results in errors reported to messages.log. SQL queries may fail. Other errors may also occur as other subsystems try to allocate gmheap.
90,"To review gmheap usage by an instance, including IPQ usage in particular, from the home page of the Management Portal choose System Operation then System Usage, and click the Shared Memory Heap Usage link; see Generic (Shared) Memory Heap Usage in the &OpenCurlyDoubleQuoteMonitoring InterSystems IRIS Using the Management Portal&CloseCurlyDoubleQuote chapter of the Monitoring Guide for more information."
90,"To change the size of the generic memory heap or gmheap (sometimes known as the shared memory heap or SMH), from the home page of the Management Portal choose System Administration then Configuration then Additional Settings then Advanced Memory; see Memory and Startup Settings in the &OpenCurlyDoubleQuoteConfiguring InterSystems IRIS&CloseCurlyDoubleQuote chapter in System Administration Guide for more information."
90,Cached Query ConsiderationsCopy link to this section
90,"If you are running a cached SQL query which uses %PARALLEL and while this query is being initialized you do something that purges cached queries, then this query could get a <NOROUTINE> error reported from one of the worker jobs. Typical things that causes cached queries to be purged are calling $SYSTEM.SQL.Purge() or recompiling a class which this query references. Recompiling a class automatically purges any cached queries relating to that class."
90,"If this error occurs, running the query again will probably execute successfully. Removing %PARALLEL from the query will avoid any chance of getting this error."
90,SQL Statements and Plan StateCopy link to this section
90,An SQL query which uses %PARALLEL can result in multiple SQL Statements. The Plan State for these SQL Statements is Unfrozen/Parallel. A query with a plan state of Unfrozen/Parallel cannot be frozen by user action. Refer to the &OpenCurlyDoubleQuoteSQL Statements&CloseCurlyDoubleQuote chapter for further details.
90,Generate ReportCopy link to this section
90,You can use the Generate Report tool to submit a query performance report to InterSystems Worldwide Response Center (WRC) customer support for analysis. You can run the Generate Report tool from the Management Portal using either of the following:
90,"Select System Explorer, select Tools, select SQL Performance Tools, then select Generate Report."
90,"Select System Explorer, select SQL, then from the Tools drop-down menu select Generate Report."
90,"To use this reporting tool, perform the following steps:"
90,You must first get a WRC tracking number from the WRC. You can contact the WRC from the Management Portal by using the Contact button found at the top of each Management Portal page. Enter this tracking number in the WRC Number area. You can use this tracking number to report the performance of a single query or multiple queries.
90,"In the SQL Statement area, enter a query text. An X icon appears in the top right corner. You can use this icon to clear the SQL Statement area. When the query is complete, select the Save Query button. The system generates a query plan and gathers runtime statistics on the specified query. Regardless of the system-wide runtime statistics setting, the Generate Report tool always collects with Collection Option 3: record statistics for all module levels of the query. Because gathering statistics at this level may take time, it is strongly recommended that you select the Run Save Query process in the background check box. This check box is selected by default."
90,"When a background job is started, the tool displays the message ""Please wait..."", disables all the fields on the page, and show a new View Process button. Clicking the View Process button will open the Process Details page in a new tab. From the Process Details page, you can view the process, and may ""Suspend"", ""Resume"" or ""Terminate"" the process. The status of the process is reflected on the Save Query page. When the process is finished, the Currently Saved Queries table is refreshed, the View Process button disappears, and all the fields on the page are enabled."
90,"Perform Step 2 with each desired query. Each query will be added to the Currently Saved Queries table. Note that this table can contain queries with the same WRC tracking number, or with different tracking numbers. When finished with all queries, proceed to Step 4."
90,"For each listed query, you can select the Details link. This link opens a separate page that displays the full SQL Statement, the Properties (including the WRC tracking number and the IRIS software version), and the Query Plan with performance statistics for each module."
90,"To delete individual queries, check the check boxes for those queries from the Currently Saved Queries table and then click the Clear button."
90,"To delete all queries associated with a WRC tracking number, select a row from the Currently Saved Queries table. The WRC number appears in the WRC Number area at the top of the page. If you then click the Clear button, all queries for that WRC number are deleted."
90,"Use the query check boxes to select the queries you wish to report to the WRC. To select all queries associated with a WRC tracking number, select a row from the Currently Saved Queries table, rather than using the check boxes. In either case, you then select the Generate Report button. The Generate Report tool creates a xml file that includes the query statement, the query plan with runtime statistics, the class definition, and the sql int file associated with each selected query."
90,"If you select queries associated with a single WRC tracking number, the generated file will have a default name such as WRC12345.xml. If you select queries associated with more than one WRC tracking number, the generated file will have the default name WRCMultiple.xml."
90,"A dialog box appears that asks you to specify the location to save the report to. After the report is saved, you can click the Mail to link to send the report to WRC customer support. Attach the file using the mail client's attach/insert capability."
90,Cached QueriesSQL Performance Analysis Toolkit
90,View this item as PDF  |  Download all PDFs
90,"© 2021 InterSystems Corporation, Cambridge, MA. All rights reserved."
90,Privacy
90,& Terms
90,Guarantee
90,Section 508
90,Cookies SettingsCookie List
90,Content date/time: 2021-04-14 05:23:32DocReleaseID: IRISforHealth2020.4
92,HeatWave Performance Benchmark | Oracle
92,Click to view our Accessibility Policy
92,Skip to content
92,home
92,nav
92,Oracle
92,Close
92,Search
92,Search
92,Products
92,Industries
92,Resources
92,Support
92,Events
92,Developer
92,View AccountsSign In
92,Back
92,Oracle Account
92,Cloud Account
92,Sign in to Cloud
92,Sign Up for Free Cloud Tier
92,Sign-In
92,Create an Account
92,Help
92,Sign Out
92,Contact Sales
92,No results found
92,Your search did not match any results.
92,We suggest you try the following to help find what you’re looking for:
92,Check the spelling of your keyword search.
92,"Use synonyms for the keyword you typed, for example, try “application” instead of “software.”"
92,Try one of the popular searches shown below.
92,Start a new search.
92,Trending Questions
92,Close
92,Database
92,MySQL Database Service
92,HeatWave
92,"Performance comparison of HeatWave with MySQL Database, Amazon Redshift, and Amazon Aurora"
92,Setup configuration
92,HeatWave
92,MySQL Database
92,AWS Redshift
92,AWS Aurora
92,Instance shape
92,Dc2.8xlarge
92,db.r5.24xlarge
92,Cluster size
92,10 + 1 MDS
92,1. Common setup
92,The workload is derived from the TPC's TPC-H benchmark*
92,Generate TPC-H data using the TPC-H data generation tool
92,Provision and configure the target service
92,Create TPC-H schema on the target service instance
92,Import TPC-H data generated to the target service instance
92,Run queries derived from TPC-H to test the performance
92,"For best performance numbers, always do multiple runs of the query and ignore the first (cold) run"
92,You can always do a explain plan to make sure that you get the best expected plan
92,2. HeatWave specific setup
92,Use optimal encodings for the columns that will be loaded into HeatWave. 5 of the string columns In the TPC-H schema are VARLEN encoded while others are DICTIONARY encoded
92,"Use custom data placement for the tables that will be loaded into HeatWave. For LINEITEM table, l_orderkey is used as the data placement key. For the other tables, primary keys are used as the data placement key."
92,Mark the tables as offloadable and load them into HeatWave
92,"For each query, force offload to HeatWave using the hint (set_var(use_secondary_engine=forced))"
92,A straight_join hint is required for certain queries to get the optimal query plan for HeatWave
92,Reference HeatWave GitHub for specific setup details
92,3. MySQL Database specific setup
92,Use a large enough innodb_buffer_pool size
92,(e.g. 450G for a 512G DRAM)
92,"Tune innodb_sort_buffer_size, max_heap_table_size, tmp_table_size"
92,Make sure the larger tables are partitioned for faster load
92,A straight_join hint can be used if the query plan looks sub-optimal
92,4. AWS Redshift specific setup
92,"Determine the best shape and cluster size for the experiments (in our experiments, we got the best results when we had 1TB of uncompressed data per dc2.8xlarge node)"
92,"For efficient ingest, follow the guidelines for enhanced VPC routing"
92,Use the default parameters as specified by the Amazon documentation
92,Make sure that the sort keys and distribution keys for each table are optimal for queries
92,Use the scripts provided by awslabs
92,5. AWS Aurora specific setup
92,Use the largest shape possible so that as much of the data can fit into the buffer cache as possible
92,"For the 1TB and 4TB TPC-H datasets, use the db.r5.24xlarge shapes"
92,Set the innodb_buffer_pool size to 630G
92,Other settings that were modified from their default value in our experiments (innodb_max_purge_lag = 1000000; innodb_max_purge_lag_delay=300000; innodb_sort_buffer_size=67108864; lock_wait_timeout =86400; max_binlog_cache_size= 4294967296; max_heap_table_size=103079215104; tmp_table_size=103079215104)
92,Set aurora_disable_hash_join = 0 and aurora_parallel_query = ON to use parallel query
92,Follow the best practices for aurora database configuration for any other tuning
92,"For parallel query to work, make sure that none of the tables are partitioned"
92,A straight_join hint can be used if the query plan looks sub-optimal
92,6. Results
92,4TB TPC-H
92,HeatWave
92,AWS Redshift
92,AWS Aurora
92,Instance shape
92,Dc2.8xlarge
92,db.r5.24xlarge
92,Cluster size
92,10 + 1 MDS
92,Geo-mean result
92,7.3 seconds
92,19.7 seconds
92,2.5 hours
92,Annual cost
92,"USD$37,022"
92,"USD$110,560"
92,"USD$129,336"
92,400GB TPC-H
92,HeatWave
92,MySQL Database
92,Instance shape
92,Cluster size (this small config is for testing purpose only—minimum supported is 2+1)
92,1 + 1 MDS
92,Geo-mean result
92,4.2 seconds
92,1700 seconds
92,Annual cost
92,"USD$6,483"
92,"USD$3,386"
92,"*Disclaimer: Benchmark queries are derived from the TPC-H benchmark, but results are not comparable to published TPC-H benchmark results since they do not comply with the TPC-H specification."
92,Resources for
92,Developers
92,Startups
92,Students and Educators
92,Partners
92,Oracle PartnerNetwork
92,Find a Partner
92,Log in to OPN
92,Solutions
92,Artificial Intelligence
92,Internet of Things
92,Blockchain
92,What’s New
92,How we’re taking on COVID-19
92,Java 16 download
92,Try Oracle Cloud Free Tier
92,Contact Us
92,US Sales: +1.800.633.0738
92,How can we help?
92,Subscribe to emails
92,Country/Region
92,© 2021 Oracle
92,Site Map
92,Privacy/Do Not Sell My Info
92,Ad Choices
92,Careers
92,Facebook
92,Twitter
92,LinkedIn
92,YouTube
94,HAProxy version 1.8.27 - Configuration Manual
94,Toggle navigation
94,HAProxy Documentation
94,Starter Guide
94,Configuration Manual
94,Management Guide
94,HAProxy Home Page
94,Summary
94,Keywords
94,Quick reminder about HTTP
94,1.1.
94,The HTTP transaction model
94,1.2.
94,HTTP request
94,1.2.1.
94,The request line
94,1.2.2.
94,The request headers
94,1.3.
94,HTTP response
94,1.3.1.
94,The response line
94,1.3.2.
94,The response headers
94,Configuring HAProxy
94,2.1.
94,Configuration file format
94,2.2.
94,Quoting and escaping
94,2.3.
94,Environment variables
94,2.4.
94,Time format
94,2.5.
94,Examples
94,Global parameters
94,3.1.
94,Process management and security
94,3.2.
94,Performance tuning
94,3.3.
94,Debugging
94,3.4.
94,Userlists
94,3.5.
94,Peers
94,3.6.
94,Mailers
94,Proxies
94,4.1.
94,Proxy keywords matrix
94,4.2.
94,Alphabetically sorted keywords reference
94,Bind and server options
94,5.1.
94,Bind options
94,5.2.
94,Server and default-server options
94,5.3.
94,Server DNS resolution
94,5.3.1.
94,Global overview
94,5.3.2.
94,The resolvers section
94,HTTP header manipulation
94,Using ACLs and fetching samples
94,7.1.
94,ACL basics
94,7.1.1.
94,Matching booleans
94,7.1.2.
94,Matching integers
94,7.1.3.
94,Matching strings
94,7.1.4.
94,Matching regular expressions (regexes)
94,7.1.5.
94,Matching arbitrary data blocks
94,7.1.6.
94,Matching IPv4 and IPv6 addresses
94,7.2.
94,Using ACLs to form conditions
94,7.3.
94,Fetching samples
94,7.3.1.
94,Converters
94,7.3.2.
94,Fetching samples from internal states
94,7.3.3.
94,Fetching samples at Layer 4
94,7.3.4.
94,Fetching samples at Layer 5
94,7.3.5.
94,Fetching samples from buffer contents (Layer 6)
94,7.3.6.
94,Fetching HTTP samples (Layer 7)
94,7.4.
94,Pre-defined ACLs
94,Logging
94,8.1.
94,Log levels
94,8.2.
94,Log formats
94,8.2.1.
94,Default log format
94,8.2.2.
94,TCP log format
94,8.2.3.
94,HTTP log format
94,8.2.4.
94,Custom log format
94,8.2.5.
94,Error log format
94,8.3.
94,Advanced logging options
94,8.3.1.
94,Disabling logging of external tests
94,8.3.2.
94,Logging before waiting for the session to terminate
94,8.3.3.
94,Raising log level upon errors
94,8.3.4.
94,Disabling logging of successful connections
94,8.4.
94,Timing events
94,8.5.
94,Session state at disconnection
94,8.6.
94,Non-printable characters
94,8.7.
94,Capturing HTTP cookies
94,8.8.
94,Capturing HTTP headers
94,8.9.
94,Examples of logs
94,Supported filters
94,9.1.
94,Trace
94,9.2.
94,HTTP compression
94,9.3.
94,Stream Processing Offload Engine (SPOE)
94,10.
94,Cache
94,10.1.
94,Limitation
94,10.2.
94,Setup
94,10.2.1.
94,Cache section
94,10.2.2.
94,Proxy section
94,Filter
94,51d.all
94,51d.single
94,51degrees-cache-size
94,51degrees-data-file
94,51degrees-property-name-list
94,51degrees-property-separator
94,accept-netscaler-cip
94,accept-proxy
94,accepted_payload_size
94,acl
94,add
94,addr
94,agent-addr
94,agent-check
94,agent-inter
94,agent-port
94,agent-send
94,allow-0rtt (Bind options)
94,allow-0rtt (Server and default-server options)
94,alpn
94,always_false
94,always_true
94,and
94,appsession
94,avg_queue
94,b64dec
94,backlog (Alphabetically sorted keywords reference)
94,backlog (Bind options)
94,backup
94,balance
94,balance url_param
94,base
94,base32
94,base32+src
94,base64
94,be_conn
94,be_id
94,be_name
94,be_sess_rate
94,bin
94,bind
94,bind-process
94,block
94,bool (Converters)
94,bool (Fetching samples from internal states)
94,bytes
94,ca-base
94,ca-file (Bind options)
94,ca-file (Server and default-server options)
94,ca-ignore-err
94,ca-sign-file
94,ca-sign-pass
94,cache
94,capture
94,capture cookie
94,capture request
94,capture request header
94,capture response
94,capture response header
94,capture-req
94,capture-res
94,capture.req.hdr
94,capture.req.method
94,capture.req.uri
94,capture.req.ver
94,capture.res.hdr
94,capture.res.ver
94,check
94,check-send-proxy
94,check-sni
94,check-ssl
94,chroot
94,ciphers (Bind options)
94,ciphers (Server and default-server options)
94,ciphersuites (Bind options)
94,ciphersuites (Server and default-server options)
94,clitimeout
94,compression
94,compression algo
94,compression offload
94,compression type
94,connslots
94,contimeout
94,cook
94,cook_cnt
94,cook_val
94,cookie (Alphabetically sorted keywords reference)
94,cookie (Server and default-server options)
94,cookie (Fetching HTTP samples (Layer 7))
94,cpl
94,cpu-map
94,crc32
94,crl-file (Bind options)
94,crl-file (Server and default-server options)
94,crt (Bind options)
94,crt (Server and default-server options)
94,crt-base
94,crt-ignore-err
94,crt-list
94,curves
94,da-csv-conv
94,daemon
94,date
94,debug (Debugging)
94,debug (Converters)
94,declare
94,declare capture
94,default-server
94,default_backend
94,defer-accept
94,description (Process management and security)
94,description (Alphabetically sorted keywords reference)
94,deviceatlas-json-file
94,deviceatlas-log-level
94,deviceatlas-properties-cookie
94,deviceatlas-separator
94,disabled (Peers)
94,disabled (Alphabetically sorted keywords reference)
94,disabled (Server and default-server options)
94,dispatch
94,distcc_body
94,distcc_param
94,div
94,djb2
94,downinter
94,dst
94,dst_conn
94,dst_is_local
94,dst_port
94,dynamic-cookie-key
94,ecdhe
94,email-alert
94,email-alert from
94,email-alert level
94,email-alert mailers
94,email-alert myhostname
94,email-alert to
94,enable
94,enabled (Alphabetically sorted keywords reference)
94,enabled (Server and default-server options)
94,env
94,error-limit
94,errorfile
94,errorloc
94,errorloc302
94,errorloc303
94,even
94,expose-fd
94,expose-fd listeners
94,external-check
94,external-check command
94,external-check path
94,fall
94,fastinter
94,fc_fackets
94,fc_http_major
94,fc_lost
94,fc_rcvd_proxy
94,fc_reordering
94,fc_retrans
94,fc_rtt
94,fc_rttvar
94,fc_sacked
94,fc_unacked
94,fe_conn
94,fe_id
94,fe_name
94,fe_req_rate
94,fe_sess_rate
94,field
94,filter (Alphabetically sorted keywords reference)
94,filter (Trace)
94,filter (HTTP compression)
94,filter (Stream Processing Offload Engine (SPOE))
94,filter compression
94,filter spoe
94,filter trace
94,force-persist
94,force-sslv3
94,force-tlsv10
94,force-tlsv11
94,force-tlsv12
94,force-tlsv13
94,fullconn
94,generate-certificates
94,gid (Process management and security)
94,gid (Bind options)
94,grace
94,group (Process management and security)
94,group (Userlists)
94,group (Bind options)
94,hard-stop-after
94,hash-balance-factor
94,hash-type
94,hdr
94,hdr_cnt
94,hdr_ip
94,hdr_val
94,hex
94,hex2i
94,hold
94,hostname
94,http-check
94,http-check disable-on-404
94,http-check expect
94,http-check send
94,http-check send-state
94,http-request (Proxy section)
94,http-request (Alphabetically sorted keywords reference)
94,http-request cache-use
94,http-response (Proxy section)
94,http-response (Alphabetically sorted keywords reference)
94,http-response cache-store
94,http-reuse
94,http-send-name-header
94,http_auth
94,http_auth_group
94,http_date
94,http_first_req
94,id (Alphabetically sorted keywords reference)
94,id (Bind options)
94,id (Server and default-server options)
94,ignore-persist
94,in_table
94,init-addr
94,int
94,inter
94,interface
94,ipmask
94,ipv4
94,ipv6
94,json
94,language
94,level
94,load-server-state-from-file
94,log (Process management and security)
94,log (Alphabetically sorted keywords reference)
94,log global
94,log-format
94,log-format-sd
94,log-send-hostname
94,log-tag (Process management and security)
94,log-tag (Alphabetically sorted keywords reference)
94,lower
94,ltime
94,lua-load
94,mailer
94,mailers
94,map
94,master-worker
94,max-age
94,max-keep-alive-queue
94,max-spread-checks
94,maxcompcpuusage
94,maxcomprate
94,maxconn (Performance tuning)
94,maxconn (Alphabetically sorted keywords reference)
94,maxconn (Bind options)
94,maxconn (Server and default-server options)
94,maxconnrate
94,maxpipes
94,maxqueue
94,maxsessrate
94,maxsslconn
94,maxsslrate
94,maxzlibmem
94,meth
94,method
94,minconn
94,mod
94,mode
94,monitor
94,monitor fail
94,monitor-net
94,monitor-uri
94,mss
94,mul
94,name
94,nameserver
94,namespace
94,nbproc (Process management and security)
94,nbproc (Fetching samples from internal states)
94,nbsrv (Converters)
94,nbsrv (Fetching samples from internal states)
94,nbthread
94,neg
94,nice
94,no log
94,no option
94,no option abortonclose
94,no option accept-invalid-http-request
94,no option accept-invalid-http-response
94,no option allbackups
94,no option checkcache
94,no option clitcpka
94,no option dontlog-normal
94,no option dontlognull
94,no option forceclose
94,no option http-buffer-request
94,no option http-ignore-probes
94,no option http-keep-alive
94,no option http-no-delay
94,no option http-pretend-keepalive
94,no option http-server-close
94,no option http-tunnel
94,no option http-use-proxy-header
94,no option http_proxy
94,no option httpclose
94,no option independent-streams
94,no option log-health-checks
94,no option log-separate-errors
94,no option logasap
94,no option nolinger
94,no option persist
94,no option prefer-last-server
94,no option redispatch
94,no option socket-stats
94,no option splice-auto
94,no option splice-request
94,no option splice-response
94,no option srvtcpka
94,no option tcp-smart-accept
94,no option tcp-smart-connect
94,no option transparent
94,no-agent-check
94,no-backup
94,no-ca-names
94,no-check
94,no-check-ssl
94,no-send-proxy
94,no-send-proxy-v2
94,no-send-proxy-v2-ssl
94,no-send-proxy-v2-ssl-cn
94,no-ssl
94,no-ssl-reuse
94,no-sslv3
94,no-tls-tickets (Bind options)
94,no-tls-tickets (Server and default-server options)
94,no-tlsv10
94,no-tlsv11
94,no-tlsv12
94,no-tlsv13
94,no-verifyhost
94,node
94,noepoll
94,nogetaddrinfo
94,nokqueue
94,non-stick
94,nopoll
94,noreuseport
94,nosplice
94,not
94,npn
94,observe
94,odd
94,on-error
94,on-marked-down
94,on-marked-up
94,option
94,option abortonclose
94,option accept-invalid-http-request
94,option accept-invalid-http-response
94,option allbackups
94,option checkcache
94,option clitcpka
94,option contstats
94,option dontlog-normal
94,option dontlognull
94,option external-check
94,option forceclose
94,option forwardfor
94,option http-buffer-request
94,option http-ignore-probes
94,option http-keep-alive
94,option http-no-delay
94,option http-pretend-keepalive
94,option http-server-close
94,option http-tunnel
94,option http-use-proxy-header
94,option http_proxy
94,option httpchk
94,option httpclose
94,option httplog
94,option independent-streams
94,option ldap-check
94,option log-health-checks
94,option log-separate-errors
94,option logasap
94,option mysql-check
94,option nolinger
94,option originalto
94,option persist
94,option pgsql-check
94,option prefer-last-server
94,option redis-check
94,option redispatch
94,option smtpchk
94,option socket-stats
94,option splice-auto
94,option splice-request
94,option splice-response
94,option spop-check
94,option srvtcpka
94,option ssl-hello-chk
94,option tcp-check
94,option tcp-smart-accept
94,option tcp-smart-connect
94,option tcpka
94,option tcplog
94,option transparent
94,path
94,payload
94,payload_lv
94,peer
94,peers
94,persist
94,persist rdp-cookie
94,pidfile
94,port
94,prefer-client-ciphers
94,presetenv
94,proc
94,process
94,query
94,queue
94,quiet
94,rand
94,rate-limit
94,rate-limit sessions
94,rdp_cookie
94,rdp_cookie_cnt
94,redir
94,redirect
94,redirect location
94,redirect prefix
94,redirect scheme
94,redisp
94,redispatch
94,regsub
94,rep_ssl_hello_type
94,req.body
94,req.body_len
94,req.body_param
94,req.body_size
94,req.cook
94,req.cook_cnt
94,req.cook_val
94,req.fhdr
94,req.fhdr_cnt
94,req.hdr
94,req.hdr_cnt
94,req.hdr_ip
94,req.hdr_names
94,req.hdr_val
94,req.hdrs
94,req.hdrs_bin
94,req.len
94,req.payload
94,req.payload_lv
94,req.proto_http
94,req.rdp_cookie
94,req.rdp_cookie_cnt
94,req.ssl_ec_ext
94,req.ssl_hello_type
94,req.ssl_sni
94,req.ssl_st_ext
94,req.ssl_ver
94,req.ver
94,req_len
94,req_proto_http
94,req_ssl_hello_type
94,req_ssl_sni
94,req_ssl_ver
94,req_ver
94,reqadd
94,reqallow
94,reqdel
94,reqdeny
94,reqiallow
94,reqidel
94,reqideny
94,reqipass
94,reqirep
94,reqitarpit
94,reqpass
94,reqrep
94,reqtarpit
94,res.comp
94,res.comp_algo
94,res.cook
94,res.cook_cnt
94,res.cook_val
94,res.fhdr
94,res.fhdr_cnt
94,res.hdr
94,res.hdr_cnt
94,res.hdr_ip
94,res.hdr_names
94,res.hdr_val
94,res.len
94,res.payload
94,res.payload_lv
94,res.ssl_hello_type
94,res.ver
94,resetenv
94,resolution_pool_size
94,resolve-net
94,resolve-opts
94,resolve-prefer
94,resolve_retries
94,resolvers (Server and default-server options)
94,resolvers (The resolvers section)
94,resp_ver
94,retries
94,rise
94,rspadd
94,rspdel
94,rspdeny
94,rspidel
94,rspideny
94,rspirep
94,rsprep
94,sc0_bytes_in_rate
94,sc0_bytes_out_rate
94,sc0_clr_gpc0
94,sc0_conn_cnt
94,sc0_conn_cur
94,sc0_conn_rate
94,sc0_get_gpc0
94,sc0_get_gpt0
94,sc0_gpc0_rate
94,sc0_http_err_cnt
94,sc0_http_err_rate
94,sc0_http_req_cnt
94,sc0_http_req_rate
94,sc0_inc_gpc0
94,sc0_kbytes_in
94,sc0_kbytes_out
94,sc0_sess_cnt
94,sc0_sess_rate
94,sc0_tracked
94,sc0_trackers
94,sc1_bytes_in_rate
94,sc1_bytes_out_rate
94,sc1_clr_gpc0
94,sc1_conn_cnt
94,sc1_conn_cur
94,sc1_conn_rate
94,sc1_get_gpc0
94,sc1_get_gpt0
94,sc1_gpc0_rate
94,sc1_http_err_cnt
94,sc1_http_err_rate
94,sc1_http_req_cnt
94,sc1_http_req_rate
94,sc1_inc_gpc0
94,sc1_kbytes_in
94,sc1_kbytes_out
94,sc1_sess_cnt
94,sc1_sess_rate
94,sc1_tracked
94,sc1_trackers
94,sc2_bytes_in_rate
94,sc2_bytes_out_rate
94,sc2_clr_gpc0
94,sc2_conn_cnt
94,sc2_conn_cur
94,sc2_conn_rate
94,sc2_get_gpc0
94,sc2_get_gpt0
94,sc2_gpc0_rate
94,sc2_http_err_cnt
94,sc2_http_err_rate
94,sc2_http_req_cnt
94,sc2_http_req_rate
94,sc2_inc_gpc0
94,sc2_kbytes_in
94,sc2_kbytes_out
94,sc2_sess_cnt
94,sc2_sess_rate
94,sc2_tracked
94,sc2_trackers
94,sc_bytes_in_rate
94,sc_bytes_out_rate
94,sc_clr_gpc0
94,sc_conn_cnt
94,sc_conn_cur
94,sc_conn_rate
94,sc_get_gpc0
94,sc_get_gpt0
94,sc_gpc0_rate
94,sc_http_err_cnt
94,sc_http_err_rate
94,sc_http_req_cnt
94,sc_http_req_rate
94,sc_inc_gpc0
94,sc_kbytes_in
94,sc_kbytes_out
94,sc_sess_cnt
94,sc_sess_rate
94,sc_tracked
94,sc_trackers
94,scook
94,scook_cnt
94,scook_val
94,sdbm
94,send-proxy
94,send-proxy-v2
94,send-proxy-v2-ssl
94,send-proxy-v2-ssl-cn
94,server
94,server-state-base
94,server-state-file
94,server-state-file-name
94,server-template
94,set-cookie
94,setenv
94,severity-output
94,sha1
94,shdr
94,shdr_cnt
94,shdr_ip
94,shdr_val
94,slowstart
94,sni
94,so_id
94,source (Alphabetically sorted keywords reference)
94,source (Server and default-server options)
94,spread-checks
94,src
94,src_bytes_in_rate
94,src_bytes_out_rate
94,src_clr_gpc0
94,src_conn_cnt
94,src_conn_cur
94,src_conn_rate
94,src_get_gpc0
94,src_get_gpt0
94,src_gpc0_rate
94,src_http_err_cnt
94,src_http_err_rate
94,src_http_req_cnt
94,src_http_req_rate
94,src_inc_gpc0
94,src_is_local
94,src_kbytes_in
94,src_kbytes_out
94,src_port
94,src_sess_cnt
94,src_sess_rate
94,src_updt_conn_cnt
94,srv_conn
94,srv_id
94,srv_is_up
94,srv_queue
94,srv_sess_rate
94,srvtimeout
94,ssl (Bind options)
94,ssl (Server and default-server options)
94,ssl-default-bind-ciphers
94,ssl-default-bind-ciphersuites
94,ssl-default-bind-options
94,ssl-default-server-ciphers
94,ssl-default-server-ciphersuites
94,ssl-default-server-options
94,ssl-dh-param-file
94,ssl-engine
94,ssl-max-ver (Bind options)
94,ssl-max-ver (Server and default-server options)
94,ssl-min-ver (Bind options)
94,ssl-min-ver (Server and default-server options)
94,ssl-mode-async
94,ssl-reuse
94,ssl-server-verify
94,ssl_bc
94,ssl_bc_alg_keysize
94,ssl_bc_cipher
94,ssl_bc_protocol
94,ssl_bc_session_id
94,ssl_bc_unique_id
94,ssl_bc_use_keysize
94,ssl_c_ca_err
94,ssl_c_ca_err_depth
94,ssl_c_der
94,ssl_c_err
94,ssl_c_i_dn
94,ssl_c_key_alg
94,ssl_c_notafter
94,ssl_c_notbefore
94,ssl_c_s_dn
94,ssl_c_serial
94,ssl_c_sha1
94,ssl_c_sig_alg
94,ssl_c_used
94,ssl_c_verify
94,ssl_c_version
94,ssl_f_der
94,ssl_f_i_dn
94,ssl_f_key_alg
94,ssl_f_notafter
94,ssl_f_notbefore
94,ssl_f_s_dn
94,ssl_f_serial
94,ssl_f_sha1
94,ssl_f_sig_alg
94,ssl_f_version
94,ssl_fc
94,ssl_fc_alg_keysize
94,ssl_fc_alpn
94,ssl_fc_cipher
94,ssl_fc_cipherlist_bin
94,ssl_fc_cipherlist_hex
94,ssl_fc_cipherlist_str
94,ssl_fc_cipherlist_xxh
94,ssl_fc_has_crt
94,ssl_fc_has_early
94,ssl_fc_has_sni
94,ssl_fc_is_resumed
94,ssl_fc_npn
94,ssl_fc_protocol
94,ssl_fc_session_id
94,ssl_fc_sni
94,ssl_fc_unique_id
94,ssl_fc_use_keysize
94,stats (Process management and security)
94,stats (Alphabetically sorted keywords reference)
94,stats admin
94,stats auth
94,stats bind-process
94,stats enable
94,stats hide-version
94,stats http-request
94,stats maxconn
94,stats realm
94,stats refresh
94,stats scope
94,stats show-desc
94,stats show-legends
94,stats show-node
94,stats socket
94,stats timeout
94,stats uri
94,status
94,stick (Alphabetically sorted keywords reference)
94,stick (Server and default-server options)
94,stick match
94,stick on
94,stick store-request
94,stick store-response
94,stick-table
94,stick-table type
94,stopping
94,str
94,strict-sni
94,sub
94,table_avl
94,table_bytes_in_rate
94,table_bytes_out_rate
94,table_cnt
94,table_conn_cnt
94,table_conn_cur
94,table_conn_rate
94,table_gpc0
94,table_gpc0_rate
94,table_gpt0
94,table_http_err_cnt
94,table_http_err_rate
94,table_http_req_cnt
94,table_http_req_rate
94,table_kbytes_in
94,table_kbytes_out
94,table_server_id
94,table_sess_cnt
94,table_sess_rate
94,table_trackers
94,tcp-check
94,tcp-check connect
94,tcp-check expect
94,tcp-check send
94,tcp-check send-binary
94,tcp-request
94,tcp-request connection
94,tcp-request content
94,tcp-request inspect-delay
94,tcp-request session
94,tcp-response
94,tcp-response content
94,tcp-response inspect-delay
94,tcp-ut
94,tfo
94,thread
94,timeout (Mailers)
94,timeout (Alphabetically sorted keywords reference)
94,timeout (The resolvers section)
94,timeout check
94,timeout client
94,timeout client-fin
94,timeout clitimeout
94,timeout connect
94,timeout contimeout
94,timeout http-keep-alive
94,timeout http-request
94,timeout mail
94,timeout queue
94,timeout server
94,timeout server-fin
94,timeout srvtimeout
94,timeout tarpit
94,timeout tunnel
94,tls-ticket-keys
94,tls-tickets
94,total-max-size
94,track
94,transparent
94,tune.buffers.limit
94,tune.buffers.reserve
94,tune.bufsize
94,tune.chksize
94,tune.comp.maxlevel
94,tune.h2.header-table-size
94,tune.h2.initial-window-size
94,tune.h2.max-concurrent-streams
94,tune.http.cookielen
94,tune.http.logurilen
94,tune.http.maxhdr
94,tune.idletimer
94,tune.lua.forced-yield
94,tune.lua.maxmem
94,tune.lua.service-timeout
94,tune.lua.session-timeout
94,tune.lua.task-timeout
94,tune.maxaccept
94,tune.maxpollevents
94,tune.maxrewrite
94,tune.pattern.cache-size
94,tune.pipesize
94,tune.rcvbuf.client
94,tune.rcvbuf.server
94,tune.recv_enough
94,tune.sndbuf.client
94,tune.sndbuf.server
94,tune.ssl.cachesize
94,tune.ssl.capture-cipherlist-size
94,tune.ssl.default-dh-param
94,tune.ssl.force-private-cache
94,tune.ssl.lifetime
94,tune.ssl.maxrecord
94,tune.ssl.ssl-ctx-cache-size
94,tune.vars.global-max-size
94,tune.vars.proc-max-size
94,tune.vars.reqres-max-size
94,tune.vars.sess-max-size
94,tune.vars.txn-max-size
94,tune.zlib.memlevel
94,tune.zlib.windowsize
94,uid (Process management and security)
94,uid (Bind options)
94,ulimit-n
94,unique-id
94,unique-id-format
94,unique-id-header
94,unix-bind
94,unsetenv
94,upper
94,url
94,url32
94,url32+src
94,url_dec
94,url_ip
94,url_param
94,url_port
94,urlp
94,urlp_val
94,use-server
94,use_backend
94,user (Process management and security)
94,user (Userlists)
94,user (Bind options)
94,userlist
94,utime
94,v4v6
94,v6only
94,var
94,verify (Bind options)
94,verify (Server and default-server options)
94,verifyhost
94,wait_end
94,weight
94,word
94,wt6
94,wurfl-cache-size
94,wurfl-data-file
94,wurfl-engine-mode
94,wurfl-information-list
94,wurfl-information-list-separator
94,wurfl-patch-file
94,wurfl-useragent-priority
94,xor
94,xxh32
94,xxh64
94,Keyboard navigation :
94,You can use left and right arrow keys to navigate between chapters.
94,Converted with haproxy-dconv v0.4.2-12 on 2020/11/11
94,Configuration Manual
94,version 1.8.27
94,willy tarreau
94,2020/11/06
94,This document covers the configuration language as implemented in the version
94,"specified above. It does not provide any hints, examples, or advice. For such"
94,"documentation, please refer to the Reference Manual or the Architecture Manual."
94,The summary below is meant to help you find sections by name and navigate
94,through the document.
94,Note to documentation contributors :
94,"This document is formatted with 80 columns per line, with even number of"
94,spaces for indentation and without tabs. Please follow these rules strictly
94,so that it remains easily printable everywhere. If a line needs to be
94,"printed verbatim and does not fit, please end each line with a backslash"
94,"('\') and continue on next line, indented by two characters. It is also"
94,"sometimes useful to prefix all output lines (logs, console outputs) with 3"
94,closing angle brackets ('>>>') in order to emphasize the difference between
94,"inputs and outputs when they may be ambiguous. If you add sections,"
94,please update the summary below for easier searching.
94,Summary
94,Quick reminder about HTTP
94,1.1.
94,The HTTP transaction model
94,1.2.
94,HTTP request
94,1.2.1.
94,The request line
94,1.2.2.
94,The request headers
94,1.3.
94,HTTP response
94,1.3.1.
94,The response line
94,1.3.2.
94,The response headers
94,Configuring HAProxy
94,2.1.
94,Configuration file format
94,2.2.
94,Quoting and escaping
94,2.3.
94,Environment variables
94,2.4.
94,Time format
94,2.5.
94,Examples
94,Global parameters
94,3.1.
94,Process management and security
94,3.2.
94,Performance tuning
94,3.3.
94,Debugging
94,3.4.
94,Userlists
94,3.5.
94,Peers
94,3.6.
94,Mailers
94,Proxies
94,4.1.
94,Proxy keywords matrix
94,4.2.
94,Alphabetically sorted keywords reference
94,Bind and server options
94,5.1.
94,Bind options
94,5.2.
94,Server and default-server options
94,5.3.
94,Server DNS resolution
94,5.3.1.
94,Global overview
94,5.3.2.
94,The resolvers section
94,HTTP header manipulation
94,Using ACLs and fetching samples
94,7.1.
94,ACL basics
94,7.1.1.
94,Matching booleans
94,7.1.2.
94,Matching integers
94,7.1.3.
94,Matching strings
94,7.1.4.
94,Matching regular expressions (regexes)
94,7.1.5.
94,Matching arbitrary data blocks
94,7.1.6.
94,Matching IPv4 and IPv6 addresses
94,7.2.
94,Using ACLs to form conditions
94,7.3.
94,Fetching samples
94,7.3.1.
94,Converters
94,7.3.2.
94,Fetching samples from internal states
94,7.3.3.
94,Fetching samples at Layer 4
94,7.3.4.
94,Fetching samples at Layer 5
94,7.3.5.
94,Fetching samples from buffer contents (Layer 6)
94,7.3.6.
94,Fetching HTTP samples (Layer 7)
94,7.4.
94,Pre-defined ACLs
94,Logging
94,8.1.
94,Log levels
94,8.2.
94,Log formats
94,8.2.1.
94,Default log format
94,8.2.2.
94,TCP log format
94,8.2.3.
94,HTTP log format
94,8.2.4.
94,Custom log format
94,8.2.5.
94,Error log format
94,8.3.
94,Advanced logging options
94,8.3.1.
94,Disabling logging of external tests
94,8.3.2.
94,Logging before waiting for the session to terminate
94,8.3.3.
94,Raising log level upon errors
94,8.3.4.
94,Disabling logging of successful connections
94,8.4.
94,Timing events
94,8.5.
94,Session state at disconnection
94,8.6.
94,Non-printable characters
94,8.7.
94,Capturing HTTP cookies
94,8.8.
94,Capturing HTTP headers
94,8.9.
94,Examples of logs
94,Supported filters
94,9.1.
94,Trace
94,9.2.
94,HTTP compression
94,9.3.
94,Stream Processing Offload Engine (SPOE)
94,10.
94,Cache
94,10.1.
94,Limitation
94,10.2.
94,Setup
94,10.2.1.
94,Cache section
94,10.2.2.
94,Proxy section
94,1. Quick reminder about HTTP
94,"When HAProxy is running in HTTP mode, both the request and the response are"
94,"fully analyzed and indexed, thus it becomes possible to build matching criteria"
94,on almost anything found in the contents.
94,"However, it is important to understand how HTTP requests and responses are"
94,"formed, and how HAProxy decomposes them. It will then become easier to write"
94,correct rules and to debug existing configurations.
94,1.1. The HTTP transaction model
94,The HTTP protocol is transaction-driven. This means that each request will lead
94,"to one and only one response. Traditionally, a TCP connection is established"
94,"from the client to the server, a request is sent by the client through the"
94,"connection, the server responds, and the connection is closed. A new request"
94,will involve a new connection :
94,[CON1] [REQ1] ... [RESP1] [CLO1] [CON2] [REQ2] ... [RESP2] [CLO2] ...
94,"In this mode, called the ""HTTP close"" mode, there are as many connection"
94,establishments as there are HTTP transactions. Since the connection is closed
94,"by the server after the response, the client does not need to know the content"
94,length.
94,"Due to the transactional nature of the protocol, it was possible to improve it"
94,to avoid closing a connection between two subsequent transactions. In this mode
94,"however, it is mandatory that the server indicates the content length for each"
94,"response so that the client does not wait indefinitely. For this, a special"
94,"header is used: ""Content-length"". This mode is called the ""keep-alive"" mode :"
94,[CON] [REQ1] ... [RESP1] [REQ2] ... [RESP2] [CLO] ...
94,"Its advantages are a reduced latency between transactions, and less processing"
94,"power required on the server side. It is generally better than the close mode,"
94,but not always because the clients often limit their concurrent connections to
94,a smaller value.
94,Another improvement in the communications is the pipelining mode. It still uses
94,"keep-alive, but the client does not wait for the first response to send the"
94,second request. This is useful for fetching large number of images composing a
94,page :
94,[CON] [REQ1] [REQ2] ... [RESP1] [RESP2] [CLO] ...
94,This can obviously have a tremendous benefit on performance because the network
94,latency is eliminated between subsequent requests. Many HTTP agents do not
94,correctly support pipelining since there is no way to associate a response with
94,"the corresponding request in HTTP. For this reason, it is mandatory for the"
94,server to reply in the exact same order as the requests were received.
94,"The next improvement is the multiplexed mode, as implemented in HTTP/2. This"
94,"time, each transaction is assigned a single stream identifier, and all streams"
94,are multiplexed over an existing connection. Many requests can be sent in
94,"parallel by the client, and responses can arrive in any order since they also"
94,carry the stream identifier.
94,By default HAProxy operates in keep-alive mode with regards to persistent
94,"connections: for each connection it processes each request and response, and"
94,leaves the connection idle on both sides between the end of a response and the
94,"start of a new request. When it receives HTTP/2 connections from a client, it"
94,"processes all the requests in parallel and leaves the connection idling,"
94,"waiting for new requests, just as if it was a keep-alive HTTP connection."
94,HAProxy supports 5 connection modes :
94,- keep alive
94,: all requests and responses are processed (default)
94,- tunnel
94,": only the first request and response are processed,"
94,everything else is forwarded with no analysis.
94,"- passive close : tunnel with ""Connection: close"" added in both directions."
94,- server close
94,: the server-facing connection is closed after the response.
94,- forced close
94,: the connection is actively closed after end of response.
94,"For HTTP/2, the connection mode resembles more the ""server close"" mode : given"
94,"the independence of all streams, there is currently no place to hook the idle"
94,"server connection after a response, so it is closed after the response. HTTP/2"
94,"is only supported for incoming connections, not on connections going to"
94,servers.
94,1.2. HTTP request
94,"First, let's consider this HTTP request :"
94,Line
94,Contents
94,number
94,GET /serv/login.php?lang=en&profile=2 HTTP/1.1
94,Host: www.mydomain.com
94,User-agent: my small browser
94,"Accept: image/jpeg, image/gif"
94,Accept: image/png
94,1.2.1. The Request line
94,"Line 1 is the ""request line"". It is always composed of 3 fields :"
94,- a METHOD
94,: GET
94,- a URI
94,: /serv/login.php?lang=en&profile=2
94,- a version tag : HTTP/1.1
94,"All of them are delimited by what the standard calls LWS (linear white spaces),"
94,"which are commonly spaces, but can also be tabs or line feeds/carriage returns"
94,followed by spaces/tabs. The method itself cannot contain any colon (':') and
94,is limited to alphabetic letters. All those various combinations make it
94,desirable that HAProxy performs the splitting itself rather than leaving it to
94,the user to write a complex or inaccurate regular expression.
94,The URI itself can have several forms :
94,"- A ""relative URI"" :"
94,/serv/login.php?lang=en&profile=2
94,It is a complete URL without the host part. This is generally what is
94,"received by servers, reverse proxies and transparent proxies."
94,"- An ""absolute URI"", also called a ""URL"" :"
94,http://192.168.0.12:8080/serv/login.php?lang=en&profile=2
94,"It is composed of a ""scheme"" (the protocol name followed by '://'), a host"
94,"name or address, optionally a colon (':') followed by a port number, then"
94,a relative URI beginning at the first slash ('/') after the address part.
94,"This is generally what proxies receive, but a server supporting HTTP/1.1"
94,must accept this form too.
94,- a star ('*') : this form is only accepted in association with the OPTIONS
94,method and is not relayable. It is used to inquiry a next hop's
94,capabilities.
94,- an address:port combination : 192.168.0.12:80
94,"This is used with the CONNECT method, which is used to establish TCP"
94,"tunnels through HTTP proxies, generally for HTTPS, but sometimes for"
94,other protocols too.
94,"In a relative URI, two sub-parts are identified. The part before the question"
94,"mark is called the ""path"". It is typically the relative path to static objects"
94,"on the server. The part after the question mark is called the ""query string""."
94,It is mostly used with GET requests sent to dynamic scripts and is very
94,"specific to the language, framework or application in use."
94,"HTTP/2 doesn't convey a version information with the request, so the version is"
94,"assumed to be the same as the one of the underlying protocol (i.e. ""HTTP/2"")."
94,"However, haproxy natively processes HTTP/1.x requests and headers, so requests"
94,received over an HTTP/2 connection are transcoded to HTTP/1.1 before being
94,"processed. This explains why they still appear as ""HTTP/1.1"" in haproxy's logs"
94,as well as in server logs.
94,1.2.2. The request headers
94,The headers start at the second line. They are composed of a name at the
94,"beginning of the line, immediately followed by a colon (':'). Traditionally,"
94,an LWS is added after the colon but that's not required. Then come the values.
94,"Multiple identical headers may be folded into one single line, delimiting the"
94,"values with commas, provided that their order is respected. This is commonly"
94,"encountered in the ""Cookie:"" field. A header may span over multiple lines if"
94,"the subsequent lines begin with an LWS. In the example in 1.2, lines 4 and 5"
94,"define a total of 3 values for the ""Accept:"" header."
94,"Contrary to a common misconception, header names are not case-sensitive, and"
94,their values are not either if they refer to other header names (such as the
94,"""Connection:"" header). In HTTP/2, header names are always sent in lower case,"
94,as can be seen when running in debug mode.
94,The end of the headers is indicated by the first empty line. People often say
94,"that it's a double line feed, which is not exact, even if a double line feed"
94,is one valid form of empty line.
94,"Fortunately, HAProxy takes care of all these complex combinations when indexing"
94,"headers, checking values and counting them, so there is no reason to worry"
94,"about the way they could be written, but it is important not to accuse an"
94,"application of being buggy if it does unusual, valid things."
94,Important note:
94,"As suggested by RFC7231, HAProxy normalizes headers by replacing line breaks"
94,in the middle of headers by LWS in order to join multi-line headers. This
94,is necessary for proper analysis and helps less capable HTTP parsers to work
94,correctly and not to be fooled by such complex constructs.
94,1.3. HTTP response
94,An HTTP response looks very much like an HTTP request. Both are called HTTP
94,messages. Let's consider this HTTP response :
94,Line
94,Contents
94,number
94,HTTP/1.1 200 OK
94,Content-length: 350
94,Content-Type: text/html
94,"As a special case, HTTP supports so called ""Informational responses"" as status"
94,codes 1xx. These messages are special in that they don't convey any part of the
94,"response, they're just used as sort of a signaling message to ask a client to"
94,continue to post its request for instance. In the case of a status 100 response
94,the requested information will be carried by the next non-100 response message
94,following the informational one. This implies that multiple responses may be
94,"sent to a single request, and that this only works when keep-alive is enabled"
94,(1xx messages are HTTP/1.1 only). HAProxy handles these messages and is able to
94,"correctly forward and skip them, and only process the next non-100 response. As"
94,"such, these messages are neither logged nor transformed, unless explicitly"
94,state otherwise. Status 101 messages indicate that the protocol is changing
94,"over the same connection and that haproxy must switch to tunnel mode, just as"
94,if a CONNECT had occurred. Then the Upgrade header would contain additional
94,information about the type of protocol the connection is switching to.
94,1.3.1. The response line
94,"Line 1 is the ""response line"". It is always composed of 3 fields :"
94,- a version tag : HTTP/1.1
94,- a status code : 200
94,- a reason
94,: OK
94,The status code is always 3-digit. The first digit indicates a general status :
94,"- 1xx = informational message to be skipped (e.g. 100, 101)"
94,"- 2xx = OK, content is following"
94,"(e.g. 200, 206)"
94,"- 3xx = OK, no content following"
94,"(e.g. 302, 304)"
94,"- 4xx = error caused by the client (e.g. 401, 403, 404)"
94,"- 5xx = error caused by the server (e.g. 500, 502, 503)"
94,Please refer to RFC7231 for the detailed meaning of all such codes. The
94,"""reason"" field is just a hint, but is not parsed by clients. Anything can be"
94,"found there, but it's a common practice to respect the well-established"
94,"messages. It can be composed of one or multiple words, such as ""OK"", ""Found"","
94,"or ""Authentication Required""."
94,HAProxy may emit the following status codes by itself :
94,Code
94,When / reason
94,200
94,"access to stats page, and when replying to monitoring requests"
94,301
94,"when performing a redirection, depending on the configured code"
94,302
94,"when performing a redirection, depending on the configured code"
94,303
94,"when performing a redirection, depending on the configured code"
94,307
94,"when performing a redirection, depending on the configured code"
94,308
94,"when performing a redirection, depending on the configured code"
94,400
94,for an invalid or too large request
94,401
94,when an authentication is required to perform the action (when
94,accessing the stats page)
94,403
94,"when a request is forbidden by a ""block"" ACL or ""reqdeny"" filter"
94,408
94,when the request timeout strikes before the request is complete
94,500
94,"when haproxy encounters an unrecoverable internal error, such as a"
94,"memory allocation failure, which should never happen"
94,502
94,"when the server returns an empty, invalid or incomplete response, or"
94,"when an ""rspdeny"" filter blocks the response."
94,503
94,"when no server was available to handle the request, or in response to"
94,"monitoring requests which match the ""monitor fail"" condition"
94,504
94,when the response timeout strikes before the server responds
94,"The error 4xx and 5xx codes above may be customized (see ""errorloc"" in section"
94,4.2).
94,1.3.2. The response headers
94,"Response headers work exactly like request headers, and as such, HAProxy uses"
94,the same parsing function for both. Please refer to paragraph 1.2.2 for more
94,details.
94,2. Configuring HAProxy
94,2.1. Configuration file format
94,HAProxy's configuration process involves 3 major sources of parameters :
94,"- the arguments from the command-line, which always take precedence"
94,"- the ""global"" section, which sets process-wide parameters"
94,"- the proxies sections which can take form of ""defaults"", ""listen"","
94,"""frontend"" and ""backend""."
94,The configuration file syntax consists in lines beginning with a keyword
94,"referenced in this manual, optionally followed by one or several parameters"
94,delimited by spaces.
94,2.2. Quoting and escaping
94,HAProxy's configuration introduces a quoting and escaping system similar to
94,many programming languages. The configuration file supports 3 types: escaping
94,"with a backslash, weak quoting with double quotes, and strong quoting with"
94,single quotes.
94,"If spaces have to be entered in strings, then they must be escaped by preceding"
94,them by a backslash ('\') or by quoting them. Backslashes also have to be
94,escaped by doubling or strong quoting them.
94,Escaping is achieved by preceding a special character by a backslash ('\'):
94,to mark a space and differentiate it from a delimiter
94,to mark a hash and differentiate it from a comment
94,to use a backslash
94,to use a single quote and differentiate it from strong quoting
94,to use a double quote and differentiate it from weak quoting
94,"Weak quoting is achieved by using double quotes (""""). Weak quoting prevents"
94,the interpretation of:
94,space as a parameter separator
94,single quote as a strong quoting delimiter
94,hash as a comment start
94,"Weak quoting permits the interpretation of variables, if you want to use a non"
94,"-interpreted dollar within a double quoted string, you should escape it with a"
94,"backslash (""\$""), it does not work outside weak quoting."
94,Interpretation of escaping and special characters are not prevented by weak
94,quoting.
94,"Strong quoting is achieved by using single quotes (''). Inside single quotes,"
94,"nothing is interpreted, it's the efficient way to quote regexes."
94,Quoted and escaped strings are replaced in memory by their interpreted
94,"equivalent, it allows you to perform concatenation."
94,Example:
94,# those are equivalents:
94,log-format %{+Q}o\ %t\ %s\ %{-Q}r
94,"log-format ""%{+Q}o %t %s %{-Q}r"""
94,log-format &#x27;%{+Q}o %t %s %{-Q}r'
94,"log-format ""%{+Q}o %t""&#x27; %s %{-Q}r'"
94,"log-format ""%{+Q}o %t""&#x27; %s'\ %{-Q}r"
94,# those are equivalents:
94,"reqrep ""^([^\ :]*)\ /static/(.*)"""
94,\1\ /\2
94,"reqrep ""^([^ :]*)\ /static/(.*)"""
94,&#x27;\1 /\2'
94,"reqrep ""^([^ :]*)\ /static/(.*)"""
94,"""\1 /\2"""
94,"reqrep ""^([^ :]*)\ /static/(.*)"""
94,"""\1\ /\2"""
94,2.3. Environment variables
94,HAProxy's configuration supports environment variables. Those variables are
94,interpreted only within double quotes. Variables are expanded during the
94,"configuration parsing. Variable names must be preceded by a dollar (""$"") and"
94,"optionally enclosed with braces (""{}"") similarly to what is done in Bourne"
94,shell. Variable names can contain alphanumerical characters or the character
94,"underscore (""_"") but should not start with a digit."
94,Example:
94,"bind ""fd@${FD_APP1}"""
94,"log ""${LOCAL_SYSLOG}:514"" local0 notice"
94,# send to local server
94,"user ""$HAPROXY_USER"""
94,2.4. Time format
94,"Some parameters involve values representing time, such as timeouts. These"
94,values are generally expressed in milliseconds (unless explicitly stated
94,otherwise) but may be expressed in any other unit by suffixing the unit to the
94,numeric value. It is important to consider this because it will not be repeated
94,for every keyword. Supported units are :
94,- us : microseconds. 1 microsecond = 1/1000000 second
94,- ms : milliseconds. 1 millisecond = 1/1000 second. This is the default.
94,- s
94,: seconds. 1s = 1000ms
94,- m
94,: minutes. 1m = 60s = 60000ms
94,- h
94,: hours.
94,1h = 60m = 3600s = 3600000ms
94,- d
94,: days.
94,1d = 24h = 1440m = 86400s = 86400000ms
94,2.5. Examples
94,# Simple configuration for an HTTP proxy listening on port 80 on all
94,"# interfaces and forwarding requests to a single backend ""servers"" with a"
94,"# single server ""server1"" listening on 127.0.0.1:8000"
94,global
94,daemon
94,maxconn 256
94,defaults
94,mode http
94,timeout connect 5000ms
94,timeout client 50000ms
94,timeout server 50000ms
94,frontend http-in
94,bind *:80
94,default_backend servers
94,backend servers
94,server server1 127.0.0.1:8000 maxconn 32
94,# The same configuration defined with a single listen block. Shorter but
94,"# less expressive, especially in HTTP mode."
94,global
94,daemon
94,maxconn 256
94,defaults
94,mode http
94,timeout connect 5000ms
94,timeout client 50000ms
94,timeout server 50000ms
94,listen http-in
94,bind *:80
94,server server1 127.0.0.1:8000 maxconn 32
94,"Assuming haproxy is in $PATH, test these configurations in a shell with:"
94,$ sudo haproxy -f configuration.conf -c
94,3. Global parameters
94,"Parameters in the ""global"" section are process-wide and often OS-specific. They"
94,are generally set once for all and do not need being changed once correct. Some
94,of them have command-line equivalents.
94,"The following keywords are supported in the ""global"" section :"
94,* Process management and security
94,- ca-base
94,- chroot
94,- crt-base
94,- cpu-map
94,- daemon
94,- description
94,- deviceatlas-json-file
94,- deviceatlas-log-level
94,- deviceatlas-separator
94,- deviceatlas-properties-cookie
94,- external-check
94,- gid
94,- group
94,- hard-stop-after
94,- log
94,- log-tag
94,- log-send-hostname
94,- lua-load
94,- nbproc
94,- nbthread
94,- node
94,- pidfile
94,- presetenv
94,- resetenv
94,- uid
94,- ulimit-n
94,- user
94,- setenv
94,- stats
94,- ssl-default-bind-ciphers
94,- ssl-default-bind-ciphersuites
94,- ssl-default-bind-options
94,- ssl-default-server-ciphers
94,- ssl-default-server-ciphersuites
94,- ssl-default-server-options
94,- ssl-dh-param-file
94,- ssl-server-verify
94,- unix-bind
94,- unsetenv
94,- 51degrees-data-file
94,- 51degrees-property-name-list
94,- 51degrees-property-separator
94,- 51degrees-cache-size
94,- wurfl-data-file
94,- wurfl-information-list
94,- wurfl-information-list-separator
94,- wurfl-engine-mode
94,- wurfl-cache-size
94,- wurfl-useragent-priority
94,* Performance tuning
94,- max-spread-checks
94,- maxconn
94,- maxconnrate
94,- maxcomprate
94,- maxcompcpuusage
94,- maxpipes
94,- maxsessrate
94,- maxsslconn
94,- maxsslrate
94,- maxzlibmem
94,- noepoll
94,- nokqueue
94,- nopoll
94,- nosplice
94,- nogetaddrinfo
94,- noreuseport
94,- spread-checks
94,- server-state-base
94,- server-state-file
94,- ssl-engine
94,- ssl-mode-async
94,- tune.buffers.limit
94,- tune.buffers.reserve
94,- tune.bufsize
94,- tune.chksize
94,- tune.comp.maxlevel
94,- tune.h2.header-table-size
94,- tune.h2.initial-window-size
94,- tune.h2.max-concurrent-streams
94,- tune.http.cookielen
94,- tune.http.logurilen
94,- tune.http.maxhdr
94,- tune.idletimer
94,- tune.lua.forced-yield
94,- tune.lua.maxmem
94,- tune.lua.session-timeout
94,- tune.lua.task-timeout
94,- tune.lua.service-timeout
94,- tune.maxaccept
94,- tune.maxpollevents
94,- tune.maxrewrite
94,- tune.pattern.cache-size
94,- tune.pipesize
94,- tune.rcvbuf.client
94,- tune.rcvbuf.server
94,- tune.recv_enough
94,- tune.sndbuf.client
94,- tune.sndbuf.server
94,- tune.ssl.cachesize
94,- tune.ssl.lifetime
94,- tune.ssl.force-private-cache
94,- tune.ssl.maxrecord
94,- tune.ssl.default-dh-param
94,- tune.ssl.ssl-ctx-cache-size
94,- tune.ssl.capture-cipherlist-size
94,- tune.vars.global-max-size
94,- tune.vars.proc-max-size
94,- tune.vars.reqres-max-size
94,- tune.vars.sess-max-size
94,- tune.vars.txn-max-size
94,- tune.zlib.memlevel
94,- tune.zlib.windowsize
94,* Debugging
94,- debug
94,- quiet
94,3.1. Process management and security
94,ca-base <dir>Assigns a default directory to fetch SSL CA certificates and CRLs from when a
94,"relative path is used with ""ca-fileThis keyword is available in sections :Bind optionsServer and default-server options"" or ""crl-fileThis keyword is available in sections :Bind optionsServer and default-server options"" directives. Absolute"
94,"locations specified in ""ca-fileThis keyword is available in sections :Bind optionsServer and default-server options"" and ""crl-fileThis keyword is available in sections :Bind optionsServer and default-server options"" prevail and ignore ""ca-base""."
94,chroot <jail dir>Changes current directory to <jail dir> and performs a chroot() there before
94,dropping privileges. This increases the security level in case an unknown
94,"vulnerability would be exploited, since it would make it very hard for the"
94,attacker to exploit the system. This only works when the process is started
94,with superuser privileges. It is important to ensure that <jail_dir> is both
94,empty and non-writable to anyone.
94,"cpu-map [auto:]<process-set>[/<thread-set>] <cpu-set>...On Linux 2.6 and above, it is possible to bind a process or a thread to a"
94,specific CPU set. This means that the process or the thread will never run on
94,"other CPUs. The ""cpu-map"" directive specifies CPU sets for process or thread"
94,"sets. The first argument is a process set, eventually followed by a thread"
94,set. These sets have the format
94,all | odd | even | number[-[number]]
94,"<number>> must be a number between 1 and 32 or 64, depending on the machine's"
94,word size. Any process IDs above nbproc and any thread IDs above nbthread are
94,ignored. It is possible to specify a range with two such number delimited by
94,a dash ('-'). It also is possible to specify all processes at once using
94,"""all"", only odd numbers using ""odd"" or even numbers using ""even"", just like"
94,"with the ""bind-process"" directive. The second and forthcoming arguments are"
94,CPU sets. Each CPU set is either a unique number between 0 and 31 or 63 or a
94,range with two such numbers delimited by a dash ('-'). Multiple CPU numbers
94,"or ranges may be specified, and the processes or threads will be allowed to"
94,"bind to all of them. Obviously, multiple ""cpu-map"" directives may be"
94,"specified. Each ""cpu-map"" directive will replace the previous ones when they"
94,overlap. A thread will be bound on the intersection of its mapping and the
94,"one of the process on which it is attached. If the intersection is null, no"
94,specific binding will be set for the thread.
94,Ranges can be partially defined. The higher bound can be omitted. In such
94,"case, it is replaced by the corresponding maximum value, 32 or 64 depending"
94,on the machine's word size.
94,"The prefix ""auto:"" can be added before the process set to let HAProxy"
94,automatically bind a process or a thread to a CPU by incrementing
94,"process/thread and CPU sets. To be valid, both sets must have the same"
94,"size. No matter the declaration order of the CPU sets, it will be bound from"
94,the lowest to the highest bound. Having a process and a thread range with the
94,"""auto:"" prefix is not supported. Only one range is supported, the other one"
94,must be a fixed number.
94,Examples:
94,cpu-map 1-4 0-3
94,# bind processes 1 to 4 on the first 4 CPUs
94,cpu-map 1/all 0-3 # bind all threads of the first process on the
94,# first 4 CPUs
94,cpu-map 1- 0-
94,"# will be replaced by ""cpu-map 1-64 0-63"""
94,"# or ""cpu-map 1-32 0-31"" depending on the machine's"
94,# word size.
94,"# all these lines bind the process 1 to the cpu 0, the process 2 to cpu 1"
94,# and so on.
94,cpu-map auto:1-4
94,0-3
94,cpu-map auto:1-4
94,0-1 2-3
94,cpu-map auto:1-4
94,3 2 1 0
94,"# all these lines bind the thread 1 to the cpu 0, the thread 2 to cpu 1"
94,# and so on.
94,cpu-map auto:1/1-4
94,0-3
94,cpu-map auto:1/1-4
94,0-1 2-3
94,cpu-map auto:1/1-4
94,3 2 1 0
94,# bind each process to exactly one CPU using all/odd/even keyword
94,cpu-map auto:all
94,0-63
94,cpu-map auto:even
94,0-31
94,cpu-map auto:odd
94,32-63
94,# invalid cpu-map because process and CPU sets have different sizes.
94,cpu-map auto:1-4
94,# invalid
94,cpu-map auto:1
94,0-3
94,# invalid
94,# invalid cpu-map because automatic binding is used with a process range
94,# and a thread range.
94,cpu-map auto:all/all
94,0 # invalid
94,cpu-map auto:all/1-4
94,0 # invalid
94,cpu-map auto:1-4/all
94,0 # invalid
94,crt-base <dir>Assigns a default directory to fetch SSL certificates from when a relative
94,"path is used with ""crtfile"" directives. Absolute locations specified after"
94,"""crtfile"" prevail and ignore ""crt-base""."
94,daemonMakes the process fork into background. This is the recommended mode of
94,"operation. It is equivalent to the command line ""-D"" argument. It can be"
94,"disabled by the command line ""-db"" argument. This option is ignored in"
94,systemd mode.
94,deviceatlas-json-file <path>Sets the path of the DeviceAtlas JSON data file to be loaded by the API.
94,The path must be a valid JSON data file and accessible by HAProxy process.
94,deviceatlas-log-level <value>Sets the level of information returned by the API. This directive is
94,optional and set to 0 by default if not set.
94,deviceatlas-separator <char>Sets the character separator for the API properties results. This directive
94,is optional and set to | by default if not set.
94,deviceatlas-properties-cookie <name>Sets the client cookie's name used for the detection if the DeviceAtlas
94,Client-side component was used during the request. This directive is optional
94,and set to DAPROPS by default if not set.
94,external-checkAllows the use of an external agent to perform health checks.
94,This is disabled by default as a security precaution.
94,"See ""option external-check""."
94,gid <number>Changes the process' group ID to <number>. It is recommended that the group
94,ID is dedicated to HAProxy or to a small set of similar daemons. HAProxy must
94,"be started with a user belonging to this group, or with superuser privileges."
94,"Note that if haproxy is started from a user having supplementary groups, it"
94,will only be able to drop these groups if started with superuser privileges.
94,"See also ""groupThis keyword is available in sections :Process management and securityUserlistsBind options"" and ""uidThis keyword is available in sections :Process management and securityBind options""."
94,hard-stop-after <time>Defines the maximum time allowed to perform a clean soft-stop.
94,Arguments :<time>
94,is the maximum time (by default in milliseconds) for which the
94,instance will remain alive when a soft-stop is received via the
94,SIGUSR1 signal.
94,This may be used to ensure that the instance will quit even if connections
94,remain opened during a soft-stop (for example with long timeouts for a proxy
94,in tcp mode). It applies both in TCP and HTTP mode.
94,Example:
94,global
94,hard-stop-after 30s
94,"group <group name>Similar to ""gidThis keyword is available in sections :Process management and securityBind options"" but uses the GID of group name <group name> from /etc/group."
94,"See also ""gidThis keyword is available in sections :Process management and securityBind options"" and ""userThis keyword is available in sections :Process management and securityUserlistsBind options""."
94,log <address> [len <length>] [format <format>] <facility> [max level [min level]]Adds a global syslog server. Several global servers can be defined. They
94,"will receive logs for starts and exits, as well as all logs from proxies"
94,"configured with ""log global""."
94,<address> can be one of:
94,- An IPv4 address optionally followed by a colon and a UDP port. If
94,"no port is specified, 514 is used by default (the standard syslog"
94,port).
94,- An IPv6 address followed by a colon and optionally a UDP port. If
94,"no port is specified, 514 is used by default (the standard syslog"
94,port).
94,"- A filesystem path to a UNIX domain socket, keeping in mind"
94,considerations for chroot (be sure the path is accessible inside
94,the chroot) and uid/gid (be sure the path is appropriately
94,writable).
94,You may want to reference some environment variables in the address
94,"parameter, see section 2.3 about environment variables."
94,<length> is an optional maximum line length. Log lines larger than this value
94,will be truncated before being sent. The reason is that syslog
94,servers act differently on log line length. All servers support the
94,"default value of 1024, but some servers simply drop larger lines"
94,"while others do log them. If a server supports long lines, it may"
94,make sense to set this value here in order to avoid truncating long
94,"lines. Similarly, if a server drops long lines, it is preferable to"
94,truncate them before sending them. Accepted values are 80 to 65535
94,inclusive. The default value of 1024 is generally fine for all
94,standard usages. Some specific cases of long captures or
94,JSON-formatted logs may require larger values. You may also need to
94,"increase ""tune.http.logurilen"" if your request URIs are truncated."
94,<format> is the log format used when generating syslog messages. It may be
94,one of the following :
94,rfc3164
94,The RFC3164 syslog message format. This is the default.
94,(https://tools.ietf.org/html/rfc3164)
94,rfc5424
94,The RFC5424 syslog message format.
94,(https://tools.ietf.org/html/rfc5424)
94,<facility> must be one of the 24 standard syslog facilities :
94,kern
94,user
94,mail
94,daemon auth
94,syslog lpr
94,news
94,uucp
94,cron
94,auth2
94,ftp
94,ntp
94,audit
94,alert
94,cron2
94,local0 local1 local2 local3 local4 local5 local6 local7
94,"An optional level can be specified to filter outgoing messages. By default,"
94,"all messages are sent. If a maximum level is specified, only messages with a"
94,severity at least as important as this level will be sent. An optional minimum
94,"level can be specified. If it is set, logs emitted with a more severe level"
94,than this one will be capped to this level. This is used to avoid sending
94,"""emerg"" messages on all terminals on some default syslog configurations."
94,Eight levels are known :
94,emerg
94,alert
94,crit
94,err
94,warning notice info
94,debug
94,"log-send-hostname [<string>]Sets the hostname field in the syslog header. If optional ""string"" parameter"
94,"is set the header is set to the string contents, otherwise uses the hostname"
94,of the system. Generally used if one is not relaying logs through an
94,intermediate syslog server or for simply customizing the hostname printed in
94,the logs.
94,log-tag <string>Sets the tag field in the syslog header to this string. It defaults to the
94,"program name as launched from the command line, which usually is ""haproxy""."
94,Sometimes it can be useful to differentiate between multiple processes
94,"running on the same host. See also the per-proxy ""log-tagThis keyword is available in sections :Process management and securityAlphabetically sorted keywords reference"" directive."
94,lua-load <file>This global directive loads and executes a Lua file. This directive can be
94,used multiple times.
94,"master-worker [no-exit-on-failure]Master-worker mode. It is equivalent to the command line ""-W"" argument."
94,"This mode will launch a ""master"" which will monitor the ""workers"". Using"
94,"this mode, you can reload HAProxy directly by sending a SIGUSR2 signal to"
94,the master. The master-worker mode is compatible either with the foreground
94,or daemon mode. It is recommended to use this mode with multiprocess and
94,systemd.
94,"By default, if a worker exits with a bad return code, in the case of a"
94,"segfault for example, all workers will be killed, and the master will leave."
94,It is convenient to combine this behavior with Restart=on-failure in a
94,systemd unit file in order to relaunch the whole process. If you don't want
94,"this behavior, you must use the keyword ""no-exit-on-failure""."
94,"See also ""-W"" in the management guide."
94,"nbproc <number>Creates <number> processes when going daemon. This requires the ""daemon"""
94,"mode. By default, only one process is created, which is the recommended mode"
94,of operation. For systems limited to small sets of file descriptors per
94,"process, it may be needed to fork multiple daemons. USING MULTIPLE PROCESSES"
94,"IS HARDER TO DEBUG AND IS REALLY DISCOURAGED. See also ""daemon"" and"
94,"""nbthread""."
94,nbthread <number>This setting is only available when support for threads was built in. It
94,creates <number> threads for each created processes. It means if HAProxy is
94,"started in foreground, it only creates <number> threads for the first"
94,"process. See also ""nbprocThis keyword is available in sections :Process management and securityFetching samples from internal states""."
94,pidfile <pidfile>Writes PIDs of all daemons into file <pidfile>. This option is equivalent to
94,"the ""-p"" command line argument. The file must be accessible to the user"
94,"starting the process. See also ""daemon""."
94,"presetenv <name> <value>Sets environment variable <name> to value <value>. If the variable exists, it"
94,is NOT overwritten. The changes immediately take effect so that the next line
94,"in the configuration file sees the new value. See also ""setenv"", ""resetenv"","
94,"and ""unsetenv""."
94,resetenv [<name> ...]Removes all environment variables except the ones specified in argument. It
94,allows to use a clean controlled environment before setting new values with
94,setenv or unsetenv. Please note that some internal functions may make use of
94,"some environment variables, such as time manipulation functions, but also"
94,OpenSSL or even external checks. This must be used with extreme care and only
94,after complete validation. The changes immediately take effect so that the
94,next line in the configuration file sees the new environment. See also
94,"""setenv"", ""presetenv"", and ""unsetenv""."
94,stats bind-process [ all | odd | even | <process_num>[-[process_num>]] ] ...Limits the stats socket to a certain set of processes numbers. By default the
94,"stats socket is bound to all processes, causing a warning to be emitted when"
94,nbproc is greater than 1 because there is no way to select the target process
94,"when connecting. However, by using this setting, it becomes possible to pin"
94,"the stats socket to a specific set of processes, typically the first one. The"
94,"warning will automatically be disabled when this setting is used, whatever"
94,the number of processes used. The maximum process ID depends on the machine's
94,word size (32 or 64). Ranges can be partially defined. The higher bound can
94,"be omitted. In such case, it is replaced by the corresponding maximum"
94,"value. A better option consists in using the ""process"" setting of the ""stats"
94,"socket"" line to force the process on each line."
94,server-state-base <directory>Specifies the directory prefix to be prepended in front of all servers state
94,"file names which do not start with a '/'. See also ""server-state-file"","
94,"""load-server-state-from-file"" and ""server-state-file-name""."
94,server-state-file <file>Specifies the path to the file containing state of servers. If the path starts
94,"with a slash ('/'), it is considered absolute, otherwise it is considered"
94,"relative to the directory specified using ""server-state-base"" (if set) or to"
94,"the current directory. Before reloading HAProxy, it is possible to save the"
94,"servers' current state using the stats command ""show servers state"". The"
94,output of this command must be written in the file pointed by <file>. When
94,"starting up, before handling traffic, HAProxy will read, load and apply state"
94,for each server found in the file and available in its current running
94,"configuration. See also ""server-state-base"" and ""show servers state"","
94,"""load-server-state-from-file"" and ""server-state-file-name"""
94,"setenv <name> <value>Sets environment variable <name> to value <value>. If the variable exists, it"
94,is overwritten. The changes immediately take effect so that the next line in
94,"the configuration file sees the new value. See also ""presetenv"", ""resetenv"","
94,"and ""unsetenv""."
94,ssl-default-bind-ciphers <ciphers>This setting is only available when support for OpenSSL was built in. It sets
94,"the default string describing the list of cipher algorithms (""cipher suite"")"
94,that are negotiated during the SSL/TLS handshake up to TLSv1.2 for all
94,"""bind"" lines which do not explicitly define theirs. The format of the string"
94,"is defined in ""man 1 ciphers"" from OpenSSL man pages. For background"
94,information and recommendations see e.g.
94,(https://wiki.mozilla.org/Security/Server_Side_TLS) and
94,(https://mozilla.github.io/server-side-tls/ssl-config-generator/). For TLSv1.3
94,"cipher configuration, please check the ""ssl-default-bind-ciphersuites"" keyword."
94,"Please check the ""bind"" keyword for more information."
94,ssl-default-bind-ciphersuites <ciphersuites>This setting is only available when support for OpenSSL was built in and
94,OpenSSL 1.1.1 or later was used to build HAProxy. It sets the default string
94,"describing the list of cipher algorithms (""cipher suite"") that are negotiated"
94,"during the TLSv1.3 handshake for all ""bind"" lines which do not explicitly define"
94,theirs. The format of the string is defined in
94,"""man 1 ciphers"" from OpenSSL man pages under the section ""ciphersuitesThis keyword is available in sections :Bind optionsServer and default-server options"". For"
94,"cipher configuration for TLSv1.2 and earlier, please check the"
94,"""ssl-default-bind-ciphers"" keyword. Please check the ""bind"" keyword for more"
94,information.
94,ssl-default-bind-options [<option>]...This setting is only available when support for OpenSSL was built in. It sets
94,"default ssl-options to force on all ""bind"" lines. Please check the ""bind"""
94,keyword to see available options.
94,Example:
94,global
94,ssl-default-bind-options ssl-min-ver TLSv1.0 no-tls-tickets
94,ssl-default-server-ciphers <ciphers>This setting is only available when support for OpenSSL was built in. It
94,sets the default string describing the list of cipher algorithms that are
94,"negotiated during the SSL/TLS handshake up to TLSv1.2 with the server,"
94,"for all ""server"" lines which do not explicitly define theirs. The format of"
94,"the string is defined in ""man 1 ciphers"" from OpenSSL man pages. For background"
94,information and recommendations see e.g.
94,(https://wiki.mozilla.org/Security/Server_Side_TLS) and
94,(https://mozilla.github.io/server-side-tls/ssl-config-generator/).
94,"For TLSv1.3 cipher configuration, please check the"
94,"""ssl-default-server-ciphersuites"" keyword. Please check the ""server"" keyword"
94,for more information.
94,ssl-default-server-ciphersuites <ciphersuites>This setting is only available when support for OpenSSL was built in and
94,OpenSSL 1.1.1 or later was used to build HAProxy. It sets the default
94,string describing the list of cipher algorithms that are negotiated during
94,"the TLSv1.3 handshake with the server, for all ""server"" lines which do not"
94,explicitly define theirs. The format of the string is defined in
94,"""man 1 ciphers"" from OpenSSL man pages under the section ""ciphersuitesThis keyword is available in sections :Bind optionsServer and default-server options"". For"
94,"cipher configuration for TLSv1.2 and earlier, please check the"
94,"""ssl-default-server-ciphers"" keyword. Please check the ""server"" keyword for"
94,more information.
94,ssl-default-server-options [<option>]...This setting is only available when support for OpenSSL was built in. It sets
94,"default ssl-options to force on all ""server"" lines. Please check the ""server"""
94,keyword to see available options.
94,ssl-dh-param-file <file>This setting is only available when support for OpenSSL was built in. It sets
94,the default DH parameters that are used during the SSL/TLS handshake when
94,"ephemeral Diffie-Hellman (DHE) key exchange is used, for all ""bind"" lines"
94,which do not explicitly define theirs. It will be overridden by custom DH
94,parameters found in a bind certificate file if any. If custom DH parameters
94,are not specified either by using ssl-dh-param-file or by setting them
94,"directly in the certificate file, pre-generated DH parameters of the size"
94,specified by tune.ssl.default-dh-param will be used. Custom parameters are
94,known to be more secure and therefore their use is recommended.
94,Custom DH parameters may be generated by using the OpenSSL command
94,"""openssl dhparam <size>"", where size should be at least 2048, as 1024-bit DH"
94,parameters should not be considered secure anymore.
94,"ssl-server-verify [none|required]The default behavior for SSL verify on servers side. If specified to 'none',"
94,servers certificates are not verified. The default is 'required' except if
94,forced using cmdline option '-dV'.
94,stats socket [<address:port>|<path>] [param*]Binds a UNIX socket to <path> or a TCPv4/v6 address to <address:port>.
94,Connections to this socket will return various statistics outputs and even
94,allow some commands to be issued to change some runtime settings. Please
94,"consult section 9.3 ""Unix Socket commands"" of Management Guide for more"
94,details.
94,"All parameters supported by ""bind"" lines are supported, for instance to"
94,restrict access to some users or their access rights. Please consult
94,section 5.1 for more information.
94,"stats timeout <timeout, in milliseconds>The default timeout on the stats socket is set to 10 seconds. It is possible"
94,"to change this value with ""stats timeout"". The value must be passed in"
94,"milliseconds, or be suffixed by a time unit among { us, ms, s, m, h, d }."
94,"stats maxconn <connections>By default, the stats socket is limited to 10 concurrent connections. It is"
94,"possible to change this value with ""stats maxconn""."
94,uid <number>Changes the process' user ID to <number>. It is recommended that the user ID
94,is dedicated to HAProxy or to a small set of similar daemons. HAProxy must
94,be started with superuser privileges in order to be able to switch to another
94,"one. See also ""gidThis keyword is available in sections :Process management and securityBind options"" and ""userThis keyword is available in sections :Process management and securityUserlistsBind options""."
94,ulimit-n <number>Sets the maximum number of per-process file-descriptors to <number>. By
94,"default, it is automatically computed, so it is recommended not to use this"
94,option.
94,unix-bind [ prefix <prefix> ] [ mode <mode> ] [ user <user> ] [ uid <uid> ]
94,"[ group <group> ] [ gid <gid> ]Fixes common settings to UNIX listening sockets declared in ""bind"" statements."
94,This is mainly used to simplify declaration of those UNIX sockets and reduce
94,"the risk of errors, since those settings are most commonly required but are"
94,also process-specific. The <prefix> setting can be used to force all socket
94,path to be relative to that directory. This might be needed to access another
94,component's chroot. Note that those paths are resolved before haproxy chroots
94,"itself, so they are absolute. The <mode>, <user>, <uid>, <group> and <gid>"
94,"all have the same meaning as their homonyms used by the ""bind"" statement. If"
94,"both are specified, the ""bind"" statement has priority, meaning that the"
94,"""unix-bind"" settings may be seen as process-wide default settings."
94,unsetenv [<name> ...]Removes environment variables specified in arguments. This can be useful to
94,hide some sensitive information that are occasionally inherited from the
94,user's environment during some operations. Variables which did not exist are
94,"silently ignored so that after the operation, it is certain that none of"
94,these variables remain. The changes immediately take effect so that the next
94,line in the configuration file will not see these variables. See also
94,"""setenv"", ""presetenv"", and ""resetenv""."
94,"user <user name>Similar to ""uidThis keyword is available in sections :Process management and securityBind options"" but uses the UID of user name <user name> from /etc/passwd."
94,"See also ""uidThis keyword is available in sections :Process management and securityBind options"" and ""groupThis keyword is available in sections :Process management and securityUserlistsBind options""."
94,"node <name>Only letters, digits, hyphen and underscore are allowed, like in DNS names."
94,This statement is useful in HA configurations where two or more processes or
94,servers share the same IP address. By setting a different node-name on all
94,"nodes, it becomes easy to immediately spot what server is handling the"
94,traffic.
94,description <text>Add a text that describes the instance.
94,Please note that it is required to escape certain characters (# for example)
94,and this text is inserted into a html page so you should avoid using
94,"""<"" and "">"" characters."
94,51degrees-data-file <file path>The path of the 51Degrees data file to provide device detection services. The
94,file should be unzipped and accessible by HAProxy with relevant permissions.
94,Please note that this option is only available when haproxy has been
94,compiled with USE_51DEGREES.
94,51degrees-property-name-list [<string> ...]A list of 51Degrees property names to be load from the dataset. A full list
94,of names is available on the 51Degrees website:
94,https://51degrees.com/resources/property-dictionary
94,Please note that this option is only available when haproxy has been
94,compiled with USE_51DEGREES.
94,51degrees-property-separator <char>A char that will be appended to every property value in a response header
94,"containing 51Degrees results. If not set that will be set as ','."
94,Please note that this option is only available when haproxy has been
94,compiled with USE_51DEGREES.
94,51degrees-cache-size <number>Sets the size of the 51Degrees converter cache to <number> entries. This
94,is an LRU cache which reminds previous device detections and their results.
94,"By default, this cache is disabled."
94,Please note that this option is only available when haproxy has been
94,compiled with USE_51DEGREES.
94,wurfl-data-file <file path>The path of the WURFL data file to provide device detection services. The
94,file should be accessible by HAProxy with relevant permissions.
94,Please note that this option is only available when haproxy has been compiled
94,with USE_WURFL=1.
94,"wurfl-information-list [<capability>]*A space-delimited list of WURFL capabilities, virtual capabilities, property"
94,names we plan to use in injected headers. A full list of capability and
94,virtual capability names is available on the Scientiamobile website :
94,https://www.scientiamobile.com/wurflCapability
94,Valid WURFL properties are:
94,- wurfl_id
94,Contains the device ID of the matched device.
94,- wurfl_root_id
94,Contains the device root ID of the matched
94,device.
94,- wurfl_isdevroot
94,Tells if the matched device is a root device.
94,"Possible values are ""TRUE"" or ""FALSE""."
94,- wurfl_useragent
94,The original useragent coming with this
94,particular web request.
94,- wurfl_api_version
94,Contains a string representing the currently
94,used Libwurfl API version.
94,- wurfl_engine_target
94,Contains a string representing the currently
94,set WURFL Engine Target. Possible values are
94,"""HIGH_ACCURACY"", ""HIGH_PERFORMANCE"", ""INVALID""."
94,- wurfl_info
94,A string containing information on the parsed
94,wurfl.xml and its full path.
94,- wurfl_last_load_time
94,Contains the UNIX timestamp of the last time
94,WURFL has been loaded successfully.
94,- wurfl_normalized_useragent
94,The normalized useragent.
94,- wurfl_useragent_priority
94,The user agent priority used by WURFL.
94,Please note that this option is only available when haproxy has been compiled
94,with USE_WURFL=1.
94,wurfl-information-list-separator <char>A char that will be used to separate values in a response header containing
94,"WURFL results. If not set that a comma (',') will be used by default."
94,Please note that this option is only available when haproxy has been compiled
94,with USE_WURFL=1.
94,wurfl-patch-file [<file path>]A list of WURFL patch file paths. Note that patches are loaded during startup
94,thus before the chroot.
94,Please note that this option is only available when haproxy has been compiled
94,with USE_WURFL=1.
94,wurfl-engine-mode { accuracy | performance }Sets the WURFL engine target. You can choose between 'accuracy' or
94,"'performance' targets. In performance mode, desktop web browser detection is"
94,"done programmatically without referencing the WURFL data. As a result, most"
94,desktop web browsers are returned as generic_web_browser WURFL ID for
94,"performance. If either performance or accuracy are not defined, performance"
94,mode is enabled by default.
94,Please note that this option is only available when haproxy has been compiled
94,with USE_WURFL=1.
94,"wurfl-cache-size <U>[,<D>]Sets the WURFL caching strategy. Here <U> is the Useragent cache size, and"
94,<D> is the internal device cache size. There are three possibilities here :
94,"- ""0"""
94,: no cache is used.
94,- <U>
94,": the Single LRU cache is used, the size is expressed in elements."
94,"- <U>,<D> : the Double LRU cache is used, both sizes are in elements. This is"
94,the highest performing option.
94,Please note that this option is only available when haproxy has been compiled
94,with USE_WURFL=1.
94,wurfl-useragent-priority { plain | sideloaded_browser }Tells WURFL if it should prioritize use of the plain user agent ('plain')
94,over the default sideloaded browser user agent ('sideloaded_browser').
94,Please note that this option is only available when haproxy has been compiled
94,with USE_WURFL=1.
94,3.2. Performance tuning
94,"max-spread-checks <delay in milliseconds>By default, haproxy tries to spread the start of health checks across the"
94,smallest health check interval of all the servers in a farm. The principle is
94,to avoid hammering services running on the same server. But when using large
94,"check intervals (10 seconds or more), the last servers in the farm take some"
94,"time before starting to be tested, which can be a problem. This parameter is"
94,"used to enforce an upper bound on delay between the first and the last check,"
94,even if the servers' check intervals are larger. When servers run with
94,"shorter intervals, their intervals will be respected though."
94,maxconn <number>Sets the maximum per-process number of concurrent connections to <number>. It
94,"is equivalent to the command-line argument ""-n"". Proxies will stop accepting"
94,"connections when this limit is reached. The ""ulimit-n"" parameter is"
94,"automatically adjusted according to this value. See also ""ulimit-n"". Note:"
94,"the ""select"" poller cannot reliably use more than 1024 file descriptors on"
94,"some platforms. If your platform only supports select and reports ""select"
94,"FAILED"" on startup, you need to reduce maxconn until it works (slightly"
94,"below 500 in general). If this value is not set, it will default to the value"
94,set in DEFAULT_MAXCONN at build time (reported in haproxy -vv) if no memory
94,"limit is enforced, or will be computed based on the memory limit, the buffer"
94,"size, memory allocated to compression, SSL cache size, and use or not of SSL"
94,and the associated maxsslconn (which can also be automatic).
94,maxconnrate <number>Sets the maximum per-process number of connections per second to <number>.
94,Proxies will stop accepting connections when this limit is reached. It can be
94,used to limit the global capacity regardless of each frontend capacity. It is
94,"important to note that this can only be used as a service protection measure,"
94,as there will not necessarily be a fair share between frontends when the
94,"limit is reached, so it's a good idea to also limit each frontend to some"
94,"value close to its expected share. Also, lowering tune.maxaccept can improve"
94,fairness.
94,maxcomprate <number>Sets the maximum per-process input compression rate to <number> kilobytes
94,"per second. For each session, if the maximum is reached, the compression"
94,level will be decreased during the session. If the maximum is reached at the
94,"beginning of a session, the session will not compress at all. If the maximum"
94,"is not reached, the compression level will be increased up to"
94,"tune.comp.maxlevel. A value of zero means there is no limit, this is the"
94,default value.
94,maxcompcpuusage <number>Sets the maximum CPU usage HAProxy can reach before stopping the compression
94,for new requests or decreasing the compression level of current requests.
94,It works like 'maxcomprate' but measures CPU usage instead of incoming data
94,bandwidth. The value is expressed in percent of the CPU used by haproxy. In
94,"case of multiple processes (nbproc > 1), each process manages its individual"
94,usage. A value of 100 disable the limit. The default value is 100. Setting
94,a lower value will prevent the compression work from slowing the whole
94,process down and from introducing high latencies.
94,"maxpipes <number>Sets the maximum per-process number of pipes to <number>. Currently, pipes"
94,are only used by kernel-based tcp splicing. Since a pipe contains two file
94,"descriptors, the ""ulimit-n"" value will be increased accordingly. The default"
94,"value is maxconn/4, which seems to be more than enough for most heavy usages."
94,"The splice code dynamically allocates and releases pipes, and can fall back"
94,"to standard copy, so setting this value too low may only impact performance."
94,maxsessrate <number>Sets the maximum per-process number of sessions per second to <number>.
94,Proxies will stop accepting connections when this limit is reached. It can be
94,used to limit the global capacity regardless of each frontend capacity. It is
94,"important to note that this can only be used as a service protection measure,"
94,as there will not necessarily be a fair share between frontends when the
94,"limit is reached, so it's a good idea to also limit each frontend to some"
94,"value close to its expected share. Also, lowering tune.maxaccept can improve"
94,fairness.
94,maxsslconn <number>Sets the maximum per-process number of concurrent SSL connections to
94,"<number>. By default there is no SSL-specific limit, which means that the"
94,global maxconn setting will apply to all connections. Setting this limit
94,avoids having openssl use too much memory and crash when malloc returns NULL
94,(since it unfortunately does not reliably check for such conditions). Note
94,"that the limit applies both to incoming and outgoing connections, so one"
94,connection which is deciphered then ciphered accounts for 2 SSL connections.
94,"If this value is not set, but a memory limit is enforced, this value will be"
94,"automatically computed based on the memory limit, maxconn,"
94,"the buffer size,"
94,"memory allocated to compression, SSL cache size, and use of SSL in either"
94,"frontends, backends or both. If neither maxconn nor maxsslconn are specified"
94,"when there is a memory limit, haproxy will automatically adjust these values"
94,"so that 100% of the connections can be made over SSL with no risk, and will"
94,"consider the sides where it is enabled (frontend, backend, both)."
94,maxsslrate <number>Sets the maximum per-process number of SSL sessions per second to <number>.
94,SSL listeners will stop accepting connections when this limit is reached. It
94,can be used to limit the global SSL CPU usage regardless of each frontend
94,capacity. It is important to note that this can only be used as a service
94,"protection measure, as there will not necessarily be a fair share between"
94,"frontends when the limit is reached, so it's a good idea to also limit each"
94,frontend to some value close to its expected share. It is also important to
94,note that the sessions are accounted before they enter the SSL stack and not
94,"after, which also protects the stack against bad handshakes. Also, lowering"
94,tune.maxaccept can improve fairness.
94,maxzlibmem <number>Sets the maximum amount of RAM in megabytes per process usable by the zlib.
94,"When the maximum amount is reached, future sessions will not compress as long"
94,"as RAM is unavailable. When sets to 0, there is no limit."
94,The default value is 0. The value is available in bytes on the UNIX socket
94,"with ""show info"" on the line ""MaxZlibMemUsage"", the memory used by zlib is"
94,"""ZlibMemUsage"" in bytes."
94,"noepollDisables the use of the ""epoll"" event polling system on Linux. It is"
94,"equivalent to the command-line argument ""-de"". The next polling system"
94,"used will generally be ""poll"". See also ""nopoll""."
94,"nokqueueDisables the use of the ""kqueue"" event polling system on BSD. It is"
94,"equivalent to the command-line argument ""-dk"". The next polling system"
94,"used will generally be ""poll"". See also ""nopoll""."
94,"nopollDisables the use of the ""poll"" event polling system. It is equivalent to the"
94,"command-line argument ""-dp"". The next polling system used will be ""select""."
94,"It should never be needed to disable ""poll"" since it's available on all"
94,"platforms supported by HAProxy. See also ""nokqueue"" and ""noepoll""."
94,nospliceDisables the use of kernel tcp splicing between sockets on Linux. It is
94,"equivalent to the command line argument ""-dS"". Data will then be copied"
94,using conventional and more portable recv/send calls. Kernel tcp splicing is
94,limited to some very recent instances of kernel 2.6. Most versions between
94,"2.6.25 and 2.6.28 are buggy and will forward corrupted data, so they must not"
94,be used. This option makes it easier to globally disable kernel splicing in
94,"case of doubt. See also ""option splice-auto"", ""option splice-request"" and"
94,"""option splice-response""."
94,nogetaddrinfoDisables the use of getaddrinfo(3) for name resolving. It is equivalent to
94,"the command line argument ""-dG"". Deprecated gethostbyname(3) will be used."
94,noreuseportDisables the use of SO_REUSEPORT - see socket(7). It is equivalent to the
94,"command line argument ""-dR""."
94,"spread-checks <0..50, in percent>Sometimes it is desirable to avoid sending agent and health checks to"
94,"servers at exact intervals, for instance when many logical servers are"
94,"located on the same physical server. With the help of this parameter, it"
94,becomes possible to add some randomness in the check interval between 0
94,and +/- 50%. A value between 2 and 5 seems to show good results. The
94,default value remains at 0.
94,ssl-engine <name> [algo <comma-separated list of algorithms>]Sets the OpenSSL engine to <name>. List of valid values for <name> may be
94,"obtained using the command ""openssl engine"". This statement may be used"
94,"multiple times, it will simply enable multiple crypto engines. Referencing an"
94,unsupported engine will prevent haproxy from starting. Note that many engines
94,will lead to lower HTTPS performance than pure software with recent
94,"processors. The optional command ""algo"" sets the default algorithms an ENGINE"
94,will supply using the OPENSSL function ENGINE_set_default_string(). A value
94,"of ""ALL"" uses the engine for all cryptographic operations. If no list of"
94,"algo is specified then the value of ""ALL"" is used. A comma-separated list"
94,"of different algorithms may be specified, including: RSA, DSA, DH, EC, RAND,"
94,"CIPHERS, DIGESTS, PKEY, PKEY_CRYPTO, PKEY_ASN1. This is the same format that"
94,openssl configuration file uses:
94,https://www.openssl.org/docs/man1.0.2/apps/config.html
94,ssl-mode-asyncAdds SSL_MODE_ASYNC mode to the SSL context. This enables asynchronous TLS
94,I/O operations if asynchronous capable SSL engines are used. The current
94,implementation supports a maximum of 32 engines. The Openssl ASYNC API
94,doesn't support moving read/write buffers and is not compliant with
94,haproxy's buffer management. So the asynchronous mode is disabled on
94,read/write
94,operations (it is only enabled during initial and reneg
94,handshakes).
94,tune.buffers.limit <number>Sets a hard limit on the number of buffers which may be allocated per process.
94,The default value is zero which means unlimited. The minimum non-zero value
94,"will always be greater than ""tune.buffers.reserve"" and should ideally always"
94,be about twice as large. Forcing this value can be particularly useful to
94,"limit the amount of memory a process may take, while retaining a sane"
94,"behavior. When this limit is reached, sessions which need a buffer wait for"
94,another one to be released by another session. Since buffers are dynamically
94,"allocated and released, the waiting time is very short and not perceptible"
94,provided that limits remain reasonable. In fact sometimes reducing the limit
94,may even increase performance by increasing the CPU cache's efficiency. Tests
94,have shown good results on average HTTP traffic with a limit to 1/10 of the
94,"expected global maxconn setting, which also significantly reduces memory"
94,usage. The memory savings come from the fact that a number of connections
94,will not allocate 2*tune.bufsize. It is best not to touch this value unless
94,advised to do so by an haproxy core developer.
94,tune.buffers.reserve <number>Sets the number of buffers which are pre-allocated and reserved for use only
94,during memory shortage conditions resulting in failed memory allocations. The
94,minimum value is 2 and is also the default. There is no reason a user would
94,"want to change this value, it's mostly aimed at haproxy core developers."
94,tune.bufsize <number>Sets the buffer size to this size (in bytes). Lower values allow more
94,"sessions to coexist in the same amount of RAM, and higher values allow some"
94,applications with very large cookies to work. The default value is 16384 and
94,can be changed at build time. It is strongly recommended not to change this
94,"from the default value, as very low values will break some services such as"
94,"statistics, and values larger than default size will increase memory usage,"
94,possibly causing the system to run out of memory. At least the global maxconn
94,parameter should be decreased by the same factor as this one is increased. In
94,"addition, use of HTTP/2 mandates that this value must be 16384 or more. If an"
94,"HTTP request is larger than (tune.bufsize - tune.maxrewrite), haproxy will"
94,return HTTP 400 (Bad Request) error. Similarly if an HTTP response is larger
94,"than this size, haproxy will return HTTP 502 (Bad Gateway)."
94,tune.chksize <number>Sets the check buffer size to this size (in bytes). Higher values may help
94,"find string or regex patterns in very large pages, though doing so may imply"
94,more memory and CPU usage. The default value is 16384 and can be changed at
94,"build time. It is not recommended to change this value, but to use better"
94,checks whenever possible.
94,tune.comp.maxlevel <number>Sets the maximum compression level. The compression level affects CPU
94,usage during compression. This value affects CPU usage during compression.
94,Each session using compression initializes the compression algorithm with
94,this value. The default value is 1.
94,tune.h2.header-table-size <number>Sets the HTTP/2 dynamic header table size. It defaults to 4096 bytes and
94,cannot be larger than 65536 bytes. A larger value may help certain clients
94,"send more compact requests, depending on their capabilities. This amount of"
94,memory is consumed for each HTTP/2 connection. It is recommended not to
94,change it.
94,"tune.h2.initial-window-size <number>Sets the HTTP/2 initial window size, which is the number of bytes the client"
94,can upload before waiting for an acknowledgment from haproxy. This setting
94,"only affects payload contents (i.e. the body of POST requests), not headers."
94,"The default value is 65535, which roughly allows up to 5 Mbps of upload"
94,"bandwidth per client over a network showing a 100 ms ping time, or 500 Mbps"
94,over a 1-ms local network. It can make sense to increase this value to allow
94,"faster uploads, or to reduce it to increase fairness when dealing with many"
94,clients. It doesn't affect resource usage.
94,tune.h2.max-concurrent-streams <number>Sets the HTTP/2 maximum number of concurrent streams per connection (ie the
94,number of outstanding requests on a single connection). The default value is
94,100. A larger one may slightly improve page load time for complex sites when
94,"visited over high latency networks, but increases the amount of resources a"
94,single client may allocate. A value of zero disables the limit so a single
94,client may create as many streams as allocatable by haproxy. It is highly
94,recommended not to change this value.
94,tune.http.cookielen <number>Sets the maximum length of captured cookies. This is the maximum value that
94,"the ""capture cookie xxx len yyy"" will be allowed to take, and any upper value"
94,will automatically be truncated to this one. It is important not to set too
94,high a value because all cookie captures still allocate this size whatever
94,their configured value (they share a same pool). This value is per request
94,"per response, so the memory allocated is twice this value per connection."
94,"When not specified, the limit is set to 63 characters. It is recommended not"
94,to change this value.
94,tune.http.logurilen <number>Sets the maximum length of request URI in logs. This prevents truncating long
94,request URIs with valuable query strings in log lines. This is not related
94,"to syslog limits. If you increase this limit, you may also increase the"
94,'log ... len yyy' parameter. Your syslog daemon may also need specific
94,configuration directives too.
94,The default value is 1024.
94,tune.http.maxhdr <number>Sets the maximum number of headers in a request. When a request comes with a
94,"number of headers greater than this value (including the first line), it is"
94,"rejected with a ""400 Bad Request"" status code. Similarly, too large responses"
94,"are blocked with ""502 Bad Gateway"". The default value is 101, which is enough"
94,"for all usages, considering that the widely deployed Apache server uses the"
94,same limit. It can be useful to push this limit further to temporarily allow
94,a buggy application to work by the time it gets fixed. The accepted range is
94,1..32767. Keep in mind that each new header consumes 32bits of memory for
94,"each session, so don't push this limit too high."
94,tune.idletimer <timeout>Sets the duration after which haproxy will consider that an empty buffer is
94,probably associated with an idle stream. This is used to optimally adjust
94,some packet sizes while forwarding large and small data alternatively. The
94,decision to use splice() or to send large buffers in SSL is modulated by this
94,parameter. The value is in milliseconds between 0 and 65535. A value of zero
94,"means that haproxy will not try to detect idle streams. The default is 1000,"
94,which seems to correctly detect end user pauses (e.g. read a page before
94,clicking). There should be not reason for changing this value. Please check
94,tune.ssl.maxrecord below.
94,tune.lua.forced-yield <number>This directive forces the Lua engine to execute a yield each <number> of
94,instructions executed. This permits interrupting a long script and allows the
94,HAProxy scheduler to process other tasks like accepting connections or
94,forwarding traffic. The default value is 10000 instructions. If HAProxy often
94,"executes some Lua code but more responsiveness is required, this value can be"
94,lowered. If the Lua code is quite long and its result is absolutely required
94,"to process the data, the <number> can be increased."
94,tune.lua.maxmemSets the maximum amount of RAM in megabytes per process usable by Lua. By
94,default it is zero which means unlimited. It is important to set a limit to
94,ensure that a bug in a script will not result in the system running out of
94,memory.
94,tune.lua.session-timeout <timeout>This is the execution timeout for the Lua sessions. This is useful for
94,preventing infinite loops or spending too much time in Lua. This timeout
94,"counts only the pure Lua runtime. If the Lua does a sleep, the sleep is"
94,not taken in account. The default timeout is 4s.
94,"tune.lua.task-timeout <timeout>Purpose is the same as ""tune.lua.session-timeout"", but this timeout is"
94,"dedicated to the tasks. By default, this timeout isn't set because a task may"
94,"remain alive during of the lifetime of HAProxy. For example, a task used to"
94,check servers.
94,tune.lua.service-timeout <timeout>This is the execution timeout for the Lua services. This is useful for
94,preventing infinite loops or spending too much time in Lua. This timeout
94,"counts only the pure Lua runtime. If the Lua does a sleep, the sleep is"
94,not taken in account. The default timeout is 4s.
94,tune.maxaccept <number>Sets the maximum number of consecutive connections a process may accept in a
94,"row before switching to other work. In single process mode, higher numbers"
94,give better performance at high connection rates. However in multi-process
94,"modes, keeping a bit of fairness between processes generally is better to"
94,"increase performance. This value applies individually to each listener, so"
94,that the number of processes a listener is bound to is taken into account.
94,"This value defaults to 64. In multi-process mode, it is divided by twice"
94,the number of processes the listener is bound to. Setting this value to -1
94,completely disables the limitation. It should normally not be needed to tweak
94,this value.
94,tune.maxpollevents <number>Sets the maximum amount of events that can be processed at once in a call to
94,the polling system. The default value is adapted to the operating system. It
94,has been noticed that reducing it below 200 tends to slightly decrease
94,"latency at the expense of network bandwidth, and increasing it above 200"
94,tends to trade latency for slightly increased bandwidth.
94,tune.maxrewrite <number>Sets the reserved buffer space to this size in bytes. The reserved space is
94,used for header rewriting or appending. The first reads on sockets will never
94,fill more than bufsize-maxrewrite. Historically it has defaulted to half of
94,"bufsize, though that does not make much sense since there are rarely large"
94,numbers of headers to add. Setting it too high prevents processing of large
94,requests or responses. Setting it too low prevents addition of new headers
94,to already large requests or to POST requests. It is generally wise to set it
94,to about 1024. It is automatically readjusted to half of bufsize if it is
94,larger than that. This means you don't have to worry about it when changing
94,bufsize.
94,tune.pattern.cache-size <number>Sets the size of the pattern lookup cache to <number> entries. This is an LRU
94,cache which reminds previous lookups and their results. It is used by ACLs
94,"and maps on slow pattern lookups, namely the ones using the ""sub"", ""reg"","
94,"""dir"", ""dom"", ""end"", ""bin"" match methods as well as the case-insensitive"
94,strings. It applies to pattern expressions which means that it will be able
94,to memorize the result of a lookup among all the patterns specified on a
94,configuration line (including all those loaded from files). It automatically
94,invalidates entries which are updated using HTTP actions or on the CLI. The
94,"default cache size is set to 10000 entries, which limits its footprint to"
94,about 5 MB per process/thread on 32-bit systems and 8 MB per process/thread
94,"on 64-bit systems, as caches are thread/process local. There is a very low"
94,"risk of collision in this cache, which is in the order of the size of the"
94,"cache divided by 2^64. Typically, at 10000 requests per second with the"
94,"default cache size of 10000 entries, there's 1% chance that a brute force"
94,"attack could cause a single collision after 60 years, or 0.1% after 6 years."
94,This is considered much lower than the risk of a memory corruption caused by
94,"aging components. If this is not acceptable, the cache can be disabled by"
94,setting this parameter to 0.
94,"tune.pipesize <number>Sets the kernel pipe buffer size to this size (in bytes). By default, pipes"
94,"are the default size for the system. But sometimes when using TCP splicing,"
94,"it can improve performance to increase pipe sizes, especially if it is"
94,suspected that pipes are not filled and that many calls to splice() are
94,"performed. This has an impact on the kernel's memory footprint, so this must"
94,not be changed if impacts are not understood.
94,tune.rcvbuf.client <number>tune.rcvbuf.server <number>Forces the kernel socket receive buffer size on the client or the server side
94,to the specified value in bytes. This value applies to all TCP/HTTP frontends
94,"and backends. It should normally never be set, and the default size (0) lets"
94,the kernel autotune this value depending on the amount of available memory.
94,However it can sometimes help to set it to very low values (e.g. 4096) in
94,order to save kernel memory by preventing it from buffering too large amounts
94,of received data. Lower values will significantly increase CPU usage though.
94,tune.recv_enough <number>HAProxy uses some hints to detect that a short read indicates the end of the
94,socket buffers. One of them is that a read returns more than <recv_enough>
94,"bytes, which defaults to 10136 (7 segments of 1448 each). This default value"
94,may be changed by this setting to better deal with workloads involving lots
94,of short messages such as telnet or SSH sessions.
94,tune.sndbuf.client <number>tune.sndbuf.server <number>Forces the kernel socket send buffer size on the client or the server side to
94,the specified value in bytes. This value applies to all TCP/HTTP frontends
94,"and backends. It should normally never be set, and the default size (0) lets"
94,the kernel autotune this value depending on the amount of available memory.
94,However it can sometimes help to set it to very low values (e.g. 4096) in
94,order to save kernel memory by preventing it from buffering too large amounts
94,of received data. Lower values will significantly increase CPU usage though.
94,Another use case is to prevent write timeouts with extremely slow clients due
94,to the kernel waiting for a large part of the buffer to be read before
94,notifying haproxy again.
94,"tune.ssl.cachesize <number>Sets the size of the global SSL session cache, in a number of blocks. A block"
94,is large enough to contain an encoded session without peer certificate.
94,An encoded session with peer certificate is stored in multiple blocks
94,depending on the size of the peer certificate. A block uses approximately
94,"200 bytes of memory. The default value may be forced at build time, otherwise"
94,"defaults to 20000. When the cache is full, the most idle entries are purged"
94,"and reassigned. Higher values reduce the occurrence of such a purge, hence"
94,the number of CPU-intensive SSL handshakes by ensuring that all users keep
94,their session as long as possible. All entries are pre-allocated upon startup
94,"and are shared between all processes if ""nbprocThis keyword is available in sections :Process management and securityFetching samples from internal states"" is greater than 1. Setting"
94,this value to 0 disables the SSL session cache.
94,tune.ssl.force-private-cacheThis option disables SSL session cache sharing between all processes. It
94,should normally not be used since it will force many renegotiations due to
94,clients hitting a random process. But it may be required on some operating
94,systems where none of the SSL cache synchronization method may be used. In
94,"this case, adding a first layer of hash-based load balancing before the SSL"
94,layer might limit the impact of the lack of session sharing.
94,tune.ssl.lifetime <timeout>Sets how long a cached SSL session may remain valid. This time is expressed
94,in seconds and defaults to 300 (5 min). It is important to understand that it
94,"does not guarantee that sessions will last that long, because if the cache is"
94,"full, the longest idle sessions will be purged despite their configured"
94,lifetime. The real usefulness of this setting is to prevent sessions from
94,being used for too long.
94,tune.ssl.maxrecord <number>Sets the maximum amount of bytes passed to SSL_write() at a time. Default
94,"value 0 means there is no limit. Over SSL/TLS, the client can decipher the"
94,"data only once it has received a full record. With large records, it means"
94,that clients might have to download up to 16kB of data before starting to
94,process them. Limiting the value can improve page load times on browsers
94,located over high latency or low bandwidth networks. It is suggested to find
94,optimal values which fit into 1 or 2 TCP segments (generally 1448 bytes over
94,"Ethernet with TCP timestamps enabled, or 1460 when timestamps are disabled),"
94,keeping in mind that SSL/TLS add some overhead. Typical values of 1419 and
94,"2859 gave good results during tests. Use ""strace -e trace=write"" to find the"
94,best value. HAProxy will automatically switch to this setting after an idle
94,stream has been detected (see tune.idletimer above).
94,tune.ssl.default-dh-param <number>Sets the maximum size of the Diffie-Hellman parameters used for generating
94,the ephemeral/temporary Diffie-Hellman key in case of DHE key exchange. The
94,"final size will try to match the size of the server's RSA (or DSA) key (e.g,"
94,"a 2048 bits temporary DH key for a 2048 bits RSA key), but will not exceed"
94,this maximum value. Default value if 1024. Only 1024 or higher values are
94,"allowed. Higher values will increase the CPU load, and values greater than"
94,1024 bits are not supported by Java 7 and earlier clients. This value is not
94,used if static Diffie-Hellman parameters are supplied either directly
94,in the certificate file or by using the ssl-dh-param-file parameter.
94,tune.ssl.ssl-ctx-cache-size <number>Sets the size of the cache used to store generated certificates to <number>
94,entries. This is a LRU cache. Because generating a SSL certificate
94,"dynamically is expensive, they are cached. The default cache size is set to"
94,1000 entries.
94,tune.ssl.capture-cipherlist-size <number>Sets the maximum size of the buffer used for capturing client-hello cipher
94,"list. If the value is 0 (default value) the capture is disabled, otherwise"
94,a buffer is allocated for each SSL/TLS connection.
94,tune.vars.global-max-size <size>tune.vars.proc-max-size <size>tune.vars.reqres-max-size <size>tune.vars.sess-max-size <size>tune.vars.txn-max-size <size>These five tunes help to manage the maximum amount of memory used by the
94,"variables system. ""global"" limits the overall amount of memory available for"
94,"all scopes. ""proc"" limits the memory for the process scope, ""sess"" limits the"
94,"memory for the session scope, ""txn"" for the transaction scope, and ""reqres"""
94,limits the memory for each request or response processing.
94,"Memory accounting is hierarchical, meaning more coarse grained limits include"
94,"the finer grained ones: ""proc"" includes ""sess"", ""sess"" includes ""txn"", and"
94,"""txn"" includes ""reqres""."
94,"For example, when ""tune.vars.sess-max-size"" is limited to 100,"
94,"""tune.vars.txn-max-size"" and ""tune.vars.reqres-max-size"" cannot exceed"
94,"100 either. If we create a variable ""txn.var"" that contains 100 bytes,"
94,all available space is consumed.
94,Notice that exceeding the limits at runtime will not result in an error
94,"message, but values might be cut off or corrupted. So make sure to accurately"
94,plan for the amount of space needed to store all your variables.
94,tune.zlib.memlevel <number>Sets the memLevel parameter in zlib initialization for each session. It
94,defines how much memory should be allocated for the internal compression
94,state. A value of 1 uses minimum memory but is slow and reduces compression
94,"ratio, a value of 9 uses maximum memory for optimal speed. Can be a value"
94,between 1 and 9. The default value is 8.
94,tune.zlib.windowsize <number>Sets the window size (the size of the history buffer) as a parameter of the
94,zlib initialization for each session. Larger values of this parameter result
94,in better compression at the expense of memory usage. Can be a value between
94,8 and 15. The default value is 15.
94,3.3. Debugging
94,"debugEnables debug mode which dumps to stdout all exchanges, and disables forking"
94,"into background. It is the equivalent of the command-line argument ""-d"". It"
94,should never be used in a production configuration since it may prevent full
94,system startup.
94,quietDo not display any message during startup. It is equivalent to the command-
94,"line argument ""-q""."
94,3.4. Userlists
94,It is possible to control access to frontend/backend/listen sections or to
94,"http stats by allowing only authenticated and authorized users. To do this,"
94,it is required to create at least one userlist and to define users.
94,userlist <listname>Creates new userlist with name <listname>. Many independent userlists can be
94,used to store authentication & authorization data for independent customers.
94,"group <groupname> [users <user>,<user>,(...)]Adds group <groupname> to the current userlist. It is also possible to"
94,attach users to this group by using a comma separated list of names
94,"proceeded by ""users"" keyword."
94,user <username> [password|insecure-password <password>]
94,"[groups <group>,<group>,(...)]Adds user <username> to the current userlist. Both secure (encrypted) and"
94,insecure (unencrypted) passwords can be used. Encrypted passwords are
94,"evaluated using the crypt(3) function, so depending on the system's"
94,"capabilities, different algorithms are supported. For example, modern Glibc"
94,"based Linux systems support MD5, SHA-256, SHA-512, and, of course, the"
94,classic DES-based method of encrypting passwords.
94,Attention: Be aware that using encrypted passwords might cause significantly
94,"increased CPU usage, depending on the number of requests, and the algorithm"
94,"used. For any of the hashed variants, the password for each request must"
94,"be processed through the chosen algorithm, before it can be compared to the"
94,value specified in the config file. Most current algorithms are deliberately
94,designed to be expensive to compute to achieve resistance against brute
94,"force attacks. They do not simply salt/hash the clear text password once,"
94,but thousands of times. This can quickly become a major factor in haproxy's
94,overall CPU consumption!
94,Example:
94,userlist L1
94,"group G1 users tiger,scott"
94,"group G2 users xdb,scott"
94,user tiger password $6$k6y3o.eP$JlKBx9za9667qe4(...)xHSwRv6J.C0/D7cV91
94,user scott insecure-password elgato
94,user xdb insecure-password hello
94,userlist L2
94,group G1
94,group G2
94,user tiger password $6$k6y3o.eP$JlKBx(...)xHSwRv6J.C0/D7cV91 groups G1
94,"user scott insecure-password elgato groups G1,G2"
94,user xdb insecure-password hello groups G2
94,Please note that both lists are functionally identical.
94,3.5. Peers
94,It is possible to propagate entries of any data-types in stick-tables between
94,several haproxy instances over TCP connections in a multi-master fashion. Each
94,instance pushes its local updates and insertions to remote peers. The pushed
94,values overwrite remote ones without aggregation. Interrupted exchanges are
94,automatically detected and recovered from the last known point.
94,"In addition, during a soft restart, the old process connects to the new one"
94,using such a TCP connection to push all its entries before the new process
94,tries to connect to other peers. That ensures very fast replication during a
94,"reload, it typically takes a fraction of a second even for large tables."
94,"Note that Server IDs are used to identify servers remotely, so it is important"
94,that configurations look similar or at least that the same IDs are forced on
94,each server on all participants.
94,"peers <peersect>Creates a new peer list with name <peersect>. It is an independent section,"
94,which is referenced by one or more stick-tables.
94,disabledDisables a peers section. It disables both listening and any synchronization
94,related to this section. This is provided to disable synchronization of stick
94,"tables without having to comment out all ""peers"" references."
94,enableThis re-enables a disabled peers section which was previously disabled.
94,peer <peername> <ip>:<port>Defines a peer inside a peers section.
94,"If <peername> is set to the local peer name (by default hostname, or forced"
94,"using ""-L"" command line option), haproxy will listen for incoming remote peer"
94,"connection on <ip>:<port>. Otherwise, <ip>:<port> defines where to connect to"
94,"to join the remote peer, and <peername> is used at the protocol level to"
94,identify and validate the remote peer on the server side.
94,"During a soft restart, local peer <ip>:<port> is used by the old instance to"
94,connect the new one and initiate a complete replication (teaching process).
94,It is strongly recommended to have the exact same peers declaration on all
94,"peers and to only rely on the ""-L"" command line argument to change the local"
94,peer name. This makes it easier to maintain coherent configuration files
94,across all peers.
94,You may want to reference some environment variables in the address
94,"parameter, see section 2.3 about environment variables."
94,Example:
94,peers mypeers
94,peer haproxy1 192.168.0.1:1024
94,peer haproxy2 192.168.0.2:1024
94,peer haproxy3 10.2.0.1:1024
94,backend mybackend
94,mode tcp
94,balance roundrobin
94,stick-table type ip size 20k peers mypeers
94,stick on src
94,server srv1 192.168.0.30:80
94,server srv2 192.168.0.31:80
94,3.6. Mailers
94,It is possible to send email alerts when the state of servers changes.
94,If configured email alerts are sent to each mailer that is configured
94,in a mailers section. Email is sent to mailers using SMTP.
94,mailers <mailersect>Creates a new mailer list with the name <mailersect>. It is an
94,independent section which is referenced by one or more proxies.
94,mailer <mailername> <ip>:<port>Defines a mailer inside a mailers section.
94,Example:
94,mailers mymailers
94,mailer smtp1 192.168.0.1:587
94,mailer smtp2 192.168.0.2:587
94,backend mybackend
94,mode tcp
94,balance roundrobin
94,email-alert mailers mymailers
94,email-alert from test1@horms.org
94,email-alert to test2@horms.org
94,server srv1 192.168.0.30:80
94,server srv2 192.168.0.31:80
94,timeout mail <time>Defines the time available for a mail/connection to be made and send to
94,the mail-server. If not defined the default value is 10 seconds. To allow
94,for at least two SYN-ACK packets to be send during initial TCP handshake it
94,is advised to keep this value above 4 seconds.
94,Example:
94,mailers mymailers
94,timeout mail 20s
94,mailer smtp1 192.168.0.1:587
94,4. Proxies
94,Proxy configuration can be located in a set of sections :
94,- defaults [<name>]
94,- frontend <name>
94,- backend
94,<name>
94,- listen
94,<name>
94,"A ""defaults"" section sets default parameters for all other sections following"
94,"its declaration. Those default parameters are reset by the next ""defaults"""
94,"section. See below for the list of parameters which can be set in a ""defaults"""
94,section. The name is optional but its use is encouraged for better readability.
94,"A ""frontend"" section describes a set of listening sockets accepting client"
94,connections.
94,"A ""backend"" section describes a set of servers to which the proxy will connect"
94,to forward incoming connections.
94,"A ""listen"" section defines a complete proxy with its frontend and backend"
94,parts combined in one section. It is generally useful for TCP-only traffic.
94,"All proxy names must be formed from upper and lower case letters, digits,"
94,"'-' (dash), '_' (underscore) , '.' (dot) and ':' (colon). ACL names are"
94,"case-sensitive, which means that ""www"" and ""WWW"" are two different proxies."
94,"Historically, all proxy names could overlap, it just caused troubles in the"
94,"logs. Since the introduction of content switching, it is mandatory that two"
94,proxies with overlapping capabilities (frontend/backend) have different names.
94,"However, it is still permitted that a frontend and a backend share the same"
94,"name, as this configuration seems to be commonly encountered."
94,"Right now, two major proxy modes are supported : ""tcp"", also known as layer 4,"
94,"and ""http"", also known as layer 7. In layer 4 mode, HAProxy simply forwards"
94,"bidirectional traffic between two sides. In layer 7 mode, HAProxy analyzes the"
94,"protocol, and can interact with it by allowing, blocking, switching, adding,"
94,"modifying, or removing arbitrary contents in requests or responses, based on"
94,arbitrary criteria.
94,"In HTTP mode, the processing applied to requests and responses flowing over"
94,a connection depends in the combination of the frontend's HTTP options and
94,the backend's. HAProxy supports 5 connection modes :
94,"- KAL : keep alive (""option http-keep-alive"") which is the default mode : all"
94,"requests and responses are processed, and connections remain open but idle"
94,between responses and new requests.
94,"- TUN: tunnel (""option http-tunnel"") : this was the default mode for versions"
94,"1.0 to 1.5-dev21 : only the first request and response are processed, and"
94,everything else is forwarded with no analysis at all. This mode should not
94,be used as it creates lots of trouble with logging and HTTP processing.
94,"- PCL: passive close (""option httpclose"") : exactly the same as tunnel mode,"
94,"but with ""Connection: close"" appended in both directions to try to make"
94,both ends close after the first request/response exchange.
94,"- SCL: server close (""option http-server-close"") : the server-facing"
94,"connection is closed after the end of the response is received, but the"
94,client-facing connection remains open.
94,"- FCL: forced close (""option forceclose"") : the connection is actively closed"
94,after the end of the response.
94,The effective mode that will be applied to a connection passing through a
94,frontend and a backend can be determined by both proxy modes according to the
94,"following matrix, but in short, the modes are symmetric, keep-alive is the"
94,weakest option and force close is the strongest.
94,Backend mode
94,| KAL | TUN | PCL | SCL | FCL
94,----+-----+-----+-----+-----+----
94,KAL | KAL | TUN | PCL | SCL | FCL
94,----+-----+-----+-----+-----+----
94,TUN | TUN | TUN | PCL | SCL | FCL
94,Frontend
94,----+-----+-----+-----+-----+----
94,mode
94,PCL | PCL | PCL | PCL | FCL | FCL
94,----+-----+-----+-----+-----+----
94,SCL | SCL | SCL | FCL | SCL | FCL
94,----+-----+-----+-----+-----+----
94,FCL | FCL | FCL | FCL | FCL | FCL
94,4.1. Proxy keywords matrix
94,The following list of keywords is supported. Most of them may only be used in a
94,"limited set of section types. Some of them are marked as ""deprecated"" because"
94,they are inherited from an old syntax which may be confusing or functionally
94,"limited, and there are new recommended keywords to replace them. Keywords"
94,"marked with ""(*)"" can be optionally inverted using the ""no"" prefix, e.g. ""no"
94,"option contstats"". This makes sense when the option has been enabled by default"
94,and must be disabled for a specific instance. Such options may also be prefixed
94,"with ""default"" in order to restore default settings regardless of what has been"
94,"specified in a previous ""defaults"" section."
94,keyworddefaultsfrontendlistenbackend
94,acl
94,appsession
94,backlog
94,balance
94,bind
94,bind-process
94,(deprecated)block
94,capture cookie
94,capture request header
94,capture response header
94,(deprecated)clitimeout
94,compression
94,(deprecated)contimeout
94,cookie
94,declare capture
94,default-server
94,default_backend
94,description
94,disabled
94,dispatch
94,keyworddefaultsfrontendlistenbackend
94,email-alert from
94,email-alert level
94,email-alert mailers
94,email-alert myhostname
94,email-alert to
94,enabled
94,errorfile
94,errorloc
94,errorloc302
94,errorloc303
94,force-persist
94,filter
94,fullconn
94,grace
94,hash-type
94,http-check disable-on-404
94,http-check expect
94,http-check send-state
94,http-request
94,http-response
94,keyworddefaultsfrontendlistenbackend
94,http-reuse
94,http-send-name-header
94,ignore-persist
94,load-server-state-from-file
94,(*)log
94,log-format
94,log-format-sd
94,log-tag
94,max-keep-alive-queue
94,maxconn
94,mode
94,monitor fail
94,monitor-net
94,monitor-uri
94,(*)option abortonclose
94,(*)option accept-invalid-http-request
94,(*)option accept-invalid-http-response
94,(*)option allbackups
94,(*)option checkcache
94,keyworddefaultsfrontendlistenbackend
94,(*)option clitcpka
94,(*)option contstats
94,(*)option dontlog-normal
94,(*)option dontlognull
94,(*)option forceclose
94,option forwardfor
94,(*)option http-buffer-request
94,(*)option http-ignore-probes
94,(*)option http-keep-alive
94,(*)option http-no-delay
94,(*)option http-pretend-keepalive
94,(*)option http-server-close
94,(*)option http-tunnel
94,(*)option http-use-proxy-header
94,option httpchk
94,(*)option httpclose
94,option httplog
94,(*)option http_proxy
94,(*)option independent-streams
94,option ldap-check
94,keyworddefaultsfrontendlistenbackend
94,option external-check
94,(*)option log-health-checks
94,(*)option log-separate-errors
94,(*)option logasap
94,option mysql-check
94,(*)option nolinger
94,option originalto
94,(*)option persist
94,option pgsql-check
94,(*)option prefer-last-server
94,(*)option redispatch
94,option redis-check
94,option smtpchk
94,(*)option socket-stats
94,(*)option splice-auto
94,(*)option splice-request
94,(*)option splice-response
94,option spop-check
94,(*)option srvtcpka
94,option ssl-hello-chk
94,keyworddefaultsfrontendlistenbackend
94,option tcp-check
94,(*)option tcp-smart-accept
94,(*)option tcp-smart-connect
94,option tcpka
94,option tcplog
94,(*)option transparent
94,external-check command
94,external-check path
94,persist rdp-cookie
94,rate-limit sessions
94,redirect
94,(deprecated)redisp
94,(deprecated)redispatch
94,reqadd
94,reqallow
94,reqdel
94,reqdeny
94,reqiallow
94,reqidel
94,reqideny
94,keyworddefaultsfrontendlistenbackend
94,reqipass
94,reqirep
94,reqitarpit
94,reqpass
94,reqrep
94,reqtarpit
94,retries
94,rspadd
94,rspdel
94,rspdeny
94,rspidel
94,rspideny
94,rspirep
94,rsprep
94,server
94,server-state-file-name
94,server-template
94,source
94,(deprecated)srvtimeout
94,stats admin
94,keyworddefaultsfrontendlistenbackend
94,stats auth
94,stats enable
94,stats hide-version
94,stats http-request
94,stats realm
94,stats refresh
94,stats scope
94,stats show-desc
94,stats show-legends
94,stats show-node
94,stats uri
94,stick match
94,stick on
94,stick store-request
94,stick store-response
94,stick-table
94,tcp-check connect
94,tcp-check expect
94,tcp-check send
94,tcp-check send-binary
94,keyworddefaultsfrontendlistenbackend
94,tcp-request connection
94,tcp-request content
94,tcp-request inspect-delay
94,tcp-request session
94,tcp-response content
94,tcp-response inspect-delay
94,timeout check
94,timeout client
94,timeout client-fin
94,(deprecated)timeout clitimeout
94,timeout connect
94,(deprecated)timeout contimeout
94,timeout http-keep-alive
94,timeout http-request
94,timeout queue
94,timeout server
94,timeout server-fin
94,(deprecated)timeout srvtimeout
94,timeout tarpit
94,timeout tunnel
94,keyworddefaultsfrontendlistenbackend
94,(deprecated)transparent
94,unique-id-format
94,unique-id-header
94,use_backend
94,use-server
94,4.2. Alphabetically sorted keywords reference
94,This section provides a description of each keyword and its usage.
94,acl <aclname> <criterion> [flags] [operator] <value> ...Declare or complete an access list.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,Example:
94,acl invalid_src
94,src
94,0.0.0.0/7 224.0.0.0/3
94,acl invalid_src
94,src_port
94,0:1023
94,acl local_dst
94,hdr(host) -i localhost
94,See section 7 about ACL usage.
94,appsession <cookie> len <length> timeout <holdtime>
94,[request-learn] [prefix] [mode <path-parameters|query-string>]Define session stickiness on an existing application cookie.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,Arguments :<cookie>
94,this is the name of the cookie used by the application and which
94,HAProxy will have to learn for each new session.
94,<length>
94,this is the max number of characters that will be memorized and
94,checked in each cookie value.
94,<holdtime> this is the time after which the cookie will be removed from
94,"memory if unused. If no unit is specified, this time is in"
94,milliseconds.
94,request-learn
94,"If this option is specified, then haproxy will be able to learn"
94,the cookie found in the request in case the server does not
94,specify any in response. This is typically what happens with
94,"PHPSESSID cookies, or when haproxy's session expires before"
94,the application's session and the correct server is selected.
94,It is recommended to specify this option to improve reliability.
94,prefix
94,"When this option is specified, haproxy will match on the cookie"
94,prefix (or URL parameter prefix). The appsession value is the
94,data following this prefix.
94,Example :
94,appsession ASPSESSIONID len 64 timeout 3h prefix
94,"This will match the cookie ASPSESSIONIDXXX=XXXX,"
94,the appsession value will be XXX=XXXX.
94,mode
94,This option allows to change the URL parser mode.
94,2 modes are currently supported :
94,- path-parameters :
94,The parser looks for the appsession in the path parameters
94,"part (each parameter is separated by a semi-colon), which is"
94,convenient for JSESSIONID for example.
94,This is the default mode if the option is not set.
94,- query-string :
94,"In this mode, the parser will look for the appsession in the"
94,query string.
94,"As of version 1.6, appsessions was removed. It is more flexible and more"
94,"convenient to use stick-tables instead, and stick-tables support multi-master"
94,"replication and data conservation across reloads, which appsessions did not."
94,"See also : ""cookieThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server optionsFetching HTTP samples (Layer 7)"", ""capture cookie"", ""balance"", ""stickThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"", ""stick-table"", ""ignore-persist"", ""nbprocThis keyword is available in sections :Process management and securityFetching samples from internal states"" and ""bind-process""."
94,backlog <conns>Give hints to the system about the approximate listen backlog desired size
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments :<conns>
94,is the number of pending connections. Depending on the operating
94,"system, it may represent the number of already acknowledged"
94,"connections, of non-acknowledged ones, or both."
94,"In order to protect against SYN flood attacks, one solution is to increase"
94,"the system's SYN backlog size. Depending on the system, sometimes it is just"
94,"tunable via a system parameter, sometimes it is not adjustable at all, and"
94,sometimes the system relies on hints given by the application at the time of
94,"the listen() syscall. By default, HAProxy passes the frontend's maxconn value"
94,"to the listen() syscall. On systems which can make use of this value, it can"
94,"sometimes be useful to be able to specify a different value, hence this"
94,backlog parameter.
94,"On Linux 2.4, the parameter is ignored by the system. On Linux 2.6, it is"
94,used as a hint and the system accepts up to the smallest greater power of
94,"two, and never more than some limits (usually 32768)."
94,"See also : ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"" and the target operating system's tuning guide."
94,balance <algorithm> [ <arguments> ]balance url_param <param> [check_post]Define the load balancing algorithm to be used in a backend.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments :<algorithm> is the algorithm used to select a server when doing load
94,balancing. This only applies when no persistence information
94,"is available, or when a connection is redispatched to another"
94,server. <algorithm> may be one of the following :
94,roundrobin
94,"Each server is used in turns, according to their weights."
94,This is the smoothest and fairest algorithm when the server's
94,processing time remains equally distributed. This algorithm
94,"is dynamic, which means that server weights may be adjusted"
94,on the fly for slow starts for instance. It is limited by
94,design to 4095 active servers per backend. Note that in some
94,"large farms, when a server becomes up after having been down"
94,"for a very short time, it may sometimes take a few hundreds"
94,requests for it to be re-integrated into the farm and start
94,"receiving traffic. This is normal, though very rare. It is"
94,indicated here in case you would have the chance to observe
94,"it, so that you don't worry."
94,static-rr
94,"Each server is used in turns, according to their weights."
94,This algorithm is as similar to roundrobin except that it is
94,"static, which means that changing a server's weight on the"
94,"fly will have no effect. On the other hand, it has no design"
94,"limitation on the number of servers, and when a server goes"
94,"up, it is always immediately reintroduced into the farm, once"
94,the full map is recomputed. It also uses slightly less CPU to
94,run (around -1%).
94,leastconn
94,The server with the lowest number of connections receives the
94,connection. Round-robin is performed within groups of servers
94,of the same load to ensure that all servers will be used. Use
94,of this algorithm is recommended where very long sessions are
94,"expected, such as LDAP, SQL, TSE, etc... but is not very well"
94,suited for protocols using short sessions such as HTTP. This
94,"algorithm is dynamic, which means that server weights may be"
94,adjusted on the fly for slow starts for instance.
94,first
94,The first server with available connection slots receives the
94,connection. The servers are chosen from the lowest numeric
94,"identifier to the highest (see server parameter ""idThis keyword is available in sections :Alphabetically sorted keywords referenceBind optionsServer and default-server options""), which"
94,defaults to the server's position in the farm. Once a server
94,"reaches its maxconn value, the next server is used. It does"
94,not make sense to use this algorithm without setting maxconn.
94,The purpose of this algorithm is to always use the smallest
94,number of servers so that extra servers can be powered off
94,during non-intensive hours. This algorithm ignores the server
94,"weight, and brings more benefit to long session such as RDP"
94,"or IMAP than HTTP, though it can be useful there too. In"
94,"order to use this algorithm efficiently, it is recommended"
94,that a cloud controller regularly checks server usage to turn
94,"them off when unused, and regularly checks backend queue to"
94,"turn new servers on when the queue inflates. Alternatively,"
94,"using ""http-check send-state"" may inform servers on the load."
94,source
94,The source IP address is hashed and divided by the total
94,weight of the running servers to designate which server will
94,receive the request. This ensures that the same client IP
94,address will always reach the same server as long as no
94,server goes down or up. If the hash result changes due to the
94,"number of running servers changing, many clients will be"
94,directed to a different server. This algorithm is generally
94,used in TCP mode where no cookie may be inserted. It may also
94,be used on the Internet to provide a best-effort stickiness
94,to clients which refuse session cookies. This algorithm is
94,"static by default, which means that changing a server's"
94,"weight on the fly will have no effect, but this can be"
94,"changed using ""hash-type""."
94,uri
94,This algorithm hashes either the left part of the URI (before
94,"the question mark) or the whole URI (if the ""whole"" parameter"
94,is present) and divides the hash value by the total weight of
94,the running servers. The result designates which server will
94,receive the request. This ensures that the same URI will
94,always be directed to the same server as long as no server
94,goes up or down. This is used with proxy caches and
94,anti-virus proxies in order to maximize the cache hit rate.
94,Note that this algorithm may only be used in an HTTP backend.
94,"This algorithm is static by default, which means that"
94,"changing a server's weight on the fly will have no effect,"
94,"but this can be changed using ""hash-type""."
94,"This algorithm supports two optional parameters ""len"" and"
94,"""depth"", both followed by a positive integer number. These"
94,options may be helpful when it is needed to balance servers
94,"based on the beginning of the URI only. The ""len"" parameter"
94,indicates that the algorithm should only consider that many
94,characters at the beginning of the URI to compute the hash.
94,"Note that having ""len"" set to 1 rarely makes sense since most"
94,"URIs start with a leading ""/""."
94,"The ""depth"" parameter indicates the maximum directory depth"
94,to be used to compute the hash. One level is counted for each
94,"slash in the request. If both parameters are specified, the"
94,evaluation stops when either is reached.
94,url_param
94,The URL parameter specified in argument will be looked up in
94,the query string of each HTTP GET request.
94,"If the modifier ""check_post"" is used, then an HTTP POST"
94,"request entity will be searched for the parameter argument,"
94,when it is not found in a query string after a question mark
94,('?') in the URL. The message body will only start to be
94,analyzed once either the advertised amount of data has been
94,received or the request buffer is full. In the unlikely event
94,"that chunked encoding is used, only the first chunk is"
94,"scanned. Parameter values separated by a chunk boundary, may"
94,be randomly balanced if at all. This keyword used to support
94,an optional <max_wait> parameter which is now ignored.
94,If the parameter is found followed by an equal sign ('=') and
94,"a value, then the value is hashed and divided by the total"
94,weight of the running servers. The result designates which
94,server will receive the request.
94,This is used to track user identifiers in requests and ensure
94,that a same user ID will always be sent to the same server as
94,long as no server goes up or down. If no value is found or if
94,"the parameter is not found, then a round robin algorithm is"
94,applied. Note that this algorithm may only be used in an HTTP
94,"backend. This algorithm is static by default, which means"
94,that changing a server's weight on the fly will have no
94,"effect, but this can be changed using ""hash-type""."
94,hdr(<name>) The HTTP header <name> will be looked up in each HTTP
94,"request. Just as with the equivalent ACL 'hdr()' function,"
94,the header name in parenthesis is not case sensitive. If the
94,"header is absent or if it does not contain any value, the"
94,roundrobin algorithm is applied instead.
94,"An optional 'use_domain_only' parameter is available, for"
94,reducing the hash algorithm to the main domain part with some
94,"specific headers such as 'Host'. For instance, in the Host"
94,"value ""haproxy.1wt.eu"", only ""1wt"" will be considered."
94,"This algorithm is static by default, which means that"
94,"changing a server's weight on the fly will have no effect,"
94,"but this can be changed using ""hash-type""."
94,rdp-cookie
94,rdp-cookie(<name>)
94,"The RDP cookie <name> (or ""mstshash"" if omitted) will be"
94,looked up and hashed for each incoming TCP request. Just as
94,"with the equivalent ACL 'req_rdp_cookie()' function, the name"
94,is not case-sensitive. This mechanism is useful as a degraded
94,"persistence mode, as it makes it possible to always send the"
94,same user (or the same session ID) to the same server. If the
94,"cookie is not found, the normal roundrobin algorithm is"
94,used instead.
94,"Note that for this to work, the frontend must ensure that an"
94,RDP cookie is already present in the request buffer. For this
94,you must use 'tcp-request content accept' rule combined with
94,a 'req_rdp_cookie_cnt' ACL.
94,"This algorithm is static by default, which means that"
94,"changing a server's weight on the fly will have no effect,"
94,"but this can be changed using ""hash-type""."
94,See also the rdp_cookie pattern fetch function.
94,<arguments> is an optional list of arguments which may be needed by some
94,"algorithms. Right now, only ""url_param"" and ""uri"" support an"
94,optional argument.
94,The load balancing algorithm of a backend is set to roundrobin when no other
94,"algorithm, mode nor option have been set. The algorithm may only be set once"
94,for each backend.
94,"With authentication schemes that require the same connection like NTLM, URI"
94,"based alghoritms must not be used, as they would cause subsequent requests"
94,"to be routed to different backend servers, breaking the invalid assumptions"
94,NTLM relies on.
94,Examples :
94,balance roundrobin
94,balance url_param userid
94,balance url_param session_id check_post 64
94,balance hdr(User-Agent)
94,balance hdr(host)
94,balance hdr(Host) use_domain_only
94,"Note: the following caveats and limitations on using the ""check_post"""
94,"extension with ""url_param"" must be considered :"
94,"- all POST requests are eligible for consideration, because there is no way"
94,to determine if the parameters will be found in the body or entity which
94,may contain binary data. Therefore another method may be required to
94,restrict consideration of POST requests that have no URL parameters in
94,the body. (see acl reqideny http_end)
94,- using a <max_wait> value larger than the request buffer size does not
94,"make sense and is useless. The buffer size is set at build time, and"
94,defaults to 16 kB.
94,"- Content-Encoding is not supported, the parameter search will probably"
94,fail; and load balancing will fall back to Round Robin.
94,"- Expect: 100-continue is not supported, load balancing will fall back to"
94,Round Robin.
94,- Transfer-Encoding (RFC7230 3.3.1) is only supported in the first chunk.
94,"If the entire parameter value is not present in the first chunk, the"
94,"selection of server is undefined (actually, defined by how little"
94,actually appeared in the first chunk).
94,"- This feature does not support generation of a 100, 411 or 501 response."
94,"- In some cases, requesting ""check_post"" MAY attempt to scan the entire"
94,contents of a message body. Scanning normally terminates when linear
94,"white space or control characters are found, indicating the end of what"
94,might be a URL parameter list. This is probably not a concern with SGML
94,type message bodies.
94,"See also : ""dispatch"", ""cookieThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server optionsFetching HTTP samples (Layer 7)"", ""transparent"", ""hash-type"" and ""http_proxy""."
94,"bind [<address>]:<port_range> [, ...] [param*]bind /<path> [, ...] [param*]Define one or several listening addresses and/or ports in a frontend."
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesno
94,Arguments :<address>
94,"is optional and can be a host name, an IPv4 address, an IPv6"
94,"address, or '*'. It designates the address the frontend will"
94,"listen on. If unset, all IPv4 addresses of the system will be"
94,listened on. The same will apply for '*' or the system's
94,"special address ""0.0.0.0"". The IPv6 equivalent is '::'."
94,"Optionally, an address family prefix may be used before the"
94,"address to force the family regardless of the address format,"
94,which can be useful to specify a path to a unix socket with
94,no slash ('/'). Currently supported prefixes are :
94,- 'ipv4@'
94,-> address is always IPv4
94,- 'ipv6@'
94,-> address is always IPv6
94,- 'unix@'
94,-> address is a path to a local unix socket
94,- 'abns@'
94,-> address is in abstract namespace (Linux only).
94,"Note: since abstract sockets are not ""rebindable"", they"
94,do not cope well with multi-process mode during
94,"soft-restart, so it is better to avoid them if"
94,nbproc is greater than 1. The effect is that if the
94,"new process fails to start, only one of the old ones"
94,will be able to rebind to the socket.
94,- 'fd@<n>' -> use file descriptor <n> inherited from the
94,parent. The fd must be bound and may or may not already
94,be listening.
94,You may want to reference some environment variables in the
94,"address parameter, see section 2.3 about environment"
94,variables.
94,<port_range>
94,"is either a unique TCP port, or a port range for which the"
94,proxy will accept connections for the IP address specified
94,above. The port is mandatory for TCP listeners. Note that in
94,"the case of an IPv6 address, the port is always the number"
94,after the last colon (':'). A range can either be :
94,- a numerical port (ex: '80')
94,- a dash-delimited ports range explicitly stating the lower
94,and upper bounds (ex: '2000-2100') which are included in
94,the range.
94,"Particular care must be taken against port ranges, because"
94,every <address:port> couple consumes one socket (= a file
94,"descriptor), so it's easy to consume lots of descriptors"
94,"with a simple range, and to run out of sockets. Also, each"
94,<address:port> couple must be used only once among all
94,instances running on a same system. Please note that binding
94,to ports lower than 1024 generally require particular
94,"privileges to start the program, which are independent of"
94,the 'uid' parameter.
94,<path>
94,is a UNIX socket path beginning with a slash ('/'). This is
94,alternative to the TCP listening port. HAProxy will then
94,receive UNIX connections on the socket located at this place.
94,The path must begin with a slash and by default is absolute.
94,"It can be relative to the prefix defined by ""unix-bind"" in"
94,the global section. Note that the total length of the prefix
94,followed by the socket path cannot exceed some system limits
94,"for UNIX sockets, which commonly are set to 107 characters."
94,<param*>
94,is a list of parameters common to all sockets declared on the
94,same line. These numerous parameters depend on OS and build
94,options and have a complete section dedicated to them. Please
94,refer to section 5 to for more details.
94,It is possible to specify a list of address:port combinations delimited by
94,commas. The frontend will then listen on all of these addresses. There is no
94,fixed limit to the number of addresses and ports which can be listened on in
94,"a frontend, as well as there is no limit to the number of ""bind"" statements"
94,in a frontend.
94,Example :
94,listen http_proxy
94,"bind :80,:443"
94,"bind 10.0.0.1:10080,10.0.0.1:10443"
94,bind /var/run/ssl-frontend.sock user root mode 600 accept-proxy
94,listen http_https_proxy
94,bind :80
94,bind :443 ssl crt /etc/haproxy/site.pem
94,listen http_https_proxy_explicit
94,bind ipv6@:80
94,bind ipv4@public_ssl:443 ssl crt /etc/haproxy/site.pem
94,bind unix@ssl-frontend.sock user root mode 600 accept-proxy
94,listen external_bind_app1
94,"bind ""fd@${FD_APP1}"""
94,"Note: regarding Linux's abstract namespace sockets, HAProxy uses the whole"
94,sun_path length is used for the address length. Some other programs
94,such as socat use the string length only by default. Pass the option
94,""",unix-tightsocklen=0"" to any abstract socket definition in socat to"
94,make it compatible with HAProxy's.
94,"See also : ""sourceThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"", ""option forwardfor"", ""unix-bind"" and the PROXY protocol documentation, and section 5 about bind options."
94,bind-process [ all | odd | even | <process_num>[-[<process_num>]] ] ...Limit visibility of an instance to a certain set of processes numbers.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :all
94,All process will see this instance. This is the default. It
94,may be used to override a default value.
94,odd
94,"This instance will be enabled on processes 1,3,5,...63. This"
94,option may be combined with other numbers.
94,even
94,"This instance will be enabled on processes 2,4,6,...64. This"
94,option may be combined with other numbers. Do not use it
94,with less than 2 processes otherwise some instances might be
94,missing from all processes.
94,process_num
94,"The instance will be enabled on this process number or range,"
94,whose values must all be between 1 and 32 or 64 depending on
94,the machine's word size. Ranges can be partially defined. The
94,"higher bound can be omitted. In such case, it is replaced by"
94,the corresponding maximum value. If a proxy is bound to
94,"process numbers greater than the configured global.nbproc, it"
94,will either be forced to process #1 if a single process was
94,"specified, or to all processes otherwise."
94,This keyword limits binding of certain instances to certain processes. This
94,is useful in order not to have too many processes listening to the same
94,"ports. For instance, on a dual-core machine, it might make sense to set"
94,"'nbproc 2' in the global section, then distributes the listeners among 'odd'"
94,and 'even' instances.
94,"At the moment, it is not possible to reference more than 32 or 64 processes"
94,"using this keyword, but this should be more than enough for most setups."
94,Please note that 'all' really means all processes regardless of the machine's
94,"word size, and is not limited to the first 32 or 64."
94,"Each ""bind"" line may further be limited to a subset of the proxy's processes,"
94,"please consult the ""process"" bind keyword in section 5.1."
94,"When a frontend has no explicit ""bind-process"" line, it tries to bind to all"
94,"the processes referenced by its ""bind"" lines. That means that frontends can"
94,easily adapt to their listeners' processes.
94,"If some backends are referenced by frontends bound to other processes, the"
94,backend automatically inherits the frontend's processes.
94,Example :
94,listen app_ip1
94,bind 10.0.0.1:80
94,bind-process odd
94,listen app_ip2
94,bind 10.0.0.2:80
94,bind-process even
94,listen management
94,bind 10.0.0.3:80
94,bind-process 1 2 3 4
94,listen management
94,bind 10.0.0.4:80
94,bind-process 1-4
94,"See also : ""nbprocThis keyword is available in sections :Process management and securityFetching samples from internal states"" in global section, and ""process"" in section 5.1."
94,block { if | unless } <condition> (deprecated)Block a layer 7 request if/unless a condition is matched
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,The HTTP request will be blocked very early in the layer 7 processing
94,if/unless <condition> is matched. A 403 error will be returned if the request
94,is blocked. The condition has to reference ACLs (see section 7). This is
94,typically used to deny access to certain sensitive resources if some
94,conditions are met or not met. There is no fixed limit to the number of
94,"""block"" statements per instance. To block connections at layer 4 (without"
94,"sending a 403 error) see ""tcp-request connection reject"" and"
94,"""tcp-request content reject"" rules."
94,"This form is deprecated, do not use it in any new configuration, use the new"
94,"""http-request deny"" instead."
94,Example:
94,acl invalid_src
94,src
94,0.0.0.0/7 224.0.0.0/3
94,acl invalid_src
94,src_port
94,0:1023
94,acl local_dst
94,hdr(host) -i localhost
94,# block is deprecated. Use http-request deny instead:
94,#block if invalid_src || local_dst
94,http-request deny if invalid_src || local_dst
94,"See also : section 7 about ACL usage, ""http-request deny"", ""http-response deny"", ""tcp-request connection reject"" and ""tcp-request content reject""."
94,capture cookie <name> len <length>Capture and log a cookie in the request and in the response.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesno
94,Arguments :<name>
94,is the beginning of the name of the cookie to capture. In order
94,"to match the exact name, simply suffix the name with an equal"
94,"sign ('='). The full name will appear in the logs, which is"
94,useful with application servers which adjust both the cookie name
94,and value (e.g. ASPSESSIONXXX).
94,<length>
94,"is the maximum number of characters to report in the logs, which"
94,"include the cookie name, the equal sign and the value, all in the"
94,"standard ""name=value"" form. The string will be truncated on the"
94,right if it exceeds <length>.
94,"Only the first cookie is captured. Both the ""cookieThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server optionsFetching HTTP samples (Layer 7)"" request headers and the"
94,"""set-cookie"" response headers are monitored. This is particularly useful to"
94,check for application bugs causing session crossing or stealing between
94,"users, because generally the user's cookies can only change on a login page."
94,"When the cookie was not presented by the client, the associated log column"
94,"will report ""-"". When a request does not cause a cookie to be assigned by the"
94,"server, a ""-"" is reported in the response column."
94,The capture is performed in the frontend only because it is necessary that
94,the log format does not change for a given frontend depending on the
94,backends. This may change in the future. Note that there can be only one
94,"""capture cookie"" statement in a frontend. The maximum capture length is set"
94,"by the global ""tune.http.cookielen"" setting and defaults to 63 characters. It"
94,"is not possible to specify a capture in a ""defaults"" section."
94,Example:
94,capture cookie ASPSESSION len 32
94,"See also : ""capture request header"", ""capture response header"" as well as section 8 about logging."
94,capture request header <name> len <length>Capture and log the last occurrence of the specified request header.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesno
94,Arguments :<name>
94,is the name of the header to capture. The header names are not
94,"case-sensitive, but it is a common practice to write them as they"
94,"appear in the requests, with the first letter of each word in"
94,"upper case. The header name will not appear in the logs, only the"
94,"value is reported, but the position in the logs is respected."
94,<length>
94,is the maximum number of characters to extract from the value and
94,report in the logs. The string will be truncated on the right if
94,it exceeds <length>.
94,The complete value of the last occurrence of the header is captured. The
94,value will be added to the logs between braces ('{}'). If multiple headers
94,"are captured, they will be delimited by a vertical bar ('|') and will appear"
94,in the same order they were declared in the configuration. Non-existent
94,headers will be logged just as an empty string. Common uses for request
94,"header captures include the ""Host"" field in virtual hosting environments, the"
94,"""Content-length"" when uploads are supported, ""User-agent"" to quickly"
94,"differentiate between real users and robots, and ""X-Forwarded-For"" in proxied"
94,environments to find where the request came from.
94,"Note that when capturing headers such as ""User-agent"", some spaces may be"
94,"logged, making the log analysis more difficult. Thus be careful about what"
94,you log if you know your log parser is not smart enough to rely on the
94,braces.
94,There is no limit to the number of captured request headers nor to their
94,"length, though it is wise to keep them low to limit memory usage per session."
94,"In order to keep log format consistent for a same frontend, header captures"
94,can only be declared in a frontend. It is not possible to specify a capture
94,"in a ""defaults"" section."
94,Example:
94,capture request header Host len 15
94,capture request header X-Forwarded-For len 15
94,capture request header Referer len 15
94,"See also : ""capture cookie"", ""capture response header"" as well as section 8 about logging."
94,capture response header <name> len <length>Capture and log the last occurrence of the specified response header.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesno
94,Arguments :<name>
94,is the name of the header to capture. The header names are not
94,"case-sensitive, but it is a common practice to write them as they"
94,"appear in the response, with the first letter of each word in"
94,"upper case. The header name will not appear in the logs, only the"
94,"value is reported, but the position in the logs is respected."
94,<length>
94,is the maximum number of characters to extract from the value and
94,report in the logs. The string will be truncated on the right if
94,it exceeds <length>.
94,The complete value of the last occurrence of the header is captured. The
94,result will be added to the logs between braces ('{}') after the captured
94,"request headers. If multiple headers are captured, they will be delimited by"
94,a vertical bar ('|') and will appear in the same order they were declared in
94,the configuration. Non-existent headers will be logged just as an empty
94,"string. Common uses for response header captures include the ""Content-length"""
94,"header which indicates how many bytes are expected to be returned, the"
94,"""Location"" header to track redirections."
94,There is no limit to the number of captured response headers nor to their
94,"length, though it is wise to keep them low to limit memory usage per session."
94,"In order to keep log format consistent for a same frontend, header captures"
94,can only be declared in a frontend. It is not possible to specify a capture
94,"in a ""defaults"" section."
94,Example:
94,capture response header Content-length len 9
94,capture response header Location len 15
94,"See also : ""capture cookie"", ""capture request header"" as well as section 8 about logging."
94,clitimeout <timeout> (deprecated)Set the maximum inactivity time on the client side.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,"Arguments :<timeout> is the timeout value is specified in milliseconds by default, but"
94,"can be in any other unit if the number is suffixed by the unit,"
94,as explained at the top of this document.
94,The inactivity timeout applies when the client is expected to acknowledge or
94,"send data. In HTTP mode, this timeout is particularly important to consider"
94,"during the first phase, when the client sends the request, and during the"
94,response while it is reading data sent by the server. The value is specified
94,"in milliseconds by default, but can be in any other unit if the number is"
94,"suffixed by the unit, as specified at the top of this document. In TCP mode"
94,"(and to a lesser extent, in HTTP mode), it is highly recommended that the"
94,client timeout remains equal to the server timeout in order to avoid complex
94,situations to debug. It is a good practice to cover one or several TCP packet
94,losses by specifying timeouts that are slightly above multiples of 3 seconds
94,(e.g. 4 or 5 seconds).
94,"This parameter is specific to frontends, but can be specified once for all in"
94,"""defaults"" sections. This is in fact one of the easiest solutions not to"
94,"forget about it. An unspecified timeout results in an infinite timeout, which"
94,is not recommended. Such a usage is accepted and works but reports a warning
94,during startup because it may results in accumulation of expired sessions in
94,the system if the system's timeouts are not configured either.
94,This parameter is provided for compatibility but is currently deprecated.
94,"Please use ""timeout client"" instead."
94,"See also : ""timeout client"", ""timeout http-request"", ""timeout server"", and ""srvtimeout""."
94,compression algo <algorithm> ...compression type <mime type> ...compression offloadEnable HTTP compression.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :algo
94,is followed by the list of supported compression algorithms.
94,type
94,is followed by the list of MIME types that will be compressed.
94,offload
94,makes haproxy work as a compression offloader only (see notes).
94,The currently supported algorithms are :
94,identity
94,"this is mostly for debugging, and it was useful for developing"
94,the compression feature. Identity does not apply any change on
94,data.
94,gzip
94,applies gzip compression. This setting is only available when
94,support for zlib or libslz was built in.
94,deflate
94,"same as ""gzip"", but with deflate algorithm and zlib format."
94,Note that this algorithm has ambiguous support on many
94,browsers and no support at all from recent ones. It is
94,strongly recommended not to use it for anything else than
94,experimentation. This setting is only available when support
94,for zlib or libslz was built in.
94,raw-deflate
94,"same as ""deflate"" without the zlib wrapper, and used as an"
94,"alternative when the browser wants ""deflate"". All major"
94,"browsers understand it and despite violating the standards,"
94,"it is known to work better than ""deflate"", at least on MSIE"
94,and some versions of Safari. Do not use it in conjunction
94,"with ""deflate"", use either one or the other since both react"
94,to the same Accept-Encoding token. This setting is only
94,available when support for zlib or libslz was built in.
94,Compression will be activated depending on the Accept-Encoding request
94,"header. With identity, it does not take care of that header."
94,"If backend servers support HTTP compression, these directives"
94,will be no-op: haproxy will see the compressed response and will not
94,compress again. If backend servers do not support HTTP compression and
94,"there is Accept-Encoding header in request, haproxy will compress the"
94,matching response.
94,"The ""offload"" setting makes haproxy remove the Accept-Encoding header to"
94,prevent backend servers from compressing responses. It is strongly
94,recommended not to do this because this means that all the compression work
94,will be done on the single point where haproxy is located. However in some
94,"deployment scenarios, haproxy may be installed in front of a buggy gateway"
94,with broken HTTP compression implementation which can't be turned off.
94,In that case haproxy can be used to prevent that gateway from emitting
94,"invalid payloads. In this case, simply removing the header in the"
94,"configuration does not work because it applies before the header is parsed,"
94,"so that prevents haproxy from compressing. The ""offload"" setting should"
94,"then be used for such scenarios. Note: for now, the ""offload"" setting is"
94,ignored when set in a defaults section.
94,Compression is disabled when:
94,* the request does not advertise a supported compression algorithm in the
94,"""Accept-Encoding"" header"
94,* the response message is not HTTP/1.1
94,* HTTP status code is not 200
94,"* response header ""Transfer-Encoding"" contains ""chunked"" (Temporary"
94,Workaround)
94,"* response contain neither a ""Content-Length"" header nor a"
94,"""Transfer-Encoding"" whose last value is ""chunked"""
94,"* response contains a ""Content-Type"" header whose first value starts with"
94,"""multipart"""
94,"* the response contains the ""no-transform"" value in the ""Cache-control"""
94,header
94,"* User-Agent matches ""Mozilla/4"" unless it is MSIE 6 with XP SP2, or MSIE 7"
94,and later
94,"* The response contains a ""Content-Encoding"" header, indicating that the"
94,response is already compressed (see compression offload)
94,"Note: The compression does not rewrite Etag headers, and does not emit the"
94,Warning header.
94,Examples :
94,compression algo gzip
94,compression type text/html text/plain
94,contimeout <timeout> (deprecated)Set the maximum time to wait for a connection attempt to a server to succeed.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,"Arguments :<timeout> is the timeout value is specified in milliseconds by default, but"
94,"can be in any other unit if the number is suffixed by the unit,"
94,as explained at the top of this document.
94,"If the server is located on the same LAN as haproxy, the connection should be"
94,"immediate (less than a few milliseconds). Anyway, it is a good practice to"
94,cover one or several TCP packet losses by specifying timeouts that are
94,"slightly above multiples of 3 seconds (e.g. 4 or 5 seconds). By default, the"
94,connect timeout also presets the queue timeout to the same value if this one
94,"has not been specified. Historically, the contimeout was also used to set the"
94,"tarpit timeout in a listen section, which is not possible in a pure frontend."
94,"This parameter is specific to backends, but can be specified once for all in"
94,"""defaults"" sections. This is in fact one of the easiest solutions not to"
94,"forget about it. An unspecified timeout results in an infinite timeout, which"
94,is not recommended. Such a usage is accepted and works but reports a warning
94,during startup because it may results in accumulation of failed sessions in
94,the system if the system's timeouts are not configured either.
94,This parameter is provided for backwards compatibility but is currently
94,"deprecated. Please use ""timeout connect"", ""timeout queue"" or ""timeout tarpit"""
94,instead.
94,"See also : ""timeout connect"", ""timeout queue"", ""timeout tarpit"", ""timeout server"", ""contimeout""."
94,cookie <name> [ rewrite | insert | prefix ] [ indirect ] [ nocache ]
94,[ postonly ] [ preserve ] [ httponly ] [ secure ]
94,[ domain <domain> ]* [ maxidle <idle> ] [ maxlife <life> ]
94,[ dynamic ] [ attr <value> ]*Enable cookie-based persistence in a backend.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments :<name>
94,"is the name of the cookie which will be monitored, modified or"
94,inserted in order to bring persistence. This cookie is sent to
94,"the client via a ""Set-Cookie"" header in the response, and is"
94,"brought back by the client in a ""Cookie"" header in all requests."
94,Special care should be taken to choose a name which does not
94,"conflict with any likely application cookie. Also, if the same"
94,backends are subject to be used by the same clients (e.g.
94,"HTTP/HTTPS), care should be taken to use different cookie names"
94,between all backends if persistence between them is not desired.
94,rewrite
94,This keyword indicates that the cookie will be provided by the
94,server and that haproxy will have to modify its value to set the
94,server's identifier in it. This mode is handy when the management
94,"of complex combinations of ""Set-cookie"" and ""Cache-control"""
94,headers is left to the application. The application can then
94,decide whether or not it is appropriate to emit a persistence
94,"cookie. Since all responses should be monitored, this mode"
94,doesn't work in HTTP tunnel mode. Unless the application
94,"behavior is very complex and/or broken, it is advised not to"
94,start with this mode for new deployments. This keyword is
94,"incompatible with ""insert"" and ""prefix""."
94,insert
94,This keyword indicates that the persistence cookie will have to
94,be inserted by haproxy in server responses if the client did not
94,already have a cookie that would have permitted it to access this
94,"server. When used without the ""preserve"" option, if the server"
94,"emits a cookie with the same name, it will be removed before"
94,"processing. For this reason, this mode can be used to upgrade"
94,"existing configurations running in the ""rewrite"" mode. The cookie"
94,will only be a session cookie and will not be stored on the
94,"client's disk. By default, unless the ""indirect"" option is added,"
94,the server will see the cookies emitted by the client. Due to
94,"caching effects, it is generally wise to add the ""nocache"" or"
94,"""postonly"" keywords (see below). The ""insert"" keyword is not"
94,"compatible with ""rewrite"" and ""prefix""."
94,prefix
94,This keyword indicates that instead of relying on a dedicated
94,"cookie for the persistence, an existing one will be completed."
94,This may be needed in some specific environments where the client
94,does not support more than one single cookie and the application
94,"already needs it. In this case, whenever the server sets a cookie"
94,"named <name>, it will be prefixed with the server's identifier"
94,and a delimiter. The prefix will be removed from all client
94,requests so that the server still finds the cookie it emitted.
94,"Since all requests and responses are subject to being modified,"
94,"this mode doesn't work with tunnel mode. The ""prefix"" keyword is"
94,"not compatible with ""rewrite"" and ""insert"". Note: it is highly"
94,"recommended not to use ""indirect"" with ""prefix"", otherwise server"
94,cookie updates would not be sent to clients.
94,indirect
94,"When this option is specified, no cookie will be emitted to a"
94,client which already has a valid one for the server which has
94,"processed the request. If the server sets such a cookie itself,"
94,"it will be removed, unless the ""preserve"" option is also set. In"
94,"""insert"" mode, this will additionally remove cookies from the"
94,"requests transmitted to the server, making the persistence"
94,mechanism totally transparent from an application point of view.
94,"Note: it is highly recommended not to use ""indirect"" with"
94,"""prefix"", otherwise server cookie updates would not be sent to"
94,clients.
94,nocache
94,This option is recommended in conjunction with the insert mode
94,"when there is a cache between the client and HAProxy, as it"
94,ensures that a cacheable response will be tagged non-cacheable if
94,a cookie needs to be inserted. This is important because if all
94,persistence cookies are added on a cacheable home page for
94,"instance, then all customers will then fetch the page from an"
94,"outer cache and will all share the same persistence cookie,"
94,leading to one server receiving much more traffic than others.
94,"See also the ""insert"" and ""postonly"" options."
94,postonly
94,This option ensures that cookie insertion will only be performed
94,on responses to POST requests. It is an alternative to the
94,"""nocache"" option, because POST responses are not cacheable, so"
94,this ensures that the persistence cookie will never get cached.
94,Since most sites do not need any sort of persistence before the
94,"first POST which generally is a login request, this is a very"
94,efficient method to optimize caching without risking to find a
94,persistence cookie in the cache.
94,"See also the ""insert"" and ""nocache"" options."
94,preserve
94,"This option may only be used with ""insert"" and/or ""indirect"". It"
94,allows the server to emit the persistence cookie itself. In this
94,"case, if a cookie is found in the response, haproxy will leave it"
94,untouched. This is useful in order to end persistence after a
94,"logout request for instance. For this, the server just has to"
94,emit a cookie with an invalid value (e.g. empty) or with a date in
94,"the past. By combining this mechanism with the ""disable-on-404"""
94,"check option, it is possible to perform a completely graceful"
94,shutdown because users will definitely leave the server after
94,they logout.
94,httponly
94,"This option tells haproxy to add an ""HttpOnly"" cookie attribute"
94,when a cookie is inserted. This attribute is used so that a
94,user agent doesn't share the cookie with non-HTTP components.
94,Please check RFC6265 for more information on this attribute.
94,secure
94,"This option tells haproxy to add a ""Secure"" cookie attribute when"
94,a cookie is inserted. This attribute is used so that a user agent
94,"never emits this cookie over non-secure channels, which means"
94,that a cookie learned with this flag will be presented only over
94,SSL/TLS connections. Please check RFC6265 for more information on
94,this attribute.
94,domain
94,This option allows to specify the domain at which a cookie is
94,inserted. It requires exactly one parameter: a valid domain
94,"name. If the domain begins with a dot, the browser is allowed to"
94,use it for any host ending with that name. It is also possible to
94,specify several domain names by invoking this option multiple
94,times. Some browsers might have small limits on the number of
94,"domains, so be careful when doing that. For the record, sending"
94,10 domains to MSIE 6 or Firefox 2 works as expected.
94,maxidle
94,This option allows inserted cookies to be ignored after some idle
94,time. It only works with insert-mode cookies. When a cookie is
94,"sent to the client, the date this cookie was emitted is sent too."
94,"Upon further presentations of this cookie, if the date is older"
94,"than the delay indicated by the parameter (in seconds), it will"
94,"be ignored. Otherwise, it will be refreshed if needed when the"
94,response is sent to the client. This is particularly useful to
94,prevent users who never close their browsers from remaining for
94,too long on the same server (e.g. after a farm size change). When
94,"this option is set and a cookie has no date, it is always"
94,"accepted, but gets refreshed in the response. This maintains the"
94,ability for admins to access their sites. Cookies that have a
94,date in the future further than 24 hours are ignored. Doing so
94,lets admins fix timezone issues without risking kicking users off
94,the site.
94,maxlife
94,This option allows inserted cookies to be ignored after some life
94,"time, whether they're in use or not. It only works with insert"
94,"mode cookies. When a cookie is first sent to the client, the date"
94,this cookie was emitted is sent too. Upon further presentations
94,"of this cookie, if the date is older than the delay indicated by"
94,"the parameter (in seconds), it will be ignored. If the cookie in"
94,"the request has no date, it is accepted and a date will be set."
94,Cookies that have a date in the future further than 24 hours are
94,ignored. Doing so lets admins fix timezone issues without risking
94,"kicking users off the site. Contrary to maxidle, this value is"
94,"not refreshed, only the first visit date counts. Both maxidle and"
94,maxlife may be used at the time. This is particularly useful to
94,prevent users who never close their browsers from remaining for
94,too long on the same server (e.g. after a farm size change). This
94,is stronger than the maxidle method in that it forces a
94,redispatch after some absolute delay.
94,dynamic
94,"Activate dynamic cookies. When used, a session cookie is"
94,"dynamically created for each server, based on the IP and port"
94,"of the server, and a secret key, specified in the"
94,"""dynamic-cookie-key"" backend directive."
94,"The cookie will be regenerated each time the IP address change,"
94,and is only generated for IPv4/IPv6.
94,attr
94,This option tells haproxy to add an extra attribute when a
94,cookie is inserted. The attribute value can contain any
94,"characters except control ones or "";"". This option may be"
94,repeated.
94,"There can be only one persistence cookie per HTTP backend, and it can be"
94,declared in a defaults section. The value of the cookie will be the value
94,"indicated after the ""cookieThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server optionsFetching HTTP samples (Layer 7)"" keyword in a ""server"" statement. If no cookie"
94,"is declared for a given server, the cookie is not set."
94,Examples :
94,cookie JSESSIONID prefix
94,cookie SRV insert indirect nocache
94,cookie SRV insert postonly indirect
94,cookie SRV insert indirect nocache maxidle 30m maxlife 8h
94,"See also : ""balance source"", ""capture cookie"", ""server"" and ""ignore-persist""."
94,declare capture [ request | response ] len <length>Declares a capture slot.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesno
94,Arguments:<length> is the length allowed for the capture.
94,"This declaration is only available in the frontend or listen section, but the"
94,"reserved slot can be used in the backends. The ""request"" keyword allocates a"
94,"capture slot for use in the request, and ""response"" allocates a capture slot"
94,for use in the response.
94,"See also: ""capture-req"", ""capture-res"" (sample converters), ""capture.req.hdr"", ""capture.res.hdr"" (sample fetches), ""http-request capture"" and ""http-response capture""."
94,default-server [param*]Change default options for a server in a backend
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments:<param*>
94,"is a list of parameters for this server. The ""default-server"""
94,keyword accepts an important number of options and has a complete
94,section dedicated to it. Please refer to section 5 for more
94,details.
94,Example :
94,default-server inter 1000 weight 13
94,"See also: ""server"" and section 5 about server options"
94,"default_backend <backend>Specify the backend to use when no ""use_backend"" rule has been matched."
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments :<backend> is the name of the backend to use.
94,When doing content-switching between frontend and backends using the
94,"""use_backend"" keyword, it is often useful to indicate which backend will be"
94,used when no rule has matched. It generally is the dynamic backend which
94,will catch all undetermined requests.
94,Example :
94,use_backend
94,dynamic
94,url_dyn
94,use_backend
94,static
94,url_css url_img extension_img
94,default_backend dynamic
94,"See also : ""use_backend"""
94,"description <string>Describe a listen, frontend or backend."
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,Arguments : string
94,Allows to add a sentence to describe the related object in the HAProxy HTML
94,stats page. The description will be printed on the right of the object name
94,it describes.
94,No need to backslash spaces in the <string> arguments.
94,"disabledDisable a proxy, frontend or backend."
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,"The ""disabledThis keyword is available in sections :PeersAlphabetically sorted keywords referenceServer and default-server options"" keyword is used to disable an instance, mainly in order to"
94,liberate a listening port or to temporarily disable a service. The instance
94,"will still be created and its configuration will be checked, but it will be"
94,"created in the ""stopped"" state and will appear as such in the statistics. It"
94,will not receive any traffic nor will it send any health-checks or logs. It
94,"is possible to disable many instances at once by adding the ""disabledThis keyword is available in sections :PeersAlphabetically sorted keywords referenceServer and default-server options"""
94,"keyword in a ""defaults"" section."
94,"See also : ""enabledThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"""
94,dispatch <address>:<port>Set a default server address
94,May be used in sections
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,"Arguments :<address> is the IPv4 address of the default server. Alternatively, a"
94,"resolvable hostname is supported, but this name will be resolved"
94,during start-up.
94,<ports>
94,is a mandatory port specification. All connections will be sent
94,"to this port, and it is not permitted to use port offsets as is"
94,possible with normal servers.
94,"The ""dispatch"" keyword designates a default server for use when no other"
94,server can take the connection. In the past it was used to forward non
94,persistent connections to an auxiliary load balancer. Due to its simple
94,"syntax, it has also been used for simple TCP relays. It is recommended not to"
94,"use it for more clarity, and to use the ""server"" directive instead."
94,"See also : ""server"""
94,dynamic-cookie-key <string>Set the dynamic cookie secret key for a backend.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : The secret key to be used.
94,"When dynamic cookies are enabled (see the ""dynamic"" directive for cookie),"
94,a dynamic cookie is created for each server (unless one is explicitly
94,"specified on the ""server"" line), using a hash of the IP address of the"
94,"server, the TCP port, and the secret key."
94,"That way, we can ensure session persistence across multiple load-balancers,"
94,even if servers are dynamically added or removed.
94,"enabledEnable a proxy, frontend or backend."
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,"The ""enabledThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"" keyword is used to explicitly enable an instance, when the"
94,"defaults has been set to ""disabledThis keyword is available in sections :PeersAlphabetically sorted keywords referenceServer and default-server options"". This is very rarely used."
94,"See also : ""disabledThis keyword is available in sections :PeersAlphabetically sorted keywords referenceServer and default-server options"""
94,errorfile <code> <file>Return a file contents instead of errors generated by HAProxy
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :<code>
94,"is the HTTP status code. Currently, HAProxy is capable of"
94,"generating codes 200, 400, 403, 405, 408, 425, 429, 500, 502,"
94,"503, and 504."
94,<file>
94,designates a file containing the full HTTP response. It is
94,"recommended to follow the common practice of appending "".http"" to"
94,the filename so that people do not confuse the response with HTML
94,"error pages, and to use absolute paths, since files are read"
94,before any chroot is performed.
94,It is important to understand that this keyword is not meant to rewrite
94,"errors returned by the server, but errors detected and returned by HAProxy."
94,This is why the list of supported errors is limited to a small set.
94,"Code 200 is emitted in response to requests matching a ""monitor-uri"" rule."
94,The files are returned verbatim on the TCP socket. This allows any trick such
94,"as redirections to another URL or site, as well as tricks to clean cookies,"
94,"force enable or disable caching, etc... The package provides default error"
94,files returning the same contents as default errors.
94,"The files should not exceed the configured buffer size (BUFSIZE), which"
94,"generally is 8 or 16 kB, otherwise they will be truncated. It is also wise"
94,not to put any reference to local contents (e.g. images) in order to avoid
94,"loops between the client and HAProxy when all servers are down, causing an"
94,"error to be returned instead of an image. For better HTTP compliance, it is"
94,recommended that all header lines end with CR-LF and not LF alone.
94,The files are read at the same time as the configuration and kept in memory.
94,"For this reason, the errors continue to be returned even when the process is"
94,"chrooted, and no file change is considered while the process is running. A"
94,simple method for developing those files consists in associating them to the
94,403 status code and interrogating a blocked URL.
94,"See also : ""errorloc"", ""errorloc302"", ""errorloc303"""
94,Example :
94,errorfile 400 /etc/haproxy/errorfiles/400badreq.http
94,errorfile 408 /dev/null
94,# work around Chrome pre-connect bug
94,errorfile 403 /etc/haproxy/errorfiles/403forbid.http
94,errorfile 503 /etc/haproxy/errorfiles/503sorry.http
94,errorloc <code> <url>errorloc302 <code> <url>Return an HTTP redirection to a URL instead of errors generated by HAProxy
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :<code>
94,"is the HTTP status code. Currently, HAProxy is capable of"
94,"generating codes 200, 400, 403, 405, 408, 425, 429, 500, 502,"
94,"503, and 504."
94,<url>
94,"it is the exact contents of the ""Location"" header. It may contain"
94,"either a relative URI to an error page hosted on the same site,"
94,or an absolute URI designating an error page on another site.
94,Special care should be given to relative URIs to avoid redirect
94,loops if the URI itself may generate the same error (e.g. 500).
94,It is important to understand that this keyword is not meant to rewrite
94,"errors returned by the server, but errors detected and returned by HAProxy."
94,This is why the list of supported errors is limited to a small set.
94,"Code 200 is emitted in response to requests matching a ""monitor-uri"" rule."
94,"Note that both keyword return the HTTP 302 status code, which tells the"
94,client to fetch the designated URL using the same HTTP method. This can be
94,"quite problematic in case of non-GET methods such as POST, because the URL"
94,sent to the client might not be allowed for something other than GET. To
94,"work around this problem, please use ""errorloc303"" which send the HTTP 303"
94,"status code, indicating to the client that the URL must be fetched with a GET"
94,request.
94,"See also : ""errorfile"", ""errorloc303"""
94,errorloc303 <code> <url>Return an HTTP redirection to a URL instead of errors generated by HAProxy
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :<code>
94,"is the HTTP status code. Currently, HAProxy is capable of"
94,"generating codes 200, 400, 403, 405, 408, 425, 429, 500, 502,"
94,"503, and 504."
94,<url>
94,"it is the exact contents of the ""Location"" header. It may contain"
94,"either a relative URI to an error page hosted on the same site,"
94,or an absolute URI designating an error page on another site.
94,Special care should be given to relative URIs to avoid redirect
94,loops if the URI itself may generate the same error (e.g. 500).
94,It is important to understand that this keyword is not meant to rewrite
94,"errors returned by the server, but errors detected and returned by HAProxy."
94,This is why the list of supported errors is limited to a small set.
94,"Code 200 is emitted in response to requests matching a ""monitor-uri"" rule."
94,"Note that both keyword return the HTTP 303 status code, which tells the"
94,client to fetch the designated URL using the same HTTP GET method. This
94,"solves the usual problems associated with ""errorloc"" and the 302 code. It is"
94,possible that some very old browsers designed before HTTP/1.1 do not support
94,"it, but no such problem has been reported till now."
94,"See also : ""errorfile"", ""errorloc"", ""errorloc302"""
94,email-alert from <emailaddr>Declare the from email address to be used in both the envelope and header
94,of email alerts. This is the address that email alerts are sent from.
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :<emailaddr> is the from email address to use when sending email alerts
94,"Also requires ""email-alert mailers"" and ""email-alert to"" to be set"
94,and if so sending email alerts is enabled for the proxy.
94,"See also : ""email-alert level"", ""email-alert mailers"", ""email-alert myhostname"", ""email-alert to"", section 3.6 about mailers."
94,email-alert level <level>Declare the maximum log level of messages for which email alerts will be
94,sent. This acts as a filter on the sending of email alerts.
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :<level> One of the 8 syslog levels:
94,emerg alert crit err warning notice info
94,debug
94,The above syslog levels are ordered from lowest to highest.
94,By default level is alert
94,"Also requires ""email-alert from"", ""email-alert mailers"" and"
94,"""email-alert to"" to be set and if so sending email alerts is enabled"
94,for the proxy.
94,Alerts are sent when :
94,* An un-paused server is marked as down and <level> is alert or lower
94,* A paused server is marked as down and <level> is notice or lower
94,* A server is marked as up or enters the drain state and <level>
94,is notice or lower
94,"* ""option log-health-checks"" is enabled, <level> is info or lower,"
94,and a health check status update occurs
94,"See also : ""email-alert from"", ""email-alert mailers"", ""email-alert myhostname"", ""email-alert to"", section 3.6 about mailers."
94,email-alert mailers <mailersect>Declare the mailers to be used when sending email alerts
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :<mailersect> is the name of the mailers section to send email alerts.
94,"Also requires ""email-alert from"" and ""email-alert to"" to be set"
94,and if so sending email alerts is enabled for the proxy.
94,"See also : ""email-alert from"", ""email-alert level"", ""email-alert myhostname"", ""email-alert to"", section 3.6 about mailers."
94,email-alert myhostname <hostname>Declare the to hostname address to be used when communicating with
94,mailers.
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :<hostname> is the hostname to use when communicating with mailers
94,By default the systems hostname is used.
94,"Also requires ""email-alert from"", ""email-alert mailers"" and"
94,"""email-alert to"" to be set and if so sending email alerts is enabled"
94,for the proxy.
94,"See also : ""email-alert from"", ""email-alert level"", ""email-alert mailers"", ""email-alert to"", section 3.6 about mailers."
94,email-alert to <emailaddr>Declare both the recipient address in the envelope and to address in the
94,header of email alerts. This is the address that email alerts are sent to.
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :<emailaddr> is the to email address to use when sending email alerts
94,"Also requires ""email-alert mailers"" and ""email-alert to"" to be set"
94,and if so sending email alerts is enabled for the proxy.
94,"See also : ""email-alert from"", ""email-alert level"", ""email-alert mailers"", ""email-alert myhostname"", section 3.6 about mailers."
94,force-persist { if | unless } <condition>Declare a condition to force persistence on down servers
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,"By default, requests are not dispatched to down servers. It is possible to"
94,"force this using ""option persist"", but it is unconditional and redispatches"
94,"to a valid server if ""option redispatch"" is set. That leaves with very little"
94,possibilities to force some requests to reach a server which is artificially
94,marked down for maintenance operations.
94,"The ""force-persist"" statement allows one to declare various ACL-based"
94,"conditions which, when met, will cause a request to ignore the down status of"
94,a server and still try to connect to it. That makes it possible to start a
94,"server, still replying an error to the health checks, and run a specially"
94,"configured browser to test the service. Among the handy methods, one could"
94,"use a specific source IP address, or a specific cookie. The cookie also has"
94,the advantage that it can easily be added/removed on the browser from a test
94,"page. Once the service is validated, it is then possible to open the service"
94,to the world by returning a valid response to health checks.
94,"The forced persistence is enabled when an ""if"" condition is met, or unless an"
94,"""unless"" condition is met. The final redispatch is always disabled when this"
94,is used.
94,"See also : ""option redispatch"", ""ignore-persist"", ""persist"", and section 7 about ACL usage."
94,filter <name> [param*]Add the filter <name> in the filter list attached to the proxy.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,Arguments :<name>
94,is the name of the filter. Officially supported filters are
94,referenced in section 9.
94,<param*>
94,is a list of parameters accepted by the filter <name>. The
94,parsing of these parameters are the responsibility of the
94,filter. Please refer to the documentation of the corresponding
94,filter (section 9) for all details on the supported parameters.
94,Multiple occurrences of the filter line can be used for the same proxy. The
94,same filter can be referenced many times if needed.
94,Example:
94,listen
94,bind *:80
94,filter trace name BEFORE-HTTP-COMP
94,filter compression
94,filter trace name AFTER-HTTP-COMP
94,compression algo gzip
94,compression offload
94,server srv1 192.168.0.1:80
94,See also : section 9.
94,fullconn <conns>Specify at what backend load the servers will reach their maxconn
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments :<conns>
94,is the number of connections on the backend which will make the
94,servers use the maximal number of connections.
94,"When a server has a ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"" parameter specified, it means that its number"
94,"of concurrent connections will never go higher. Additionally, if it has a"
94,"""minconn"" parameter, it indicates a dynamic limit following the backend's"
94,"load. The server will then always accept at least <minconn> connections,"
94,"never more than <maxconn>, and the limit will be on the ramp between both"
94,values when the backend has less than <conns> concurrent connections. This
94,"makes it possible to limit the load on the servers during normal loads, but"
94,push it further for important loads without overloading the servers during
94,exceptional loads.
94,"Since it's hard to get this value right, haproxy automatically sets it to"
94,10% of the sum of the maxconns of all frontends that may branch to this
94,"backend (based on ""use_backend"" and ""default_backend"" rules). That way it's"
94,"safe to leave it unset. However, ""use_backend"" involving dynamic names are"
94,not counted since there is no way to know if they could match or not.
94,Example :
94,# The servers will accept between 100 and 1000 concurrent connections each
94,# and the maximum of 1000 will be reached when the backend reaches 10000
94,# connections.
94,backend dynamic
94,fullconn
94,10000
94,server
94,srv1
94,dyn1:80 minconn 100 maxconn 1000
94,server
94,srv2
94,dyn2:80 minconn 100 maxconn 1000
94,"See also : ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"", ""server"""
94,grace <time>Maintain a proxy operational for some time after a soft stop
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :<time>
94,is the time (by default in milliseconds) for which the instance
94,will remain operational with the frontend sockets still listening
94,when a soft-stop is received via the SIGUSR1 signal.
94,This may be used to ensure that the services disappear in a certain order.
94,This was designed so that frontends which are dedicated to monitoring by an
94,external equipment fail immediately while other ones remain up for the time
94,needed by the equipment to detect the failure.
94,"Note that currently, there is very little benefit in using this parameter,"
94,and it may in fact complicate the soft-reconfiguration process more than
94,simplify it.
94,hash-balance-factor <factor>Specify the balancing factor for bounded-load consistent hashing
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnonoyes
94,Arguments :<factor> is the control for the maximum number of concurrent requests to
94,"send to a server, expressed as a percentage of the average number"
94,of concurrent requests across all of the active servers.
94,"Specifying a ""hash-balance-factor"" for a server with ""hash-type consistent"""
94,enables an algorithm that prevents any one server from getting too many
94,"requests at once, even if some hash buckets receive many more requests than"
94,"others. Setting <factor> to 0 (the default) disables the feature. Otherwise,"
94,"<factor> is a percentage greater than 100. For example, if <factor> is 150,"
94,then no server will be allowed to have a load more than 1.5 times the average.
94,"If server weights are used, they will be respected."
94,"If the first-choice server is disqualified, the algorithm will choose another"
94,"server based on the request hash, until a server with additional capacity is"
94,"found. A higher <factor> allows more imbalance between the servers, while a"
94,"lower <factor> means that more servers will be checked on average, affecting"
94,performance. Reasonable values are from 125 to 200.
94,"See also : ""balance"" and ""hash-type""."
94,hash-type <method> <function> <modifier>Specify a method to use for mapping hashes to servers
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments :<method> is the method used to select a server from the hash computed by
94,the <function> :
94,map-based
94,the hash table is a static array containing all alive servers.
94,"The hashes will be very smooth, will consider weights, but"
94,will be static in that weight changes while a server is up
94,will be ignored. This means that there will be no slow start.
94,"Also, since a server is selected by its position in the array,"
94,most mappings are changed when the server count changes. This
94,"means that when a server goes up or down, or when a server is"
94,"added to a farm, most connections will be redistributed to"
94,different servers. This can be inconvenient with caches for
94,instance.
94,consistent
94,the hash table is a tree filled with many occurrences of each
94,server. The hash key is looked up in the tree and the closest
94,"server is chosen. This hash is dynamic, it supports changing"
94,"weights while the servers are up, so it is compatible with the"
94,slow start feature. It has the advantage that when a server
94,"goes up or down, only its associations are moved. When a"
94,"server is added to the farm, only a few part of the mappings"
94,"are redistributed, making it an ideal method for caches."
94,"However, due to its principle, the distribution will never be"
94,very smooth and it may sometimes be necessary to adjust a
94,server's weight or its ID to get a more balanced distribution.
94,In order to get the same distribution on multiple load
94,"balancers, it is important that all servers have the exact"
94,same IDs. Note: consistent hash uses sdbm and avalanche if no
94,hash function is specified.
94,<function> is the hash function to be used :
94,sdbm
94,this function was created initially for sdbm (a public-domain
94,reimplementation of ndbm) database library. It was found to do
94,"well in scrambling bits, causing better distribution of the keys"
94,and fewer splits. It also happens to be a good general hashing
94,"function with good distribution, unless the total server weight"
94,"is a multiple of 64, in which case applying the avalanche"
94,modifier may help.
94,djb2
94,this function was first proposed by Dan Bernstein many years ago
94,on comp.lang.c. Studies have shown that for certain workload this
94,function provides a better distribution than sdbm. It generally
94,works well with text-based inputs though it can perform extremely
94,poorly with numeric-only input or when the total server weight is
94,"a multiple of 33, unless the avalanche modifier is also used."
94,wt6
94,this function was designed for haproxy while testing other
94,"functions in the past. It is not as smooth as the other ones, but"
94,is much less sensible to the input data set or to the number of
94,servers. It can make sense as an alternative to sdbm+avalanche or
94,djb2+avalanche for consistent hashing or when hashing on numeric
94,data such as a source IP address or a visitor identifier in a URL
94,parameter.
94,crc32
94,"this is the most common CRC32 implementation as used in Ethernet,"
94,"gzip, PNG, etc. It is slower than the other ones but may provide"
94,a better distribution or less predictable results especially when
94,used on strings.
94,<modifier> indicates an optional method applied after hashing the key :
94,avalanche
94,This directive indicates that the result from the hash
94,function above should not be used in its raw form but that
94,a 4-byte full avalanche hash must be applied first. The
94,purpose of this step is to mix the resulting bits from the
94,previous hash in order to avoid any undesired effect when
94,the input contains some limited values or when the number of
94,servers is a multiple of one of the hash's components (64
94,"for SDBM, 33 for DJB2). Enabling avalanche tends to make the"
94,"result less predictable, but it's also not as smooth as when"
94,using the original function. Some testing might be needed
94,with some workloads. This hash is one of the many proposed
94,by Bob Jenkins.
94,"The default hash type is ""map-based"" and is recommended for most usages. The"
94,"default function is ""sdbm"", the selection of a function should be based on"
94,the range of the values being hashed.
94,"See also : ""balance"", ""hash-balance-factor"", ""server"""
94,http-check disable-on-404Enable a maintenance mode upon HTTP/404 response to health-checks
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : none
94,"When this option is set, a server which returns an HTTP code 404 will be"
94,"excluded from further load-balancing, but will still receive persistent"
94,connections. This provides a very convenient method for Web administrators
94,to perform a graceful shutdown of their servers. It is also important to note
94,that a server which is detected as failed while it was in this mode will not
94,"generate an alert, just a notice. If the server responds 2xx or 3xx again, it"
94,will immediately be reinserted into the farm. The status on the stats page
94,"reports ""NOLB"" for a server in this mode. It is important to note that this"
94,"option only works in conjunction with the ""httpchk"" option. If this option"
94,"is used with ""http-check expect"", then it has precedence over it so that 404"
94,responses will still be considered as soft-stop.
94,"See also : ""option httpchk"", ""http-check expect"""
94,http-check expect [!] <match> <pattern>Make HTTP health checks consider response contents or specific status codes
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments :<match>
94,is a keyword indicating how to look for a specific pattern in the
94,"response. The keyword may be one of ""status"", ""rstatus"","
94,"""string"", or ""rstring"". The keyword may be preceded by an"
94,"exclamation mark (""!"") to negate the match. Spaces are allowed"
94,between the exclamation mark and the keyword. See below for more
94,details on the supported keywords.
94,<pattern> is the pattern to look for. It may be a string or a regular
94,"expression. If the pattern contains spaces, they must be escaped"
94,with the usual backslash ('\').
94,"By default, ""option httpchk"" considers that response statuses 2xx and 3xx"
94,"are valid, and that others are invalid. When ""http-check expect"" is used,"
94,"it defines what is considered valid or invalid. Only one ""http-check"""
94,statement is supported in a backend. If a server fails to respond or times
94,"out, the check obviously fails. The available matches are :"
94,status <string> : test the exact string match for the HTTP status code.
94,A health check response will be considered valid if the
94,response's status code is exactly this string. If the
94,"""status"" keyword is prefixed with ""!"", then the response"
94,will be considered invalid if the status code matches.
94,rstatus <regex> : test a regular expression for the HTTP status code.
94,A health check response will be considered valid if the
94,response's status code matches the expression. If the
94,"""rstatus"" keyword is prefixed with ""!"", then the response"
94,will be considered invalid if the status code matches.
94,This is mostly used to check for multiple codes.
94,string <string> : test the exact string match in the HTTP response body.
94,A health check response will be considered valid if the
94,response's body contains this exact string. If the
94,"""string"" keyword is prefixed with ""!"", then the response"
94,will be considered invalid if the body contains this
94,string. This can be used to look for a mandatory word at
94,"the end of a dynamic page, or to detect a failure when a"
94,specific error appears on the check page (e.g. a stack
94,trace).
94,rstring <regex> : test a regular expression on the HTTP response body.
94,A health check response will be considered valid if the
94,"response's body matches this expression. If the ""rstring"""
94,"keyword is prefixed with ""!"", then the response will be"
94,considered invalid if the body matches the expression.
94,This can be used to look for a mandatory word at the end
94,"of a dynamic page, or to detect a failure when a specific"
94,error appears on the check page (e.g. a stack trace).
94,It is important to note that the responses will be limited to a certain size
94,"defined by the global ""tune.chksize"" option, which defaults to 16384 bytes."
94,"Thus, too large responses may not contain the mandatory pattern when using"
94,"""string"" or ""rstring"". If a large response is absolutely required, it is"
94,possible to change the default max size by setting the global variable.
94,"However, it is worth keeping in mind that parsing very large responses can"
94,"waste some CPU cycles, especially when regular expressions are used, and that"
94,it is always better to focus the checks on smaller resources.
94,"Also ""http-check expect"" doesn't support HTTP keep-alive. Keep in mind that it"
94,"will automatically append a ""Connection: close"" header, meaning that this"
94,"header should not be present in the request provided by ""option httpchk""."
94,"Last, if ""http-check expect"" is combined with ""http-check disable-on-404"","
94,then this last one has precedence when the server responds with 404.
94,Examples :
94,# only accept status 200 as valid
94,http-check expect status 200
94,# consider SQL errors as errors
94,http-check expect ! string SQL\ Error
94,# consider status 5xx only as errors
94,http-check expect ! rstatus ^5
94,# check that we have a correct hexadecimal tag before /html
94,http-check expect rstring <!--tag:[0-9a-f]*--></html>
94,"See also : ""option httpchk"", ""http-check disable-on-404"""
94,http-check send [hdr <name> <value>]* [body <string>]Add a possible list of headers and/or a body to the request sent during HTTP
94,health checks.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments :hdr <name> <value>
94,adds the HTTP header field whose name is specified in
94,<name> and whose value is defined by <value> to the
94,request sent during HTTP health checks.
94,body <string>
94,add the body defined by <string> to the request sent
94,"sent during HTTP health checks. If defined, the"
94,"""Content-Length"" header is thus automatically added"
94,to the request.
94,"In addition to the request line defined by the ""option httpchk"" directive,"
94,this one is the valid way to add some headers and optionally a body to the
94,"request sent during HTTP health checks. If a body is defined, the associate"
94,"""Content-Length"" header is automatically added. The old trick consisting to"
94,"add headers after the version string on the ""option httpchk"" line is now"
94,"deprecated. Note also the ""Connection: close"" header is still added if a"
94,"""http-check expect"" direcive is defined independently of this directive, just"
94,"like the state header if the directive ""http-check send-state"" is defined."
94,"See also : ""option httpchk"", ""http-check send-state"" and ""http-check expect"""
94,http-check send-stateEnable emission of a state header with HTTP health checks
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : none
94,"When this option is set, haproxy will systematically send a special header"
94,"""X-Haproxy-Server-State"" with a list of parameters indicating to each server"
94,how they are seen by haproxy. This can be used for instance when a server is
94,manipulated without access to haproxy and the operator needs to know whether
94,"haproxy still sees it up or not, or if the server is the last one in a farm."
94,"The header is composed of fields delimited by semi-colons, the first of which"
94,"is a word (""UP"", ""DOWN"", ""NOLB""), possibly followed by a number of valid"
94,"checks on the total number before transition, just as appears in the stats"
94,"interface. Next headers are in the form ""<variable>=<value>"", indicating in"
94,no specific order some values available in the stats interface :
94,"- a variable ""address"", containing the address of the backend server."
94,This corresponds to the <address> field in the server declaration. For
94,"unix domain sockets, it will read ""unix""."
94,"- a variable ""port"", containing the port of the backend server. This"
94,corresponds to the <port> field in the server declaration. For unix
94,"domain sockets, it will read ""unix""."
94,"- a variable ""name"", containing the name of the backend followed by a slash"
94,"(""/"") then the name of the server. This can be used when a server is"
94,checked in multiple backends.
94,"- a variable ""node"" containing the name of the haproxy node, as set in the"
94,"global ""node"" variable, otherwise the system's hostname if unspecified."
94,"- a variable ""weight"" indicating the weight of the server, a slash (""/"")"
94,and the total weight of the farm (just counting usable servers). This
94,helps to know if other servers are available to handle the load when this
94,one fails.
94,"- a variable ""scur"" indicating the current number of concurrent connections"
94,"on the server, followed by a slash (""/"") then the total number of"
94,connections on all servers of the same backend.
94,"- a variable ""qcur"" indicating the current number of requests in the"
94,server's queue.
94,Example of a header received by the application server :
94,>>>
94,X-Haproxy-Server-State: UP 2/3; name=bck/srv2; node=lb1; weight=1/2; \
94,scur=13/22; qcur=0
94,"See also : ""option httpchk"", ""http-check disable-on-404"""
94,http-request { allow | auth [realm <realm>] | redirect <rule> | reject |
94,tarpit [deny_status <status>] | deny [deny_status <status>] |
94,add-header <name> <fmt> | set-header <name> <fmt> |
94,capture <sample> [ len <length> | id <id> ] |
94,del-header <name> | set-nice <nice> | set-log-level <level> |
94,replace-header <name> <match-regex> <replace-fmt> |
94,replace-value <name> <match-regex> <replace-fmt> |
94,set-method <fmt> | set-path <fmt> | set-query <fmt> |
94,set-uri <fmt> | set-tos <tos> | set-mark <mark> |
94,add-acl(<file name>) <key fmt> |
94,del-acl(<file name>) <key fmt> |
94,del-map(<file name>) <key fmt> |
94,set-map(<file name>) <key fmt> <value fmt> |
94,set-var(<var name>) <expr> |
94,unset-var(<var name>) |
94,{ track-sc0 | track-sc1 | track-sc2 } <key> [table <table>] |
94,sc-inc-gpc0(<sc-id>) |
94,sc-set-gpt0(<sc-id>) <int> |
94,silent-drop |
94,send-spoe-group |
94,cache-use
94,[ { if | unless } <condition> ]Access control for Layer 7 requests
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,The http-request statement defines a set of rules which apply to layer 7
94,processing. The rules are evaluated in their declaration order when they are
94,"met in a frontend, listen or backend section. Any rule may optionally be"
94,"followed by an ACL-based condition, in which case it will only be evaluated"
94,if the condition is true.
94,The first keyword is the rule's action. Currently supported actions include :
94,"- ""allow"" : this stops the evaluation of the rules and lets the request"
94,"pass the check. No further ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"" rules are evaluated."
94,"- ""deny"" : this stops the evaluation of the rules and immediately rejects"
94,"the request and emits an HTTP 403 error, or optionally the status code"
94,"specified as an argument to ""deny_status"". The list of permitted status"
94,"codes is limited to those that can be overridden by the ""errorfile"""
94,"directive. No further ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"" rules are evaluated."
94,"- ""reject"" : this stops the evaluation of the rules and immediately closes"
94,the connection without sending any response. It acts similarly to the
94,"""tcp-request content reject"" rules. It can be useful to force an"
94,immediate connection closure on HTTP/2 connections.
94,"- ""tarpit"" : this stops the evaluation of the rules and immediately blocks"
94,"the request without responding for a delay specified by ""timeout tarpit"""
94,"or ""timeout connect"" if the former is not set. After that delay, if the"
94,"client is still connected, an HTTP error 500 (or optionally the status"
94,"code specified as an argument to ""deny_status"") is returned so that the"
94,client does not suspect it has been tarpitted. Logs will report the flags
94,"""PT"". The goal of the tarpit rule is to slow down robots during an attack"
94,when they're limited on the number of concurrent requests. It can be very
94,"efficient against very dumb robots, and will significantly reduce the"
94,"load on firewalls compared to a ""deny"" rule. But when facing ""correctly"""
94,"developed robots, it can make things worse by forcing haproxy and the"
94,front firewall to support insane number of concurrent connections. See
94,"also the ""silent-drop"" action below."
94,"- ""auth"" : this stops the evaluation of the rules and immediately responds"
94,with an HTTP 401 or 407 error code to invite the user to present a valid
94,"user name and password. No further ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"" rules are evaluated. An"
94,"optional ""realm"" parameter is supported, it sets the authentication realm"
94,that is returned with the response (typically the application's name).
94,"- ""redirect"" : this performs an HTTP redirection based on a redirect rule."
94,"This is exactly the same as the ""redirect"" statement except that it"
94,inserts a redirect rule which can be processed in the middle of other
94,"""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"" rules and that these rules use the ""log-format"" strings."
94,"See the ""redirect"" keyword for the rule's syntax."
94,"- ""add-header"" appends an HTTP header field whose name is specified in"
94,<name> and whose value is defined by <fmt> which follows the log-format
94,rules (see Custom Log Format in section 8.2.4). This is particularly
94,useful to pass connection-specific information to the server (e.g. the
94,"client's SSL certificate), or to combine several headers into one. This"
94,"rule is not final, so it is possible to add other similar rules. Note"
94,"that header addition is performed immediately, so one rule might reuse"
94,the resulting header from a previous rule.
94,"- ""set-header"" does the same as ""add-header"" except that the header name"
94,is first removed if it existed. This is useful when passing security
94,"information to the server, where the header must not be manipulated by"
94,external users. Note that the new value is computed before the removal so
94,it is possible to concatenate a value to an existing header.
94,"- ""del-header"" removes all HTTP header fields whose name is specified in"
94,<name>.
94,"- ""replace-header"" matches the regular expression in all occurrences of"
94,"header field <name> according to <match-regex>, and replaces them with"
94,the <replace-fmt> argument. Format characters are allowed in replace-fmt
94,"and work like in <fmt> arguments in ""add-header"". The match is only"
94,case-sensitive. It is important to understand that this action only
94,"considers whole header lines, regardless of the number of values they"
94,may contain. This usage is suited to headers naturally containing commas
94,"in their value, such as If-Modified-Since and so on."
94,Example:
94,http-request replace-header Cookie foo=([^;]*);(.*) foo=\1;ip=%bi;\2
94,applied to:
94,"Cookie: foo=foobar; expires=Tue, 14-Jun-2016 01:40:45 GMT;"
94,outputs:
94,"Cookie: foo=foobar;ip=192.168.1.20; expires=Tue, 14-Jun-2016 01:40:45 GMT;"
94,assuming the backend IP is 192.168.1.20
94,"- ""replace-value"" works like ""replace-header"" except that it matches the"
94,regex against every comma-delimited value of the header field <name>
94,instead of the entire header. This is suited for all headers which are
94,allowed to carry more than one value. An example could be the Accept
94,header.
94,Example:
94,http-request replace-value X-Forwarded-For ^192\.168\.(.*)$ 172.16.\1
94,applied to:
94,"X-Forwarded-For: 192.168.10.1, 192.168.13.24, 10.0.0.37"
94,outputs:
94,"X-Forwarded-For: 172.16.10.1, 172.16.13.24, 10.0.0.37"
94,"- ""set-method"" rewrites the request method with the result of the"
94,evaluation of format string <fmt>. There should be very few valid reasons
94,for having to do so as this is more likely to break something than to fix
94,it.
94,"- ""set-path"" rewrites the request path with the result of the evaluation of"
94,"format string <fmt>. The query string, if any, is left intact. If a"
94,"scheme and authority is found before the path, they are left intact as"
94,"well. If the request doesn't have a path (""*""), this one is replaced with"
94,the format. This can be used to prepend a directory component in front of
94,"a path for example. See also ""set-query"" and ""set-uri""."
94,Example :
94,# prepend the host name before the path
94,http-request set-path /%[hdr(host)]%[path]
94,"- ""set-query"" rewrites the request's query string which appears after the"
94,"first question mark (""?"") with the result of the evaluation of format"
94,string <fmt>. The part prior to the question mark is left intact. If the
94,"request doesn't contain a question mark and the new value is not empty,"
94,"then one is added at the end of the URI, followed by the new value. If"
94,"a question mark was present, it will never be removed even if the value"
94,is empty. This can be used to add or remove parameters from the query
94,"string. See also ""set-query"" and ""set-uri""."
94,Example :
94,"# replace ""%3D"" with ""="" in the query string"
94,"http-request set-query %[query,regsub(%3D,=,g)]"
94,"- ""set-uri"" rewrites the request URI with the result of the evaluation of"
94,"format string <fmt>. The scheme, authority, path and query string are all"
94,"replaced at once. This can be used to rewrite hosts in front of proxies,"
94,or to perform complex modifications to the URI such as moving parts
94,"between the path and the query string. See also ""set-path"" and"
94,"""set-query""."
94,"- ""set-nice"" sets the ""nice"" factor of the current request being processed."
94,It only has effect against the other requests being processed at the same
94,"time. The default value is 0, unless altered by the ""nice"" setting on the"
94,"""bind"" line. The accepted range is -1024..1024. The higher the value, the"
94,nicest the request will be. Lower values will make the request more
94,important than other ones. This can be useful to improve the speed of
94,"some requests, or lower the priority of non-important requests. Using"
94,this setting without prior experimentation can cause some major slowdown.
94,"- ""set-log-level"" is used to change the log level of the current request"
94,when a certain condition is met. Valid levels are the 8 syslog levels
94,"(see the ""logThis keyword is available in sections :Process management and securityAlphabetically sorted keywords reference"" keyword) plus the special level ""silent"" which disables"
94,logging for this request. This rule is not final so the last matching
94,rule wins. This rule can be useful to disable health checks coming from
94,another equipment.
94,"- ""set-tos"" is used to set the TOS or DSCP field value of packets sent to"
94,the client to the value passed in <tos> on platforms which support this.
94,"This value represents the whole 8 bits of the IP TOS field, and can be"
94,"expressed both in decimal or hexadecimal format (prefixed by ""0x""). Note"
94,"that only the 6 higher bits are used in DSCP or TOS, and the two lower"
94,bits are always 0. This can be used to adjust some routing behavior on
94,"border routers based on some information from the request. See RFC 2474,"
94,"2597, 3260 and 4594 for more information."
94,"- ""set-mark"" is used to set the Netfilter MARK on all packets sent to the"
94,client to the value passed in <mark> on platforms which support it. This
94,value is an unsigned 32 bit value which can be matched by netfilter and
94,by the routing table. It can be expressed both in decimal or hexadecimal
94,"format (prefixed by ""0x""). This can be useful to force certain packets to"
94,take a different route (for example a cheaper network path for bulk
94,downloads). This works on Linux kernels 2.6.32 and above and requires
94,admin privileges.
94,"- ""add-acl"" is used to add a new entry into an ACL. The ACL must be loaded"
94,from a file (even a dummy empty file). The file name of the ACL to be
94,"updated is passed between parentheses. It takes one argument: <key fmt>,"
94,"which follows log-format rules, to collect content of the new entry. It"
94,"performs a lookup in the ACL before insertion, to avoid duplicated (or"
94,more) values. This lookup is done by a linear search and can be expensive
94,"with large lists! It is the equivalent of the ""add acl"" command from the"
94,"stats socket, but can be triggered by an HTTP request."
94,"- ""del-acl"" is used to delete an entry from an ACL. The ACL must be loaded"
94,from a file (even a dummy empty file). The file name of the ACL to be
94,"updated is passed between parentheses. It takes one argument: <key fmt>,"
94,"which follows log-format rules, to collect content of the entry to delete."
94,"It is the equivalent of the ""del acl"" command from the stats socket, but"
94,can be triggered by an HTTP request.
94,"- ""del-map"" is used to delete an entry from a MAP. The MAP must be loaded"
94,from a file (even a dummy empty file). The file name of the MAP to be
94,"updated is passed between parentheses. It takes one argument: <key fmt>,"
94,"which follows log-format rules, to collect content of the entry to delete."
94,"It takes one argument: ""file name"" It is the equivalent of the ""del map"""
94,"command from the stats socket, but can be triggered by an HTTP request."
94,"- ""set-map"" is used to add a new entry into a MAP. The MAP must be loaded"
94,from a file (even a dummy empty file). The file name of the MAP to be
94,"updated is passed between parentheses. It takes 2 arguments: <key fmt>,"
94,"which follows log-format rules, used to collect MAP key, and <value fmt>,"
94,"which follows log-format rules, used to collect content for the new entry."
94,"It performs a lookup in the MAP before insertion, to avoid duplicated (or"
94,more) values. This lookup is done by a linear search and can be expensive
94,"with large lists! It is the equivalent of the ""set map"" command from the"
94,"stats socket, but can be triggered by an HTTP request."
94,- capture <sample> [ len <length> | id <id> ] :
94,"captures sample expression <sample> from the request buffer, and converts"
94,it to a string of at most <len> characters. The resulting string is
94,"stored into the next request ""capture"" slot, so it will possibly appear"
94,next to some captured HTTP headers. It will then automatically appear in
94,"the logs, and it will be possible to extract it using sample fetch rules"
94,to feed it into headers or anything. The length should be limited given
94,that this size will be allocated for each capture during the whole
94,"session life. Please check section 7.3 (Fetching samples) and ""capture"
94,"request header"" for more information."
94,"If the keyword ""idThis keyword is available in sections :Alphabetically sorted keywords referenceBind optionsServer and default-server options"" is used instead of ""len"", the action tries to store"
94,the captured string in a previously declared capture slot. This is useful
94,to run captures in backends. The slot id can be declared by a previous
94,"directive ""http-request capture"" or with the ""declare capture"" keyword."
94,"When using this action in a backend, double check that the relevant"
94,"frontend(s) have the required capture slots otherwise, this rule will be"
94,ignored at run time. This can't be detected at configuration parsing time
94,due to HAProxy's ability to dynamically resolve backend name at runtime.
94,- cache-use <name> :
94,See section 10.2 about cache setup.
94,- { track-sc0 | track-sc1 | track-sc2 } <key> [table <table>] :
94,enables tracking of sticky counters from current request. These rules
94,do not stop evaluation and do not change default action. The number of
94,counters that may be simultaneously tracked by the same connection is set
94,in MAX_SESS_STKCTR at build time (reported in haproxy -vv) which defaults
94,"to 3, so the track-sc number is between 0 and (MAX_SESS_STCKTR-1). The"
94,"first ""track-sc0"" rule executed enables tracking of the counters of the"
94,"specified table as the first set. The first ""track-sc1"" rule executed"
94,enables tracking of the counters of the specified table as the second
94,"set. The first ""track-sc2"" rule executed enables tracking of the"
94,counters of the specified table as the third set. It is a recommended
94,practice to use the first set of counters for the per-frontend counters
94,and the second set for the per-backend ones. But this is just a
94,"guideline, all may be used everywhere."
94,These actions take one or two arguments :
94,<key>
94,"is mandatory, and is a sample expression rule as described"
94,in section 7.3. It describes what elements of the incoming
94,"request or connection will be analyzed, extracted, combined,"
94,and used to select which table entry to update the counters.
94,"<table> is an optional table to be used instead of the default one,"
94,which is the stick-table declared in the current proxy. All
94,the counters for the matches and updates for the key will
94,then be performed in that table until the session ends.
94,"Once a ""track-sc*"" rule is executed, the key is looked up in the table"
94,"and if it is not found, an entry is allocated for it. Then a pointer to"
94,"that entry is kept during all the session's life, and this entry's"
94,"counters are updated as often as possible, every time the session's"
94,"counters are updated, and also systematically when the session ends."
94,Counters are only updated for events that happen after the tracking has
94,"been started. As an exception, connection counters and request counters"
94,are systematically updated so that they reflect useful information.
94,"If the entry tracks concurrent connection counters, one connection is"
94,"counted for as long as the entry is tracked, and the entry will not"
94,expire during that time. Tracking counters also provides a performance
94,"advantage over just checking the keys, because only one table lookup is"
94,performed for all ACL checks that make use of it.
94,- sc-set-gpt0(<sc-id>) <int> :
94,This action sets the GPT0 tag according to the sticky counter designated
94,by <sc-id> and the value of <int>. The expected result is a boolean. If
94,"an error occurs, this action silently fails and the actions evaluation"
94,continues.
94,- sc-inc-gpc0(<sc-id>):
94,This action increments the GPC0 counter according with the sticky counter
94,"designated by <sc-id>. If an error occurs, this action silently fails and"
94,the actions evaluation continues.
94,- set-var(<var-name>) <expr> :
94,Is used to set the contents of a variable. The variable is declared
94,inline.
94,<var-name> The name of the variable starts with an indication about
94,its scope. The scopes allowed are:
94,"""proc"" : the variable is shared with the whole process"
94,"""sess"" : the variable is shared with the whole session"
94,"""txn"""
94,: the variable is shared with the transaction
94,(request and response)
94,"""req"""
94,: the variable is shared only during request
94,processing
94,"""res"""
94,: the variable is shared only during response
94,processing
94,This prefix is followed by a name. The separator is a '.'.
94,"The name may only contain characters 'a-z', 'A-Z', '0-9'"
94,and '_'.
94,<expr>
94,Is a standard HAProxy expression formed by a sample-fetch
94,followed by some converters.
94,Example:
94,"http-request set-var(req.my_var) req.fhdr(user-agent),lower"
94,- unset-var(<var-name>) :
94,Is used to unset a variable. See above for details about <var-name>.
94,Example:
94,http-request unset-var(req.my_var)
94,- set-src <expr> :
94,Is used to set the source IP address to the value of specified
94,"expression. Useful when a proxy in front of HAProxy rewrites source IP,"
94,but provides the correct IP in a HTTP header; or you want to mask
94,"source IP for privacy. All subsequent calls to ""src"" fetch will return"
94,this value (see example).
94,<expr>
94,Is a standard HAProxy expression formed by a sample-fetch
94,followed by some converters.
94,"See also ""option forwardfor""."
94,Example:
94,http-request set-src hdr(x-forwarded-for)
94,"http-request set-src src,ipmask(24)"
94,# After the masking this will track connections
94,# based on the IP address with the last byte zeroed out.
94,http-request track-sc0 src
94,"When possible, set-src preserves the original source port as long as the"
94,"address family allows it, otherwise the source port is set to 0."
94,- set-src-port <expr> :
94,Is used to set the source port address to the value of specified
94,expression.
94,<expr>
94,Is a standard HAProxy expression formed by a sample-fetch
94,followed by some converters.
94,Example:
94,http-request set-src-port hdr(x-port)
94,http-request set-src-port int(4000)
94,"When possible, set-src-port preserves the original source address as long"
94,"as the address family supports a port, otherwise it forces the source"
94,"address to IPv4 ""0.0.0.0"" before rewriting the port."
94,- set-dst <expr> :
94,Is used to set the destination IP address to the value of specified
94,expression. Useful when a proxy in front of HAProxy rewrites destination
94,"IP, but provides the correct IP in a HTTP header; or you want to mask"
94,"the IP for privacy. If you want to connect to the new address/port, use"
94,'0.0.0.0:0' as a server address in the backend.
94,<expr>
94,Is a standard HAProxy expression formed by a sample-fetch
94,followed by some converters.
94,Example:
94,http-request set-dst hdr(x-dst)
94,"http-request set-dst dst,ipmask(24)"
94,"When possible, set-dst preserves the original destination port as long as"
94,"the address family allows it, otherwise the destination port is set to 0."
94,- set-dst-port <expr> :
94,Is used to set the destination port address to the value of specified
94,"expression. If you want to connect to the new address/port, use"
94,'0.0.0.0:0' as a server address in the backend.
94,<expr>
94,Is a standard HAProxy expression formed by a sample-fetch
94,followed by some converters.
94,Example:
94,http-request set-dst-port hdr(x-port)
94,http-request set-dst-port int(4000)
94,"When possible, set-dst-port preserves the original destination address as"
94,"long as the address family supports a port, otherwise it forces the"
94,"destination address to IPv4 ""0.0.0.0"" before rewriting the port."
94,"- ""silent-drop"" : this stops the evaluation of the rules and makes the"
94,client-facing connection suddenly disappear using a system-dependent way
94,that tries to prevent the client from being notified. The effect it then
94,that the client still sees an established connection while there's none
94,"on HAProxy. The purpose is to achieve a comparable effect to ""tarpit"""
94,except that it doesn't use any local resource at all on the machine
94,"running HAProxy. It can resist much higher loads than ""tarpit"", and slow"
94,down stronger attackers. It is important to understand the impact of using
94,this mechanism. All stateful equipment placed between the client and
94,"HAProxy (firewalls, proxies, load balancers) will also keep the"
94,established connection for a long time and may suffer from this action.
94,"On modern Linux systems running with enough privileges, the TCP_REPAIR"
94,socket option is used to block the emission of a TCP reset. On other
94,"systems, the socket's TTL is reduced to 1 so that the TCP reset doesn't"
94,"pass the first router, though it's still delivered to local networks. Do"
94,not use it unless you fully understand how it works.
94,"- ""wait-for-handshake"" : this will delay the processing of the request"
94,until the SSL handshake happened. This is mostly useful to delay
94,processing early data until we're sure they are valid.
94,- send-spoe-group <engine-name> <group-name> :
94,This action is used to trigger sending of a group of SPOE messages. To do
94,"so, the SPOE engine used to send messages must be defined, as well as the"
94,"SPOE group to send. Of course, the SPOE engine must refer to an existing"
94,"SPOE filter. If not engine name is provided on the SPOE filter line, the"
94,SPOE agent name must be used.
94,<engine-name> The SPOE engine name.
94,<group-name>
94,The SPOE group name as specified in the engine
94,configuration.
94,There is no limit to the number of http-request statements per instance.
94,It is important to know that http-request rules are processed very early in
94,"the HTTP processing, just after ""block"" rules and before ""reqdel"" or ""reqrep"""
94,"or ""reqadd"" rules. That way, headers added by ""add-header""/""set-header"" are"
94,visible by almost all further ACL rules.
94,"Using ""reqadd""/""reqdel""/""reqrep"" to manipulate request headers is discouraged"
94,in newer versions (>= 1.5). But if you need to use regular expression to
94,"delete headers, you can still use ""reqdel"". Also please use"
94,"""http-request deny/allow/tarpit"" instead of ""reqdeny""/""reqpass""/""reqtarpit""."
94,Example:
94,acl nagios src 192.168.129.3
94,acl local_net src 192.168.0.0/16
94,acl auth_ok http_auth(L1)
94,http-request allow if nagios
94,http-request allow if local_net auth_ok
94,http-request auth realm Gimme if local_net auth_ok
94,http-request deny
94,Example:
94,acl auth_ok http_auth_group(L1) G1
94,http-request auth unless auth_ok
94,Example:
94,http-request set-header X-Haproxy-Current-Date %T
94,http-request set-header X-SSL
94,%[ssl_fc]
94,http-request set-header X-SSL-Session_ID
94,"%[ssl_fc_session_id,hex]"
94,http-request set-header X-SSL-Client-Verify
94,%[ssl_c_verify]
94,http-request set-header X-SSL-Client-DN
94,%{+Q}[ssl_c_s_dn]
94,http-request set-header X-SSL-Client-CN
94,%{+Q}[ssl_c_s_dn(cn)]
94,http-request set-header X-SSL-Issuer
94,%{+Q}[ssl_c_i_dn]
94,http-request set-header X-SSL-Client-NotBefore %{+Q}[ssl_c_notbefore]
94,http-request set-header X-SSL-Client-NotAfter
94,%{+Q}[ssl_c_notafter]
94,Example:
94,acl key req.hdr(X-Add-Acl-Key) -m found
94,acl add path /addacl
94,acl del path /delacl
94,acl myhost hdr(Host) -f myhost.lst
94,http-request add-acl(myhost.lst) %[req.hdr(X-Add-Acl-Key)] if key add
94,http-request del-acl(myhost.lst) %[req.hdr(X-Add-Acl-Key)] if key del
94,Example:
94,acl value
94,req.hdr(X-Value) -m found
94,acl setmap path /setmap
94,acl delmap path /delmap
94,"use_backend bk_appli if { hdr(Host),map_str(map.lst) -m found }"
94,http-request set-map(map.lst) %[src] %[req.hdr(X-Value)] if setmap value
94,http-request del-map(map.lst) %[src]
94,if delmap
94,"See also : ""stats http-request"", section 3.4 about userlists and section 7 about ACL usage."
94,http-response { allow | deny | add-header <name> <fmt> | set-nice <nice> |
94,capture <sample> id <id> | redirect <rule> |
94,set-header <name> <fmt> | del-header <name> |
94,replace-header <name> <regex-match> <replace-fmt> |
94,replace-value <name> <regex-match> <replace-fmt> |
94,set-status <status> [reason <str>] |
94,set-log-level <level> | set-mark <mark> | set-tos <tos> |
94,add-acl(<file name>) <key fmt> |
94,del-acl(<file name>) <key fmt> |
94,del-map(<file name>) <key fmt> |
94,set-map(<file name>) <key fmt> <value fmt> |
94,set-var(<var-name>) <expr> |
94,unset-var(<var-name>) |
94,{ track-sc0 | track-sc1 | track-sc2 } <key> [table <table>] |
94,sc-inc-gpc0(<sc-id>) |
94,sc-set-gpt0(<sc-id>) <int> |
94,silent-drop |
94,send-spoe-group |
94,cache-store
94,[ { if | unless } <condition> ]Access control for Layer 7 responses
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,The http-response statement defines a set of rules which apply to layer 7
94,processing. The rules are evaluated in their declaration order when they are
94,"met in a frontend, listen or backend section. Any rule may optionally be"
94,"followed by an ACL-based condition, in which case it will only be evaluated"
94,"if the condition is true. Since these rules apply on responses, the backend"
94,"rules are applied first, followed by the frontend's rules."
94,The first keyword is the rule's action. Currently supported actions include :
94,"- ""allow"" : this stops the evaluation of the rules and lets the response"
94,"pass the check. No further ""http-responseThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"" rules are evaluated for the"
94,current section.
94,"- ""deny"" : this stops the evaluation of the rules and immediately rejects"
94,"the response and emits an HTTP 502 error. No further ""http-responseThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"""
94,rules are evaluated.
94,"- ""add-header"" appends an HTTP header field whose name is specified in"
94,<name> and whose value is defined by <fmt> which follows the log-format
94,rules (see Custom Log Format in section 8.2.4). This may be used to send
94,"a cookie to a client for example, or to pass some internal information."
94,"This rule is not final, so it is possible to add other similar rules."
94,"Note that header addition is performed immediately, so one rule might"
94,reuse the resulting header from a previous rule.
94,"- ""set-header"" does the same as ""add-header"" except that the header name"
94,is first removed if it existed. This is useful when passing security
94,"information to the server, where the header must not be manipulated by"
94,external users.
94,"- ""del-header"" removes all HTTP header fields whose name is specified in"
94,<name>.
94,"- ""replace-header"" matches the regular expression in all occurrences of"
94,"header field <name> according to <match-regex>, and replaces them with"
94,the <replace-fmt> argument. Format characters are allowed in replace-fmt
94,"and work like in <fmt> arguments in ""add-header"". The match is only"
94,case-sensitive. It is important to understand that this action only
94,"considers whole header lines, regardless of the number of values they"
94,may contain. This usage is suited to headers naturally containing commas
94,"in their value, such as Set-Cookie, Expires and so on."
94,Example:
94,http-response replace-header Set-Cookie (C=[^;]*);(.*) \1;ip=%bi;\2
94,applied to:
94,"Set-Cookie: C=1; expires=Tue, 14-Jun-2016 01:40:45 GMT"
94,outputs:
94,"Set-Cookie: C=1;ip=192.168.1.20; expires=Tue, 14-Jun-2016 01:40:45 GMT"
94,assuming the backend IP is 192.168.1.20.
94,"- ""replace-value"" works like ""replace-header"" except that it matches the"
94,regex against every comma-delimited value of the header field <name>
94,instead of the entire header. This is suited for all headers which are
94,allowed to carry more than one value. An example could be the Accept
94,header.
94,Example:
94,http-response replace-value Cache-control ^public$ private
94,applied to:
94,"Cache-Control: max-age=3600, public"
94,outputs:
94,"Cache-Control: max-age=3600, private"
94,"- ""set-status"" replaces the response status code with <status> which must"
94,"be an integer between 100 and 999. Optionally, a custom reason text can be"
94,"provided defined by <str>, or the default reason for the specified code"
94,will be used as a fallback.
94,Example:
94,"# return ""431 Request Header Fields Too Large"""
94,http-response set-status 431
94,"# return ""503 Slow Down"", custom reason"
94,"http-response set-status 503 reason ""Slow Down""."
94,"- ""set-nice"" sets the ""nice"" factor of the current request being processed."
94,It only has effect against the other requests being processed at the same
94,"time. The default value is 0, unless altered by the ""nice"" setting on the"
94,"""bind"" line. The accepted range is -1024..1024. The higher the value, the"
94,nicest the request will be. Lower values will make the request more
94,important than other ones. This can be useful to improve the speed of
94,"some requests, or lower the priority of non-important requests. Using"
94,this setting without prior experimentation can cause some major slowdown.
94,"- ""set-log-level"" is used to change the log level of the current request"
94,when a certain condition is met. Valid levels are the 8 syslog levels
94,"(see the ""logThis keyword is available in sections :Process management and securityAlphabetically sorted keywords reference"" keyword) plus the special level ""silent"" which disables"
94,logging for this request. This rule is not final so the last matching
94,rule wins. This rule can be useful to disable health checks coming from
94,another equipment.
94,"- ""set-tos"" is used to set the TOS or DSCP field value of packets sent to"
94,the client to the value passed in <tos> on platforms which support this.
94,"This value represents the whole 8 bits of the IP TOS field, and can be"
94,"expressed both in decimal or hexadecimal format (prefixed by ""0x""). Note"
94,"that only the 6 higher bits are used in DSCP or TOS, and the two lower"
94,bits are always 0. This can be used to adjust some routing behavior on
94,"border routers based on some information from the request. See RFC 2474,"
94,"2597, 3260 and 4594 for more information."
94,"- ""set-mark"" is used to set the Netfilter MARK on all packets sent to the"
94,client to the value passed in <mark> on platforms which support it. This
94,value is an unsigned 32 bit value which can be matched by netfilter and
94,by the routing table. It can be expressed both in decimal or hexadecimal
94,"format (prefixed by ""0x""). This can be useful to force certain packets to"
94,take a different route (for example a cheaper network path for bulk
94,downloads). This works on Linux kernels 2.6.32 and above and requires
94,admin privileges.
94,"- ""add-acl"" is used to add a new entry into an ACL. The ACL must be loaded"
94,from a file (even a dummy empty file). The file name of the ACL to be
94,"updated is passed between parentheses. It takes one argument: <key fmt>,"
94,"which follows log-format rules, to collect content of the new entry. It"
94,"performs a lookup in the ACL before insertion, to avoid duplicated (or"
94,more) values. This lookup is done by a linear search and can be expensive
94,"with large lists! It is the equivalent of the ""add acl"" command from the"
94,"stats socket, but can be triggered by an HTTP response."
94,"- ""del-acl"" is used to delete an entry from an ACL. The ACL must be loaded"
94,from a file (even a dummy empty file). The file name of the ACL to be
94,"updated is passed between parentheses. It takes one argument: <key fmt>,"
94,"which follows log-format rules, to collect content of the entry to delete."
94,"It is the equivalent of the ""del acl"" command from the stats socket, but"
94,can be triggered by an HTTP response.
94,"- ""del-map"" is used to delete an entry from a MAP. The MAP must be loaded"
94,from a file (even a dummy empty file). The file name of the MAP to be
94,"updated is passed between parentheses. It takes one argument: <key fmt>,"
94,"which follows log-format rules, to collect content of the entry to delete."
94,"It takes one argument: ""file name"" It is the equivalent of the ""del map"""
94,"command from the stats socket, but can be triggered by an HTTP response."
94,"- ""set-map"" is used to add a new entry into a MAP. The MAP must be loaded"
94,from a file (even a dummy empty file). The file name of the MAP to be
94,"updated is passed between parentheses. It takes 2 arguments: <key fmt>,"
94,"which follows log-format rules, used to collect MAP key, and <value fmt>,"
94,"which follows log-format rules, used to collect content for the new entry."
94,"It performs a lookup in the MAP before insertion, to avoid duplicated (or"
94,more) values. This lookup is done by a linear search and can be expensive
94,"with large lists! It is the equivalent of the ""set map"" command from the"
94,"stats socket, but can be triggered by an HTTP response."
94,- capture <sample> id <id> :
94,"captures sample expression <sample> from the response buffer, and converts"
94,it to a string. The resulting string is stored into the next request
94,"""capture"" slot, so it will possibly appear next to some captured HTTP"
94,"headers. It will then automatically appear in the logs, and it will be"
94,possible to extract it using sample fetch rules to feed it into headers or
94,"anything. Please check section 7.3 (Fetching samples) and ""capture"
94,"response header"" for more information."
94,"The keyword ""idThis keyword is available in sections :Alphabetically sorted keywords referenceBind optionsServer and default-server options"" is the id of the capture slot which is used for storing"
94,the string. The capture slot must be defined in an associated frontend.
94,This is useful to run captures in backends. The slot id can be declared by
94,"a previous directive ""http-response capture"" or with the ""declare capture"""
94,keyword.
94,"When using this action in a backend, double check that the relevant"
94,"frontend(s) have the required capture slots otherwise, this rule will be"
94,ignored at run time. This can't be detected at configuration parsing time
94,due to HAProxy's ability to dynamically resolve backend name at runtime.
94,- cache-store <name> :
94,See section 10.2 about cache setup.
94,"- ""redirect"" : this performs an HTTP redirection based on a redirect rule."
94,"This supports a format string similarly to ""http-request redirect"" rules,"
94,"with the exception that only the ""location"" type of redirect is possible"
94,"on the response. See the ""redirect"" keyword for the rule's syntax. When"
94,"a redirect rule is applied during a response, connections to the server"
94,are closed so that no data can be forwarded from the server to the client.
94,- set-var(<var-name>) expr:
94,Is used to set the contents of a variable. The variable is declared
94,inline.
94,<var-name> The name of the variable starts with an indication about
94,its scope. The scopes allowed are:
94,"""proc"" : the variable is shared with the whole process"
94,"""sess"" : the variable is shared with the whole session"
94,"""txn"""
94,: the variable is shared with the transaction
94,(request and response)
94,"""req"""
94,: the variable is shared only during request
94,processing
94,"""res"""
94,: the variable is shared only during response
94,processing
94,This prefix is followed by a name. The separator is a '.'.
94,"The name may only contain characters 'a-z', 'A-Z', '0-9',"
94,'.' and '_'.
94,<expr>
94,Is a standard HAProxy expression formed by a sample-fetch
94,followed by some converters.
94,Example:
94,http-response set-var(sess.last_redir) res.hdr(location)
94,- unset-var(<var-name>) :
94,Is used to unset a variable. See above for details about <var-name>.
94,Example:
94,http-response unset-var(sess.last_redir)
94,- { track-sc0 | track-sc1 | track-sc2 } <key> [table <table>] :
94,enables tracking of sticky counters from current response. Please refer to
94,"""http-request track-sc"" for a complete description. The only difference"
94,"from ""http-request track-sc"" is the <key> sample expression can only make"
94,"use of samples in response (e.g. res.*, status etc.) and samples below"
94,"Layer 6 (e.g. SSL-related samples, see section 7.3.4). If the sample is"
94,"not supported, haproxy will fail and warn while parsing the config."
94,- sc-set-gpt0(<sc-id>) <int> :
94,This action sets the GPT0 tag according to the sticky counter designated
94,by <sc-id> and the value of <int>. The expected result is a boolean. If
94,"an error occurs, this action silently fails and the actions evaluation"
94,continues.
94,- sc-inc-gpc0(<sc-id>):
94,This action increments the GPC0 counter according with the sticky counter
94,"designated by <sc-id>. If an error occurs, this action silently fails and"
94,the actions evaluation continues.
94,"- ""silent-drop"" : this stops the evaluation of the rules and makes the"
94,client-facing connection suddenly disappear using a system-dependent way
94,that tries to prevent the client from being notified. The effect it then
94,that the client still sees an established connection while there's none
94,"on HAProxy. The purpose is to achieve a comparable effect to ""tarpit"""
94,except that it doesn't use any local resource at all on the machine
94,"running HAProxy. It can resist much higher loads than ""tarpit"", and slow"
94,down stronger attackers. It is important to understand the impact of using
94,this mechanism. All stateful equipment placed between the client and
94,"HAProxy (firewalls, proxies, load balancers) will also keep the"
94,established connection for a long time and may suffer from this action.
94,"On modern Linux systems running with enough privileges, the TCP_REPAIR"
94,socket option is used to block the emission of a TCP reset. On other
94,"systems, the socket's TTL is reduced to 1 so that the TCP reset doesn't"
94,"pass the first router, though it's still delivered to local networks. Do"
94,not use it unless you fully understand how it works.
94,- send-spoe-group <engine-name> <group-name> :
94,This action is used to trigger sending of a group of SPOE messages. To do
94,"so, the SPOE engine used to send messages must be defined, as well as the"
94,"SPOE group to send. Of course, the SPOE engine must refer to an existing"
94,"SPOE filter. If not engine name is provided on the SPOE filter line, the"
94,SPOE agent name must be used.
94,<engine-name> The SPOE engine name.
94,<group-name>
94,The SPOE group name as specified in the engine
94,configuration.
94,There is no limit to the number of http-response statements per instance.
94,It is important to know that http-response rules are processed very early in
94,"the HTTP processing, before ""rspdel"" or ""rsprep"" or ""rspadd"" rules. That way,"
94,"headers added by ""add-header""/""set-header"" are visible by almost all further"
94,ACL rules.
94,"Using ""rspadd""/""rspdel""/""rsprep"" to manipulate request headers is discouraged"
94,in newer versions (>= 1.5). But if you need to use regular expression to
94,"delete headers, you can still use ""rspdel"". Also please use"
94,"""http-response deny"" instead of ""rspdeny""."
94,Example:
94,acl key_acl res.hdr(X-Acl-Key) -m found
94,acl myhost hdr(Host) -f myhost.lst
94,http-response add-acl(myhost.lst) %[res.hdr(X-Acl-Key)] if key_acl
94,http-response del-acl(myhost.lst) %[res.hdr(X-Acl-Key)] if key_acl
94,Example:
94,acl value
94,res.hdr(X-Value) -m found
94,"use_backend bk_appli if { hdr(Host),map_str(map.lst) -m found }"
94,http-response set-map(map.lst) %[src] %[res.hdr(X-Value)] if value
94,http-response del-map(map.lst) %[src]
94,if ! value
94,"See also : ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"", section 3.4 about userlists and section 7 about ACL usage."
94,http-reuse { never | safe | aggressive | always }Declare how idle HTTP connections may be shared between requests
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,"By default, a connection established between haproxy and the backend server"
94,belongs to the session that initiated it. The downside is that between the
94,"response and the next request, the connection remains idle and is not used."
94,In many cases for performance reasons it is desirable to make it possible to
94,reuse these idle connections to serve other requests from different sessions.
94,This directive allows to tune this behavior.
94,The argument indicates the desired connection reuse strategy :
94,"- ""never"""
94,: idle connections are never shared between sessions. This is
94,the default choice. It may be enforced to cancel a different
94,strategy inherited from a defaults section or for
94,"troubleshooting. For example, if an old bogus application"
94,considers that multiple requests over the same connection come
94,from the same client and it is not possible to fix the
94,"application, it may be desirable to disable connection sharing"
94,in a single backend. An example of such an application could
94,be an old haproxy using cookie insertion in tunnel mode and
94,not checking any request past the first one.
94,"- ""safe"""
94,: this is the recommended strategy. The first request of a
94,"session is always sent over its own connection, and only"
94,subsequent requests may be dispatched over other existing
94,connections. This ensures that in case the server closes the
94,"connection when the request is being sent, the browser can"
94,decide to silently retry it. Since it is exactly equivalent to
94,"regular keep-alive, there should be no side effects."
94,"- ""aggressive"" : this mode may be useful in webservices environments where"
94,all servers are not necessarily known and where it would be
94,appreciable to deliver most first requests over existing
94,"connections. In this case, first requests are only delivered"
94,"over existing connections that have been reused at least once,"
94,proving that the server correctly supports connection reuse.
94,It should only be used when it's sure that the client can
94,retry a failed request once in a while and where the benefit
94,of aggressive connection reuse significantly outweighs the
94,downsides of rare connection failures.
94,"- ""always"" : this mode is only recommended when the path to the server is"
94,known for never breaking existing connections quickly after
94,releasing them. It allows the first request of a session to be
94,sent to an existing connection. This can provide a significant
94,"performance increase over the ""safe"" strategy when the backend"
94,"is a cache farm, since such components tend to show a"
94,consistent behavior and will benefit from the connection
94,"sharing. It is recommended that the ""http-keep-alive"" timeout"
94,remains low in this mode so that no dead connections remain
94,"usable. In most cases, this will lead to the same performance"
94,"gains as ""aggressive"" but with more risks. It should only be"
94,"used when it improves the situation over ""aggressive""."
94,"When http connection sharing is enabled, a great care is taken to respect the"
94,connection properties and compatibility. Specifically :
94,"- connections made with ""usesrc"" followed by a client-dependent value"
94,"(""client"", ""clientip"", ""hdr_ip"") are marked private and never shared;"
94,- connections sent to a server with a TLS SNI extension are marked private
94,and are never shared;
94,- connections with certain bogus authentication schemes (relying on the
94,"connection) like NTLM are detected, marked private and are never shared;"
94,"No connection pool is involved, once a session dies, the last idle connection"
94,it was attached to is deleted at the same time. This ensures that connections
94,may not last after all sessions are closed.
94,"Note: connection reuse improves the accuracy of the ""server maxconn"" setting,"
94,because almost no new connection will be established while idle connections
94,"remain available. This is particularly true with the ""always"" strategy."
94,"See also : ""option http-keep-alive"", ""server maxconn"""
94,http-send-name-header [<header>]Add the server name to a request. Use the header string given by <header>
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments :<header>
94,The header string to use to send the server name
94,"The ""http-send-name-header"" statement causes the header field named <header>"
94,to be set to the name of the target server at the moment the request is about
94,to be sent on the wire. Any existing occurrences of this header are removed.
94,"Upon retries and redispatches, the header field is updated to always reflect"
94,the server being attempted to connect to. Given that this header is modified
94,"very late in the connection setup, it may have unexpected effects on already"
94,modified headers. For example using it with transport-level header such as
94,"connection, content-length, transfer-encoding and so on will likely result in"
94,invalid requests being sent to the server. Additionally it has been reported
94,that this directive is currently being used as a way to overwrite the Host
94,header field in outgoing requests; while this trick has been known to work
94,"as a side effect of the feature for some time, it is not officially supported"
94,and might possibly not work anymore in a future version depending on the
94,technical difficulties this feature induces. A long-term solution instead
94,consists in fixing the application which required this trick so that it binds
94,to the correct host name.
94,"See also : ""server"""
94,id <value>Set a persistent ID to a proxy.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,Arguments : none
94,Set a persistent ID for the proxy. This ID must be unique and positive.
94,An unused ID will automatically be assigned if unset. The first assigned
94,value will be 1. This ID is currently only returned in statistics.
94,ignore-persist { if | unless } <condition>Declare a condition to ignore persistence
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,"By default, when cookie persistence is enabled, every requests containing"
94,the cookie are unconditionally persistent (assuming the target server is up
94,and running).
94,"The ""ignore-persist"" statement allows one to declare various ACL-based"
94,"conditions which, when met, will cause a request to ignore persistence."
94,"This is sometimes useful to load balance requests for static files, which"
94,often don't require persistence. This can also be used to fully disable
94,"persistence for a specific User-Agent (for example, some web crawler bots)."
94,"The persistence is ignored when an ""if"" condition is met, or unless an"
94,"""unless"" condition is met."
94,Example:
94,acl url_static
94,path_beg
94,/static /images /img /css
94,acl url_static
94,path_end
94,.gif .png .jpg .css .js
94,ignore-persist
94,if url_static
94,"See also : ""force-persist"", ""cookieThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server optionsFetching HTTP samples (Layer 7)"", and section 7 about ACL usage."
94,load-server-state-from-file { global | local | none }Allow seamless reload of HAProxy
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,This directive points HAProxy to a file where server state from previous
94,"running process has been saved. That way, when starting up, before handling"
94,"traffic, the new process can apply old states to servers exactly has if no"
94,"reload occurred. The purpose of the ""load-server-state-from-file"" directive is"
94,"to tell haproxy which file to use. For now, only 2 arguments to either prevent"
94,loading state or load states from a file containing all backends and servers.
94,"The state file can be generated by running the command ""show servers state"""
94,over the stats socket and redirect output.
94,"The format of the file is versioned and is very specific. To understand it,"
94,"please read the documentation of the ""show servers state"" command (chapter"
94,9.3 of Management Guide).
94,Arguments:global
94,load the content of the file pointed by the global directive
94,"named ""server-state-file""."
94,local
94,load the content of the file pointed by the directive
94,"""server-state-file-name"" if set. If not set, then the backend"
94,name is used as a file name.
94,none
94,don't load any stat for this backend
94,Notes:
94,"- server's IP address is preserved across reloads by default, but the"
94,"order can be changed thanks to the server's ""init-addr"" setting. This"
94,means that an IP address change performed on the CLI at run time will
94,"be preserved, and that any change to the local resolver (e.g. /etc/hosts)"
94,will possibly not have any effect if the state file is in use.
94,- server's weight is applied from previous running process unless it has
94,has changed between previous and new configuration files.
94,Example:
94,Minimal configurationglobal
94,stats socket /tmp/socket
94,server-state-file /tmp/server_state
94,defaults
94,load-server-state-from-file global
94,backend bk
94,server s1 127.0.0.1:22 check weight 11
94,server s2 127.0.0.1:22 check weight 12
94,Then one can run :
94,"socat /tmp/socket - <<< ""show servers state"" > /tmp/server_state"
94,Content of the file /tmp/server_state would be like this:
94,# <field names skipped for the doc example>
94,1 bk 1 s1 127.0.0.1 2 0 11 11 4 6 3 4 6 0 0
94,1 bk 2 s2 127.0.0.1 2 0 12 12 4 6 3 4 6 0 0
94,Example:
94,Minimal configurationglobal
94,stats socket /tmp/socket
94,server-state-base /etc/haproxy/states
94,defaults
94,load-server-state-from-file local
94,backend bk
94,server s1 127.0.0.1:22 check weight 11
94,server s2 127.0.0.1:22 check weight 12
94,Then one can run :
94,"socat /tmp/socket - <<< ""show servers state bk"" > /etc/haproxy/states/bk"
94,Content of the file /etc/haproxy/states/bk would be like this:
94,# <field names skipped for the doc example>
94,1 bk 1 s1 127.0.0.1 2 0 11 11 4 6 3 4 6 0 0
94,1 bk 2 s2 127.0.0.1 2 0 12 12 4 6 3 4 6 0 0
94,"See also: ""server-state-file"", ""server-state-file-name"", and ""show servers state"""
94,log globallog <address> [len <length>] <facility> [<level> [<minlevel>]]no logEnable per-instance logging of events and traffic.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Prefix :
94,"should be used when the logger list must be flushed. For example,"
94,if you don't want to inherit from the default logger list. This
94,prefix does not allow arguments.
94,Arguments :global
94,should be used when the instance's logging parameters are the
94,"same as the global ones. This is the most common usage. ""global"""
94,"replaces <address>, <facility> and <level> with those of the log"
94,"entries found in the ""global"" section. Only one ""log global"""
94,"statement may be used per instance, and this form takes no other"
94,parameter.
94,<address>
94,indicates where to send the logs. It takes the same format as
94,"for the ""global"" section's logs, and can be one of :"
94,- An IPv4 address optionally followed by a colon (':') and a UDP
94,"port. If no port is specified, 514 is used by default (the"
94,standard syslog port).
94,- An IPv6 address followed by a colon (':') and optionally a UDP
94,"port. If no port is specified, 514 is used by default (the"
94,standard syslog port).
94,"- A filesystem path to a UNIX domain socket, keeping in mind"
94,considerations for chroot (be sure the path is accessible
94,inside the chroot) and uid/gid (be sure the path is
94,appropriately writable).
94,You may want to reference some environment variables in the
94,"address parameter, see section 2.3 about environment variables."
94,<length>
94,is an optional maximum line length. Log lines larger than this
94,value will be truncated before being sent. The reason is that
94,syslog servers act differently on log line length. All servers
94,"support the default value of 1024, but some servers simply drop"
94,larger lines while others do log them. If a server supports long
94,"lines, it may make sense to set this value here in order to avoid"
94,"truncating long lines. Similarly, if a server drops long lines,"
94,it is preferable to truncate them before sending them. Accepted
94,values are 80 to 65535 inclusive. The default value of 1024 is
94,generally fine for all standard usages. Some specific cases of
94,long captures or JSON-formatted logs may require larger values.
94,<facility> must be one of the 24 standard syslog facilities :
94,kern
94,user
94,mail
94,daemon auth
94,syslog lpr
94,news
94,uucp
94,cron
94,auth2
94,ftp
94,ntp
94,audit
94,alert
94,cron2
94,local0 local1 local2 local3 local4 local5 local6 local7
94,<level>
94,is optional and can be specified to filter outgoing messages. By
94,"default, all messages are sent. If a level is specified, only"
94,messages with a severity at least as important as this level
94,will be sent. An optional minimum level can be specified. If it
94,"is set, logs emitted with a more severe level than this one will"
94,"be capped to this level. This is used to avoid sending ""emerg"""
94,messages on all terminals on some default syslog configurations.
94,Eight levels are known :
94,emerg
94,alert
94,crit
94,err
94,warning notice info
94,debug
94,It is important to keep in mind that it is the frontend which decides what to
94,"log from a connection, and that in case of content switching, the log entries"
94,"from the backend will be ignored. Connections are logged at level ""info""."
94,"However, backend log declaration define how and where servers status changes"
94,"will be logged. Level ""notice"" will be used to indicate a server going up,"
94,"""warning"" will be used for termination signals and definitive service"
94,"termination, and ""alert"" will be used for when a server goes down."
94,"Note : According to RFC3164, messages are truncated to 1024 bytes before"
94,being emitted.
94,Example :
94,log global
94,log 127.0.0.1:514 local0 notice
94,# only send important events
94,log 127.0.0.1:514 local0 notice notice
94,# same but limit output level
94,"log ""${LOCAL_SYSLOG}:514"" local0 notice"
94,# send to local server
94,log-format <string>Specifies the log format string to use for traffic logs
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,This directive specifies the log format string that will be used for all logs
94,resulting from traffic passing through the frontend using this line. If the
94,"directive is used in a defaults section, all subsequent frontends will use"
94,the same log format. Please see section 8.2.4 which covers the log format
94,string in depth.
94,"""log-format"" directive overrides previous ""option tcplog"", ""log-format"" and"
94,"""option httplog"" directives."
94,log-format-sd <string>Specifies the RFC5424 structured-data log format string
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,This directive specifies the RFC5424 structured-data log format string that
94,will be used for all logs resulting from traffic passing through the frontend
94,"using this line. If the directive is used in a defaults section, all"
94,subsequent frontends will use the same log format. Please see section 8.2.4
94,which covers the log format string in depth.
94,See https://tools.ietf.org/html/rfc5424#section-6.3 for more information
94,about the RFC5424 structured-data part.
94,Note : This log format string will be used only for loggers that have set
94,"log format to ""rfc5424""."
94,Example :
94,"log-format-sd [exampleSDID@1234\ bytes=\""%B\""\ status=\""%ST\""]"
94,log-tag <string>Specifies the log tag to use for all outgoing logs
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Sets the tag field in the syslog header to this string. It defaults to the
94,"log-tag set in the global section, otherwise the program name as launched"
94,"from the command line, which usually is ""haproxy"". Sometimes it can be useful"
94,"to differentiate between multiple processes running on the same host, or to"
94,"differentiate customer instances running in the same process. In the backend,"
94,"logs about servers up/down will use this tag. As a hint, it can be convenient"
94,to set a log-tag related to a hosted customer in a defaults section then put
94,"all the frontends and backends for that customer, then start another customer"
94,"in a new defaults section. See also the global ""log-tagThis keyword is available in sections :Process management and securityAlphabetically sorted keywords reference"" directive."
94,max-keep-alive-queue <value>Set the maximum server queue size for maintaining keep-alive connections
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,"HTTP keep-alive tries to reuse the same server connection whenever possible,"
94,"but sometimes it can be counter-productive, for example if a server has a lot"
94,of connections while other ones are idle. This is especially true for static
94,servers.
94,The purpose of this setting is to set a threshold on the number of queued
94,connections at which haproxy stops trying to reuse the same server and prefers
94,"to find another one. The default value, -1, means there is no limit. A value"
94,of zero means that keep-alive requests will never be queued. For very close
94,servers which can be reached with a low latency and which are not sensible to
94,"breaking keep-alive, a low value is recommended (e.g. local static server can"
94,"use a value of 10 or less). For remote servers suffering from a high latency,"
94,higher values might be needed to cover for the latency and/or the cost of
94,picking a different server.
94,Note that this has no impact on responses which are maintained to the same
94,server consecutively to a 401 response. They will still go to the same server
94,even if they have to be queued.
94,"See also : ""option http-server-close"", ""option prefer-last-server"", server ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"" and cookie persistence."
94,maxconn <conns>Fix the maximum number of concurrent connections on a frontend
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments :<conns>
94,is the maximum number of concurrent connections the frontend will
94,accept to serve. Excess connections will be queued by the system
94,in the socket's listen queue and will be served once a connection
94,closes.
94,"If the system supports it, it can be useful on big sites to raise this limit"
94,"very high so that haproxy manages connection queues, instead of leaving the"
94,clients with unanswered connection attempts. This value should not exceed the
94,"global maxconn. Also, keep in mind that a connection contains two buffers"
94,"of tune.bufsize (16kB by default) each, as well as some other data resulting"
94,in about 33 kB of RAM being consumed per established connection. That means
94,that a medium system equipped with 1GB of RAM can withstand around
94,20000-25000 concurrent connections if properly tuned.
94,"Also, when <conns> is set to large values, it is possible that the servers"
94,"are not sized to accept such loads, and for this reason it is generally wise"
94,to assign them some reasonable connection limits.
94,"By default, this value is set to 2000."
94,"See also : ""server"", global section's ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"", ""fullconn"""
94,mode { tcp|http|health }Set the running mode or protocol of the instance
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :tcp
94,The instance will work in pure TCP mode. A full-duplex connection
94,"will be established between clients and servers, and no layer 7"
94,examination will be performed. This is the default mode. It
94,"should be used for SSL, SSH, SMTP, ..."
94,http
94,The instance will work in HTTP mode. The client request will be
94,analyzed in depth before connecting to any server. Any request
94,"which is not RFC-compliant will be rejected. Layer 7 filtering,"
94,processing and switching will be possible. This is the mode which
94,brings HAProxy most of its value.
94,health
94,"The instance will work in ""health"" mode. It will just reply ""OK"""
94,"to incoming connections and close the connection. Alternatively,"
94,"If the ""httpchk"" option is set, ""HTTP/1.0 200 OK"" will be sent"
94,instead. Nothing will be logged in either case. This mode is used
94,to reply to external components health checks. This mode is
94,deprecated and should not be used anymore as it is possible to do
94,the same and even better by combining TCP or HTTP modes with the
94,"""monitor"" keyword."
94,"When doing content switching, it is mandatory that the frontend and the"
94,"backend are in the same mode (generally HTTP), otherwise the configuration"
94,will be refused.
94,Example :
94,defaults http_instances
94,mode http
94,"See also : ""monitor"", ""monitor-net"""
94,monitor fail { if | unless } <condition>Add a condition to report a failure to a monitor HTTP request.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesno
94,Arguments :if <cond>
94,"the monitor request will fail if the condition is satisfied,"
94,and will succeed otherwise. The condition should describe a
94,combined test which must induce a failure if all conditions
94,"are met, for instance a low number of servers both in a"
94,backend and its backup.
94,unless <cond> the monitor request will succeed only if the condition is
94,"satisfied, and will fail otherwise. Such a condition may be"
94,based on a test on the presence of a minimum number of active
94,servers in a list of backends.
94,This statement adds a condition which can force the response to a monitor
94,"request to report a failure. By default, when an external component queries"
94,"the URI dedicated to monitoring, a 200 response is returned. When one of the"
94,"conditions above is met, haproxy will return 503 instead of 200. This is"
94,very useful to report a site failure to an external component which may base
94,routing advertisements between multiple sites on the availability reported by
94,"haproxy. In this case, one would rely on an ACL involving the ""nbsrvThis keyword is available in sections :ConvertersFetching samples from internal states"""
94,"criterion. Note that ""monitor fail"" only works in HTTP mode. Both status"
94,"messages may be tweaked using ""errorfile"" or ""errorloc"" if needed."
94,Example:
94,frontend www
94,mode http
94,acl site_dead nbsrv(dynamic) lt 2
94,acl site_dead nbsrv(static)
94,lt 2
94,monitor-uri
94,/site_alive
94,monitor fail
94,if site_dead
94,"See also : ""monitor-net"", ""monitor-uri"", ""errorfile"", ""errorloc"""
94,monitor-net <source>Declare a source network which is limited to monitor requests
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments :<source>
94,is the source IPv4 address or network which will only be able to
94,get monitor responses to any request. It can be either an IPv4
94,"address, a host name, or an address followed by a slash ('/')"
94,followed by a mask.
94,"In TCP mode, any connection coming from a source matching <source> will cause"
94,the connection to be immediately closed without any log. This allows another
94,"equipment to probe the port and verify that it is still listening, without"
94,forwarding the connection to a remote server.
94,"In HTTP mode, a connection coming from a source matching <source> will be"
94,"accepted, the following response will be sent without waiting for a request,"
94,"then the connection will be closed : ""HTTP/1.0 200 OK"". This is normally"
94,enough for any front-end HTTP probe to detect that the service is UP and
94,running without forwarding the request to a backend server. Note that this
94,"response is sent in raw format, without any transformation. This is important"
94,as it means that it will not be SSL-encrypted on SSL listeners.
94,"Monitor requests are processed very early, just after tcp-request connection"
94,ACLs which are the only ones able to block them. These connections are short
94,"lived and never wait for any data from the client. They cannot be logged, and"
94,it is the intended purpose. They are only used to report HAProxy's health to
94,"an upper component, nothing more. Please note that ""monitor fail"" rules do"
94,"not apply to connections intercepted by ""monitor-net""."
94,"Last, please note that only one ""monitor-net"" statement can be specified in"
94,"a frontend. If more than one is found, only the last one will be considered."
94,Example :
94,# addresses .252 and .253 are just probing us.
94,frontend www
94,monitor-net 192.168.0.252/31
94,"See also : ""monitor fail"", ""monitor-uri"""
94,monitor-uri <uri>Intercept a URI used by external components' monitor requests
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments :<uri>
94,is the exact URI which we want to intercept to return HAProxy's
94,health status instead of forwarding the request.
94,"When an HTTP request referencing <uri> will be received on a frontend,"
94,"HAProxy will not forward it nor log it, but instead will return either"
94,"""HTTP/1.0 200 OK"" or ""HTTP/1.0 503 Service unavailable"", depending on failure"
94,"conditions defined with ""monitor fail"". This is normally enough for any"
94,front-end HTTP probe to detect that the service is UP and running without
94,"forwarding the request to a backend server. Note that the HTTP method, the"
94,"version and all headers are ignored, but the request must at least be valid"
94,at the HTTP level. This keyword may only be used with an HTTP-mode frontend.
94,Monitor requests are processed very early. It is not possible to block nor
94,"divert them using ACLs. They cannot be logged either, and it is the intended"
94,"purpose. They are only used to report HAProxy's health to an upper component,"
94,"nothing more. However, it is possible to add any number of conditions using"
94,"""monitor fail"" and ACLs so that the result can be adjusted to whatever check"
94,can be imagined (most often the number of available servers in a backend).
94,Example :
94,# Use /haproxy_test to report haproxy's status
94,frontend www
94,mode http
94,monitor-uri /haproxy_test
94,"See also : ""monitor fail"", ""monitor-net"""
94,option abortoncloseno option abortoncloseEnable or disable early dropping of aborted requests pending in queues.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : none
94,"In presence of very high loads, the servers will take some time to respond."
94,"The per-instance connection queue will inflate, and the response time will"
94,increase respective to the size of the queue times the average per-session
94,"response time. When clients will wait for more than a few seconds, they will"
94,"often hit the ""STOP"" button on their browser, leaving a useless request in"
94,"the queue, and slowing down other users, and the servers as well, because the"
94,"request will eventually be served, then aborted at the first error"
94,encountered while delivering the response.
94,As there is no way to distinguish between a full STOP and a simple output
94,"close on the client side, HTTP agents should be conservative and consider"
94,that the client might only have closed its output channel while waiting for
94,"the response. However, this introduces risks of congestion when lots of users"
94,"do the same, and is completely useless nowadays because probably no client at"
94,all will close the session while waiting for the response. Some HTTP agents
94,"support this behavior (Squid, Apache, HAProxy), and others do not (TUX, most"
94,hardware-based load balancers). So the probability for a closed input channel
94,"to represent a user hitting the ""STOP"" button is close to 100%, and the risk"
94,of being the single component to break rare but valid traffic is extremely
94,"low, which adds to the temptation to be able to abort a session early while"
94,still not served and not pollute the servers.
94,"In HAProxy, the user can choose the desired behavior using the option"
94,"""abortonclose"". By default (without the option) the behavior is HTTP"
94,compliant and aborted requests will be served. But when the option is
94,"specified, a session with an incoming channel closed will be aborted while"
94,"it is still possible, either pending in the queue for a connection slot, or"
94,during the connection establishment if the server has not yet acknowledged
94,the connection request. This considerably reduces the queue size and the load
94,"on saturated servers when users are tempted to click on STOP, which in turn"
94,reduces the response time for other users.
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""timeout queue"" and server's ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"" and ""maxqueue"" parameters"
94,option accept-invalid-http-requestno option accept-invalid-http-requestEnable or disable relaxing of HTTP request parsing
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments : none
94,"By default, HAProxy complies with RFC7230 in terms of message parsing. This"
94,means that invalid characters in header names are not permitted and cause an
94,error to be returned to the client. This is the desired behavior as such
94,forbidden characters are essentially used to build attacks exploiting server
94,"weaknesses, and bypass security filtering. Sometimes, a buggy browser or"
94,"server will emit invalid header names for whatever reason (configuration,"
94,"implementation) and the issue will not be immediately fixed. In such a case,"
94,it is possible to relax HAProxy's header name parser to accept any character
94,"even if that does not make sense, by specifying this option. Similarly, the"
94,"list of characters allowed to appear in a URI is well defined by RFC3986, and"
94,"chars 0-31, 32 (space), 34 ('""'), 60 ('<'), 62 ('>'), 92 ('\'), 94 ('^'), 96"
94,"('`'), 123 ('{'), 124 ('|'), 125 ('}'), 127 (delete) and anything above are"
94,"not allowed at all. HAProxy always blocks a number of them (0..32, 127). The"
94,remaining ones are blocked by default unless this option is enabled. This
94,"option also relaxes the test on the HTTP version, it allows HTTP/0.9 requests"
94,to pass through (no version specified) and multiple digits for both the major
94,and the minor version.
94,This option should never be enabled by default as it hides application bugs
94,and open security breaches. It should only be deployed after a problem has
94,been confirmed.
94,"When this option is enabled, erroneous header names will still be accepted in"
94,"requests, but the complete request will be captured in order to permit later"
94,"analysis using the ""show errors"" request on the UNIX stats socket. Similarly,"
94,requests containing invalid chars in the URI part will be logged. Doing this
94,also helps confirming that the issue has been solved.
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""option accept-invalid-http-response"" and ""show errors"" on the stats socket."
94,option accept-invalid-http-responseno option accept-invalid-http-responseEnable or disable relaxing of HTTP response parsing
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : none
94,"By default, HAProxy complies with RFC7230 in terms of message parsing. This"
94,means that invalid characters in header names are not permitted and cause an
94,error to be returned to the client. This is the desired behavior as such
94,forbidden characters are essentially used to build attacks exploiting server
94,"weaknesses, and bypass security filtering. Sometimes, a buggy browser or"
94,"server will emit invalid header names for whatever reason (configuration,"
94,"implementation) and the issue will not be immediately fixed. In such a case,"
94,it is possible to relax HAProxy's header name parser to accept any character
94,"even if that does not make sense, by specifying this option. This option also"
94,"relaxes the test on the HTTP version format, it allows multiple digits for"
94,both the major and the minor version.
94,This option should never be enabled by default as it hides application bugs
94,and open security breaches. It should only be deployed after a problem has
94,been confirmed.
94,"When this option is enabled, erroneous header names will still be accepted in"
94,"responses, but the complete response will be captured in order to permit"
94,"later analysis using the ""show errors"" request on the UNIX stats socket."
94,Doing this also helps confirming that the issue has been solved.
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""option accept-invalid-http-request"" and ""show errors"" on the stats socket."
94,option allbackupsno option allbackupsUse either all backup servers at a time or only the first one
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : none
94,"By default, the first operational backup server gets all traffic when normal"
94,"servers are all down. Sometimes, it may be preferred to use multiple backups"
94,"at once, because one will not be enough. When ""option allbackups"" is enabled,"
94,the load balancing will be performed among all backup servers when all normal
94,ones are unavailable. The same load balancing algorithm will be used and the
94,"servers' weights will be respected. Thus, there will not be any priority"
94,order between the backup servers anymore.
94,This option is mostly used with static server farms dedicated to return a
94,"""sorry"" page when an application is completely offline."
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,option checkcacheno option checkcacheAnalyze all server responses and block responses with cacheable cookies
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : none
94,Some high-level frameworks set application cookies everywhere and do not
94,always let enough control to the developer to manage how the responses should
94,"be cached. When a session cookie is returned on a cacheable object, there is a"
94,high risk of session crossing or stealing between users traversing the same
94,"caches. In some situations, it is better to block the response than to let"
94,some sensitive session information go in the wild.
94,"The option ""checkcache"" enables deep inspection of all server responses for"
94,strict compliance with HTTP specification in terms of cacheability. It
94,"carefully checks ""Cache-control"", ""Pragma"" and ""Set-cookie"" headers in server"
94,response to check if there's a risk of caching a cookie on a client-side
94,"proxy. When this option is enabled, the only responses which can be delivered"
94,to the client are :
94,"- all those without ""Set-Cookie"" header;"
94,"- all those with a return code other than 200, 203, 204, 206, 300, 301,"
94,"404, 405, 410, 414, 501, provided that the server has not set a"
94,"""Cache-control: public"" header field;"
94,"- all those that result from a request using a method other than GET, HEAD,"
94,"OPTIONS, TRACE, provided that the server has not set a 'Cache-Control:"
94,public' header field;
94,- those with a 'Pragma: no-cache' header
94,- those with a 'Cache-control: private' header
94,- those with a 'Cache-control: no-store' header
94,- those with a 'Cache-control: max-age=0' header
94,- those with a 'Cache-control: s-maxage=0' header
94,- those with a 'Cache-control: no-cache' header
94,"- those with a 'Cache-control: no-cache=""set-cookie""' header"
94,"- those with a 'Cache-control: no-cache=""set-cookie,' header"
94,(allowing other fields after set-cookie)
94,"If a response doesn't respect these requirements, then it will be blocked"
94,"just as if it was from an ""rspdeny"" filter, with an ""HTTP 502 bad gateway""."
94,"The session state shows ""PH--"" meaning that the proxy blocked the response"
94,"during headers processing. Additionally, an alert will be sent in the logs so"
94,that admins are informed that there's something to be fixed.
94,"Due to the high impact on the application, the application should be tested"
94,in depth with the option enabled before going to production. It is also a
94,"good practice to always activate it during tests, even if it is not used in"
94,"production, as it will report potentially dangerous application behaviors."
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,option clitcpkano option clitcpkaEnable or disable the sending of TCP keepalive packets on the client side
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments : none
94,When there is a firewall or any session-aware component between a client and
94,"a server, and when the protocol involves very long sessions with long idle"
94,"periods (e.g. remote desktops), there is a risk that one of the intermediate"
94,components decides to expire a session which has remained idle for too long.
94,Enabling socket-level TCP keep-alives makes the system regularly send packets
94,"to the other end of the connection, leaving it active. The delay between"
94,keep-alive probes is controlled by the system only and depends both on the
94,operating system and its tuning parameters.
94,It is important to understand that keep-alive packets are neither emitted nor
94,received at the application level. It is only the network stacks which sees
94,"them. For this reason, even if one side of the proxy already uses keep-alives"
94,"to maintain its connection alive, those keep-alive packets will not be"
94,forwarded to the other side of the proxy.
94,Please note that this has nothing to do with HTTP keep-alive.
94,"Using option ""clitcpka"" enables the emission of TCP keep-alive probes on the"
94,"client side of a connection, which should help when session expirations are"
94,noticed between HAProxy and a client.
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""option srvtcpka"", ""option tcpka"""
94,option contstatsEnable continuous traffic statistics updates
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments : none
94,"By default, counters used for statistics calculation are incremented"
94,only when a session finishes. It works quite well when serving small
94,"objects, but with big ones (for example large images or archives) or"
94,"with A/V streaming, a graph generated from haproxy counters looks like"
94,a hedgehog. With this option enabled counters get incremented frequently
94,"along the session, typically every 5 seconds, which is often enough to"
94,produce clean graphs. Recounting touches a hotpath directly so it is not
94,"not enabled by default, as it can cause a lot of wakeups for very large"
94,session counts and cause a small performance drop.
94,"option dontlog-normalno option dontlog-normalEnable or disable logging of normal, successful connections"
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments : none
94,There are large sites dealing with several thousand connections per second
94,and for which logging is a major pain. Some of them are even forced to turn
94,logs off and cannot debug production issues. Setting this option ensures that
94,"normal connections, those which experience no error, no timeout, no retry nor"
94,"redispatch, will not be logged. This leaves disk space for anomalies. In HTTP"
94,"mode, the response status code is checked and return codes 5xx will still be"
94,logged.
94,"It is strongly discouraged to use this option as most of the time, the key to"
94,complex issues is in the normal logs which will not be logged here. If you
94,"need to separate logs, see the ""log-separate-errors"" option instead."
94,"See also : ""logThis keyword is available in sections :Process management and securityAlphabetically sorted keywords reference"", ""dontlognull"", ""log-separate-errors"" and section 8 about logging."
94,option dontlognullno option dontlognullEnable or disable logging of null connections
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments : none
94,"In certain environments, there are components which will regularly connect to"
94,various systems to ensure that they are still alive. It can be the case from
94,"another load balancer as well as from monitoring systems. By default, even a"
94,simple port probe or scan will produce a log. If those connections pollute
94,"the logs too much, it is possible to enable option ""dontlognull"" to indicate"
94,"that a connection on which no data has been transferred will not be logged,"
94,which typically corresponds to those probes. Note that errors will still be
94,returned to the client and accounted for in the stats. If this is not what is
94,"desired, option http-ignore-probes can be used instead."
94,It is generally recommended not to use this option in uncontrolled
94,"environments (e.g. internet), otherwise scans and other malicious activities"
94,would not be logged.
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""logThis keyword is available in sections :Process management and securityAlphabetically sorted keywords reference"", ""http-ignore-probes"", ""monitor-net"", ""monitor-uri"", and section 8 about logging."
94,option forcecloseno option forcecloseEnable or disable active connection closing after response is transferred.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,Some HTTP servers do not necessarily close the connections when they receive
94,"the ""Connection: close"" set by ""option httpclose"", and if the client does not"
94,"close either, then the connection remains open till the timeout expires. This"
94,causes high number of simultaneous connections on the servers and shows high
94,global session times in the logs.
94,"When this happens, it is possible to use ""option forceclose"". It will"
94,actively close the outgoing server channel as soon as the server has finished
94,"to respond and release some resources earlier than with ""option httpclose""."
94,"This option may also be combined with ""option http-pretend-keepalive"", which"
94,"will disable sending of the ""Connection: close"" header, but will still cause"
94,the connection to be closed once the whole response is received.
94,"This option disables and replaces any previous ""option httpclose"", ""option"
94,"http-server-close"", ""option http-keep-alive"", or ""option http-tunnel""."
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""option httpclose"" and ""option http-pretend-keepalive"""
94,option forwardfor [ except <network> ] [ header <name> ] [ if-none ]Enable insertion of the X-Forwarded-For header to requests sent to servers
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :<network> is an optional argument used to disable this option for sources
94,matching <network>
94,<name>
94,"an optional argument to specify a different ""X-Forwarded-For"""
94,header name.
94,"Since HAProxy works in reverse-proxy mode, the servers see its IP address as"
94,their client address. This is sometimes annoying when the client's IP address
94,"is expected in server logs. To solve this problem, the well-known HTTP header"
94,"""X-Forwarded-For"" may be added by HAProxy to all requests sent to the server."
94,This header contains a value representing the client's IP address. Since this
94,"header is always appended at the end of the existing header list, the server"
94,must be configured to always use the last occurrence of this header only. See
94,the server's manual to find how to enable use of this standard header. Note
94,"that only the last occurrence of the header must be used, since it is really"
94,possible that the client has already brought one.
94,"The keyword ""header"" may be used to supply a different header name to replace"
94,"the default ""X-Forwarded-For"". This can be useful where you might already"
94,"have a ""X-Forwarded-For"" header from a different application (e.g. stunnel),"
94,and you need preserve it. Also if your backend server doesn't use the
94,"""X-Forwarded-For"" header and requires different one (e.g. Zeus Web Servers"
94,"require ""X-Cluster-Client-IP"")."
94,"Sometimes, a same HAProxy instance may be shared between a direct client"
94,access and a reverse-proxy access (for instance when an SSL reverse-proxy is
94,used to decrypt HTTPS traffic). It is possible to disable the addition of the
94,"header for a known source address or network by adding the ""except"" keyword"
94,"followed by the network address. In this case, any source IP matching the"
94,network will not cause an addition of this header. Most common uses are with
94,private networks or 127.0.0.1.
94,"Alternatively, the keyword ""if-none"" states that the header will only be"
94,added if it is not present. This should only be used in perfectly trusted
94,"environment, as this might cause a security issue if headers reaching haproxy"
94,are under the control of the end-user.
94,This option may be specified either in the frontend or in the backend. If at
94,"least one of them uses it, the header will be added. Note that the backend's"
94,setting of the header subargument takes precedence over the frontend's if
94,"both are defined. In the case of the ""if-none"" argument, if at least one of"
94,"the frontend or the backend does not specify it, it wants the addition to be"
94,"mandatory, so it wins."
94,Example :
94,# Public HTTP address also used by stunnel on the same machine
94,frontend www
94,mode http
94,option forwardfor except 127.0.0.1
94,# stunnel already adds the header
94,# Those servers want the IP Address in X-Client
94,backend www
94,mode http
94,option forwardfor header X-Client
94,"See also : ""option httpclose"", ""option http-server-close"", ""option forceclose"", ""option http-keep-alive"""
94,option http-buffer-requestno option http-buffer-requestEnable or disable waiting for whole HTTP request body before proceeding
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,It is sometimes desirable to wait for the body of an HTTP request before
94,"taking a decision. This is what is being done by ""balance url_param"" for"
94,example. The first use case is to buffer requests from slow clients before
94,connecting to the server. Another use case consists in taking the routing
94,decision based on the request body's contents. This option placed in a
94,frontend or backend forces the HTTP processing to wait until either the whole
94,"body is received, or the request buffer is full, or the first chunk is"
94,complete in case of chunked encoding. It can have undesired side effects with
94,some applications abusing HTTP by expecting unbuffered transmissions between
94,"the frontend and the backend, so this should definitely not be used by"
94,default.
94,"See also : ""option http-no-delay"", ""timeout http-request"""
94,option http-ignore-probesno option http-ignore-probesEnable or disable logging of null connections and request timeouts
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments : none
94,"Recently some browsers started to implement a ""pre-connect"" feature"
94,consisting in speculatively connecting to some recently visited web sites
94,just in case the user would like to visit them. This results in many
94,"connections being established to web sites, which end up in 408 Request"
94,"Timeout if the timeout strikes first, or 400 Bad Request when the browser"
94,decides to close them first. These ones pollute the log and feed the error
94,"counters. There was already ""option dontlognull"" but it's insufficient in"
94,"this case. Instead, this option does the following things :"
94,- prevent any 400/408 message from being sent to the client if nothing
94,was received over a connection before it was closed;
94,- prevent any log from being emitted in this situation;
94,- prevent any error counter from being incremented
94,That way the empty connection is silently ignored. Note that it is better
94,"not to use this unless it is clear that it is needed, because it will hide"
94,real problems. The most common reason for not receiving a request and seeing
94,a 408 is due to an MTU inconsistency between the client and an intermediary
94,"element such as a VPN, which blocks too large packets. These issues are"
94,generally seen with POST requests as well as GET with large cookies. The logs
94,are often the only way to detect them.
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""logThis keyword is available in sections :Process management and securityAlphabetically sorted keywords reference"", ""dontlognull"", ""errorfile"", and section 8 about logging."
94,option http-keep-aliveno option http-keep-aliveEnable or disable HTTP keep-alive from client to server
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,By default HAProxy operates in keep-alive mode with regards to persistent
94,"connections: for each connection it processes each request and response, and"
94,leaves the connection idle on both sides between the end of a response and the
94,start of a new request. This mode may be changed by several options such as
94,"""option http-server-close"", ""option forceclose"", ""option httpclose"" or"
94,"""option http-tunnel"". This option allows to set back the keep-alive mode,"
94,which can be useful when another mode was used in a defaults section.
94,"Setting ""option http-keep-alive"" enables HTTP keep-alive mode on the client-"
94,and server- sides. This provides the lowest latency on the client side (slow
94,network) and the fastest session reuse on the server side at the expense
94,"of maintaining idle connections to the servers. In general, it is possible"
94,with this option to achieve approximately twice the request rate that the
94,"""http-server-close"" option achieves on small objects. There are mainly two"
94,situations where this option may be useful :
94,- when the server is non-HTTP compliant and authenticates the connection
94,instead of requests (e.g. NTLM authentication)
94,- when the cost of establishing the connection to the server is significant
94,compared to the cost of retrieving the associated object from the server.
94,This last case can happen when the server is a fast static server of cache.
94,"In this case, the server will need to be properly tuned to support high enough"
94,connection counts because connections will last until the client sends another
94,request.
94,If the client request has to go to another backend or another server due to
94,"content switching or the load balancing algorithm, the idle connection will"
94,"immediately be closed and a new one re-opened. Option ""prefer-last-server"" is"
94,available to try optimize server selection so that if the server currently
94,"attached to an idle connection is usable, it will be used."
94,"At the moment, logs will not indicate whether requests came from the same"
94,session or not. The accept date reported in the logs corresponds to the end
94,"of the previous request, and the request time corresponds to the time spent"
94,waiting for a new request. The keep-alive request time is still bound to the
94,"timeout defined by ""timeout http-keep-alive"" or ""timeout http-request"" if"
94,not set.
94,"This option disables and replaces any previous ""option httpclose"", ""option"
94,"http-server-close"", ""option forceclose"" or ""option http-tunnel"". When backend"
94,"and frontend options differ, all of these 4 options have precedence over"
94,"""option http-keep-alive""."
94,"See also : ""option forceclose"", ""option http-server-close"", ""option prefer-last-server"", ""option http-pretend-keepalive"", ""option httpclose"", and ""1.1. The HTTP transaction model""."
94,option http-no-delayno option http-no-delayInstruct the system to favor low interactive delays over performance in HTTP
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,"In HTTP, each payload is unidirectional and has no notion of interactivity."
94,Any agent is expected to queue data somewhat for a reasonably low delay.
94,There are some very rare server-to-server applications that abuse the HTTP
94,"protocol and expect the payload phase to be highly interactive, with many"
94,interleaved data chunks in both directions within a single request. This is
94,absolutely not supported by the HTTP specification and will not work across
94,most proxies or servers. When such applications attempt to do this through
94,"haproxy, it works but they will experience high delays due to the network"
94,optimizations which favor performance by instructing the system to wait for
94,enough data to be available in order to only send full packets. Typical
94,delays are around 200 ms per round trip. Note that this only happens with
94,abnormal uses. Normal uses such as CONNECT requests nor WebSockets are not
94,affected.
94,"When ""option http-no-delay"" is present in either the frontend or the backend"
94,"used by a connection, all such optimizations will be disabled in order to"
94,make the exchanges as fast as possible. Of course this offers no guarantee on
94,"the functionality, as it may break at any other place. But if it works via"
94,"HAProxy, it will work as fast as possible. This option should never be used"
94,"by default, and should never be used at all unless such a buggy application"
94,is discovered. The impact of using this option is an increase of bandwidth
94,"usage and CPU usage, which may significantly lower performance in high"
94,latency environments.
94,"See also : ""option http-buffer-request"""
94,option http-pretend-keepaliveno option http-pretend-keepaliveDefine whether haproxy will announce keepalive to the server or not
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,"When running with ""option http-server-close"" or ""option forceclose"", haproxy"
94,"adds a ""Connection: close"" header to the request forwarded to the server."
94,"Unfortunately, when some servers see this header, they automatically refrain"
94,"from using the chunked encoding for responses of unknown length, while this"
94,is totally unrelated. The immediate effect is that this prevents haproxy from
94,maintaining the client connection alive. A second effect is that a client or
94,"a cache could receive an incomplete response without being aware of it, and"
94,consider the response complete.
94,"By setting ""option http-pretend-keepalive"", haproxy will make the server"
94,believe it will keep the connection alive. The server will then not fall back
94,"to the abnormal undesired above. When haproxy gets the whole response, it"
94,will close the connection with the server just as it would do with the
94,"""forceclose"" option. That way the client gets a normal response and the"
94,connection is correctly closed on the server side.
94,"It is recommended not to enable this option by default, because most servers"
94,"will more efficiently close the connection themselves after the last packet,"
94,"and release its buffers slightly earlier. Also, the added packet on the"
94,network could slightly reduce the overall peak performance. However it is
94,"worth noting that when this option is enabled, haproxy will have slightly"
94,"less work to do. So if haproxy is the bottleneck on the whole architecture,"
94,enabling this option might save a few CPU cycles.
94,This option may be set both in a frontend and in a backend. It is enabled if
94,at least one of the frontend or backend holding a connection has it enabled.
94,"This option may be combined with ""option httpclose"", which will cause"
94,keepalive to be announced to the server and close to be announced to the
94,client. This practice is discouraged though.
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""option forceclose"", ""option http-server-close"", and ""option http-keep-alive"""
94,option http-server-closeno option http-server-closeEnable or disable HTTP connection closing on the server side
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,By default HAProxy operates in keep-alive mode with regards to persistent
94,"connections: for each connection it processes each request and response, and"
94,leaves the connection idle on both sides between the end of a response and
94,the start of a new request. This mode may be changed by several options such
94,"as ""option http-server-close"", ""option forceclose"", ""option httpclose"" or"
94,"""option http-tunnel"". Setting ""option http-server-close"" enables HTTP"
94,connection-close mode on the server side while keeping the ability to support
94,HTTP keep-alive and pipelining on the client side. This provides the lowest
94,latency on the client side (slow network) and the fastest session reuse on
94,"the server side to save server resources, similarly to ""option forceclose""."
94,It also permits non-keepalive capable servers to be served in keep-alive mode
94,to the clients if they conform to the requirements of RFC7230. Please note
94,that some servers do not always conform to those requirements when they see
94,"""Connection: close"" in the request. The effect will be that keep-alive will"
94,"never be used. A workaround consists in enabling ""option"
94,"http-pretend-keepalive""."
94,"At the moment, logs will not indicate whether requests came from the same"
94,session or not. The accept date reported in the logs corresponds to the end
94,"of the previous request, and the request time corresponds to the time spent"
94,waiting for a new request. The keep-alive request time is still bound to the
94,"timeout defined by ""timeout http-keep-alive"" or ""timeout http-request"" if"
94,not set.
94,This option may be set both in a frontend and in a backend. It is enabled if
94,at least one of the frontend or backend holding a connection has it enabled.
94,"It disables and replaces any previous ""option httpclose"", ""option forceclose"","
94,"""option http-tunnel"" or ""option http-keep-alive"". Please check section 4"
94,"(""Proxies"") to see how this option combines with others when frontend and"
94,backend options differ.
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""option forceclose"", ""option http-pretend-keepalive"", ""option httpclose"", ""option http-keep-alive"", and ""1.1. The HTTP transaction model""."
94,option http-tunnelno option http-tunnelDisable or enable HTTP connection processing after first transaction
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,By default HAProxy operates in keep-alive mode with regards to persistent
94,"connections: for each connection it processes each request and response, and"
94,leaves the connection idle on both sides between the end of a response and
94,the start of a new request. This mode may be changed by several options such
94,"as ""option http-server-close"", ""option forceclose"", ""option httpclose"" or"
94,"""option http-tunnel""."
94,"Option ""http-tunnel"" disables any HTTP processing past the first request and"
94,the first response. This is the mode which was used by default in versions
94,"1.0 to 1.5-dev21. It is the mode with the lowest processing overhead, which"
94,is normally not needed anymore unless in very specific cases such as when
94,"using an in-house protocol that looks like HTTP but is not compatible, or"
94,just to log one request per client in order to reduce log size. Note that
94,"everything which works at the HTTP level, including header parsing/addition,"
94,cookie processing or content switching will only work for the first request
94,and will be ignored after the first response.
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""option forceclose"", ""option http-server-close"", ""option httpclose"", ""option http-keep-alive"", and ""1.1. The HTTP transaction model""."
94,option http-use-proxy-headerno option http-use-proxy-headerMake use of non-standard Proxy-Connection header instead of Connection
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments : none
94,While RFC7230 explicitly states that HTTP/1.1 agents must use the
94,Connection header to indicate their wish of persistent or non-persistent
94,"connections, both browsers and proxies ignore this header for proxied"
94,"connections and make use of the undocumented, non-standard Proxy-Connection"
94,header instead. The issue begins when trying to put a load balancer between
94,"browsers and such proxies, because there will be a difference between what"
94,haproxy understands and what the client and the proxy agree on.
94,"By setting this option in a frontend, haproxy can automatically switch to use"
94,that non-standard header if it sees proxied requests. A proxied request is
94,defined here as one where the URI begins with neither a '/' nor a '*'. This
94,is incompatible with the HTTP tunnel mode. Note that this option can only be
94,specified in a frontend and will affect the request along its whole life.
94,"Also, when this option is set, a request which requires authentication will"
94,automatically switch to use proxy authentication headers if it is itself a
94,proxied request. That makes it possible to check or enforce authentication in
94,front of an existing proxy.
94,"This option should normally never be used, except in front of a proxy."
94,"See also : ""option httpclose"", ""option forceclose"" and ""option http-server-close""."
94,option httpchkoption httpchk <uri>option httpchk <method> <uri>option httpchk <method> <uri> <version>Enable HTTP protocol to check on the servers health
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments :<method>
94,"is the optional HTTP method used with the requests. When not set,"
94,"the ""OPTIONS"" method is used, as it generally requires low server"
94,processing and is easy to filter out from the logs. Any method
94,"may be used, though it is not recommended to invent non-standard"
94,ones.
94,<uri>
94,"is the URI referenced in the HTTP requests. It defaults to "" / """
94,"which is accessible by default on almost any server, but may be"
94,changed to any other URI. Query strings are permitted.
94,"<version> is the optional HTTP version string. It defaults to ""HTTP/1.0"""
94,"but some servers might behave incorrectly in HTTP 1.0, so turning"
94,it to HTTP/1.1 may sometimes help. Note that the Host field is
94,"mandatory in HTTP/1.1, use ""http-check send"" directive to add it."
94,"By default, server health checks only consist in trying to establish a TCP"
94,"connection. When ""option httpchk"" is specified, a complete HTTP request is"
94,"sent once the TCP connection is established, and responses 2xx and 3xx are"
94,"considered valid, while all other ones indicate a server failure, including"
94,the lack of any response.
94,The port and interval are specified in the server configuration.
94,"This option does not necessarily require an HTTP backend, it also works with"
94,plain TCP backends. This is particularly useful to check simple scripts bound
94,to some dedicated ports using the inetd daemon.
94,"Note : For a while, there was no way to add headers or body in the request"
94,used for HTTP health checks. So a workaround was to hide it at the end
94,"of the version string with a ""\r\n"" after the version. It is now"
94,"deprecated. The directive ""http-check send"" must be used instead."
94,Examples :
94,# Relay HTTPS traffic to Apache instance and check service availability
94,"# using HTTP request ""OPTIONS * HTTP/1.1"" on port 80."
94,backend https_relay
94,mode tcp
94,option httpchk OPTIONS * HTTP/1.1
94,http-check send hdr Host www
94,server apache1 192.168.1.1:443 check port 80
94,"See also : ""option ssl-hello-chk"", ""option smtpchk"", ""option mysql-check"", ""option pgsql-check"", ""http-check"" and the ""check"", ""port"" and ""inter"" server options."
94,option httpcloseno option httpcloseEnable or disable passive HTTP connection closing
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,By default HAProxy operates in keep-alive mode with regards to persistent
94,"connections: for each connection it processes each request and response, and"
94,leaves the connection idle on both sides between the end of a response and
94,the start of a new request. This mode may be changed by several options such
94,"as ""option http-server-close"", ""option forceclose"", ""option httpclose"" or"
94,"""option http-tunnel""."
94,"If ""option httpclose"" is set, HAProxy will work in HTTP tunnel mode and check"
94,"if a ""Connection: close"" header is already set in each direction, and will"
94,add one if missing. Each end should react to this by actively closing the TCP
94,"connection after each transfer, thus resulting in a switch to the HTTP close"
94,"mode. Any ""Connection"" header different from ""close"" will also be removed."
94,Note that this option is deprecated since what it does is very cheap but not
94,"reliable. Using ""option http-server-close"" or ""option forceclose"" is strongly"
94,recommended instead.
94,It seldom happens that some servers incorrectly ignore this header and do not
94,"close the connection even though they reply ""Connection: close"". For this"
94,"reason, they are not compatible with older HTTP 1.0 browsers. If this happens"
94,"it is possible to use the ""option forceclose"" which actively closes the"
94,"request connection once the server responds. Option ""forceclose"" also"
94,releases the server connection earlier because it does not have to wait for
94,the client to acknowledge it.
94,This option may be set both in a frontend and in a backend. It is enabled if
94,at least one of the frontend or backend holding a connection has it enabled.
94,"It disables and replaces any previous ""option http-server-close"","
94,"""option forceclose"", ""option http-keep-alive"" or ""option http-tunnel"". Please"
94,"check section 4 (""Proxies"") to see how this option combines with others when"
94,frontend and backend options differ.
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""option forceclose"", ""option http-server-close"" and ""1.1. The HTTP transaction model""."
94,"option httplog [ clf ]Enable logging of HTTP request, session state and timers"
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments :clf
94,"if the ""clf"" argument is added, then the output format will be"
94,the CLF format instead of HAProxy's default HTTP format. You can
94,use this when you need to feed HAProxy's logs through a specific
94,log analyzer which only support the CLF format and which is not
94,extensible.
94,"By default, the log output format is very poor, as it only contains the"
94,"source and destination addresses, and the instance name. By specifying"
94,"""option httplog"", each log line turns into a much richer format including,"
94,"but not limited to, the HTTP request, the connection timers, the session"
94,"status, the connections numbers, the captured headers and cookies, the"
94,"frontend, backend and server name, and of course the source address and"
94,ports.
94,"Specifying only ""option httplog"" will automatically clear the 'clf' mode"
94,if it was set by default.
94,"""option httplog"" overrides any previous ""log-format"" directive."
94,See also : section 8 about logging.
94,option http_proxyno option http_proxyEnable or disable plain HTTP proxy mode
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,It sometimes happens that people need a pure HTTP proxy which understands
94,"basic proxy requests without caching nor any fancy feature. In this case,"
94,"it may be worth setting up an HAProxy instance with the ""option http_proxy"""
94,"set. In this mode, no server is declared, and the connection is forwarded to"
94,"the IP address and port found in the URL after the ""http://"" scheme."
94,"No host address resolution is performed, so this only works when pure IP"
94,"addresses are passed. Since this option's usage perimeter is rather limited,"
94,it will probably be used only by experts who know they need exactly it. This
94,is incompatible with the HTTP tunnel mode.
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,Example :
94,# this backend understands HTTP proxy requests and forwards them directly.
94,backend direct_forward
94,option httpclose
94,option http_proxy
94,"See also : ""option httpclose"""
94,option independent-streamsno option independent-streamsEnable or disable independent timeout processing for both directions
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,"By default, when data is sent over a socket, both the write timeout and the"
94,"read timeout for that socket are refreshed, because we consider that there is"
94,"activity on that socket, and we have no other means of guessing if we should"
94,receive data or not.
94,"While this default behavior is desirable for almost all applications, there"
94,"exists a situation where it is desirable to disable it, and only refresh the"
94,read timeout if there are incoming data. This happens on sessions with large
94,timeouts and low amounts of exchanged data such as telnet session. If the
94,"server suddenly disappears, the output data accumulates in the system's"
94,"socket buffers, both timeouts are correctly refreshed, and there is no way"
94,"to know the server does not receive them, so we don't timeout. However, when"
94,"the underlying protocol always echoes sent data, it would be enough by itself"
94,to detect the issue using the read timeout. Note that this problem does not
94,happen with more verbose protocols because data won't accumulate long in the
94,socket buffers.
94,"When this option is set on the frontend, it will disable read timeout updates"
94,on data sent to the client. There probably is little use of this case. When
94,"the option is set on the backend, it will disable read timeout updates on"
94,data sent to the server. Doing so will typically break large HTTP posts from
94,"slow lines, so use it with caution."
94,"Note: older versions used to call this setting ""option independant-streams"""
94,with a spelling mistake. This spelling is still supported but
94,deprecated.
94,"See also : ""timeout client"", ""timeout server"" and ""timeout tunnel"""
94,option ldap-checkUse LDAPv3 health checks for server testing
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : none
94,It is possible to test that the server correctly talks LDAPv3 instead of just
94,"testing that it accepts the TCP connection. When this option is set, an"
94,"LDAPv3 anonymous simple bind message is sent to the server, and the response"
94,is analyzed to find an LDAPv3 bind response message.
94,The server is considered valid only when the LDAP response contains success
94,resultCode (http://tools.ietf.org/html/rfc4511#section-4.1.9).
94,Logging of bind requests is server dependent see your documentation how to
94,configure it.
94,Example :
94,option ldap-check
94,"See also : ""option httpchk"""
94,option external-checkUse external processes for server health checks
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,It is possible to test the health of a server using an external command.
94,"This is achieved by running the executable set using ""external-check"
94,"command""."
94,"Requires the ""external-checkThis keyword is available in sections :Process management and securityAlphabetically sorted keywords reference"" global to be set."
94,"See also : ""external-checkThis keyword is available in sections :Process management and securityAlphabetically sorted keywords reference"", ""external-check command"", ""external-check path"""
94,option log-health-checksno option log-health-checksEnable or disable logging of health checks status updates
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : none
94,"By default, failed health check are logged if server is UP and successful"
94,"health checks are logged if server is DOWN, so the amount of additional"
94,information is limited.
94,"When this option is enabled, any change of the health check status or to"
94,"the server's health will be logged, so that it becomes possible to know"
94,"that a server was failing occasional checks before crashing, or exactly when"
94,"it failed to respond a valid HTTP status, then when the port started to"
94,"reject connections, then when the server stopped responding at all."
94,Note that status changes not caused by health checks (e.g. enable/disable on
94,the CLI) are intentionally not logged by this option.
94,"See also: ""option httpchk"", ""option ldap-check"", ""option mysql-check"", ""option pgsql-check"", ""option redis-check"", ""option smtpchk"", ""option tcp-check"", ""logThis keyword is available in sections :Process management and securityAlphabetically sorted keywords reference"" and section 8 about logging."
94,option log-separate-errorsno option log-separate-errorsChange log level for non-completely successful connections
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments : none
94,Sometimes looking for errors in logs is not easy. This option makes haproxy
94,raise the level of logs containing potentially interesting information such
94,"as errors, timeouts, retries, redispatches, or HTTP status codes 5xx. The"
94,"level changes from ""info"" to ""err"". This makes it possible to log them"
94,separately to a different file with most syslog daemons. Be careful not to
94,"remove them from the original file, otherwise you would lose ordering which"
94,provides very important information.
94,"Using this option, large sites dealing with several thousand connections per"
94,second may log normal traffic to a rotating buffer and only archive smaller
94,error logs.
94,"See also : ""logThis keyword is available in sections :Process management and securityAlphabetically sorted keywords reference"", ""dontlognull"", ""dontlog-normal"" and section 8 about logging."
94,option logasapno option logasapEnable or disable early logging.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments : none
94,"By default, logs are emitted when all the log format variables and sample"
94,"fetches used in the definition of the log-format string return a value, or"
94,when the session is terminated. This allows the built in log-format strings
94,"to account for the transfer time, or the number of bytes in log messages."
94,"When handling long lived connections such as large file transfers or RDP,"
94,it may take a while for the request or connection to appear in the logs.
94,"Using ""option logasap"", the log message is created as soon as the server"
94,"connection is established in mode tcp, or as soon as the server sends the"
94,complete headers in mode http. Missing information in the logs will be the
94,total number of bytes which will only indicate the amount of data transfered
94,before the message was created and the total time which will not take the
94,remainder of the connection life or transfer time into account. For the case
94,"of HTTP, it is good practice to capture the Content-Length response header"
94,so that the logs at least indicate how many bytes are expected to be
94,transfered.
94,Examples :
94,listen http_proxy 0.0.0.0:80
94,mode http
94,option httplog
94,option logasap
94,log 192.168.2.200 local3
94,>>> Feb
94,6 12:14:14 localhost \
94,haproxy[14389]: 10.0.1.2:33317 [06/Feb/2009:12:14:14.655] http-in \
94,static/srv1 9/10/7/14/+30 200 +243 - - ---- 3/1/1/1/0 1/0 \
94,"""GET /image.iso HTTP/1.0"""
94,"See also : ""option httplog"", ""capture response header"", and section 8 about logging."
94,option mysql-check [ user <username> [ post-41 ] ]Use MySQL health checks for server testing
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments :<username> This is the username which will be used when connecting to MySQL
94,server.
94,post-41
94,Send post v4.1 client compatible checks
94,"If you specify a username, the check consists of sending two MySQL packet,"
94,"one Client Authentication packet, and one QUIT packet, to correctly close"
94,MySQL session. We then parse the MySQL Handshake Initialization packet and/or
94,Error packet. It is a basic but useful test which does not produce error nor
94,"aborted connect on the server. However, it requires adding an authorization"
94,"in the MySQL table, like this :"
94,USE mysql;
94,"INSERT INTO user (Host,User) values ('<ip_of_haproxy>','<username>');"
94,FLUSH PRIVILEGES;
94,"If you don't specify a username (it is deprecated and not recommended), the"
94,check only consists in parsing the Mysql Handshake Initialization packet or
94,"Error packet, we don't send anything in this mode. It was reported that it"
94,can generate lockout if check is too frequent and/or if there is not enough
94,"traffic. In fact, you need in this case to check MySQL ""max_connect_errors"""
94,value as if a connection is established successfully within fewer than MySQL
94,"""max_connect_errors"" attempts after a previous connection was interrupted,"
94,the error count for the host is cleared to zero. If HAProxy's server get
94,"blocked, the ""FLUSH HOSTS"" statement is the only way to unblock it."
94,Remember that this does not check database presence nor database consistency.
94,"To do this, you can use an external check with xinetd for example."
94,"The check requires MySQL >=3.22, for older version, please use TCP check."
94,"Most often, an incoming MySQL server needs to see the client's IP address for"
94,"various purposes, including IP privilege matching and connection logging."
94,"When possible, it is often wise to masquerade the client's IP address when"
94,"connecting to the server using the ""usesrc"" argument of the ""sourceThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"" keyword,"
94,"which requires the transparent proxy feature to be compiled in, and the MySQL"
94,server to route the client via the machine hosting haproxy.
94,"See also: ""option httpchk"""
94,option nolingerno option nolingerEnable or disable immediate session resource cleaning after close
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,When clients or servers abort connections in a dirty way (e.g. they are
94,"physically disconnected), the session timeouts triggers and the session is"
94,"closed. But it will remain in FIN_WAIT1 state for some time in the system,"
94,using some resources and possibly limiting the ability to establish newer
94,connections.
94,"When this happens, it is possible to activate ""option nolinger"" which forces"
94,"the system to immediately remove any socket's pending data on close. Thus,"
94,the session is instantly purged from the system's tables. This usually has
94,side effects such as increased number of TCP resets due to old retransmits
94,getting immediately rejected. Some firewalls may sometimes complain about
94,this too.
94,"For this reason, it is not recommended to use this option when not absolutely"
94,needed. You know that you need it when you have thousands of FIN_WAIT1
94,sessions on your system (TIME_WAIT ones do not count).
94,"This option may be used both on frontends and backends, depending on the side"
94,"where it is required. Use it on the frontend for clients, and on the backend"
94,for servers.
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,option originalto [ except <network> ] [ header <name> ]Enable insertion of the X-Original-To header to requests sent to servers
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :<network> is an optional argument used to disable this option for sources
94,matching <network>
94,<name>
94,"an optional argument to specify a different ""X-Original-To"""
94,header name.
94,"Since HAProxy can work in transparent mode, every request from a client can"
94,be redirected to the proxy and HAProxy itself can proxy every request to a
94,complex SQUID environment and the destination host from SO_ORIGINAL_DST will
94,be lost. This is annoying when you want access rules based on destination ip
94,"addresses. To solve this problem, a new HTTP header ""X-Original-To"" may be"
94,added by HAProxy to all requests sent to the server. This header contains a
94,value representing the original destination IP address. Since this must be
94,configured to always use the last occurrence of this header only. Note that
94,"only the last occurrence of the header must be used, since it is really"
94,possible that the client has already brought one.
94,"The keyword ""header"" may be used to supply a different header name to replace"
94,"the default ""X-Original-To"". This can be useful where you might already"
94,"have a ""X-Original-To"" header from a different application, and you need"
94,"preserve it. Also if your backend server doesn't use the ""X-Original-To"""
94,header and requires different one.
94,"Sometimes, a same HAProxy instance may be shared between a direct client"
94,access and a reverse-proxy access (for instance when an SSL reverse-proxy is
94,used to decrypt HTTPS traffic). It is possible to disable the addition of the
94,"header for a known source address or network by adding the ""except"" keyword"
94,"followed by the network address. In this case, any source IP matching the"
94,network will not cause an addition of this header. Most common uses are with
94,private networks or 127.0.0.1.
94,This option may be specified either in the frontend or in the backend. If at
94,"least one of them uses it, the header will be added. Note that the backend's"
94,setting of the header subargument takes precedence over the frontend's if
94,both are defined.
94,Examples :
94,# Original Destination address
94,frontend www
94,mode http
94,option originalto except 127.0.0.1
94,# Those servers want the IP Address in X-Client-Dst
94,backend www
94,mode http
94,option originalto header X-Client-Dst
94,"See also : ""option httpclose"", ""option http-server-close"", ""option forceclose"""
94,option persistno option persistEnable or disable forced persistence on down servers
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : none
94,When an HTTP request reaches a backend with a cookie which references a dead
94,"server, by default it is redispatched to another server. It is possible to"
94,"force the request to be sent to the dead server first using ""option persist"""
94,if absolutely needed. A common use case is when servers are under extreme
94,"load and spend their time flapping. In this case, the users would still be"
94,"directed to the server they opened the session on, in the hope they would be"
94,"correctly served. It is recommended to use ""option redispatch"" in conjunction"
94,with this option so that in the event it would not be possible to connect to
94,"the server at all (server definitely dead), the client would finally be"
94,redirected to another valid server.
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""option redispatch"", ""retries"", ""force-persist"""
94,option pgsql-check [ user <username> ]Use PostgreSQL health checks for server testing
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments :<username> This is the username which will be used when connecting to
94,PostgreSQL server.
94,The check sends a PostgreSQL StartupMessage and waits for either
94,Authentication request or ErrorResponse message. It is a basic but useful
94,test which does not produce error nor aborted connect on the server.
94,"This check is identical with the ""mysql-check""."
94,"See also: ""option httpchk"""
94,option prefer-last-serverno option prefer-last-serverAllow multiple load balanced requests to remain on the same server
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : none
94,"When the load balancing algorithm in use is not deterministic, and a previous"
94,"request was sent to a server to which haproxy still holds a connection, it is"
94,sometimes desirable that subsequent requests on a same session go to the same
94,"server as much as possible. Note that this is different from persistence, as"
94,we only indicate a preference which haproxy tries to apply without any form
94,of warranty. The real use is for keep-alive connections sent to servers. When
94,"this option is used, haproxy will try to reuse the same connection that is"
94,"attached to the server instead of rebalancing to another server, causing a"
94,close of the connection. This can make sense for static file servers. It does
94,"not make much sense to use this in combination with hashing algorithms. Note,"
94,haproxy already automatically tries to stick to a server which sends a 401 or
94,"to a proxy which sends a 407 (authentication required), when the load"
94,balancing algorithm is not deterministic. This is mandatory for use with the
94,"broken NTLM authentication challenge, and significantly helps in"
94,troubleshooting some faulty applications. Option prefer-last-server might be
94,"desirable in these environments as well, to avoid redistributing the traffic"
94,after every other response.
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also: ""option http-keep-alive"""
94,option redispatchoption redispatch <interval>no option redispatchEnable or disable session redistribution in case of connection failure
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments :<interval> The optional integer value that controls how often redispatches
94,occur when retrying connections. Positive value P indicates a
94,"redispatch is desired on every Pth retry, and negative value"
94,N indicate a redispatch is desired on the Nth retry prior to the
94,"last retry. For example, the default of -1 preserves the"
94,"historical behavior of redispatching on the last retry, a"
94,"positive value of 1 would indicate a redispatch on every retry,"
94,and a positive value of 3 would indicate a redispatch on every
94,third retry. You can disable redispatches with a value of 0.
94,"In HTTP mode, if a server designated by a cookie is down, clients may"
94,"definitely stick to it because they cannot flush the cookie, so they will not"
94,be able to access the service anymore.
94,"Specifying ""option redispatch"" will allow the proxy to break cookie or"
94,consistent hash based persistence and redistribute them to a working server.
94,It also allows to retry connections to another server in case of multiple
94,"connection failures. Of course, it requires having ""retries"" set to a nonzero"
94,value.
94,"This form is the preferred form, which replaces both the ""redispatch"" and"
94,"""redisp"" keywords."
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""redispatch"", ""retries"", ""force-persist"""
94,option redis-checkUse redis health checks for server testing
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : none
94,It is possible to test that the server correctly talks REDIS protocol instead
94,"of just testing that it accepts the TCP connection. When this option is set,"
94,"a PING redis command is sent to the server, and the response is analyzed to"
94,"find the ""+PONG"" response message."
94,Example :
94,option redis-check
94,"See also : ""option httpchk"", ""option tcp-check"", ""tcp-check expect"""
94,option smtpchkoption smtpchk <hello> <domain>Use SMTP health checks for server testing
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments :<hello>
94,"is an optional argument. It is the ""hello"" command to use. It can"
94,"be either ""HELO"" (for SMTP) or ""EHLO"" (for ESMTP). All other"
94,"values will be turned into the default command (""HELO"")."
94,<domain>
94,is the domain name to present to the server. It may only be
94,specified (and is mandatory) if the hello command has been
94,"specified. By default, ""localhost"" is used."
94,"When ""option smtpchk"" is set, the health checks will consist in TCP"
94,"connections followed by an SMTP command. By default, this command is"
94,"""HELO localhost"". The server's return code is analyzed and only return codes"
94,"starting with a ""2"" will be considered as valid. All other responses,"
94,including a lack of response will constitute an error and will indicate a
94,dead server.
94,This test is meant to be used with SMTP servers or relays. Depending on the
94,"request, it is possible that some servers do not log each connection attempt,"
94,so you may want to experiment to improve the behavior. Using telnet on port
94,25 is often easier than adjusting the configuration.
94,"Most often, an incoming SMTP server needs to see the client's IP address for"
94,"various purposes, including spam filtering, anti-spoofing and logging. When"
94,"possible, it is often wise to masquerade the client's IP address when"
94,"connecting to the server using the ""usesrc"" argument of the ""sourceThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"" keyword,"
94,which requires the transparent proxy feature to be compiled in.
94,Example :
94,option smtpchk HELO mydomain.org
94,"See also : ""option httpchk"", ""sourceThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"""
94,option socket-statsno option socket-statsEnable or disable collecting & providing separate statistics for each socket.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments : none
94,option splice-autono option splice-autoEnable or disable automatic kernel acceleration on sockets in both directions
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,"When this option is enabled either on a frontend or on a backend, haproxy"
94,will automatically evaluate the opportunity to use kernel tcp splicing to
94,"forward data between the client and the server, in either direction. HAProxy"
94,uses heuristics to estimate if kernel splicing might improve performance or
94,not. Both directions are handled independently. Note that the heuristics used
94,are not much aggressive in order to limit excessive use of splicing. This
94,"option requires splicing to be enabled at compile time, and may be globally"
94,"disabled with the global option ""nosplice"". Since splice uses pipes, using it"
94,requires that there are enough spare pipes.
94,Important note: kernel-based TCP splicing is a Linux-specific feature which
94,first appeared in kernel 2.6.25. It offers kernel-based acceleration to
94,"transfer data between sockets without copying these data to user-space, thus"
94,providing noticeable performance gains and CPU cycles savings. Since many
94,"early implementations are buggy, corrupt data and/or are inefficient, this"
94,"feature is not enabled by default, and it should be used with extreme care."
94,"While it is not possible to detect the correctness of an implementation,"
94,2.6.29 is the first version offering a properly working implementation. In
94,"case of doubt, splicing may be globally disabled using the global ""nosplice"""
94,keyword.
94,Example :
94,option splice-auto
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""option splice-request"", ""option splice-response"", and global options ""nosplice"" and ""maxpipes"""
94,option splice-requestno option splice-requestEnable or disable automatic kernel acceleration on sockets for requests
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,"When this option is enabled either on a frontend or on a backend, haproxy"
94,will use kernel tcp splicing whenever possible to forward data going from
94,the client to the server. It might still use the recv/send scheme if there
94,are no spare pipes left. This option requires splicing to be enabled at
94,"compile time, and may be globally disabled with the global option ""nosplice""."
94,"Since splice uses pipes, using it requires that there are enough spare pipes."
94,"Important note: see ""option splice-auto"" for usage limitations."
94,Example :
94,option splice-request
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""option splice-auto"", ""option splice-response"", and global options ""nosplice"" and ""maxpipes"""
94,option splice-responseno option splice-responseEnable or disable automatic kernel acceleration on sockets for responses
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,"When this option is enabled either on a frontend or on a backend, haproxy"
94,will use kernel tcp splicing whenever possible to forward data going from
94,the server to the client. It might still use the recv/send scheme if there
94,are no spare pipes left. This option requires splicing to be enabled at
94,"compile time, and may be globally disabled with the global option ""nosplice""."
94,"Since splice uses pipes, using it requires that there are enough spare pipes."
94,"Important note: see ""option splice-auto"" for usage limitations."
94,Example :
94,option splice-response
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""option splice-auto"", ""option splice-request"", and global options ""nosplice"" and ""maxpipes"""
94,option spop-checkUse SPOP health checks for server testing
94,May be used in sections
94,defaultsfrontendlistenbackend
94,nononoyes
94,Arguments : none
94,It is possible to test that the server correctly talks SPOP protocol instead
94,"of just testing that it accepts the TCP connection. When this option is set,"
94,"a HELLO handshake is performed between HAProxy and the server, and the"
94,response is analyzed to check no error is reported.
94,Example :
94,option spop-check
94,"See also : ""option httpchk"""
94,option srvtcpkano option srvtcpkaEnable or disable the sending of TCP keepalive packets on the server side
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : none
94,When there is a firewall or any session-aware component between a client and
94,"a server, and when the protocol involves very long sessions with long idle"
94,"periods (e.g. remote desktops), there is a risk that one of the intermediate"
94,components decides to expire a session which has remained idle for too long.
94,Enabling socket-level TCP keep-alives makes the system regularly send packets
94,"to the other end of the connection, leaving it active. The delay between"
94,keep-alive probes is controlled by the system only and depends both on the
94,operating system and its tuning parameters.
94,It is important to understand that keep-alive packets are neither emitted nor
94,received at the application level. It is only the network stacks which sees
94,"them. For this reason, even if one side of the proxy already uses keep-alives"
94,"to maintain its connection alive, those keep-alive packets will not be"
94,forwarded to the other side of the proxy.
94,Please note that this has nothing to do with HTTP keep-alive.
94,"Using option ""srvtcpka"" enables the emission of TCP keep-alive probes on the"
94,"server side of a connection, which should help when session expirations are"
94,noticed between HAProxy and a server.
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""option clitcpka"", ""option tcpka"""
94,option ssl-hello-chkUse SSLv3 client hello health checks for server testing
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : none
94,"When some SSL-based protocols are relayed in TCP mode through HAProxy, it is"
94,possible to test that the server correctly talks SSL instead of just testing
94,"that it accepts the TCP connection. When ""option ssl-hello-chk"" is set, pure"
94,SSLv3 client hello messages are sent once the connection is established to
94,"the server, and the response is analyzed to find an SSL server hello message."
94,The server is considered valid only when the response contains this server
94,hello message.
94,"All servers tested till there correctly reply to SSLv3 client hello messages,"
94,and most servers tested do not even log the requests containing only hello
94,"messages, which is appreciable."
94,Note that this check works even when SSL support was not built into haproxy
94,"because it forges the SSL message. When SSL support is available, it is best"
94,to use native SSL health checks instead of this one.
94,"See also: ""option httpchk"", ""check-ssl"""
94,option tcp-checkPerform health checks using tcp-check send/expect sequences
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,"This health check method is intended to be combined with ""tcp-check"" command"
94,lists in order to support send/expect types of health check sequences.
94,TCP checks currently support 4 modes of operations :
94,"- no ""tcp-check"" directive : the health check only consists in a connection"
94,"attempt, which remains the default mode."
94,"- ""tcp-check send"" or ""tcp-check send-binary"" only is mentioned : this is"
94,used to send a string along with a connection opening. With some
94,"protocols, it helps sending a ""QUIT"" message for example that prevents"
94,the server from logging a connection error for each health check. The
94,check result will still be based on the ability to open the connection
94,only.
94,"- ""tcp-check expect"" only is mentioned : this is used to test a banner."
94,The connection is opened and haproxy waits for the server to present some
94,contents which must validate some rules. The check result will be based
94,on the matching between the contents and the rules. This is suited for
94,"POP, IMAP, SMTP, FTP, SSH, TELNET."
94,"- both ""tcp-check send"" and ""tcp-check expect"" are mentioned : this is"
94,"used to test a hello-type protocol. HAProxy sends a message, the server"
94,responds and its response is analyzed. the check result will be based on
94,the matching between the response contents and the rules. This is often
94,suited for protocols which require a binding or a request/response model.
94,"LDAP, MySQL, Redis and SSL are example of such protocols, though they"
94,already all have their dedicated checks with a deeper understanding of
94,the respective protocols.
94,"In this mode, many questions may be sent and many answers may be"
94,analyzed.
94,A fifth mode can be used to insert comments in different steps of the
94,script.
94,"For each tcp-check rule you create, you can add a ""comment"" directive,"
94,followed by a string. This string will be reported in the log and stderr
94,in debug mode. It is useful to make user-friendly error reporting.
94,"The ""comment"" is of course optional."
94,Examples :
94,# perform a POP check (analyze only server's banner)
94,option tcp-check
94,tcp-check expect string +OK\ POP3\ ready comment POP\ protocol
94,# perform an IMAP check (analyze only server's banner)
94,option tcp-check
94,tcp-check expect string *\ OK\ IMAP4\ ready comment IMAP\ protocol
94,# look for the redis master server after ensuring it speaks well
94,"# redis protocol, then it exits properly."
94,# (send a command then analyze the response 3 times)
94,option tcp-check
94,tcp-check comment PING\ phase
94,tcp-check send PING\r\n
94,tcp-check expect string +PONG
94,tcp-check comment role\ check
94,tcp-check send info\ replication\r\n
94,tcp-check expect string role:master
94,tcp-check comment QUIT\ phase
94,tcp-check send QUIT\r\n
94,tcp-check expect string +OK
94,"forge a HTTP request, then analyze the response"
94,(send many headers before analyzing)
94,option tcp-check
94,tcp-check comment forge\ and\ send\ HTTP\ request
94,tcp-check send HEAD\ /\ HTTP/1.1\r\n
94,tcp-check send Host:\ www.mydomain.com\r\n
94,tcp-check send User-Agent:\ HAProxy\ tcpcheck\r\n
94,tcp-check send \r\n
94,tcp-check expect rstring HTTP/1\..\ (2..|3..) comment check\ HTTP\ response
94,"See also : ""tcp-check expect"", ""tcp-check send"""
94,option tcp-smart-acceptno option tcp-smart-acceptEnable or disable the saving of one ACK packet during the accept sequence
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments : none
94,"When an HTTP connection request comes in, the system acknowledges it on"
94,"behalf of HAProxy, then the client immediately sends its request, and the"
94,system acknowledges it too while it is notifying HAProxy about the new
94,connection. HAProxy then reads the request and responds. This means that we
94,"have one TCP ACK sent by the system for nothing, because the request could"
94,very well be acknowledged by HAProxy when it sends its response.
94,"For this reason, in HTTP mode, HAProxy automatically asks the system to avoid"
94,sending this useless ACK on platforms which support it (currently at least
94,"Linux). It must not cause any problem, because the system will send it anyway"
94,after 40 ms if the response takes more time than expected to come.
94,"During complex network debugging sessions, it may be desirable to disable"
94,this optimization because delayed ACKs can make troubleshooting more complex
94,when trying to identify where packets are delayed. It is then possible to
94,"fall back to normal behavior by specifying ""no option tcp-smart-accept""."
94,It is also possible to force it for non-HTTP proxies by simply specifying
94,"""option tcp-smart-accept"". For instance, it can make sense with some services"
94,such as SMTP where the server speaks first.
94,It is recommended to avoid forcing this option in a defaults section. In case
94,"of doubt, consider setting it back to automatic values by prepending the"
94,"""default"" keyword before it, or disabling it using the ""no"" keyword."
94,"See also : ""option tcp-smart-connect"""
94,option tcp-smart-connectno option tcp-smart-connectEnable or disable the saving of one ACK packet during the connect sequence
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : none
94,"On certain systems (at least Linux), HAProxy can ask the kernel not to"
94,"immediately send an empty ACK upon a connection request, but to directly"
94,send the buffer request instead. This saves one packet on the network and
94,"thus boosts performance. It can also be useful for some servers, because they"
94,immediately get the request along with the incoming connection.
94,"This feature is enabled when ""option tcp-smart-connect"" is set in a backend."
94,It is not enabled by default because it makes network troubleshooting more
94,complex.
94,It only makes sense to enable it with protocols where the client speaks first
94,"such as HTTP. In other situations, if there is no data to send in place of"
94,"the ACK, a normal ACK is sent."
94,"If this option has been enabled in a ""defaults"" section, it can be disabled"
94,"in a specific instance by prepending the ""no"" keyword before it."
94,"See also : ""option tcp-smart-accept"""
94,option tcpkaEnable or disable the sending of TCP keepalive packets on both sides
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,When there is a firewall or any session-aware component between a client and
94,"a server, and when the protocol involves very long sessions with long idle"
94,"periods (e.g. remote desktops), there is a risk that one of the intermediate"
94,components decides to expire a session which has remained idle for too long.
94,Enabling socket-level TCP keep-alives makes the system regularly send packets
94,"to the other end of the connection, leaving it active. The delay between"
94,keep-alive probes is controlled by the system only and depends both on the
94,operating system and its tuning parameters.
94,It is important to understand that keep-alive packets are neither emitted nor
94,received at the application level. It is only the network stacks which sees
94,"them. For this reason, even if one side of the proxy already uses keep-alives"
94,"to maintain its connection alive, those keep-alive packets will not be"
94,forwarded to the other side of the proxy.
94,Please note that this has nothing to do with HTTP keep-alive.
94,"Using option ""tcpka"" enables the emission of TCP keep-alive probes on both"
94,the client and server sides of a connection. Note that this is meaningful
94,"only in ""defaults"" or ""listen"" sections. If this option is used in a"
94,"frontend, only the client side will get keep-alives, and if this option is"
94,"used in a backend, only the server side will get keep-alives. For this"
94,"reason, it is strongly recommended to explicitly use ""option clitcpka"" and"
94,"""option srvtcpka"" when the configuration is split between frontends and"
94,backends.
94,"See also : ""option clitcpka"", ""option srvtcpka"""
94,option tcplogEnable advanced logging of TCP connections with session state and timers
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments : none
94,"By default, the log output format is very poor, as it only contains the"
94,"source and destination addresses, and the instance name. By specifying"
94,"""option tcplog"", each log line turns into a much richer format including, but"
94,"not limited to, the connection timers, the session status, the connections"
94,"numbers, the frontend, backend and server name, and of course the source"
94,address and ports. This option is useful for pure TCP proxies in order to
94,find which of the client or server disconnects or times out. For normal HTTP
94,"proxies, it's better to use ""option httplog"" which is even more complete."
94,"""option tcplog"" overrides any previous ""log-format"" directive."
94,"See also : ""option httplog"", and section 8 about logging."
94,option transparentno option transparentEnable client-side transparent proxying
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : none
94,This option was introduced in order to provide layer 7 persistence to layer 3
94,load balancers. The idea is to use the OS's ability to redirect an incoming
94,"connection for a remote address to a local process (here HAProxy), and let"
94,this process know what address was initially requested. When this option is
94,"used, sessions without cookies will be forwarded to the original destination"
94,IP address of the incoming request (which should match that of another
94,"equipment), while requests with cookies will still be forwarded to the"
94,appropriate server.
94,"Note that contrary to a common belief, this option does NOT make HAProxy"
94,present the client's IP to the server when establishing the connection.
94,"See also: the ""usesrc"" argument of the ""sourceThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"" keyword, and the ""transparent"" option of the ""bind"" keyword."
94,external-check command <command>Executable to run when performing an external-check
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments :<command> is the external command to run
94,The arguments passed to the to the command are:
94,<proxy_address> <proxy_port> <server_address> <server_port>
94,The <proxy_address> and <proxy_port> are derived from the first listener
94,"that is either IPv4, IPv6 or a UNIX socket. In the case of a UNIX socket"
94,listener the proxy_address will be the path of the socket and the
94,"<proxy_port> will be the string ""NOT_USED"". In a backend section, it's not"
94,"possible to determine a listener, and both <proxy_address> and <proxy_port>"
94,"will have the string value ""NOT_USED""."
94,Some values are also provided through environment variables.
94,Environment variables :
94,HAPROXY_PROXY_ADDR
94,The first bind address if available (or empty if not
94,"applicable, for example in a ""backend"" section)."
94,HAPROXY_PROXY_ID
94,The backend id.
94,HAPROXY_PROXY_NAME
94,The backend name.
94,HAPROXY_PROXY_PORT
94,The first bind port if available (or empty if not
94,"applicable, for example in a ""backend"" section or"
94,for a UNIX socket).
94,HAPROXY_SERVER_ADDR
94,The server address.
94,HAPROXY_SERVER_CURCONN
94,The current number of connections on the server.
94,HAPROXY_SERVER_ID
94,The server id.
94,HAPROXY_SERVER_MAXCONN
94,The server max connections.
94,HAPROXY_SERVER_NAME
94,The server name.
94,HAPROXY_SERVER_PORT
94,The server port if available (or empty for a UNIX
94,socket).
94,PATH
94,The PATH environment variable used when executing
94,"the command may be set using ""external-check path""."
94,If the command executed and exits with a zero status then the check is
94,"considered to have passed, otherwise the check is considered to have"
94,failed.
94,Example :
94,external-check command /bin/true
94,"See also : ""external-checkThis keyword is available in sections :Process management and securityAlphabetically sorted keywords reference"", ""option external-check"", ""external-check path"""
94,external-check path <path>The value of the PATH environment variable used when running an external-check
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments :<path> is the path used when executing external command to run
94,"The default path is """"."
94,Example :
94,"external-check path ""/usr/bin:/bin"""
94,"See also : ""external-checkThis keyword is available in sections :Process management and securityAlphabetically sorted keywords reference"", ""option external-check"", ""external-check command"""
94,persist rdp-cookiepersist rdp-cookie(<name>)Enable RDP cookie-based persistence
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments :<name>
94,"is the optional name of the RDP cookie to check. If omitted, the"
94,"default cookie name ""msts"" will be used. There currently is no"
94,valid reason to change this name.
94,This statement enables persistence based on an RDP cookie. The RDP cookie
94,contains all information required to find the server in the list of known
94,"servers. So when this option is set in the backend, the request is analyzed"
94,"and if an RDP cookie is found, it is decoded. If it matches a known server"
94,"which is still UP (or if ""option persist"" is set), then the connection is"
94,forwarded to this server.
94,"Note that this only makes sense in a TCP backend, but for this to work, the"
94,frontend must have waited long enough to ensure that an RDP cookie is present
94,"in the request buffer. This is the same requirement as with the ""rdp-cookie"""
94,load-balancing method. Thus it is highly recommended to put all statements in
94,"a single ""listen"" section."
94,"Also, it is important to understand that the terminal server will emit this"
94,"RDP cookie only if it is configured for ""token redirection mode"", which means"
94,"that the ""IP address redirection"" option is disabled."
94,Example :
94,listen tse-farm
94,bind :3389
94,# wait up to 5s for an RDP cookie in the request
94,tcp-request inspect-delay 5s
94,tcp-request content accept if RDP_COOKIE
94,# apply RDP cookie persistence
94,persist rdp-cookie
94,"# if server is unknown, let's balance on the same cookie."
94,"# alternatively, ""balance leastconn"" may be useful too."
94,balance rdp-cookie
94,server srv1 1.1.1.1:3389
94,server srv2 1.1.1.2:3389
94,"See also : ""balance rdp-cookie"", ""tcp-request"", the ""req_rdp_cookie"" ACL and the rdp_cookie pattern fetch function."
94,rate-limit sessions <rate>Set a limit on the number of new sessions accepted per second on a frontend
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments :<rate>
94,The <rate> parameter is an integer designating the maximum number
94,of new sessions per second to accept on the frontend.
94,"When the frontend reaches the specified number of new sessions per second, it"
94,stops accepting new connections until the rate drops below the limit again.
94,"During this time, the pending sessions will be kept in the socket's backlog"
94,(in system buffers) and haproxy will not even be aware that sessions are
94,"pending. When applying very low limit on a highly loaded service, it may make"
94,"sense to increase the socket's backlog using the ""backlogThis keyword is available in sections :Alphabetically sorted keywords referenceBind options"" keyword."
94,This feature is particularly efficient at blocking connection-based attacks
94,or service abuse on fragile servers. Since the session rate is measured every
94,"millisecond, it is extremely accurate. Also, the limit applies immediately,"
94,no delay is needed at all to detect the threshold.
94,Example :
94,Limit the connection rate on SMTP to 10 per second maxlisten smtp
94,mode tcp
94,bind :25
94,rate-limit sessions 10
94,server smtp1 127.0.0.1:1025
94,"Note : when the maximum rate is reached, the frontend's status is not changed"
94,"but its sockets appear as ""WAITING"" in the statistics if the"
94,"""socket-stats"" option is enabled."
94,"See also : the ""backlogThis keyword is available in sections :Alphabetically sorted keywords referenceBind options"" keyword and the ""fe_sess_rate"" ACL criterion."
94,redirect location <loc> [code <code>] <option> [{if | unless} <condition>]redirect prefix <pfx> [code <code>] <option> [{if | unless} <condition>]redirect scheme <sch> [code <code>] <option> [{if | unless} <condition>]Return an HTTP redirection if/unless a condition is matched
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,"If/unless the condition is matched, the HTTP request will lead to a redirect"
94,"response. If no condition is specified, the redirect applies unconditionally."
94,Arguments :<loc>
94,"With ""redirect location"", the exact value in <loc> is placed into"
94,"the HTTP ""Location"" header. When used in an ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"" rule,"
94,<loc> value follows the log-format rules and can include some
94,dynamic values (see Custom Log Format in section 8.2.4).
94,<pfx>
94,"With ""redirect prefix"", the ""Location"" header is built from the"
94,"concatenation of <pfx> and the complete URI path, including the"
94,"query string, unless the ""drop-query"" option is specified (see"
94,"below). As a special case, if <pfx> equals exactly ""/"", then"
94,nothing is inserted before the original URI. It allows one to
94,"redirect to the same URL (for instance, to insert a cookie). When"
94,"used in an ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"" rule, <pfx> value follows the log-format"
94,rules and can include some dynamic values (see Custom Log Format
94,in section 8.2.4).
94,<sch>
94,"With ""redirect scheme"", then the ""Location"" header is built by"
94,"concatenating <sch> with ""://"" then the first occurrence of the"
94,"""Host"" header, and then the URI path, including the query string"
94,"unless the ""drop-query"" option is specified (see below). If no"
94,"path is found or if the path is ""*"", then ""/"" is used instead. If"
94,"no ""Host"" header is found, then an empty host component will be"
94,"returned, which most recent browsers interpret as redirecting to"
94,the same host. This directive is mostly used to redirect HTTP to
94,"HTTPS. When used in an ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"" rule, <sch> value follows"
94,the log-format rules and can include some dynamic values (see
94,Custom Log Format in section 8.2.4).
94,<code>
94,The code is optional. It indicates which type of HTTP redirection
94,"is desired. Only codes 301, 302, 303, 307 and 308 are supported,"
94,with 302 used by default if no code is specified. 301 means
94,"""Moved permanently"", and a browser may cache the Location. 302"
94,"means ""Moved temporarily"" and means that the browser should not"
94,cache the redirection. 303 is equivalent to 302 except that the
94,browser will fetch the location with a GET method. 307 is just
94,like 302 but makes it clear that the same method must be reused.
94,"Likewise, 308 replaces 301 if the same method must be used."
94,<option>
94,There are several options which can be specified to adjust the
94,expected behavior of a redirection :
94,"- ""drop-query"""
94,"When this keyword is used in a prefix-based redirection, then the"
94,"location will be set without any possible query-string, which is useful"
94,for directing users to a non-secure page for instance. It has no effect
94,with a location-type redirect.
94,"- ""append-slash"""
94,"This keyword may be used in conjunction with ""drop-query"" to redirect"
94,users who use a URL not ending with a '/' to the same one with the '/'.
94,It can be useful to ensure that search engines will only see one URL.
94,"For this, a return code 301 is preferred."
94,"- ""set-cookie NAME[=value]"""
94,"A ""Set-Cookie"" header will be added with NAME (and optionally ""=value"")"
94,to the response. This is sometimes used to indicate that a user has
94,"been seen, for instance to protect against some types of DoS. No other"
94,"cookie option is added, so the cookie will be a session cookie. Note"
94,"that for a browser, a sole cookie name without an equal sign is"
94,different from a cookie with an equal sign.
94,"- ""clear-cookie NAME[=]"""
94,"A ""Set-Cookie"" header will be added with NAME (and optionally ""=""), but"
94,"with the ""Max-Age"" attribute set to zero. This will tell the browser to"
94,delete this cookie. It is useful for instance on logout pages. It is
94,"important to note that clearing the cookie ""NAME"" will not remove a"
94,"cookie set with ""NAME=value"". You have to clear the cookie ""NAME="" for"
94,"that, because the browser makes the difference."
94,Example:
94,Move the login URL only to HTTPS.acl clear
94,dst_port
94,acl secure
94,dst_port
94,8080
94,acl login_page url_beg
94,/login
94,acl logout
94,url_beg
94,/logout
94,acl uid_given
94,url_reg
94,/login?userid=[^&]+
94,acl cookie_set hdr_sub(cookie) SEEN=1
94,redirect prefix
94,https://mysite.com set-cookie SEEN=1 if !cookie_set
94,redirect prefix
94,https://mysite.com
94,if login_page !secure
94,redirect prefix
94,http://mysite.com drop-query if login_page !uid_given
94,redirect location http://mysite.com/
94,if !login_page secure
94,redirect location / clear-cookie USERID=
94,if logout
94,Example:
94,Send redirects for request for articles without a '/'.acl missing_slash path_reg ^/article/[^/]*$
94,redirect code 301 prefix / drop-query append-slash if missing_slash
94,Example:
94,Redirect all HTTP traffic to HTTPS when SSL is handled by haproxy.redirect scheme https if !{ ssl_fc }
94,Example:
94,Append 'www.' prefix in front of all hosts not having ithttp-request redirect code 301 location
94,http://www.%[hdr(host)]%[capture.req.uri]
94,unless { hdr_beg(host) -i www }
94,See section 7 about ACL usage.
94,redisp (deprecated)redispatch (deprecated)Enable or disable session redistribution in case of connection failure
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : none
94,"In HTTP mode, if a server designated by a cookie is down, clients may"
94,"definitely stick to it because they cannot flush the cookie, so they will not"
94,be able to access the service anymore.
94,"Specifying ""redispatch"" will allow the proxy to break their persistence and"
94,redistribute them to a working server.
94,It also allows to retry last connection to another server in case of multiple
94,"connection failures. Of course, it requires having ""retries"" set to a nonzero"
94,value.
94,"This form is deprecated, do not use it in any new configuration, use the new"
94,"""option redispatch"" instead."
94,"See also : ""option redispatch"""
94,reqadd <string> [{if | unless} <cond>]Add a header at the end of the HTTP request
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,Arguments :<string>
94,is the complete line to be added. Any space or known delimiter
94,must be escaped using a backslash ('\'). Please refer to section
94,6 about HTTP header manipulation for more information.
94,<cond>
94,is an optional matching condition built from ACLs. It makes it
94,possible to ignore this rule when other conditions are not met.
94,A new line consisting in <string> followed by a line feed will be added after
94,the last header of an HTTP request.
94,"Header transformations only apply to traffic which passes through HAProxy,"
94,"and not to traffic generated by HAProxy, such as health-checks or error"
94,responses.
94,Example :
94,"Add ""X-Proto: SSL"" to requests coming via port 81acl is-ssl"
94,dst_port
94,reqadd
94,X-Proto:\ SSL
94,if is-ssl
94,"See also: ""rspadd"", ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"", section 6 about HTTP header manipulation, and section 7 about ACLs."
94,reqallow <search> [{if | unless} <cond>]reqiallow <search> [{if | unless} <cond>] (ignore case)Definitely allow an HTTP request if a line matches a regular expression
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,Arguments :<search>
94,is the regular expression applied to HTTP headers and to the
94,request line. This is an extended regular expression. Parenthesis
94,grouping is supported and no preliminary backslash is required.
94,Any space or known delimiter must be escaped using a backslash
94,('\'). The pattern applies to a full line at a time. The
94,"""reqallow"" keyword strictly matches case while ""reqiallow"""
94,ignores case.
94,<cond>
94,is an optional matching condition built from ACLs. It makes it
94,possible to ignore this rule when other conditions are not met.
94,A request containing any line which matches extended regular expression
94,"<search> will mark the request as allowed, even if any later test would"
94,result in a deny. The test applies both to the request line and to request
94,headers. Keep in mind that URLs in request line are case-sensitive while
94,header names are not.
94,"It is easier, faster and more powerful to use ACLs to write access policies."
94,"Reqdeny, reqallow and reqpass should be avoided in new designs."
94,Example :
94,# allow www.* but refuse *.local
94,reqiallow ^Host:\ www\.
94,reqideny
94,^Host:\ .*\.local
94,"See also: ""reqdeny"", ""block"", ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"","
94,"section 6 about HTTP header manipulation, and section 7 about ACLs."
94,reqdel <search> [{if | unless} <cond>]reqidel <search> [{if | unless} <cond>]
94,(ignore case)Delete all headers matching a regular expression in an HTTP request
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,Arguments :<search>
94,is the regular expression applied to HTTP headers and to the
94,request line. This is an extended regular expression. Parenthesis
94,grouping is supported and no preliminary backslash is required.
94,Any space or known delimiter must be escaped using a backslash
94,"('\'). The pattern applies to a full line at a time. The ""reqdel"""
94,"keyword strictly matches case while ""reqidel"" ignores case."
94,<cond>
94,is an optional matching condition built from ACLs. It makes it
94,possible to ignore this rule when other conditions are not met.
94,Any header line matching extended regular expression <search> in the request
94,will be completely deleted. Most common use of this is to remove unwanted
94,and/or dangerous headers or cookies from a request before passing it to the
94,next servers.
94,"Header transformations only apply to traffic which passes through HAProxy,"
94,"and not to traffic generated by HAProxy, such as health-checks or error"
94,responses. Keep in mind that header names are not case-sensitive.
94,Example :
94,# remove X-Forwarded-For header and SERVER cookie
94,reqidel ^X-Forwarded-For:.*
94,reqidel ^Cookie:.*SERVER=
94,"See also: ""reqadd"", ""reqrep"", ""rspdel"", ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"", section 6 about HTTP header manipulation, and section 7 about ACLs."
94,reqdeny <search> [{if | unless} <cond>]reqideny <search> [{if | unless} <cond>]
94,(ignore case)Deny an HTTP request if a line matches a regular expression
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,Arguments :<search>
94,is the regular expression applied to HTTP headers and to the
94,request line. This is an extended regular expression. Parenthesis
94,grouping is supported and no preliminary backslash is required.
94,Any space or known delimiter must be escaped using a backslash
94,('\'). The pattern applies to a full line at a time. The
94,"""reqdeny"" keyword strictly matches case while ""reqideny"" ignores"
94,case.
94,<cond>
94,is an optional matching condition built from ACLs. It makes it
94,possible to ignore this rule when other conditions are not met.
94,A request containing any line which matches extended regular expression
94,"<search> will mark the request as denied, even if any later test would"
94,result in an allow. The test applies both to the request line and to request
94,headers. Keep in mind that URLs in request line are case-sensitive while
94,header names are not.
94,"A denied request will generate an ""HTTP 403 forbidden"" response once the"
94,complete request has been parsed. This is consistent with what is practiced
94,using ACLs.
94,"It is easier, faster and more powerful to use ACLs to write access policies."
94,"Reqdeny, reqallow and reqpass should be avoided in new designs."
94,Example :
94,"# refuse *.local, then allow www.*"
94,reqideny
94,^Host:\ .*\.local
94,reqiallow ^Host:\ www\.
94,"See also: ""reqallow"", ""rspdeny"", ""block"", ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"", section 6 about HTTP header manipulation, and section 7 about ACLs."
94,reqpass <search> [{if | unless} <cond>]reqipass <search> [{if | unless} <cond>]
94,(ignore case)Ignore any HTTP request line matching a regular expression in next rules
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,Arguments :<search>
94,is the regular expression applied to HTTP headers and to the
94,request line. This is an extended regular expression. Parenthesis
94,grouping is supported and no preliminary backslash is required.
94,Any space or known delimiter must be escaped using a backslash
94,('\'). The pattern applies to a full line at a time. The
94,"""reqpass"" keyword strictly matches case while ""reqipass"" ignores"
94,case.
94,<cond>
94,is an optional matching condition built from ACLs. It makes it
94,possible to ignore this rule when other conditions are not met.
94,A request containing any line which matches extended regular expression
94,"<search> will skip next rules, without assigning any deny or allow verdict."
94,The test applies both to the request line and to request headers. Keep in
94,mind that URLs in request line are case-sensitive while header names are not.
94,"It is easier, faster and more powerful to use ACLs to write access policies."
94,"Reqdeny, reqallow and reqpass should be avoided in new designs."
94,Example :
94,"# refuse *.local, then allow www.*, but ignore ""www.private.local"""
94,reqipass
94,^Host:\ www.private\.local
94,reqideny
94,^Host:\ .*\.local
94,reqiallow ^Host:\ www\.
94,"See also: ""reqallow"", ""reqdeny"", ""block"", ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"", section 6 about HTTP header manipulation, and section 7 about ACLs."
94,reqrep <search> <string> [{if | unless} <cond>]reqirep <search> <string> [{if | unless} <cond>]
94,(ignore case)Replace a regular expression with a string in an HTTP request line
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,Arguments :<search>
94,is the regular expression applied to HTTP headers and to the
94,request line. This is an extended regular expression. Parenthesis
94,grouping is supported and no preliminary backslash is required.
94,Any space or known delimiter must be escaped using a backslash
94,"('\'). The pattern applies to a full line at a time. The ""reqrep"""
94,"keyword strictly matches case while ""reqirep"" ignores case."
94,<string>
94,is the complete line to be added. Any space or known delimiter
94,must be escaped using a backslash ('\'). References to matched
94,"pattern groups are possible using the common \N form, with N"
94,being a single digit between 0 and 9. Please refer to section
94,6 about HTTP header manipulation for more information.
94,<cond>
94,is an optional matching condition built from ACLs. It makes it
94,possible to ignore this rule when other conditions are not met.
94,Any line matching extended regular expression <search> in the request (both
94,the request line and header lines) will be completely replaced with <string>.
94,"Most common use of this is to rewrite URLs or domain names in ""Host"" headers."
94,"Header transformations only apply to traffic which passes through HAProxy,"
94,"and not to traffic generated by HAProxy, such as health-checks or error"
94,"responses. Note that for increased readability, it is suggested to add enough"
94,spaces between the request and the response. Keep in mind that URLs in
94,request line are case-sensitive while header names are not.
94,Example :
94,"# replace ""/static/"" with ""/"" at the beginning of any request path."
94,reqrep ^([^\ :]*)\ /static/(.*)
94,\1\ /\2
94,"# replace ""www.mydomain.com"" with ""www"" in the host name."
94,reqirep ^Host:\ www.mydomain.com
94,Host:\ www
94,"See also: ""reqadd"", ""reqdel"", ""rsprep"", ""tune.bufsize"", ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"", section 6 about HTTP header manipulation, and section 7 about ACLs."
94,reqtarpit <search> [{if | unless} <cond>]reqitarpit <search> [{if | unless} <cond>]
94,(ignore case)Tarpit an HTTP request containing a line matching a regular expression
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,Arguments :<search>
94,is the regular expression applied to HTTP headers and to the
94,request line. This is an extended regular expression. Parenthesis
94,grouping is supported and no preliminary backslash is required.
94,Any space or known delimiter must be escaped using a backslash
94,('\'). The pattern applies to a full line at a time. The
94,"""reqtarpit"" keyword strictly matches case while ""reqitarpit"""
94,ignores case.
94,<cond>
94,is an optional matching condition built from ACLs. It makes it
94,possible to ignore this rule when other conditions are not met.
94,A request containing any line which matches extended regular expression
94,"<search> will be tarpitted, which means that it will connect to nowhere, will"
94,"be kept open for a pre-defined time, then will return an HTTP error 500 so"
94,that the attacker does not suspect it has been tarpitted. The status 500 will
94,"be reported in the logs, but the completion flags will indicate ""PT"". The"
94,"delay is defined by ""timeout tarpit"", or ""timeout connect"" if the former is"
94,not set.
94,The goal of the tarpit is to slow down robots attacking servers with
94,identifiable requests. Many robots limit their outgoing number of connections
94,and stay connected waiting for a reply which can take several minutes to
94,"come. Depending on the environment and attack, it may be particularly"
94,efficient at reducing the load on the network and firewalls.
94,Examples :
94,"# ignore user-agents reporting any flavor of ""Mozilla"" or ""MSIE"", but"
94,# block all others.
94,reqipass
94,^User-Agent:\.*(Mozilla|MSIE)
94,reqitarpit ^User-Agent:
94,# block bad guys
94,acl badguys src 10.1.0.3 172.16.13.20/28
94,reqitarpit . if badguys
94,"See also: ""reqallow"", ""reqdeny"", ""reqpass"", ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"","
94,"section 6 about HTTP header manipulation, and section 7 about ACLs."
94,retries <value>Set the number of retries to perform on a server after a connection failure
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments :<value>
94,is the number of times a connection attempt should be retried on
94,a server when a connection either is refused or times out. The
94,default value is 3.
94,It is important to understand that this value applies to the number of
94,"connection attempts, not full requests. When a connection has effectively"
94,"been established to a server, there will be no more retry."
94,"In order to avoid immediate reconnections to a server which is restarting,"
94,"a turn-around timer of min(""timeout connect"", one second) is applied before"
94,a retry occurs.
94,"When ""option redispatch"" is set, the last retry may be performed on another"
94,server even if a cookie references a different server.
94,"See also : ""option redispatch"""
94,rspadd <string> [{if | unless} <cond>]Add a header at the end of the HTTP response
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,Arguments :<string>
94,is the complete line to be added. Any space or known delimiter
94,must be escaped using a backslash ('\'). Please refer to section
94,6 about HTTP header manipulation for more information.
94,<cond>
94,is an optional matching condition built from ACLs. It makes it
94,possible to ignore this rule when other conditions are not met.
94,A new line consisting in <string> followed by a line feed will be added after
94,the last header of an HTTP response.
94,"Header transformations only apply to traffic which passes through HAProxy,"
94,"and not to traffic generated by HAProxy, such as health-checks or error"
94,responses.
94,"See also: ""rspdel"" ""reqadd"", ""http-responseThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"", section 6 about HTTP header manipulation, and section 7 about ACLs."
94,rspdel <search> [{if | unless} <cond>]rspidel <search> [{if | unless} <cond>]
94,(ignore case)Delete all headers matching a regular expression in an HTTP response
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,Arguments :<search>
94,is the regular expression applied to HTTP headers and to the
94,"response line. This is an extended regular expression, so"
94,parenthesis grouping is supported and no preliminary backslash
94,is required. Any space or known delimiter must be escaped using
94,a backslash ('\'). The pattern applies to a full line at a time.
94,"The ""rspdel"" keyword strictly matches case while ""rspidel"""
94,ignores case.
94,<cond>
94,is an optional matching condition built from ACLs. It makes it
94,possible to ignore this rule when other conditions are not met.
94,Any header line matching extended regular expression <search> in the response
94,will be completely deleted. Most common use of this is to remove unwanted
94,and/or sensitive headers or cookies from a response before passing it to the
94,client.
94,"Header transformations only apply to traffic which passes through HAProxy,"
94,"and not to traffic generated by HAProxy, such as health-checks or error"
94,responses. Keep in mind that header names are not case-sensitive.
94,Example :
94,# remove the Server header from responses
94,rspidel ^Server:.*
94,"See also: ""rspadd"", ""rsprep"", ""reqdel"", ""http-responseThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"","
94,"section 6 about HTTP header manipulation, and section 7 about ACLs."
94,rspdeny <search> [{if | unless} <cond>]rspideny <search> [{if | unless} <cond>]
94,(ignore case)Block an HTTP response if a line matches a regular expression
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,Arguments :<search>
94,is the regular expression applied to HTTP headers and to the
94,"response line. This is an extended regular expression, so"
94,parenthesis grouping is supported and no preliminary backslash
94,is required. Any space or known delimiter must be escaped using
94,a backslash ('\'). The pattern applies to a full line at a time.
94,"The ""rspdeny"" keyword strictly matches case while ""rspideny"""
94,ignores case.
94,<cond>
94,is an optional matching condition built from ACLs. It makes it
94,possible to ignore this rule when other conditions are not met.
94,A response containing any line which matches extended regular expression
94,<search> will mark the request as denied. The test applies both to the
94,response line and to response headers. Keep in mind that header names are not
94,case-sensitive.
94,Main use of this keyword is to prevent sensitive information leak and to
94,"block the response before it reaches the client. If a response is denied, it"
94,will be replaced with an HTTP 502 error so that the client never retrieves
94,any sensitive data.
94,"It is easier, faster and more powerful to use ACLs to write access policies."
94,Rspdeny should be avoided in new designs.
94,Example :
94,# Ensure that no content type matching ms-word will leak
94,rspideny
94,^Content-type:\.*/ms-word
94,"See also: ""reqdeny"", ""acl"", ""block"", ""http-responseThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"","
94,section 6 about HTTP header manipulation and section 7 about ACLs.
94,rsprep <search> <string> [{if | unless} <cond>]rspirep <search> <string> [{if | unless} <cond>]
94,(ignore case)Replace a regular expression with a string in an HTTP response line
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,Arguments :<search>
94,is the regular expression applied to HTTP headers and to the
94,"response line. This is an extended regular expression, so"
94,parenthesis grouping is supported and no preliminary backslash
94,is required. Any space or known delimiter must be escaped using
94,a backslash ('\'). The pattern applies to a full line at a time.
94,"The ""rsprep"" keyword strictly matches case while ""rspirep"""
94,ignores case.
94,<string>
94,is the complete line to be added. Any space or known delimiter
94,must be escaped using a backslash ('\'). References to matched
94,"pattern groups are possible using the common \N form, with N"
94,being a single digit between 0 and 9. Please refer to section
94,6 about HTTP header manipulation for more information.
94,<cond>
94,is an optional matching condition built from ACLs. It makes it
94,possible to ignore this rule when other conditions are not met.
94,Any line matching extended regular expression <search> in the response (both
94,the response line and header lines) will be completely replaced with
94,<string>. Most common use of this is to rewrite Location headers.
94,"Header transformations only apply to traffic which passes through HAProxy,"
94,"and not to traffic generated by HAProxy, such as health-checks or error"
94,"responses. Note that for increased readability, it is suggested to add enough"
94,spaces between the request and the response. Keep in mind that header names
94,are not case-sensitive.
94,Example :
94,"# replace ""Location: 127.0.0.1:8080"" with ""Location: www.mydomain.com"""
94,rspirep ^Location:\ 127.0.0.1:8080
94,Location:\ www.mydomain.com
94,"See also: ""rspadd"", ""rspdel"", ""reqrep"", ""http-responseThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"", section 6 about HTTP header manipulation, and section 7 about ACLs."
94,server <name> <address>[:[port]] [param*]Declare a server in a backend
94,May be used in sections
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,Arguments :<name>
94,is the internal name assigned to this server. This name will
94,"appear in logs and alerts. If ""http-send-name-header"" is"
94,"set, it will be added to the request header sent to the server."
94,"<address> is the IPv4 or IPv6 address of the server. Alternatively, a"
94,"resolvable hostname is supported, but this name will be resolved"
94,"during start-up. Address ""0.0.0.0"" or ""*"" has a special meaning."
94,It indicates that the connection will be forwarded to the same IP
94,address as the one from the client connection. This is useful in
94,transparent proxy architectures where the client's connection is
94,intercepted and haproxy must forward to the original destination
94,"address. This is more or less what the ""transparent"" keyword does"
94,except that with a server it's possible to limit concurrency and
94,"to report statistics. Optionally, an address family prefix may be"
94,used before the address to force the family regardless of the
94,"address format, which can be useful to specify a path to a unix"
94,socket with no slash ('/'). Currently supported prefixes are :
94,- 'ipv4@'
94,-> address is always IPv4
94,- 'ipv6@'
94,-> address is always IPv6
94,- 'unix@'
94,-> address is a path to a local unix socket
94,- 'abns@'
94,-> address is in abstract namespace (Linux only)
94,You may want to reference some environment variables in the
94,"address parameter, see section 2.3 about environment"
94,"variables. The ""init-addr"" setting can be used to modify the way"
94,IP addresses should be resolved upon startup.
94,<port>
94,"is an optional port specification. If set, all connections will"
94,"be sent to this port. If unset, the same port the client"
94,"connected to will be used. The port may also be prefixed by a ""+"""
94,"or a ""-"". In this case, the server's port will be determined by"
94,adding this value to the client's port.
94,<param*>
94,"is a list of parameters for this server. The ""server"" keywords"
94,accepts an important number of options and has a complete section
94,dedicated to it. Please refer to section 5 for more details.
94,Examples :
94,server first
94,10.1.1.1:1080 cookie first
94,check inter 1000
94,server second 10.1.1.2:1080 cookie second check inter 1000
94,server transp ipv4@
94,"server backup ""${SRV_BACKUP}:1080"" backup"
94,"server www1_dc1 ""${LAN_DC1}.101:80"""
94,"server www1_dc2 ""${LAN_DC2}.101:80"""
94,"Note: regarding Linux's abstract namespace sockets, HAProxy uses the whole"
94,sun_path length is used for the address length. Some other programs
94,such as socat use the string length only by default. Pass the option
94,""",unix-tightsocklen=0"" to any abstract socket definition in socat to"
94,make it compatible with HAProxy's.
94,"See also: ""default-server"", ""http-send-name-header"" and section 5 about server options"
94,"server-state-file-name [<file>]Set the server state file to read, load and apply to servers available in"
94,"this backend. It only applies when the directive ""load-server-state-from-file"""
94,"is set to ""local"". When <file> is not provided or if this directive is not"
94,"set, then backend name is used. If <file> starts with a slash '/', then it is"
94,"considered as an absolute path. Otherwise, <file> is concatenated to the"
94,"global directive ""server-state-file-base""."
94,Example:
94,The minimal configuration below would make HAProxy look for the state server file '/etc/haproxy/states/bk':global
94,server-state-file-base /etc/haproxy/states
94,backend bk
94,load-server-state-from-file
94,"See also: ""server-state-file-base"", ""load-server-state-from-file"", and ""show servers state"""
94,server-template <prefix> <num | range> <fqdn>[:<port>] [params*]Set a template to initialize servers with shared parameters.
94,The names of these servers are built from <prefix> and <num | range> parameters.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,Arguments:<prefix>
94,A prefix for the server names to be built.
94,<num | range>
94,"If <num> is provided, this template initializes <num> servers"
94,with 1 up to <num> as server name suffixes. A range of numbers
94,<num_low>-<num_high> may also be used to use <num_low> up to
94,<num_high> as server name suffixes.
94,<fqdn>
94,A FQDN for all the servers this template initializes.
94,<port>
94,"Same meaning as ""server"" <port> argument (see ""server"" keyword)."
94,<params*>
94,"Remaining server parameters among all those supported by ""server"""
94,keyword.
94,Examples:
94,"# Initializes 3 servers with srv1, srv2 and srv3 as names,"
94,"# google.com as FQDN, and health-check enabled."
94,server-template srv 1-3 google.com:80 check
94,# or
94,server-template srv 3 google.com:80 check
94,# would be equivalent to:
94,server srv1 google.com:80 check
94,server srv2 google.com:80 check
94,server srv3 google.com:80 check
94,"source <addr>[:<port>] [usesrc { <addr2>[:<port2>] | client | clientip } ]source <addr>[:<port>] [usesrc { <addr2>[:<port2>] | hdr_ip(<hdr>[,<occ>]) } ]source <addr>[:<port>] [interface <name>]Set the source address for outgoing connections"
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments :<addr>
94,is the IPv4 address HAProxy will bind to before connecting to a
94,server. This address is also used as a source for health checks.
94,The default value of 0.0.0.0 means that the system will select
94,the most appropriate address to reach its destination. Optionally
94,an address family prefix may be used before the address to force
94,"the family regardless of the address format, which can be useful"
94,to specify a path to a unix socket with no slash ('/'). Currently
94,supported prefixes are :
94,- 'ipv4@' -> address is always IPv4
94,- 'ipv6@' -> address is always IPv6
94,- 'unix@' -> address is a path to a local unix socket
94,- 'abns@' -> address is in abstract namespace (Linux only)
94,You may want to reference some environment variables in the
94,"address parameter, see section 2.3 about environment variables."
94,<port>
94,is an optional port. It is normally not needed but may be useful
94,in some very specific contexts. The default value of zero means
94,the system will select a free port. Note that port ranges are not
94,"supported in the backend. If you want to force port ranges, you"
94,"have to specify them on each ""server"" line."
94,<addr2>
94,is the IP address to present to the server when connections are
94,forwarded in full transparent proxy mode. This is currently only
94,supported on some patched Linux kernels. When this address is
94,"specified, clients connecting to the server will be presented"
94,"with this address, while health checks will still use the address"
94,<addr>.
94,<port2>
94,is the optional port to present to the server when connections
94,are forwarded in full transparent proxy mode (see <addr2> above).
94,The default value of zero means the system will select a free
94,port.
94,<hdr>
94,is the name of a HTTP header in which to fetch the IP to bind to.
94,This is the name of a comma-separated header list which can
94,"contain multiple IP addresses. By default, the last occurrence is"
94,used. This is designed to work with the X-Forwarded-For header
94,and to automatically bind to the client's IP address as seen
94,"by previous proxy, typically Stunnel. In order to use another"
94,"occurrence from the last one, please see the <occ> parameter"
94,"below. When the header (or occurrence) is not found, no binding"
94,is performed so that the proxy's default IP address is used. Also
94,"keep in mind that the header name is case insensitive, as for any"
94,HTTP header.
94,<occ>
94,is the occurrence number of a value to be used in a multi-value
94,"header. This is to be used in conjunction with ""hdr_ip(<hdr>)"","
94,in order to specify which occurrence to use for the source IP
94,address. Positive values indicate a position from the first
94,"occurrence, 1 being the first one. Negative values indicate"
94,"positions relative to the last one, -1 being the last one. This"
94,is helpful for situations where an X-Forwarded-For header is set
94,at the entry point of an infrastructure and must be used several
94,"proxy layers away. When this value is not specified, -1 is"
94,assumed. Passing a zero here disables the feature.
94,<name>
94,is an optional interface name to which to bind to for outgoing
94,"traffic. On systems supporting this features (currently, only"
94,"Linux), this allows one to bind all traffic to the server to"
94,this interface even if it is not the one the system would select
94,based on routing tables. This should be used with extreme care.
94,Note that using this option requires root privileges.
94,"The ""sourceThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"" keyword is useful in complex environments where a specific"
94,address only is allowed to connect to the servers. It may be needed when a
94,"private address must be used through a public gateway for instance, and it is"
94,known that the system cannot determine the adequate source address by itself.
94,An extension which is available on certain patched Linux kernels may be used
94,"through the ""usesrc"" optional keyword. It makes it possible to connect to the"
94,servers with an IP address which does not belong to the system itself. This
94,"is called ""full transparent proxy mode"". For this to work, the destination"
94,servers have to route their traffic back to this address through the machine
94,"running HAProxy, and IP forwarding must generally be enabled on this machine."
94,"In this ""full transparent proxy"" mode, it is possible to force a specific IP"
94,address to be presented to the servers. This is not much used in fact. A more
94,"common use is to tell HAProxy to present the client's IP address. For this,"
94,there are two methods :
94,- present the client's IP and port addresses. This is the most transparent
94,"mode, but it can cause problems when IP connection tracking is enabled on"
94,"the machine, because a same connection may be seen twice with different"
94,"states. However, this solution presents the huge advantage of not"
94,"limiting the system to the 64k outgoing address+port couples, because all"
94,of the client ranges may be used.
94,- present only the client's IP address and select a spare port. This
94,solution is still quite elegant but slightly less transparent (downstream
94,firewalls logs will not match upstream's). It also presents the downside
94,of limiting the number of concurrent connections to the usual 64k ports.
94,"However, since the upstream and downstream ports are different, local IP"
94,connection tracking on the machine will not be upset by the reuse of the
94,same session.
94,This option sets the default source for all servers in the backend. It may
94,"also be specified in a ""defaults"" section. Finer source address specification"
94,"is possible at the server level using the ""sourceThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"" server option. Refer to"
94,section 5 for more information.
94,"In order to work, ""usesrc"" requires root privileges."
94,Examples :
94,backend private
94,# Connect to the servers using our 192.168.1.200 source address
94,source 192.168.1.200
94,backend transparent_ssl1
94,# Connect to the SSL farm from the client's source address
94,source 192.168.1.200 usesrc clientip
94,backend transparent_ssl2
94,# Connect to the SSL farm from the client's source address and port
94,# not recommended if IP conntrack is present on the local machine.
94,source 192.168.1.200 usesrc client
94,backend transparent_ssl3
94,# Connect to the SSL farm from the client's source address. It
94,# is more conntrack-friendly.
94,source 192.168.1.200 usesrc clientip
94,backend transparent_smtp
94,# Connect to the SMTP farm from the client's source address/port
94,# with Tproxy version 4.
94,source 0.0.0.0 usesrc clientip
94,backend transparent_http
94,# Connect to the servers using the client's IP as seen by previous
94,# proxy.
94,"source 0.0.0.0 usesrc hdr_ip(x-forwarded-for,-1)"
94,"See also : the ""sourceThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"" server option in section 5, the Tproxy patches for the Linux kernel on www.balabit.com, the ""bind"" keyword."
94,srvtimeout <timeout> (deprecated)Set the maximum inactivity time on the server side.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,"Arguments :<timeout> is the timeout value specified in milliseconds by default, but"
94,"can be in any other unit if the number is suffixed by the unit,"
94,as explained at the top of this document.
94,The inactivity timeout applies when the server is expected to acknowledge or
94,"send data. In HTTP mode, this timeout is particularly important to consider"
94,"during the first phase of the server's response, when it has to send the"
94,"headers, as it directly represents the server's processing time for the"
94,"request. To find out what value to put there, it's often good to start with"
94,"what would be considered as unacceptable response times, then check the logs"
94,"to observe the response time distribution, and adjust the value accordingly."
94,"The value is specified in milliseconds by default, but can be in any other"
94,"unit if the number is suffixed by the unit, as specified at the top of this"
94,"document. In TCP mode (and to a lesser extent, in HTTP mode), it is highly"
94,recommended that the client timeout remains equal to the server timeout in
94,order to avoid complex situations to debug. Whatever the expected server
94,"response times, it is a good practice to cover at least one or several TCP"
94,packet losses by specifying timeouts that are slightly above multiples of 3
94,seconds (e.g. 4 or 5 seconds minimum).
94,"This parameter is specific to backends, but can be specified once for all in"
94,"""defaults"" sections. This is in fact one of the easiest solutions not to"
94,"forget about it. An unspecified timeout results in an infinite timeout, which"
94,is not recommended. Such a usage is accepted and works but reports a warning
94,during startup because it may results in accumulation of expired sessions in
94,the system if the system's timeouts are not configured either.
94,This parameter is provided for compatibility but is currently deprecated.
94,"Please use ""timeout server"" instead."
94,"See also : ""timeout server"", ""timeout tunnel"", ""timeout client"" and ""clitimeout""."
94,stats admin { if | unless } <cond>Enable statistics admin level if/unless a condition is matched
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,This statement enables the statistics admin level if/unless a condition is
94,matched.
94,The admin level allows to enable/disable servers from the web interface. By
94,"default, statistics page is read-only for security reasons."
94,Note : Consider not using this feature in multi-process mode (nbproc > 1)
94,unless you know what you do : memory is not shared between the
94,"processes, which can result in random behaviors."
94,"Currently, the POST request is limited to the buffer size minus the reserved"
94,"buffer space, which means that if the list of servers is too long, the"
94,request won't be processed. It is recommended to alter few servers at a
94,time.
94,Example :
94,# statistics admin level only for localhost
94,backend stats_localhost
94,stats enable
94,stats admin if LOCALHOST
94,Example :
94,# statistics admin level always enabled because of the authentication
94,backend stats_auth
94,stats enable
94,stats auth
94,admin:AdMiN123
94,stats admin if TRUE
94,Example :
94,# statistics admin level depends on the authenticated user
94,userlist stats-auth
94,group admin
94,users admin
94,user
94,admin
94,insecure-password AdMiN123
94,group readonly users haproxy
94,user
94,haproxy
94,insecure-password haproxy
94,backend stats_auth
94,stats enable
94,acl AUTH
94,http_auth(stats-auth)
94,acl AUTH_ADMIN http_auth_group(stats-auth) admin
94,stats http-request auth unless AUTH
94,stats admin if AUTH_ADMIN
94,"See also : ""stats enable"", ""stats auth"", ""stats http-request"", ""nbprocThis keyword is available in sections :Process management and securityFetching samples from internal states"", ""bind-process"", section 3.4 about userlists and section 7 about ACL usage."
94,stats auth <user>:<passwd>Enable statistics with authentication and grant access to an account
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :<user>
94,is a user name to grant access to
94,<passwd>
94,is the cleartext password associated to this user
94,"This statement enables statistics with default settings, and restricts access"
94,to declared users only. It may be repeated as many times as necessary to
94,allow as many users as desired. When a user tries to access the statistics
94,"without a valid account, a ""401 Forbidden"" response will be returned so that"
94,the browser asks the user to provide a valid user and password. The real
94,"which will be returned to the browser is configurable using ""stats realm""."
94,"Since the authentication method is HTTP Basic Authentication, the passwords"
94,"circulate in cleartext on the network. Thus, it was decided that the"
94,configuration file would also use cleartext passwords to remind the users
94,that those ones should not be sensitive and not shared with any other account.
94,It is also possible to reduce the scope of the proxies which appear in the
94,"report using ""stats scope""."
94,"Though this statement alone is enough to enable statistics reporting, it is"
94,recommended to set all other settings in order to avoid relying on default
94,unobvious parameters.
94,Example :
94,# public access (limited to this backend only)
94,backend public_www
94,server srv1 192.168.0.1:80
94,stats enable
94,stats hide-version
94,stats scope
94,stats uri
94,/admin?stats
94,stats realm
94,HAProxy\ Statistics
94,stats auth
94,admin1:AdMiN123
94,stats auth
94,admin2:AdMiN321
94,# internal monitoring access (unlimited)
94,backend private_monitoring
94,stats enable
94,stats uri
94,/admin?stats
94,stats refresh 5s
94,"See also : ""stats enable"", ""stats realm"", ""stats scope"", ""stats uri"""
94,stats enableEnable statistics reporting with default settings
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,This statement enables statistics reporting with default settings defined
94,"at build time. Unless stated otherwise, these settings are used :"
94,- stats uri
94,: /haproxy?stats
94,"- stats realm : ""HAProxy Statistics"""
94,- stats auth
94,: no authentication
94,- stats scope : no restriction
94,"Though this statement alone is enough to enable statistics reporting, it is"
94,recommended to set all other settings in order to avoid relying on default
94,unobvious parameters.
94,Example :
94,# public access (limited to this backend only)
94,backend public_www
94,server srv1 192.168.0.1:80
94,stats enable
94,stats hide-version
94,stats scope
94,stats uri
94,/admin?stats
94,stats realm
94,HAProxy\ Statistics
94,stats auth
94,admin1:AdMiN123
94,stats auth
94,admin2:AdMiN321
94,# internal monitoring access (unlimited)
94,backend private_monitoring
94,stats enable
94,stats uri
94,/admin?stats
94,stats refresh 5s
94,"See also : ""stats auth"", ""stats realm"", ""stats uri"""
94,stats hide-versionEnable statistics and hide HAProxy version reporting
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,"By default, the stats page reports some useful status information along with"
94,"the statistics. Among them is HAProxy's version. However, it is generally"
94,"considered dangerous to report precise version to anyone, as it can help them"
94,"target known weaknesses with specific attacks. The ""stats hide-version"""
94,statement removes the version from the statistics report. This is recommended
94,for public sites or any site with a weak login/password.
94,"Though this statement alone is enough to enable statistics reporting, it is"
94,recommended to set all other settings in order to avoid relying on default
94,unobvious parameters.
94,Example :
94,# public access (limited to this backend only)
94,backend public_www
94,server srv1 192.168.0.1:80
94,stats enable
94,stats hide-version
94,stats scope
94,stats uri
94,/admin?stats
94,stats realm
94,HAProxy\ Statistics
94,stats auth
94,admin1:AdMiN123
94,stats auth
94,admin2:AdMiN321
94,# internal monitoring access (unlimited)
94,backend private_monitoring
94,stats enable
94,stats uri
94,/admin?stats
94,stats refresh 5s
94,"See also : ""stats auth"", ""stats enable"", ""stats realm"", ""stats uri"""
94,stats http-request { allow | deny | auth [realm <realm>] }
94,[ { if | unless } <condition> ]Access control for statistics
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,"As ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"", these set of options allow to fine control access to"
94,statistics. Each option may be followed by if/unless and acl.
94,First option with matched condition (or option without condition) is final.
94,"For ""deny"" a 403 error will be returned, for ""allow"" normal processing is"
94,"performed, for ""auth"" a 401/407 error code is returned so the client"
94,should be asked to enter a username and password.
94,There is no fixed limit to the number of http-request statements per
94,instance.
94,"See also : ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"", section 3.4 about userlists and section 7 about ACL usage."
94,stats realm <realm>Enable statistics and set authentication realm
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :<realm>
94,is the name of the HTTP Basic Authentication realm reported to
94,the browser. The browser uses it to display it in the pop-up
94,inviting the user to enter a valid username and password.
94,"The realm is read as a single word, so any spaces in it should be escaped"
94,using a backslash ('\').
94,"This statement is useful only in conjunction with ""stats auth"" since it is"
94,only related to authentication.
94,"Though this statement alone is enough to enable statistics reporting, it is"
94,recommended to set all other settings in order to avoid relying on default
94,unobvious parameters.
94,Example :
94,# public access (limited to this backend only)
94,backend public_www
94,server srv1 192.168.0.1:80
94,stats enable
94,stats hide-version
94,stats scope
94,stats uri
94,/admin?stats
94,stats realm
94,HAProxy\ Statistics
94,stats auth
94,admin1:AdMiN123
94,stats auth
94,admin2:AdMiN321
94,# internal monitoring access (unlimited)
94,backend private_monitoring
94,stats enable
94,stats uri
94,/admin?stats
94,stats refresh 5s
94,"See also : ""stats auth"", ""stats enable"", ""stats uri"""
94,stats refresh <delay>Enable statistics with automatic refresh
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :<delay>
94,"is the suggested refresh delay, specified in seconds, which will"
94,be returned to the browser consulting the report page. While the
94,"browser is free to apply any delay, it will generally respect it"
94,and refresh the page this every seconds. The refresh interval may
94,"be specified in any other non-default time unit, by suffixing the"
94,"unit after the value, as explained at the top of this document."
94,This statement is useful on monitoring displays with a permanent page
94,"reporting the load balancer's activity. When set, the HTML report page will"
94,"include a link ""refresh""/""stop refresh"" so that the user can select whether"
94,he wants automatic refresh of the page or not.
94,"Though this statement alone is enough to enable statistics reporting, it is"
94,recommended to set all other settings in order to avoid relying on default
94,unobvious parameters.
94,Example :
94,# public access (limited to this backend only)
94,backend public_www
94,server srv1 192.168.0.1:80
94,stats enable
94,stats hide-version
94,stats scope
94,stats uri
94,/admin?stats
94,stats realm
94,HAProxy\ Statistics
94,stats auth
94,admin1:AdMiN123
94,stats auth
94,admin2:AdMiN321
94,# internal monitoring access (unlimited)
94,backend private_monitoring
94,stats enable
94,stats uri
94,/admin?stats
94,stats refresh 5s
94,"See also : ""stats auth"", ""stats enable"", ""stats realm"", ""stats uri"""
94,"stats scope { <name> | ""."" }Enable statistics and limit access scope"
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :<name>
94,"is the name of a listen, frontend or backend section to be"
94,"reported. The special name ""."" (a single dot) designates the"
94,section in which the statement appears.
94,"When this statement is specified, only the sections enumerated with this"
94,statement will appear in the report. All other ones will be hidden. This
94,statement may appear as many times as needed if multiple sections need to be
94,reported. Please note that the name checking is performed as simple string
94,"comparisons, and that it is never checked that a give section name really"
94,exists.
94,"Though this statement alone is enough to enable statistics reporting, it is"
94,recommended to set all other settings in order to avoid relying on default
94,unobvious parameters.
94,Example :
94,# public access (limited to this backend only)
94,backend public_www
94,server srv1 192.168.0.1:80
94,stats enable
94,stats hide-version
94,stats scope
94,stats uri
94,/admin?stats
94,stats realm
94,HAProxy\ Statistics
94,stats auth
94,admin1:AdMiN123
94,stats auth
94,admin2:AdMiN321
94,# internal monitoring access (unlimited)
94,backend private_monitoring
94,stats enable
94,stats uri
94,/admin?stats
94,stats refresh 5s
94,"See also : ""stats auth"", ""stats enable"", ""stats realm"", ""stats uri"""
94,stats show-desc [ <desc> ]Enable reporting of a description on the statistics page.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,<desc>
94,"is an optional description to be reported. If unspecified, the"
94,description from global section is automatically used instead.
94,This statement is useful for users that offer shared services to their
94,"customers, where node or description should be different for each customer."
94,"Though this statement alone is enough to enable statistics reporting, it is"
94,recommended to set all other settings in order to avoid relying on default
94,unobvious parameters. By default description is not shown.
94,Example :
94,# internal monitoring access (unlimited)
94,backend private_monitoring
94,stats enable
94,"stats show-desc Master node for Europe, Asia, Africa"
94,stats uri
94,/admin?stats
94,stats refresh
94,"See also: ""show-node"", ""stats enable"", ""stats uri"" and ""descriptionThis keyword is available in sections :Process management and securityAlphabetically sorted keywords reference"" in global section."
94,stats show-legendsEnable reporting additional information on the statistics page
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments : none
94,Enable reporting additional information on the statistics page :
94,- cap: capabilities (proxy)
94,"- mode: one of tcp, http or health (proxy)"
94,"- id: SNMP ID (proxy, socket, server)"
94,"- IP (socket, server)"
94,"- cookie (backend, server)"
94,"Though this statement alone is enough to enable statistics reporting, it is"
94,recommended to set all other settings in order to avoid relying on default
94,unobvious parameters. Default behavior is not to show this information.
94,"See also: ""stats enable"", ""stats uri""."
94,stats show-node [ <name> ]Enable reporting of a host name on the statistics page.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments:<name>
94,"is an optional name to be reported. If unspecified, the"
94,node name from global section is automatically used instead.
94,This statement is useful for users that offer shared services to their
94,"customers, where node or description might be different on a stats page"
94,provided for each customer. Default behavior is not to show host name.
94,"Though this statement alone is enough to enable statistics reporting, it is"
94,recommended to set all other settings in order to avoid relying on default
94,unobvious parameters.
94,Example:
94,# internal monitoring access (unlimited)
94,backend private_monitoring
94,stats enable
94,stats show-node Europe-1
94,stats uri
94,/admin?stats
94,stats refresh
94,"See also: ""show-desc"", ""stats enable"", ""stats uri"", and ""node"" in global section."
94,stats uri <prefix>Enable statistics and define the URI prefix to access them
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,Arguments :<prefix>
94,is the prefix of any URI which will be redirected to stats. This
94,prefix may contain a question mark ('?') to indicate part of a
94,query string.
94,"The statistics URI is intercepted on the relayed traffic, so it appears as a"
94,page within the normal application. It is strongly advised to ensure that the
94,"selected URI will never appear in the application, otherwise it will never be"
94,possible to reach it in the application.
94,"The default URI compiled in haproxy is ""/haproxy?stats"", but this may be"
94,"changed at build time, so it's better to always explicitly specify it here."
94,It is generally a good idea to include a question mark in the URI so that
94,"intermediate proxies refrain from caching the results. Also, since any string"
94,"beginning with the prefix will be accepted as a stats request, the question"
94,mark helps ensuring that no valid URI will begin with the same words.
94,"It is sometimes very convenient to use ""/"" as the URI prefix, and put that"
94,"statement in a ""listen"" instance of its own. That makes it easy to dedicate"
94,an address or a port to statistics only.
94,"Though this statement alone is enough to enable statistics reporting, it is"
94,recommended to set all other settings in order to avoid relying on default
94,unobvious parameters.
94,Example :
94,# public access (limited to this backend only)
94,backend public_www
94,server srv1 192.168.0.1:80
94,stats enable
94,stats hide-version
94,stats scope
94,stats uri
94,/admin?stats
94,stats realm
94,HAProxy\ Statistics
94,stats auth
94,admin1:AdMiN123
94,stats auth
94,admin2:AdMiN321
94,# internal monitoring access (unlimited)
94,backend private_monitoring
94,stats enable
94,stats uri
94,/admin?stats
94,stats refresh 5s
94,"See also : ""stats auth"", ""stats enable"", ""stats realm"""
94,stick match <pattern> [table <table>] [{if | unless} <cond>]Define a request pattern matching condition to stick a user to a server
94,May be used in sections
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,Arguments :<pattern>
94,is a sample expression rule as described in section 7.3. It
94,describes what elements of the incoming request or connection
94,will be analyzed in the hope to find a matching entry in a
94,stickiness table. This rule is mandatory.
94,<table>
94,"is an optional stickiness table name. If unspecified, the same"
94,backend's table is used. A stickiness table is declared using
94,"the ""stick-table"" statement."
94,<cond>
94,is an optional matching condition. It makes it possible to match
94,on a certain criterion only when other conditions are met (or
94,"not met). For instance, it could be used to match on a source IP"
94,"address except when a request passes through a known proxy, in"
94,which case we'd match on a header containing that IP address.
94,Some protocols or applications require complex stickiness rules and cannot
94,"always simply rely on cookies nor hashing. The ""stick match"" statement"
94,describes a rule to extract the stickiness criterion from an incoming request
94,or connection. See section 7 for a complete list of possible patterns and
94,transformation rules.
94,"The table has to be declared using the ""stick-table"" statement. It must be of"
94,a type compatible with the pattern. By default it is the one which is present
94,in the same backend. It is possible to share a table with other backends by
94,"referencing it using the ""table"" keyword. If another table is referenced,"
94,"the server's ID inside the backends are used. By default, all server IDs"
94,"start at 1 in each backend, so the server ordering is enough. But in case of"
94,"doubt, it is highly recommended to force server IDs using their ""idThis keyword is available in sections :Alphabetically sorted keywords referenceBind optionsServer and default-server options"" setting."
94,"It is possible to restrict the conditions where a ""stick match"" statement"
94,"will apply, using ""if"" or ""unless"" followed by a condition. See section 7 for"
94,ACL based conditions.
94,"There is no limit on the number of ""stick match"" statements. The first that"
94,applies and matches will cause the request to be directed to the same server
94,"as was used for the request which created the entry. That way, multiple"
94,matches can be used as fallbacks.
94,"The stick rules are checked after the persistence cookies, so they will not"
94,affect stickiness if a cookie has already been used to select a server. That
94,"way, it becomes very easy to insert cookies and match on IP addresses in"
94,order to maintain stickiness between HTTP and HTTPS.
94,Note : Consider not using this feature in multi-process mode (nbproc > 1)
94,unless you know what you do : memory is not shared between the
94,"processes, which can result in random behaviors."
94,Example :
94,# forward SMTP users to the same server they just used for POP in the
94,# last 30 minutes
94,backend pop
94,mode tcp
94,balance roundrobin
94,stick store-request src
94,stick-table type ip size 200k expire 30m
94,server s1 192.168.1.1:110
94,server s2 192.168.1.1:110
94,backend smtp
94,mode tcp
94,balance roundrobin
94,stick match src table pop
94,server s1 192.168.1.1:25
94,server s2 192.168.1.1:25
94,"See also : ""stick-table"", ""stick on"", ""nbprocThis keyword is available in sections :Process management and securityFetching samples from internal states"", ""bind-process"" and section 7 about ACLs and samples fetching."
94,stick on <pattern> [table <table>] [{if | unless} <condition>]Define a request pattern to associate a user to a server
94,May be used in sections
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,"Note : This form is exactly equivalent to ""stick match"" followed by"
94,"""stick store-request"", all with the same arguments. Please refer"
94,to both keywords for details. It is only provided as a convenience
94,for writing more maintainable configurations.
94,Note : Consider not using this feature in multi-process mode (nbproc > 1)
94,unless you know what you do : memory is not shared between the
94,"processes, which can result in random behaviors."
94,Examples :
94,# The following form ...
94,stick on src table pop if !localhost
94,# ...is strictly equivalent to this one :
94,stick match src table pop if !localhost
94,stick store-request src table pop if !localhost
94,"# Use cookie persistence for HTTP, and stick on source address for HTTPS as"
94,# well as HTTP without cookie. Share the same table between both accesses.
94,backend http
94,mode http
94,balance roundrobin
94,stick on src table https
94,cookie SRV insert indirect nocache
94,server s1 192.168.1.1:80 cookie s1
94,server s2 192.168.1.1:80 cookie s2
94,backend https
94,mode tcp
94,balance roundrobin
94,stick-table type ip size 200k expire 30m
94,stick on src
94,server s1 192.168.1.1:443
94,server s2 192.168.1.1:443
94,"See also : ""stick match"", ""stick store-request"", ""nbprocThis keyword is available in sections :Process management and securityFetching samples from internal states"" and ""bind-process""."
94,stick store-request <pattern> [table <table>] [{if | unless} <condition>]Define a request pattern used to create an entry in a stickiness table
94,May be used in sections
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,Arguments :<pattern>
94,is a sample expression rule as described in section 7.3. It
94,describes what elements of the incoming request or connection
94,"will be analyzed, extracted and stored in the table once a"
94,server is selected.
94,<table>
94,"is an optional stickiness table name. If unspecified, the same"
94,backend's table is used. A stickiness table is declared using
94,"the ""stick-table"" statement."
94,<cond>
94,is an optional storage condition. It makes it possible to store
94,certain criteria only when some conditions are met (or not met).
94,"For instance, it could be used to store the source IP address"
94,"except when the request passes through a known proxy, in which"
94,case we'd store a converted form of a header containing that IP
94,address.
94,Some protocols or applications require complex stickiness rules and cannot
94,"always simply rely on cookies nor hashing. The ""stick store-request"" statement"
94,describes a rule to decide what to extract from the request and when to do
94,"it, in order to store it into a stickiness table for further requests to"
94,"match it using the ""stick match"" statement. Obviously the extracted part must"
94,make sense and have a chance to be matched in a further request. Storing a
94,client's IP address for instance often makes sense. Storing an ID found in a
94,URL parameter also makes sense. Storing a source port will almost never make
94,any sense because it will be randomly matched. See section 7 for a complete
94,list of possible patterns and transformation rules.
94,"The table has to be declared using the ""stick-table"" statement. It must be of"
94,a type compatible with the pattern. By default it is the one which is present
94,in the same backend. It is possible to share a table with other backends by
94,"referencing it using the ""table"" keyword. If another table is referenced,"
94,"the server's ID inside the backends are used. By default, all server IDs"
94,"start at 1 in each backend, so the server ordering is enough. But in case of"
94,"doubt, it is highly recommended to force server IDs using their ""idThis keyword is available in sections :Alphabetically sorted keywords referenceBind optionsServer and default-server options"" setting."
94,"It is possible to restrict the conditions where a ""stick store-request"""
94,"statement will apply, using ""if"" or ""unless"" followed by a condition. This"
94,"condition will be evaluated while parsing the request, so any criteria can be"
94,used. See section 7 for ACL based conditions.
94,"There is no limit on the number of ""stick store-request"" statements, but"
94,there is a limit of 8 simultaneous stores per request or response. This
94,"makes it possible to store up to 8 criteria, all extracted from either the"
94,"request or the response, regardless of the number of rules. Only the 8 first"
94,"ones which match will be kept. Using this, it is possible to feed multiple"
94,tables at once in the hope to increase the chance to recognize a user on
94,another protocol or access method. Using multiple store-request rules with
94,the same table is possible and may be used to find the best criterion to rely
94,"on, by arranging the rules by decreasing preference order. Only the first"
94,extracted criterion for a given table will be stored. All subsequent store-
94,request rules referencing the same table will be skipped and their ACLs will
94,not be evaluated.
94,"The ""store-request"" rules are evaluated once the server connection has been"
94,"established, so that the table will contain the real server that processed"
94,the request.
94,Note : Consider not using this feature in multi-process mode (nbproc > 1)
94,unless you know what you do : memory is not shared between the
94,"processes, which can result in random behaviors."
94,Example :
94,# forward SMTP users to the same server they just used for POP in the
94,# last 30 minutes
94,backend pop
94,mode tcp
94,balance roundrobin
94,stick store-request src
94,stick-table type ip size 200k expire 30m
94,server s1 192.168.1.1:110
94,server s2 192.168.1.1:110
94,backend smtp
94,mode tcp
94,balance roundrobin
94,stick match src table pop
94,server s1 192.168.1.1:25
94,server s2 192.168.1.1:25
94,"See also : ""stick-table"", ""stick on"", ""nbprocThis keyword is available in sections :Process management and securityFetching samples from internal states"", ""bind-process"" and section 7 about ACLs and sample fetching."
94,stick-table type {ip | integer | string [len <length>] | binary [len <length>]}
94,size <size> [expire <expire>] [nopurge] [peers <peersect>]
94,[store <data_type>]*Configure the stickiness table for the current section
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,Arguments : ip
94,"a table declared with ""type ip"" will only store IPv4 addresses."
94,This form is very compact (about 50 bytes per entry) and allows
94,very fast entry lookup and stores with almost no overhead. This
94,is mainly used to store client source IP addresses.
94,ipv6
94,"a table declared with ""type ipv6"" will only store IPv6 addresses."
94,This form is very compact (about 60 bytes per entry) and allows
94,very fast entry lookup and stores with almost no overhead. This
94,is mainly used to store client source IP addresses.
94,integer
94,"a table declared with ""type integer"" will store 32bit integers"
94,which can represent a client identifier found in a request for
94,instance.
94,string
94,"a table declared with ""type string"" will store substrings of up"
94,to <len> characters. If the string provided by the pattern
94,"extractor is larger than <len>, it will be truncated before"
94,"being stored. During matching, at most <len> characters will be"
94,compared between the string in the table and the extracted
94,"pattern. When not specified, the string is automatically limited"
94,to 32 characters.
94,binary
94,"a table declared with ""type binary"" will store binary blocks"
94,of <len> bytes. If the block provided by the pattern
94,"extractor is larger than <len>, it will be truncated before"
94,being stored. If the block provided by the sample expression
94,"is shorter than <len>, it will be padded by 0. When not"
94,"specified, the block is automatically limited to 32 bytes."
94,<length>
94,is the maximum number of characters that will be stored in a
94,"""string"" type table (See type ""string"" above). Or the number"
94,"of bytes of the block in ""binary"" type table. Be careful when"
94,changing this parameter as memory usage will proportionally
94,increase.
94,<size>
94,is the maximum number of entries that can fit in the table. This
94,value directly impacts memory usage. Count approximately
94,"50 bytes per entry, plus the size of a string if any. The size"
94,"supports suffixes ""k"", ""m"", ""g"" for 2^10, 2^20 and 2^30 factors."
94,[nopurge]
94,indicates that we refuse to purge older entries when the table
94,is full. When not specified and the table is full when haproxy
94,"wants to store an entry in it, it will flush a few of the oldest"
94,entries in order to release some space for the new ones. This is
94,"most often the desired behavior. In some specific cases, it"
94,be desirable to refuse new entries instead of purging the older
94,ones. That may be the case when the amount of data to store is
94,far above the hardware limits and we prefer not to offer access
94,to new clients than to reject the ones already connected. When
94,"using this parameter, be sure to properly set the ""expire"""
94,parameter (see below).
94,<peersect> is the name of the peers section to use for replication. Entries
94,which associate keys to server IDs are kept synchronized with
94,the remote peers declared in this section. All entries are also
94,automatically learned from the local peer (old process) during a
94,soft restart.
94,NOTE : each peers section may be referenced only by tables
94,belonging to the same unique process.
94,<expire>
94,defines the maximum duration of an entry in the table since it
94,"was last created, refreshed or matched. The expiration delay is"
94,"defined using the standard time format, similarly as the various"
94,timeouts. The maximum duration is slightly above 24 days. See
94,"section 2.4 for more information. If this delay is not specified,"
94,"the session won't automatically expire, but older entries will"
94,"be removed once full. Be sure not to use the ""nopurge"" parameter"
94,if not expiration delay is specified.
94,<data_type> is used to store additional information in the stick-table. This
94,may be used by ACLs in order to control various criteria related
94,to the activity of the client matching the stick-table. For each
94,"item specified here, the size of each entry will be inflated so"
94,that the additional data can fit. Several data types may be
94,stored with an entry. Multiple data types may be specified after
94,"the ""store"" keyword, as a comma-separated list. Alternatively,"
94,"it is possible to repeat the ""store"" keyword followed by one or"
94,"several data types. Except for the ""server_id"" type which is"
94,"automatically detected and enabled, all data types must be"
94,explicitly declared to be stored. If an ACL references a data
94,"type which is not stored, the ACL will simply not match. Some"
94,data types require an argument which must be passed just after
94,the type between parenthesis. See below for the supported data
94,types and their arguments.
94,The data types that can be stored with an entry are the following :
94,- server_id : this is an integer which holds the numeric ID of the server a
94,"request was assigned to. It is used by the ""stick match"", ""stick store"","
94,"and ""stick on"" rules. It is automatically enabled when referenced."
94,- gpc0 : first General Purpose Counter. It is a positive 32-bit integer
94,integer which may be used for anything. Most of the time it will be used
94,"to put a special tag on some entries, for instance to note that a"
94,specific behavior was detected and must be known for future matches.
94,- gpc0_rate(<period>) : increment rate of the first General Purpose Counter
94,over a period. It is a positive 32-bit integer integer which may be used
94,"for anything. Just like <gpc0>, it counts events, but instead of keeping"
94,"a cumulative number, it maintains the rate at which the counter is"
94,incremented. Most of the time it will be used to measure the frequency of
94,occurrence of certain events (e.g. requests to a specific URL).
94,- conn_cnt : Connection Count. It is a positive 32-bit integer which counts
94,the absolute number of connections received from clients which matched
94,"this entry. It does not mean the connections were accepted, just that"
94,they were received.
94,- conn_cur : Current Connections. It is a positive 32-bit integer which
94,stores the concurrent connection counts for the entry. It is incremented
94,"once an incoming connection matches the entry, and decremented once the"
94,connection leaves. That way it is possible to know at any time the exact
94,number of concurrent connections for an entry.
94,- conn_rate(<period>) : frequency counter (takes 12 bytes). It takes an
94,integer parameter <period> which indicates in milliseconds the length
94,of the period over which the average is measured. It reports the average
94,"incoming connection rate over that period, in connections per period. The"
94,result is an integer which can be matched using ACLs.
94,- sess_cnt : Session Count. It is a positive 32-bit integer which counts
94,the absolute number of sessions received from clients which matched this
94,entry. A session is a connection that was accepted by the layer 4 rules.
94,- sess_rate(<period>) : frequency counter (takes 12 bytes). It takes an
94,integer parameter <period> which indicates in milliseconds the length
94,of the period over which the average is measured. It reports the average
94,"incoming session rate over that period, in sessions per period. The"
94,result is an integer which can be matched using ACLs.
94,- http_req_cnt : HTTP request Count. It is a positive 32-bit integer which
94,counts the absolute number of HTTP requests received from clients which
94,matched this entry. It does not matter whether they are valid requests or
94,not. Note that this is different from sessions when keep-alive is used on
94,the client side.
94,- http_req_rate(<period>) : frequency counter (takes 12 bytes). It takes an
94,integer parameter <period> which indicates in milliseconds the length
94,of the period over which the average is measured. It reports the average
94,"HTTP request rate over that period, in requests per period. The result is"
94,an integer which can be matched using ACLs. It does not matter whether
94,they are valid requests or not. Note that this is different from sessions
94,when keep-alive is used on the client side.
94,- http_err_cnt : HTTP Error Count. It is a positive 32-bit integer which
94,counts the absolute number of HTTP requests errors induced by clients
94,which matched this entry. Errors are counted on invalid and truncated
94,"requests, as well as on denied or tarpitted requests, and on failed"
94,"authentications. If the server responds with 4xx, then the request is"
94,also counted as an error since it's an error triggered by the client
94,(e.g. vulnerability scan).
94,- http_err_rate(<period>) : frequency counter (takes 12 bytes). It takes an
94,integer parameter <period> which indicates in milliseconds the length
94,of the period over which the average is measured. It reports the average
94,"HTTP request error rate over that period, in requests per period (see"
94,http_err_cnt above for what is accounted as an error). The result is an
94,integer which can be matched using ACLs.
94,- bytes_in_cnt : client to server byte count. It is a positive 64-bit
94,integer which counts the cumulative number of bytes received from clients
94,which matched this entry. Headers are included in the count. This may be
94,used to limit abuse of upload features on photo or video servers.
94,- bytes_in_rate(<period>) : frequency counter (takes 12 bytes). It takes an
94,integer parameter <period> which indicates in milliseconds the length
94,of the period over which the average is measured. It reports the average
94,"incoming bytes rate over that period, in bytes per period. It may be used"
94,to detect users which upload too much and too fast. Warning: with large
94,"uploads, it is possible that the amount of uploaded data will be counted"
94,"once upon termination, thus causing spikes in the average transfer speed"
94,instead of having a smooth one. This may partially be smoothed with
94,"""option contstats"" though this is not perfect yet. Use of byte_in_cnt is"
94,recommended for better fairness.
94,- bytes_out_cnt : server to client byte count. It is a positive 64-bit
94,integer which counts the cumulative number of bytes sent to clients which
94,matched this entry. Headers are included in the count. This may be used
94,to limit abuse of bots sucking the whole site.
94,- bytes_out_rate(<period>) : frequency counter (takes 12 bytes). It takes
94,an integer parameter <period> which indicates in milliseconds the length
94,of the period over which the average is measured. It reports the average
94,"outgoing bytes rate over that period, in bytes per period. It may be used"
94,to detect users which download too much and too fast. Warning: with large
94,"transfers, it is possible that the amount of transferred data will be"
94,"counted once upon termination, thus causing spikes in the average"
94,transfer speed instead of having a smooth one. This may partially be
94,"smoothed with ""option contstats"" though this is not perfect yet. Use of"
94,byte_out_cnt is recommended for better fairness.
94,"There is only one stick-table per proxy. At the moment of writing this doc,"
94,it does not seem useful to have multiple tables per proxy. If this happens
94,"to be required, simply create a dummy backend with a stick-table in it and"
94,reference it.
94,It is important to understand that stickiness based on learning information
94,"has some limitations, including the fact that all learned associations are"
94,lost upon restart unless peers are properly configured to transfer such
94,information upon restart (recommended). In general it can be good as a
94,complement but not always as an exclusive stickiness.
94,"Last, memory requirements may be important when storing many data types."
94,"Indeed, storing all indicators above at once in each entry requires 116 bytes"
94,"per entry, or 116 MB for a 1-million entries table. This is definitely not"
94,something that can be ignored.
94,Example:
94,# Keep track of counters of up to 1 million IP addresses over 5 minutes
94,# and store a general purpose counter and the average connection rate
94,# computed over a sliding window of 30 seconds.
94,"stick-table type ip size 1m expire 5m store gpc0,conn_rate(30s)"
94,"See also : ""stick match"", ""stick on"", ""stick store-request"", section 2.4 about time format and section 7 about ACLs."
94,stick store-response <pattern> [table <table>] [{if | unless} <condition>]Define a response pattern used to create an entry in a stickiness table
94,May be used in sections
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,Arguments :<pattern>
94,is a sample expression rule as described in section 7.3. It
94,describes what elements of the response or connection will
94,"be analyzed, extracted and stored in the table once a"
94,server is selected.
94,<table>
94,"is an optional stickiness table name. If unspecified, the same"
94,backend's table is used. A stickiness table is declared using
94,"the ""stick-table"" statement."
94,<cond>
94,is an optional storage condition. It makes it possible to store
94,certain criteria only when some conditions are met (or not met).
94,"For instance, it could be used to store the SSL session ID only"
94,when the response is a SSL server hello.
94,Some protocols or applications require complex stickiness rules and cannot
94,"always simply rely on cookies nor hashing. The ""stick store-response"""
94,statement
94,describes a rule to decide what to extract from the response and
94,"when to do it, in order to store it into a stickiness table for further"
94,"requests to match it using the ""stick match"" statement. Obviously the"
94,extracted part must make sense and have a chance to be matched in a further
94,request. Storing an ID found in a header of a response makes sense.
94,See section 7 for a complete list of possible patterns and transformation
94,rules.
94,"The table has to be declared using the ""stick-table"" statement. It must be of"
94,a type compatible with the pattern. By default it is the one which is present
94,in the same backend. It is possible to share a table with other backends by
94,"referencing it using the ""table"" keyword. If another table is referenced,"
94,"the server's ID inside the backends are used. By default, all server IDs"
94,"start at 1 in each backend, so the server ordering is enough. But in case of"
94,"doubt, it is highly recommended to force server IDs using their ""idThis keyword is available in sections :Alphabetically sorted keywords referenceBind optionsServer and default-server options"" setting."
94,"It is possible to restrict the conditions where a ""stick store-response"""
94,"statement will apply, using ""if"" or ""unless"" followed by a condition. This"
94,"condition will be evaluated while parsing the response, so any criteria can"
94,be used. See section 7 for ACL based conditions.
94,"There is no limit on the number of ""stick store-response"" statements, but"
94,there is a limit of 8 simultaneous stores per request or response. This
94,"makes it possible to store up to 8 criteria, all extracted from either the"
94,"request or the response, regardless of the number of rules. Only the 8 first"
94,"ones which match will be kept. Using this, it is possible to feed multiple"
94,tables at once in the hope to increase the chance to recognize a user on
94,another protocol or access method. Using multiple store-response rules with
94,the same table is possible and may be used to find the best criterion to rely
94,"on, by arranging the rules by decreasing preference order. Only the first"
94,extracted criterion for a given table will be stored. All subsequent store-
94,response rules referencing the same table will be skipped and their ACLs will
94,"not be evaluated. However, even if a store-request rule references a table, a"
94,store-response rule may also use the same table. This means that each table
94,may learn exactly one element from the request and one element from the
94,response at once.
94,The table will contain the real server that processed the request.
94,Example :
94,# Learn SSL session ID from both request and response and create affinity.
94,backend https
94,mode tcp
94,balance roundrobin
94,# maximum SSL session ID length is 32 bytes.
94,stick-table type binary len 32 size 30k expire 30m
94,acl clienthello req_ssl_hello_type 1
94,acl serverhello rep_ssl_hello_type 2
94,# use tcp content accepts to detects ssl client and server hello.
94,tcp-request inspect-delay 5s
94,tcp-request content accept if clienthello
94,# no timeout on response inspect delay by default.
94,tcp-response content accept if serverhello
94,# SSL session ID (SSLID) may be present on a client or server hello.
94,# Its length is coded on 1 byte at offset 43 and its value starts
94,# at offset 44.
94,# Match and learn on request if client hello.
94,"stick on payload_lv(43,1) if clienthello"
94,# Learn on response if server hello.
94,"stick store-response payload_lv(43,1) if serverhello"
94,server s1 192.168.1.1:443
94,server s2 192.168.1.1:443
94,"See also : ""stick-table"", ""stick on"", and section 7 about ACLs and pattern extraction."
94,tcp-check connect [params*]Opens a new connection
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,When an application lies on more than a single TCP port or when HAProxy
94,"load-balance many services in a single backend, it makes sense to probe all"
94,the services individually before considering a server as operational.
94,When there are no TCP port configured on the server line neither server port
94,"directive, then the 'tcp-check connect port <port>' must be the first step"
94,of the sequence.
94,"In a tcp-check ruleset a 'connect' is required, it is also mandatory to start"
94,the ruleset with a 'connect' rule. Purpose is to ensure admin know what they
94,do.
94,Parameters :
94,They are optional and can be used to describe how HAProxy should open and
94,use the TCP connection.
94,port
94,"if not set, check port or server port is used."
94,It tells HAProxy where to open the connection to.
94,"<port> must be a valid TCP port source integer, from 1 to 65535."
94,send-proxy
94,send a PROXY protocol string
94,ssl
94,opens a ciphered connection
94,Examples:
94,# check HTTP and HTTPs services on a server.
94,"# first open port 80 thanks to server line port directive, then"
94,"# tcp-check opens port 443, ciphered and run a request on it:"
94,option tcp-check
94,tcp-check connect
94,tcp-check send GET\ /\ HTTP/1.0\r\n
94,tcp-check send Host:\ haproxy.1wt.eu\r\n
94,tcp-check send \r\n
94,tcp-check expect rstring (2..|3..)
94,tcp-check connect port 443 ssl
94,tcp-check send GET\ /\ HTTP/1.0\r\n
94,tcp-check send Host:\ haproxy.1wt.eu\r\n
94,tcp-check send \r\n
94,tcp-check expect rstring (2..|3..)
94,server www 10.0.0.1 check port 80
94,# check both POP and IMAP from a single server:
94,option tcp-check
94,tcp-check connect port 110
94,tcp-check expect string +OK\ POP3\ ready
94,tcp-check connect port 143
94,tcp-check expect string *\ OK\ IMAP4\ ready
94,server mail 10.0.0.1 check
94,"See also : ""option tcp-check"", ""tcp-check send"", ""tcp-check expect"""
94,tcp-check expect [!] <match> <pattern>Specify data to be collected and analyzed during a generic health check
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,Arguments :<match>
94,is a keyword indicating how to look for a specific pattern in the
94,"response. The keyword may be one of ""string"", ""rstring"" or"
94,binary.
94,"The keyword may be preceded by an exclamation mark (""!"") to negate"
94,the match. Spaces are allowed between the exclamation mark and the
94,keyword. See below for more details on the supported keywords.
94,<pattern> is the pattern to look for. It may be a string or a regular
94,"expression. If the pattern contains spaces, they must be escaped"
94,with the usual backslash ('\').
94,"If the match is set to binary, then the pattern must be passed as"
94,a series of hexadecimal digits in an even number. Each sequence of
94,two digits will represent a byte. The hexadecimal digits may be
94,used upper or lower case.
94,The available matches are intentionally similar to their http-check cousins :
94,string <string> : test the exact string matches in the response buffer.
94,A health check response will be considered valid if the
94,response's buffer contains this exact string. If the
94,"""string"" keyword is prefixed with ""!"", then the response"
94,will be considered invalid if the body contains this
94,string. This can be used to look for a mandatory pattern
94,"in a protocol response, or to detect a failure when a"
94,specific error appears in a protocol banner.
94,rstring <regex> : test a regular expression on the response buffer.
94,A health check response will be considered valid if the
94,response's buffer matches this expression. If the
94,"""rstring"" keyword is prefixed with ""!"", then the response"
94,will be considered invalid if the body matches the
94,expression.
94,binary <hexstring> : test the exact string in its hexadecimal form matches
94,in the response buffer. A health check response will
94,be considered valid if the response's buffer contains
94,this exact hexadecimal string.
94,Purpose is to match data on binary protocols.
94,It is important to note that the responses will be limited to a certain size
94,"defined by the global ""tune.chksize"" option, which defaults to 16384 bytes."
94,"Thus, too large responses may not contain the mandatory pattern when using"
94,"""string"", ""rstring"" or binary. If a large response is absolutely required, it"
94,is possible to change the default max size by setting the global variable.
94,"However, it is worth keeping in mind that parsing very large responses can"
94,"waste some CPU cycles, especially when regular expressions are used, and that"
94,"it is always better to focus the checks on smaller resources. Also, in its"
94,"current state, the check will not find any string nor regex past a null"
94,character in the response. Similarly it is not possible to request matching
94,the null character.
94,Examples :
94,# perform a POP check
94,option tcp-check
94,tcp-check expect string +OK\ POP3\ ready
94,# perform an IMAP check
94,option tcp-check
94,tcp-check expect string *\ OK\ IMAP4\ ready
94,# look for the redis master server
94,option tcp-check
94,tcp-check send PING\r\n
94,tcp-check expect string +PONG
94,tcp-check send info\ replication\r\n
94,tcp-check expect string role:master
94,tcp-check send QUIT\r\n
94,tcp-check expect string +OK
94,"See also : ""option tcp-check"", ""tcp-check connect"", ""tcp-check send"", ""tcp-check send-binary"", ""http-check expect"", tune.chksize"
94,tcp-check send <data>Specify a string to be sent as a question during a generic health check
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,<data> : the data to be sent as a question during a generic health check
94,"session. For now, <data> must be a string."
94,Examples :
94,# look for the redis master server
94,option tcp-check
94,tcp-check send info\ replication\r\n
94,tcp-check expect string role:master
94,"See also : ""option tcp-check"", ""tcp-check connect"", ""tcp-check expect"", ""tcp-check send-binary"", tune.chksize"
94,tcp-check send-binary <hexstring>Specify a hex digits string to be sent as a binary question during a raw
94,tcp health check
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,<data> : the data to be sent as a question during a generic health check
94,"session. For now, <data> must be a string."
94,<hexstring> : test the exact string in its hexadecimal form matches in the
94,response buffer. A health check response will be considered
94,valid if the response's buffer contains this exact
94,hexadecimal string.
94,Purpose is to send binary data to ask on binary protocols.
94,Examples :
94,# redis check in binary
94,option tcp-check
94,tcp-check send-binary 50494e470d0a # PING\r\n
94,tcp-check expect binary 2b504F4e47 # +PONG
94,"See also : ""option tcp-check"", ""tcp-check connect"", ""tcp-check expect"", ""tcp-check send"", tune.chksize"
94,tcp-request connection <action> [{if | unless} <condition>]Perform an action on an incoming connection depending on a layer 4 condition
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesno
94,Arguments :<action>
94,defines the action to perform if the condition applies. See
94,below.
94,<condition> is a standard layer4-only ACL-based condition (see section 7).
94,"Immediately after acceptance of a new incoming connection, it is possible to"
94,evaluate some conditions to decide whether this connection must be accepted
94,or dropped or have its counters tracked. Those conditions cannot make use of
94,"any data contents because the connection has not been read from yet, and the"
94,buffers are not yet allocated. This is used to selectively and very quickly
94,accept or drop connections from various sources with a very low overhead. If
94,"some contents need to be inspected in order to take the decision, the"
94,"""tcp-request content"" statements must be used instead."
94,"The ""tcp-request connection"" rules are evaluated in their exact declaration"
94,"order. If no rule matches or if there is no rule, the default action is to"
94,accept the incoming connection. There is no specific limit to the number of
94,rules which may be inserted.
94,Four types of actions are supported :
94,- accept :
94,"accepts the connection if the condition is true (when used with ""if"")"
94,"or false (when used with ""unless""). The first such rule executed ends"
94,the rules evaluation.
94,- reject :
94,"rejects the connection if the condition is true (when used with ""if"")"
94,"or false (when used with ""unless""). The first such rule executed ends"
94,the rules evaluation. Rejected connections do not even become a
94,"session, which is why they are accounted separately for in the stats,"
94,"as ""denied connections"". They are not considered for the session"
94,rate-limit and are not logged either. The reason is that these rules
94,should only be used to filter extremely high connection rates such as
94,the ones encountered during a massive DDoS attack. Under these extreme
94,"conditions, the simple action of logging each event would make the"
94,system collapse and would considerably lower the filtering capacity. If
94,"logging is absolutely desired, then ""tcp-request content"" rules should"
94,"be used instead, as ""tcp-request session"" rules will not log either."
94,- expect-proxy layer4 :
94,configures the client-facing connection to receive a PROXY protocol
94,header before any byte is read from the socket. This is equivalent to
94,"having the ""accept-proxy"" keyword on the ""bind"" line, except that using"
94,the TCP rule allows the PROXY protocol to be accepted only for certain
94,IP address ranges using an ACL. This is convenient when multiple layers
94,of load balancers are passed through by traffic coming from public
94,hosts.
94,- expect-netscaler-cip layer4 :
94,configures the client-facing connection to receive a NetScaler Client
94,IP insertion protocol header before any byte is read from the socket.
94,"This is equivalent to having the ""accept-netscaler-cip"" keyword on the"
94,"""bind"" line, except that using the TCP rule allows the PROXY protocol"
94,to be accepted only for certain IP address ranges using an ACL. This
94,is convenient when multiple layers of load balancers are passed
94,through by traffic coming from public hosts.
94,- capture <sample> len <length> :
94,"This only applies to ""tcp-request content"" rules. It captures sample"
94,"expression <sample> from the request buffer, and converts it to a"
94,string of at most <len> characters. The resulting string is stored into
94,"the next request ""capture"" slot, so it will possibly appear next to"
94,some captured HTTP headers. It will then automatically appear in the
94,"logs, and it will be possible to extract it using sample fetch rules to"
94,feed it into headers or anything. The length should be limited given
94,that this size will be allocated for each capture during the whole
94,"session life. Please check section 7.3 (Fetching samples) and ""capture"
94,"request header"" for more information."
94,- { track-sc0 | track-sc1 | track-sc2 } <key> [table <table>] :
94,enables tracking of sticky counters from current connection. These
94,rules do not stop evaluation and do not change default action. The
94,number of counters that may be simultaneously tracked by the same
94,connection is set in MAX_SESS_STKCTR at build time (reported in
94,"haproxy -vv) whichs defaults to 3, so the track-sc number is between 0"
94,"and (MAX_SESS_STCKTR-1). The first ""track-sc0"" rule executed enables"
94,tracking of the counters of the specified table as the first set. The
94,"first ""track-sc1"" rule executed enables tracking of the counters of the"
94,"specified table as the second set. The first ""track-sc2"" rule executed"
94,enables tracking of the counters of the specified table as the third
94,set. It is a recommended practice to use the first set of counters for
94,the per-frontend counters and the second set for the per-backend ones.
94,"But this is just a guideline, all may be used everywhere."
94,These actions take one or two arguments :
94,<key>
94,"is mandatory, and is a sample expression rule as described"
94,in section 7.3. It describes what elements of the incoming
94,"request or connection will be analyzed, extracted, combined,"
94,and used to select which table entry to update the counters.
94,"Note that ""tcp-request connection"" cannot use content-based"
94,fetches.
94,<table>
94,"is an optional table to be used instead of the default one,"
94,which is the stick-table declared in the current proxy. All
94,the counters for the matches and updates for the key will
94,then be performed in that table until the session ends.
94,"Once a ""track-sc*"" rule is executed, the key is looked up in the table"
94,"and if it is not found, an entry is allocated for it. Then a pointer to"
94,"that entry is kept during all the session's life, and this entry's"
94,"counters are updated as often as possible, every time the session's"
94,"counters are updated, and also systematically when the session ends."
94,Counters are only updated for events that happen after the tracking has
94,"been started. For example, connection counters will not be updated when"
94,"tracking layer 7 information, since the connection event happens before"
94,layer7 information is extracted.
94,"If the entry tracks concurrent connection counters, one connection is"
94,"counted for as long as the entry is tracked, and the entry will not"
94,expire during that time. Tracking counters also provides a performance
94,"advantage over just checking the keys, because only one table lookup is"
94,performed for all ACL checks that make use of it.
94,- sc-inc-gpc0(<sc-id>):
94,"The ""sc-inc-gpc0"" increments the GPC0 counter according to the sticky"
94,"counter designated by <sc-id>. If an error occurs, this action silently"
94,fails and the actions evaluation continues.
94,- sc-set-gpt0(<sc-id>) <int>:
94,This action sets the GPT0 tag according to the sticky counter designated
94,by <sc-id> and the value of <int>. The expected result is a boolean. If
94,"an error occurs, this action silently fails and the actions evaluation"
94,continues.
94,- set-src <expr> :
94,Is used to set the source IP address to the value of specified
94,expression. Useful if you want to mask source IP for privacy.
94,"If you want to provide an IP from a HTTP header use ""http-request"
94,"set-src"""
94,<expr>
94,Is a standard HAProxy expression formed by a sample-fetch
94,followed by some converters.
94,Example:
94,"tcp-request connection set-src src,ipmask(24)"
94,"When possible, set-src preserves the original source port as long as the"
94,"address family allows it, otherwise the source port is set to 0."
94,- set-src-port <expr> :
94,Is used to set the source port address to the value of specified
94,expression.
94,<expr>
94,Is a standard HAProxy expression formed by a sample-fetch
94,followed by some converters.
94,Example:
94,tcp-request connection set-src-port int(4000)
94,"When possible, set-src-port preserves the original source address as long"
94,"as the address family supports a port, otherwise it forces the source"
94,"address to IPv4 ""0.0.0.0"" before rewriting the port."
94,- set-dst <expr> :
94,Is used to set the destination IP address to the value of specified
94,expression. Useful if you want to mask IP for privacy in log.
94,"If you want to provide an IP from a HTTP header use ""http-request"
94,"set-dst"". If you want to connect to the new address/port, use"
94,'0.0.0.0:0' as a server address in the backend.
94,<expr>
94,Is a standard HAProxy expression formed by a sample-fetch
94,followed by some converters.
94,Example:
94,"tcp-request connection set-dst dst,ipmask(24)"
94,tcp-request connection set-dst ipv4(10.0.0.1)
94,"When possible, set-dst preserves the original destination port as long as"
94,"the address family allows it, otherwise the destination port is set to 0."
94,- set-dst-port <expr> :
94,Is used to set the destination port address to the value of specified
94,"expression. If you want to connect to the new address/port, use"
94,'0.0.0.0:0' as a server address in the backend.
94,<expr>
94,Is a standard HAProxy expression formed by a sample-fetch
94,followed by some converters.
94,Example:
94,tcp-request connection set-dst-port int(4000)
94,"When possible, set-dst-port preserves the original destination address as"
94,"long as the address family supports a port, otherwise it forces the"
94,"destination address to IPv4 ""0.0.0.0"" before rewriting the port."
94,"- ""silent-drop"" :"
94,This stops the evaluation of the rules and makes the client-facing
94,connection suddenly disappear using a system-dependent way that tries
94,to prevent the client from being notified. The effect it then that the
94,client still sees an established connection while there's none on
94,"HAProxy. The purpose is to achieve a comparable effect to ""tarpit"""
94,except that it doesn't use any local resource at all on the machine
94,"running HAProxy. It can resist much higher loads than ""tarpit"", and"
94,slow down stronger attackers. It is important to understand the impact
94,of using this mechanism. All stateful equipment placed between the
94,"client and HAProxy (firewalls, proxies, load balancers) will also keep"
94,the established connection for a long time and may suffer from this
94,"action. On modern Linux systems running with enough privileges, the"
94,TCP_REPAIR socket option is used to block the emission of a TCP
94,"reset. On other systems, the socket's TTL is reduced to 1 so that the"
94,"TCP reset doesn't pass the first router, though it's still delivered to"
94,local networks. Do not use it unless you fully understand how it works.
94,"Note that the ""if/unless"" condition is optional. If no condition is set on"
94,"the action, it is simply performed unconditionally. That can be useful for"
94,"""track-sc*"" actions as well as for changing the default action to a reject."
94,Example:
94,"Accept all connections from white-listed hosts, reject too fast connection without counting them, and track accepted connections. This results in connection rate being capped from abusive sources.tcp-request connection accept if { src -f /etc/haproxy/whitelist.lst }"
94,tcp-request connection reject if { src_conn_rate gt 10 }
94,tcp-request connection track-sc0 src
94,Example:
94,"Accept all connections from white-listed hosts, count all other connections and reject too fast ones. This results in abusive ones being blocked as long as they don't slow down.tcp-request connection accept if { src -f /etc/haproxy/whitelist.lst }"
94,tcp-request connection track-sc0 src
94,tcp-request connection reject if { sc0_conn_rate gt 10 }
94,Example:
94,Enable the PROXY protocol for traffic coming from all known proxies.tcp-request connection expect-proxy layer4 if { src -f proxies.lst }
94,See section 7 about ACL usage.
94,"See also : ""tcp-request session"", ""tcp-request content"", ""stick-table"""
94,tcp-request content <action> [{if | unless} <condition>]Perform an action on a new session depending on a layer 4-7 condition
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,Arguments :<action>
94,defines the action to perform if the condition applies. See
94,below.
94,<condition> is a standard layer 4-7 ACL-based condition (see section 7).
94,A request's contents can be analyzed at an early stage of request processing
94,"called ""TCP content inspection"". During this stage, ACL-based rules are"
94,"evaluated every time the request contents are updated, until either an"
94,"""accept"" or a ""reject"" rule matches, or the TCP request inspection delay"
94,expires with no matching rule.
94,"The first difference between these rules and ""tcp-request connection"" rules"
94,"is that ""tcp-request content"" rules can make use of contents to take a"
94,"decision. Most often, these decisions will consider a protocol recognition or"
94,validity. The second difference is that content-based rules can be used in
94,"both frontends and backends. In case of HTTP keep-alive with the client, all"
94,"tcp-request content rules are evaluated again, so haproxy keeps a record of"
94,"what sticky counters were assigned by a ""tcp-request connection"" versus a"
94,"""tcp-request content"" rule, and flushes all the content-related ones after"
94,"processing an HTTP request, so that they may be evaluated again by the rules"
94,being evaluated again for the next request. This is of particular importance
94,when the rule tracks some L7 information or when it is conditioned by an
94,"L7-based ACL, since tracking may change between requests."
94,Content-based rules are evaluated in their exact declaration order. If no
94,"rule matches or if there is no rule, the default action is to accept the"
94,contents. There is no specific limit to the number of rules which may be
94,inserted.
94,Several types of actions are supported :
94,- accept : the request is accepted
94,- reject : the request is rejected and the connection is closed
94,- capture : the specified sample expression is captured
94,- { track-sc0 | track-sc1 | track-sc2 } <key> [table <table>]
94,- sc-inc-gpc0(<sc-id>)
94,- sc-set-gpt0(<sc-id>) <int>
94,- set-var(<var-name>) <expr>
94,- unset-var(<var-name>)
94,- silent-drop
94,- send-spoe-group <engine-name> <group-name>
94,"They have the same meaning as their counter-parts in ""tcp-request connection"""
94,so please refer to that section for a complete description.
94,"While there is nothing mandatory about it, it is recommended to use the"
94,"track-sc0 in ""tcp-request connection"" rules, track-sc1 for ""tcp-request"
94,"content"" rules in the frontend, and track-sc2 for ""tcp-request content"""
94,"rules in the backend, because that makes the configuration more readable"
94,"and easier to troubleshoot, but this is just a guideline and all counters"
94,may be used everywhere.
94,"Note that the ""if/unless"" condition is optional. If no condition is set on"
94,"the action, it is simply performed unconditionally. That can be useful for"
94,"""track-sc*"" actions as well as for changing the default action to a reject."
94,"It is perfectly possible to match layer 7 contents with ""tcp-request content"""
94,"rules, since HTTP-specific ACL matches are able to preliminarily parse the"
94,contents of a buffer before extracting the required data. If the buffered
94,"contents do not parse as a valid HTTP message, then the ACL does not match."
94,The parser which is involved there is exactly the same as for all other HTTP
94,"processing, so there is no risk of parsing something differently. In an HTTP"
94,"backend connected to from an HTTP frontend, it is guaranteed that HTTP"
94,contents will always be immediately present when the rule is evaluated first.
94,Tracking layer7 information is also possible provided that the information
94,are present when the rule is processed. The rule processing engine is able to
94,wait until the inspect delay expires when the data to be tracked is not yet
94,available.
94,"The ""set-var"" is used to set the content of a variable. The variable is"
94,"declared inline. For ""tcp-request session"" rules, only session-level"
94,"variables can be used, without any layer7 contents."
94,<var-name> The name of the variable starts with an indication about
94,its scope. The scopes allowed are:
94,"""proc"" : the variable is shared with the whole process"
94,"""sess"" : the variable is shared with the whole session"
94,"""txn"""
94,: the variable is shared with the transaction
94,(request and response)
94,"""req"""
94,: the variable is shared only during request
94,processing
94,"""res"""
94,: the variable is shared only during response
94,processing
94,This prefix is followed by a name. The separator is a '.'.
94,"The name may only contain characters 'a-z', 'A-Z', '0-9',"
94,'.' and '_'.
94,<expr>
94,Is a standard HAProxy expression formed by a sample-fetch
94,followed by some converters.
94,"The ""unset-var"" is used to unset a variable. See above for details about"
94,<var-name>.
94,"The ""send-spoe-group"" is used to trigger sending of a group of SPOE"
94,"messages. To do so, the SPOE engine used to send messages must be defined, as"
94,"well as the SPOE group to send. Of course, the SPOE engine must refer to an"
94,"existing SPOE filter. If not engine name is provided on the SPOE filter line,"
94,the SPOE agent name must be used.
94,<engine-name> The SPOE engine name.
94,<group-name>
94,The SPOE group name as specified in the engine configuration.
94,Example:
94,tcp-request content set-var(sess.my_var) src
94,tcp-request content unset-var(sess.my_var2)
94,Example:
94,"# Accept HTTP requests containing a Host header saying ""example.com"""
94,# and reject everything else.
94,acl is_host_com hdr(Host) -i example.com
94,tcp-request inspect-delay 30s
94,tcp-request content accept if is_host_com
94,tcp-request content reject
94,Example:
94,# reject SMTP connection if client speaks first
94,tcp-request inspect-delay 30s
94,acl content_present req_len gt 0
94,tcp-request content reject if content_present
94,# Forward HTTPS connection only if client speaks
94,tcp-request inspect-delay 30s
94,acl content_present req_len gt 0
94,tcp-request content accept if content_present
94,tcp-request content reject
94,Example:
94,# Track the last IP(stick-table type string) from X-Forwarded-For
94,tcp-request inspect-delay 10s
94,"tcp-request content track-sc0 hdr(x-forwarded-for,-1)"
94,# Or track the last IP(stick-table type ip|ipv6) from X-Forwarded-For
94,"tcp-request content track-sc0 req.hdr_ip(x-forwarded-for,-1)"
94,Example:
94,"# track request counts per ""base"" (concatenation of Host+URL)"
94,tcp-request inspect-delay 10s
94,tcp-request content track-sc0 base table req-rate
94,Example:
94,"Track per-frontend and per-backend counters, block abusers at the frontend when the backend detects abuse(and marks gpc0).frontend http"
94,# Use General Purpose Counter 0 in SC0 as a global abuse counter
94,# protecting all our sites
94,stick-table type ip size 1m expire 5m store gpc0
94,tcp-request connection track-sc0 src
94,tcp-request connection reject if { sc0_get_gpc0 gt 0 }
94,...
94,use_backend http_dynamic if { path_end .php }
94,backend http_dynamic
94,# if a source makes too fast requests to this dynamic site (tracked
94,"# by SC1), block it globally in the frontend."
94,stick-table type ip size 1m expire 5m store http_req_rate(10s)
94,acl click_too_fast sc1_http_req_rate gt 10
94,acl mark_as_abuser sc0_inc_gpc0(http) gt 0
94,tcp-request content track-sc1 src
94,tcp-request content reject if click_too_fast mark_as_abuser
94,See section 7 about ACL usage.
94,"See also : ""tcp-request connection"", ""tcp-request session"", ""tcp-request inspect-delay"", and ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference""."
94,tcp-request inspect-delay <timeout>Set the maximum allowed time to wait for data during content inspection
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesyes
94,"Arguments :<timeout> is the timeout value specified in milliseconds by default, but"
94,"can be in any other unit if the number is suffixed by the unit,"
94,as explained at the top of this document.
94,People using haproxy primarily as a TCP relay are often worried about the
94,risk of passing any type of protocol to a server without any analysis. In
94,"order to be able to analyze the request contents, we must first withhold"
94,the data then analyze them. This statement simply enables withholding of
94,data for at most the specified amount of time.
94,TCP content inspection applies very early when a connection reaches a
94,"frontend, then very early when the connection is forwarded to a backend. This"
94,means that a connection may experience a first delay in the frontend and a
94,second delay in the backend if both have tcp-request rules.
94,"Note that when performing content inspection, haproxy will evaluate the whole"
94,"rules for every new chunk which gets in, taking into account the fact that"
94,"those data are partial. If no rule matches before the aforementioned delay,"
94,"a last check is performed upon expiration, this time considering that the"
94,"contents are definitive. If no delay is set, haproxy will not wait at all"
94,and will immediately apply a verdict based on the available information.
94,"Obviously this is unlikely to be very useful and might even be racy, so such"
94,setups are not recommended.
94,"As soon as a rule matches, the request is released and continues as usual. If"
94,"the timeout is reached and no rule matches, the default policy will be to let"
94,it pass through unaffected.
94,"For most protocols, it is enough to set it to a few seconds, as most clients"
94,send the full request immediately upon connection. Add 3 or more seconds to
94,"cover TCP retransmits but that's all. For some protocols, it may make sense"
94,"to use large values, for instance to ensure that the client never talks"
94,"before the server (e.g. SMTP), or to wait for a client to talk before passing"
94,data to the server (e.g. SSL). Note that the client timeout must cover at
94,"least the inspection delay, otherwise it will expire first. If the client"
94,"closes the connection or if the buffer is full, the delay immediately expires"
94,since the contents will not be able to change anymore.
94,"See also : ""tcp-request content accept"", ""tcp-request content reject"", ""timeout client""."
94,tcp-response content <action> [{if | unless} <condition>]Perform an action on a session response depending on a layer 4-7 condition
94,May be used in sections
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,Arguments :<action>
94,defines the action to perform if the condition applies. See
94,below.
94,<condition> is a standard layer 4-7 ACL-based condition (see section 7).
94,Response contents can be analyzed at an early stage of response processing
94,"called ""TCP content inspection"". During this stage, ACL-based rules are"
94,"evaluated every time the response contents are updated, until either an"
94,"""accept"", ""close"" or a ""reject"" rule matches, or a TCP response inspection"
94,delay is set and expires with no matching rule.
94,"Most often, these decisions will consider a protocol recognition or validity."
94,Content-based rules are evaluated in their exact declaration order. If no
94,"rule matches or if there is no rule, the default action is to accept the"
94,contents. There is no specific limit to the number of rules which may be
94,inserted.
94,Several types of actions are supported :
94,- accept :
94,"accepts the response if the condition is true (when used with ""if"")"
94,"or false (when used with ""unless""). The first such rule executed ends"
94,the rules evaluation.
94,- close :
94,immediately closes the connection with the server if the condition is
94,"true (when used with ""if""), or false (when used with ""unless""). The"
94,first such rule executed ends the rules evaluation. The main purpose of
94,this action is to force a connection to be finished between a client
94,and a server after an exchange when the application protocol expects
94,some long time outs to elapse first. The goal is to eliminate idle
94,connections which take significant resources on servers with certain
94,protocols.
94,- reject :
94,"rejects the response if the condition is true (when used with ""if"")"
94,"or false (when used with ""unless""). The first such rule executed ends"
94,the rules evaluation. Rejected session are immediately closed.
94,- set-var(<var-name>) <expr>
94,Sets a variable.
94,- unset-var(<var-name>)
94,Unsets a variable.
94,- sc-inc-gpc0(<sc-id>):
94,This action increments the GPC0 counter according to the sticky
94,"counter designated by <sc-id>. If an error occurs, this action fails"
94,silently and the actions evaluation continues.
94,- sc-set-gpt0(<sc-id>) <int> :
94,This action sets the GPT0 tag according to the sticky counter designated
94,by <sc-id> and the value of <int>. The expected result is a boolean. If
94,"an error occurs, this action silently fails and the actions evaluation"
94,continues.
94,"- ""silent-drop"" :"
94,This stops the evaluation of the rules and makes the client-facing
94,connection suddenly disappear using a system-dependent way that tries
94,to prevent the client from being notified. The effect it then that the
94,client still sees an established connection while there's none on
94,"HAProxy. The purpose is to achieve a comparable effect to ""tarpit"""
94,except that it doesn't use any local resource at all on the machine
94,"running HAProxy. It can resist much higher loads than ""tarpit"", and"
94,slow down stronger attackers. It is important to understand the impact
94,of using this mechanism. All stateful equipment placed between the
94,"client and HAProxy (firewalls, proxies, load balancers) will also keep"
94,the established connection for a long time and may suffer from this
94,"action. On modern Linux systems running with enough privileges, the"
94,TCP_REPAIR socket option is used to block the emission of a TCP
94,"reset. On other systems, the socket's TTL is reduced to 1 so that the"
94,"TCP reset doesn't pass the first router, though it's still delivered to"
94,local networks. Do not use it unless you fully understand how it works.
94,- send-spoe-group <engine-name> <group-name>
94,Send a group of SPOE messages.
94,"Note that the ""if/unless"" condition is optional. If no condition is set on"
94,"the action, it is simply performed unconditionally. That can be useful for"
94,for changing the default action to a reject.
94,"It is perfectly possible to match layer 7 contents with ""tcp-response"
94,"content"" rules, but then it is important to ensure that a full response has"
94,"been buffered, otherwise no contents will match. In order to achieve this,"
94,the best solution involves detecting the HTTP protocol during the inspection
94,period.
94,"The ""set-var"" is used to set the content of a variable. The variable is"
94,declared inline.
94,<var-name> The name of the variable starts with an indication about
94,its scope. The scopes allowed are:
94,"""proc"" : the variable is shared with the whole process"
94,"""sess"" : the variable is shared with the whole session"
94,"""txn"""
94,: the variable is shared with the transaction
94,(request and response)
94,"""req"""
94,: the variable is shared only during request
94,processing
94,"""res"""
94,: the variable is shared only during response
94,processing
94,This prefix is followed by a name. The separator is a '.'.
94,"The name may only contain characters 'a-z', 'A-Z', '0-9',"
94,'.' and '_'.
94,<expr>
94,Is a standard HAProxy expression formed by a sample-fetch
94,followed by some converters.
94,Example:
94,tcp-request content set-var(sess.my_var) src
94,"The ""unset-var"" is used to unset a variable. See above for details about"
94,<var-name>.
94,Example:
94,tcp-request content unset-var(sess.my_var)
94,"The ""send-spoe-group"" is used to trigger sending of a group of SPOE"
94,"messages. To do so, the SPOE engine used to send messages must be defined, as"
94,"well as the SPOE group to send. Of course, the SPOE engine must refer to an"
94,"existing SPOE filter. If not engine name is provided on the SPOE filter line,"
94,the SPOE agent name must be used.
94,<engine-name> The SPOE engine name.
94,<group-name>
94,The SPOE group name as specified in the engine configuration.
94,See section 7 about ACL usage.
94,"See also : ""tcp-request content"", ""tcp-response inspect-delay"""
94,tcp-request session <action> [{if | unless} <condition>]Perform an action on a validated session depending on a layer 5 condition
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesno
94,Arguments :<action>
94,defines the action to perform if the condition applies. See
94,below.
94,<condition> is a standard layer5-only ACL-based condition (see section 7).
94,"Once a session is validated, (i.e. after all handshakes have been completed),"
94,it is possible to evaluate some conditions to decide whether this session
94,must be accepted or dropped or have its counters tracked. Those conditions
94,cannot make use of any data contents because no buffers are allocated yet and
94,the processing cannot wait at this stage. The main use case it to copy some
94,early information into variables (since variables are accessible in the
94,"session), or to keep track of some information collected after the handshake,"
94,"such as SSL-level elements (SNI, ciphers, client cert's CN) or information"
94,from the PROXY protocol header (e.g. track a source forwarded this way). The
94,extracted information can thus be copied to a variable or tracked using
94,"""track-sc"" rules. Of course it is also possible to decide to accept/reject as"
94,with other rulesets. Most operations performed here could also be performed
94,"in ""tcp-request content"" rules, except that in HTTP these rules are evaluated"
94,"for each new request, and that might not always be acceptable. For example a"
94,rule might increment a counter on each evaluation. It would also be possible
94,"that a country is resolved by geolocation from the source IP address,"
94,"assigned to a session-wide variable, then the source address rewritten from"
94,an HTTP header for all requests. If some contents need to be inspected in
94,"order to take the decision, the ""tcp-request content"" statements must be used"
94,instead.
94,"The ""tcp-request session"" rules are evaluated in their exact declaration"
94,"order. If no rule matches or if there is no rule, the default action is to"
94,accept the incoming session. There is no specific limit to the number of
94,rules which may be inserted.
94,Several types of actions are supported :
94,- accept : the request is accepted
94,- reject : the request is rejected and the connection is closed
94,- { track-sc0 | track-sc1 | track-sc2 } <key> [table <table>]
94,- sc-inc-gpc0(<sc-id>)
94,- sc-set-gpt0(<sc-id>) <int>
94,- set-var(<var-name>) <expr>
94,- unset-var(<var-name>)
94,- silent-drop
94,These actions have the same meaning as their respective counter-parts in
94,"""tcp-request connection"" and ""tcp-request content"", so please refer to these"
94,sections for a complete description.
94,"Note that the ""if/unless"" condition is optional. If no condition is set on"
94,"the action, it is simply performed unconditionally. That can be useful for"
94,"""track-sc*"" actions as well as for changing the default action to a reject."
94,Example:
94,"Track the original source address by default, or the one advertised in the PROXY protocol header for connection coming from the local proxies. The first connection-level rule enables receipt of the PROXY protocol for these ones, the second rule tracks whatever address we decide to keep after optional decoding.tcp-request connection expect-proxy layer4 if { src -f proxies.lst }"
94,tcp-request session track-sc0 src
94,Example:
94,"Accept all sessions from white-listed hosts, reject too fast sessions without counting them, and track accepted sessions. This results in session rate being capped from abusive sources.tcp-request session accept if { src -f /etc/haproxy/whitelist.lst }"
94,tcp-request session reject if { src_sess_rate gt 10 }
94,tcp-request session track-sc0 src
94,Example:
94,"Accept all sessions from white-listed hosts, count all other sessions and reject too fast ones. This results in abusive ones being blocked as long as they don't slow down.tcp-request session accept if { src -f /etc/haproxy/whitelist.lst }"
94,tcp-request session track-sc0 src
94,tcp-request session reject if { sc0_sess_rate gt 10 }
94,See section 7 about ACL usage.
94,"See also : ""tcp-request connection"", ""tcp-request content"", ""stick-table"""
94,tcp-response inspect-delay <timeout>Set the maximum allowed time to wait for a response during content inspection
94,May be used in sections
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,"Arguments :<timeout> is the timeout value specified in milliseconds by default, but"
94,"can be in any other unit if the number is suffixed by the unit,"
94,as explained at the top of this document.
94,"See also : ""tcp-response content"", ""tcp-request inspect-delay""."
94,"timeout check <timeout>Set additional check timeout, but only after a connection has been already"
94,established.
94,May be used in sections :
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,"Arguments:<timeout> is the timeout value specified in milliseconds by default, but"
94,"can be in any other unit if the number is suffixed by the unit,"
94,as explained at the top of this document.
94,"If set, haproxy uses min(""timeout connect"", ""inter"") as a connect timeout"
94,"for check and ""timeout check"" as an additional read timeout. The ""min"" is"
94,"used so that people running with *very* long ""timeout connect"" (e.g. those"
94,who needed this due to the queue or tarpit) do not slow down their checks.
94,(Please also note that there is no valid reason to have such long connect
94,"timeouts, because ""timeout queue"" and ""timeout tarpit"" can always be used to"
94,avoid that).
94,"If ""timeout check"" is not set haproxy uses ""inter"" for complete check"
94,timeout (connect + read) exactly like all <1.3.15 version.
94,In most cases check request is much simpler and faster to handle than normal
94,requests and people may want to kick out laggy servers so this timeout should
94,"be smaller than ""timeout server""."
94,"This parameter is specific to backends, but can be specified once for all in"
94,"""defaults"" sections. This is in fact one of the easiest solutions not to"
94,forget about it.
94,"See also: ""timeout connect"", ""timeout queue"", ""timeout server"", ""timeout tarpit""."
94,timeout client <timeout>timeout clitimeout <timeout> (deprecated)Set the maximum inactivity time on the client side.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,"Arguments :<timeout> is the timeout value specified in milliseconds by default, but"
94,"can be in any other unit if the number is suffixed by the unit,"
94,as explained at the top of this document.
94,The inactivity timeout applies when the client is expected to acknowledge or
94,"send data. In HTTP mode, this timeout is particularly important to consider"
94,"during the first phase, when the client sends the request, and during the"
94,"response while it is reading data sent by the server. That said, for the"
94,"first phase, it is preferable to set the ""timeout http-request"" to better"
94,protect HAProxy from Slowloris like attacks. The value is specified in
94,"milliseconds by default, but can be in any other unit if the number is"
94,"suffixed by the unit, as specified at the top of this document. In TCP mode"
94,"(and to a lesser extent, in HTTP mode), it is highly recommended that the"
94,client timeout remains equal to the server timeout in order to avoid complex
94,situations to debug. It is a good practice to cover one or several TCP packet
94,losses by specifying timeouts that are slightly above multiples of 3 seconds
94,(e.g. 4 or 5 seconds). If some long-lived sessions are mixed with short-lived
94,"sessions (e.g. WebSocket and HTTP), it's worth considering ""timeout tunnel"","
94,"which overrides ""timeout client"" and ""timeout server"" for tunnels, as well as"
94,"""timeout client-fin"" for half-closed connections."
94,"This parameter is specific to frontends, but can be specified once for all in"
94,"""defaults"" sections. This is in fact one of the easiest solutions not to"
94,"forget about it. An unspecified timeout results in an infinite timeout, which"
94,is not recommended. Such a usage is accepted and works but reports a warning
94,during startup because it may results in accumulation of expired sessions in
94,the system if the system's timeouts are not configured either.
94,"This also applies to HTTP/2 connections, which will be closed with GOAWAY."
94,"This parameter replaces the old, deprecated ""clitimeout"". It is recommended"
94,"to use it to write new configurations. The form ""timeout clitimeout"" is"
94,provided only by backwards compatibility but its use is strongly discouraged.
94,"See also : ""clitimeout"", ""timeout server"", ""timeout tunnel"", ""timeout http-request""."
94,timeout client-fin <timeout>Set the inactivity timeout on the client side for half-closed connections.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,"Arguments :<timeout> is the timeout value specified in milliseconds by default, but"
94,"can be in any other unit if the number is suffixed by the unit,"
94,as explained at the top of this document.
94,The inactivity timeout applies when the client is expected to acknowledge or
94,send data while one direction is already shut down. This timeout is different
94,"from ""timeout client"" in that it only applies to connections which are closed"
94,in one direction. This is particularly useful to avoid keeping connections in
94,FIN_WAIT state for too long when clients do not disconnect cleanly. This
94,problem is particularly common long connections such as RDP or WebSocket.
94,"Note that this timeout can override ""timeout tunnel"" when a connection shuts"
94,down in one direction. It is applied to idle HTTP/2 connections once a GOAWAY
94,"frame was sent, often indicating an expectation that the connection quickly"
94,ends.
94,"This parameter is specific to frontends, but can be specified once for all in"
94,"""defaults"" sections. By default it is not set, so half-closed connections"
94,will use the other timeouts (timeout.client or timeout.tunnel).
94,"See also : ""timeout client"", ""timeout server-fin"", and ""timeout tunnel""."
94,timeout connect <timeout>timeout contimeout <timeout> (deprecated)Set the maximum time to wait for a connection attempt to a server to succeed.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,"Arguments :<timeout> is the timeout value specified in milliseconds by default, but"
94,"can be in any other unit if the number is suffixed by the unit,"
94,as explained at the top of this document.
94,"If the server is located on the same LAN as haproxy, the connection should be"
94,"immediate (less than a few milliseconds). Anyway, it is a good practice to"
94,cover one or several TCP packet losses by specifying timeouts that are
94,"slightly above multiples of 3 seconds (e.g. 4 or 5 seconds). By default, the"
94,connect timeout also presets both queue and tarpit timeouts to the same value
94,if these have not been specified.
94,"This parameter is specific to backends, but can be specified once for all in"
94,"""defaults"" sections. This is in fact one of the easiest solutions not to"
94,"forget about it. An unspecified timeout results in an infinite timeout, which"
94,is not recommended. Such a usage is accepted and works but reports a warning
94,during startup because it may results in accumulation of failed sessions in
94,the system if the system's timeouts are not configured either.
94,"This parameter replaces the old, deprecated ""contimeout"". It is recommended"
94,"to use it to write new configurations. The form ""timeout contimeout"" is"
94,provided only by backwards compatibility but its use is strongly discouraged.
94,"See also: ""timeout check"", ""timeout queue"", ""timeout server"", ""contimeout"", ""timeout tarpit""."
94,timeout http-keep-alive <timeout>Set the maximum allowed time to wait for a new HTTP request to appear
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,"Arguments :<timeout> is the timeout value specified in milliseconds by default, but"
94,"can be in any other unit if the number is suffixed by the unit,"
94,as explained at the top of this document.
94,"By default, the time to wait for a new request in case of keep-alive is set"
94,"by ""timeout http-request"". However this is not always convenient because some"
94,people want very short keep-alive timeouts in order to release connections
94,"faster, and others prefer to have larger ones but still have short timeouts"
94,once the request has started to present itself.
94,"The ""http-keep-alive"" timeout covers these needs. It will define how long to"
94,wait for a new HTTP request to start coming after a response was sent. Once
94,"the first byte of request has been seen, the ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"" timeout is used"
94,to wait for the complete request to come. Note that empty lines prior to a
94,new request do not refresh the timeout and are not counted as a new request.
94,There is also another difference between the two timeouts : when a connection
94,"expires during timeout http-keep-alive, no error is returned, the connection"
94,"just closes. If the connection expires in ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"" while waiting for a"
94,"connection to complete, a HTTP 408 error is returned."
94,In general it is optimal to set this value to a few tens to hundreds of
94,"milliseconds, to allow users to fetch all objects of a page at once but"
94,"without waiting for further clicks. Also, if set to a very small value (e.g."
94,1 millisecond) it will probably only accept pipelined requests but not the
94,non-pipelined ones. It may be a nice trade-off for very large sites running
94,with tens to hundreds of thousands of clients.
94,"If this parameter is not set, the ""http-requestThis keyword is available in sections :Proxy sectionAlphabetically sorted keywords reference"" timeout applies, and if both"
94,"are not set, ""timeout client"" still applies at the lower level. It should be"
94,"set in the frontend to take effect, unless the frontend is in TCP mode, in"
94,which case the HTTP backend's timeout will be used.
94,"When using HTTP/2 ""timeout client"" is applied instead. This is so we can keep"
94,using short keep-alive timeouts in HTTP/1.1 while using longer ones in HTTP/2
94,(where we only have one connection per client and a connection setup).
94,"See also : ""timeout http-request"", ""timeout client""."
94,timeout http-request <timeout>Set the maximum allowed time to wait for a complete HTTP request
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,"Arguments :<timeout> is the timeout value specified in milliseconds by default, but"
94,"can be in any other unit if the number is suffixed by the unit,"
94,as explained at the top of this document.
94,"In order to offer DoS protection, it may be required to lower the maximum"
94,accepted time to receive a complete HTTP request without affecting the client
94,timeout. This helps protecting against established connections on which
94,nothing is sent. The client timeout cannot offer a good protection against
94,"this abuse because it is an inactivity timeout, which means that if the"
94,"attacker sends one character every now and then, the timeout will not"
94,"trigger. With the HTTP request timeout, no matter what speed the client"
94,"types, the request will be aborted if it does not complete in time. When the"
94,"timeout expires, an HTTP 408 response is sent to the client to inform it"
94,"about the problem, and the connection is closed. The logs will report"
94,"termination codes ""cR"". Some recent browsers are having problems with this"
94,"standard, well-documented behavior, so it might be needed to hide the 408"
94,"code using ""option http-ignore-probes"" or ""errorfile 408 /dev/null"". See"
94,"more details in the explanations of the ""cR"" termination code in section 8.5."
94,"By default, this timeout only applies to the header part of the request,"
94,"and not to any data. As soon as the empty line is received, this timeout is"
94,"not used anymore. When combined with ""option http-buffer-request"", this"
94,timeout also applies to the body of the request..
94,It is used again on keep-alive connections to wait for a second
94,"request if ""timeout http-keep-alive"" is not set."
94,"Generally it is enough to set it to a few seconds, as most clients send the"
94,full request immediately upon connection. Add 3 or more seconds to cover TCP
94,retransmits but that's all. Setting it to very low values (e.g. 50 ms) will
94,generally work on local networks as long as there are no packet losses. This
94,will prevent people from sending bare HTTP requests using telnet.
94,"If this parameter is not set, the client timeout still applies between each"
94,chunk of the incoming request. It should be set in the frontend to take
94,"effect, unless the frontend is in TCP mode, in which case the HTTP backend's"
94,timeout will be used.
94,"See also : ""errorfile"", ""http-ignore-probes"", ""timeout http-keep-alive"", and ""timeout client"", ""option http-buffer-request""."
94,timeout queue <timeout>Set the maximum time to wait in the queue for a connection slot to be free
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,"Arguments :<timeout> is the timeout value specified in milliseconds by default, but"
94,"can be in any other unit if the number is suffixed by the unit,"
94,as explained at the top of this document.
94,"When a server's maxconn is reached, connections are left pending in a queue"
94,which may be server-specific or global to the backend. In order not to wait
94,"indefinitely, a timeout is applied to requests pending in the queue. If the"
94,"timeout is reached, it is considered that the request will almost never be"
94,"served, so it is dropped and a 503 error is returned to the client."
94,"The ""timeout queue"" statement allows to fix the maximum time for a request to"
94,"be left pending in a queue. If unspecified, the same value as the backend's"
94,"connection timeout (""timeout connect"") is used, for backwards compatibility"
94,"with older versions with no ""timeout queue"" parameter."
94,"See also : ""timeout connect"", ""contimeout""."
94,timeout server <timeout>timeout srvtimeout <timeout> (deprecated)Set the maximum inactivity time on the server side.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,"Arguments :<timeout> is the timeout value specified in milliseconds by default, but"
94,"can be in any other unit if the number is suffixed by the unit,"
94,as explained at the top of this document.
94,The inactivity timeout applies when the server is expected to acknowledge or
94,"send data. In HTTP mode, this timeout is particularly important to consider"
94,"during the first phase of the server's response, when it has to send the"
94,"headers, as it directly represents the server's processing time for the"
94,"request. To find out what value to put there, it's often good to start with"
94,"what would be considered as unacceptable response times, then check the logs"
94,"to observe the response time distribution, and adjust the value accordingly."
94,"The value is specified in milliseconds by default, but can be in any other"
94,"unit if the number is suffixed by the unit, as specified at the top of this"
94,"document. In TCP mode (and to a lesser extent, in HTTP mode), it is highly"
94,recommended that the client timeout remains equal to the server timeout in
94,order to avoid complex situations to debug. Whatever the expected server
94,"response times, it is a good practice to cover at least one or several TCP"
94,packet losses by specifying timeouts that are slightly above multiples of 3
94,seconds (e.g. 4 or 5 seconds minimum). If some long-lived sessions are mixed
94,"with short-lived sessions (e.g. WebSocket and HTTP), it's worth considering"
94,"""timeout tunnel"", which overrides ""timeout client"" and ""timeout server"" for"
94,tunnels.
94,"This parameter is specific to backends, but can be specified once for all in"
94,"""defaults"" sections. This is in fact one of the easiest solutions not to"
94,"forget about it. An unspecified timeout results in an infinite timeout, which"
94,is not recommended. Such a usage is accepted and works but reports a warning
94,during startup because it may results in accumulation of expired sessions in
94,the system if the system's timeouts are not configured either.
94,"This parameter replaces the old, deprecated ""srvtimeout"". It is recommended"
94,"to use it to write new configurations. The form ""timeout srvtimeout"" is"
94,provided only by backwards compatibility but its use is strongly discouraged.
94,"See also : ""srvtimeout"", ""timeout client"" and ""timeout tunnel""."
94,timeout server-fin <timeout>Set the inactivity timeout on the server side for half-closed connections.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,"Arguments :<timeout> is the timeout value specified in milliseconds by default, but"
94,"can be in any other unit if the number is suffixed by the unit,"
94,as explained at the top of this document.
94,The inactivity timeout applies when the server is expected to acknowledge or
94,send data while one direction is already shut down. This timeout is different
94,"from ""timeout server"" in that it only applies to connections which are closed"
94,in one direction. This is particularly useful to avoid keeping connections in
94,FIN_WAIT state for too long when a remote server does not disconnect cleanly.
94,This problem is particularly common long connections such as RDP or WebSocket.
94,"Note that this timeout can override ""timeout tunnel"" when a connection shuts"
94,"down in one direction. This setting was provided for completeness, but in most"
94,"situations, it should not be needed."
94,"This parameter is specific to backends, but can be specified once for all in"
94,"""defaults"" sections. By default it is not set, so half-closed connections"
94,will use the other timeouts (timeout.server or timeout.tunnel).
94,"See also : ""timeout client-fin"", ""timeout server"", and ""timeout tunnel""."
94,timeout tarpit <timeout>Set the duration for which tarpitted connections will be maintained
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesyes
94,"Arguments :<timeout> is the tarpit duration specified in milliseconds by default, but"
94,"can be in any other unit if the number is suffixed by the unit,"
94,as explained at the top of this document.
94,"When a connection is tarpitted using ""http-request tarpit"" or"
94,"""reqtarpit"", it is maintained open with no activity for a certain"
94,"amount of time, then closed. ""timeout tarpit"" defines how long it will"
94,be maintained open.
94,"The value is specified in milliseconds by default, but can be in any other"
94,"unit if the number is suffixed by the unit, as specified at the top of this"
94,"document. If unspecified, the same value as the backend's connection timeout"
94,"(""timeout connect"") is used, for backwards compatibility with older versions"
94,"with no ""timeout tarpit"" parameter."
94,"See also : ""timeout connect"", ""contimeout""."
94,timeout tunnel <timeout>Set the maximum inactivity time on the client and server side for tunnels.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,"Arguments :<timeout> is the timeout value specified in milliseconds by default, but"
94,"can be in any other unit if the number is suffixed by the unit,"
94,as explained at the top of this document.
94,The tunnel timeout applies when a bidirectional connection is established
94,"between a client and a server, and the connection remains inactive in both"
94,directions. This timeout supersedes both the client and server timeouts once
94,"the connection becomes a tunnel. In TCP, this timeout is used as soon as no"
94,analyzer remains attached to either connection (e.g. tcp content rules are
94,"accepted). In HTTP, this timeout is used when a connection is upgraded (e.g."
94,"when switching to the WebSocket protocol, or forwarding a CONNECT request"
94,"to a proxy), or after the first response when no keepalive/close option is"
94,specified.
94,"Since this timeout is usually used in conjunction with long-lived connections,"
94,"it usually is a good idea to also set ""timeout client-fin"" to handle the"
94,situation where a client suddenly disappears from the net and does not
94,"acknowledge a close, or sends a shutdown and does not acknowledge pending"
94,"data anymore. This can happen in lossy networks where firewalls are present,"
94,and is detected by the presence of large amounts of sessions in a FIN_WAIT
94,state.
94,"The value is specified in milliseconds by default, but can be in any other"
94,"unit if the number is suffixed by the unit, as specified at the top of this"
94,"document. Whatever the expected normal idle time, it is a good practice to"
94,cover at least one or several TCP packet losses by specifying timeouts that
94,are slightly above multiples of 3 seconds (e.g. 4 or 5 seconds minimum).
94,"This parameter is specific to backends, but can be specified once for all in"
94,"""defaults"" sections. This is in fact one of the easiest solutions not to"
94,forget about it.
94,Example :
94,defaults http
94,option http-server-close
94,timeout connect 5s
94,timeout client 30s
94,timeout client-fin 30s
94,timeout server 30s
94,timeout tunnel
94,# timeout to use with WebSocket and CONNECT
94,"See also : ""timeout client"", ""timeout client-fin"", ""timeout server""."
94,transparent (deprecated)Enable client-side transparent proxying
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesnoyesyes
94,Arguments : none
94,This keyword was introduced in order to provide layer 7 persistence to layer
94,3 load balancers. The idea is to use the OS's ability to redirect an incoming
94,"connection for a remote address to a local process (here HAProxy), and let"
94,this process know what address was initially requested. When this option is
94,"used, sessions without cookies will be forwarded to the original destination"
94,IP address of the incoming request (which should match that of another
94,"equipment), while requests with cookies will still be forwarded to the"
94,appropriate server.
94,"The ""transparent"" keyword is deprecated, use ""option transparent"" instead."
94,"Note that contrary to a common belief, this option does NOT make HAProxy"
94,present the client's IP to the server when establishing the connection.
94,"See also: ""option transparent"""
94,unique-id-format <string>Generate a unique ID for each request.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments :<string>
94,is a log-format string.
94,This keyword creates a ID for each request using the custom log format. A
94,unique ID is useful to trace a request passing through many components of
94,a complex infrastructure. The newly created ID may also be logged using the
94,%ID tag the log-format string.
94,The format should be composed from elements that are guaranteed to be
94,"unique when combined together. For instance, if multiple haproxy instances"
94,"are involved, it might be important to include the node name. It is often"
94,needed to log the incoming connection's source and destination addresses
94,and ports. Note that since multiple requests may be performed over the same
94,"connection, including a request counter may help differentiate them."
94,"Similarly, a timestamp may protect against a rollover of the counter."
94,Logging the process ID will avoid collisions after a service restart.
94,It is recommended to use hexadecimal notation for many fields since it
94,makes them more compact and saves space in logs.
94,Example:
94,unique-id-format %{+X}o\ %ci:%cp_%fi:%fp_%Ts_%rt:%pid
94,will generate:
94,7F000001:8296_7F00001E:1F90_4F7B0A69_0003:790A
94,"See also: ""unique-id-header"""
94,unique-id-header <name>Add a unique ID header in the HTTP request.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,yesyesyesno
94,Arguments :<name>
94,is the name of the header.
94,"Add a unique-id header in the HTTP request sent to the server, using the"
94,unique-id-format. It can't work if the unique-id-format doesn't exist.
94,Example:
94,unique-id-format %{+X}o\ %ci:%cp_%fi:%fp_%Ts_%rt:%pid
94,unique-id-header X-Unique-ID
94,will generate:
94,X-Unique-ID: 7F000001:8296_7F00001E:1F90_4F7B0A69_0003:790A
94,"See also: ""unique-id-format"""
94,use_backend <backend> [{if | unless} <condition>]Switch to a specific backend if/unless an ACL-based condition is matched.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,noyesyesno
94,Arguments :<backend>
94,"is the name of a valid backend or ""listen"" section, or a"
94,"""log-format"" string resolving to a backend name."
94,"<condition> is a condition composed of ACLs, as described in section 7. If"
94,"it is omitted, the rule is unconditionally applied."
94,"When doing content-switching, connections arrive on a frontend and are then"
94,dispatched to various backends depending on a number of conditions. The
94,relation between the conditions and the backends is described with the
94,"""use_backend"" keyword. While it is normally used with HTTP processing, it can"
94,"also be used in pure TCP, either without content using stateless ACLs (e.g."
94,"source address validation) or combined with a ""tcp-request"" rule to wait for"
94,some payload.
94,"There may be as many ""use_backend"" rules as desired. All of these rules are"
94,"evaluated in their declaration order, and the first one which matches will"
94,assign the backend.
94,"In the first form, the backend will be used if the condition is met. In the"
94,"second form, the backend will be used if the condition is not met. If no"
94,"condition is valid, the backend defined with ""default_backend"" will be used."
94,"If no default backend is defined, either the servers in the same section are"
94,"used (in case of a ""listen"" section) or, in case of a frontend, no server is"
94,used and a 503 service unavailable response is returned.
94,Note that it is possible to switch from a TCP frontend to an HTTP backend. In
94,"this case, either the frontend has already checked that the protocol is HTTP,"
94,"and backend processing will immediately follow, or the backend will wait for"
94,a complete HTTP request to get in. This feature is useful when a frontend
94,"must decode several protocols on a unique port, one of them being HTTP."
94,"When <backend> is a simple name, it is resolved at configuration time, and an"
94,error is reported if the specified backend does not exist. If <backend> is
94,"a log-format string instead, no check may be done at configuration time, so"
94,the backend name is resolved dynamically at run time. If the resulting
94,"backend name does not correspond to any valid backend, no other rule is"
94,"evaluated, and the default_backend directive is applied instead. Note that"
94,"when using dynamic backend names, it is highly recommended to use a prefix"
94,that no other backend uses in order to ensure that an unauthorized backend
94,cannot be forced from the request.
94,"It is worth mentioning that ""use_backend"" rules with an explicit name are"
94,used to detect the association between frontends and backends to compute the
94,"backend's ""fullconn"" setting. This cannot be done for dynamic names."
94,"See also: ""default_backend"", ""tcp-request"", ""fullconn"", ""log-format"", and section 7 about ACLs."
94,use-server <server> if <condition>use-server <server> unless <condition>Only use a specific server if/unless an ACL-based condition is matched.
94,May be used in sections
94,defaultsfrontendlistenbackend
94,nonoyesyes
94,Arguments :<server>
94,is the name of a valid server in the same backend section.
94,"<condition> is a condition composed of ACLs, as described in section 7."
94,"By default, connections which arrive to a backend are load-balanced across"
94,"the available servers according to the configured algorithm, unless a"
94,persistence mechanism such as a cookie is used and found in the request.
94,Sometimes it is desirable to forward a particular request to a specific
94,server without having to declare a dedicated backend for this server. This
94,"can be achieved using the ""use-server"" rules. These rules are evaluated after"
94,"the ""redirect"" rules and before evaluating cookies, and they have precedence"
94,"on them. There may be as many ""use-server"" rules as desired. All of these"
94,"rules are evaluated in their declaration order, and the first one which"
94,matches will assign the server.
94,"If a rule designates a server which is down, and ""option persist"" is not used"
94,"and no force-persist rule was validated, it is ignored and evaluation goes on"
94,with the next rules until one matches.
94,"In the first form, the server will be used if the condition is met. In the"
94,"second form, the server will be used if the condition is not met. If no"
94,"condition is valid, the processing continues and the server will be assigned"
94,according to other persistence mechanisms.
94,"Note that even if a rule is matched, cookie processing is still performed but"
94,does not assign the server. This allows prefixed cookies to have their prefix
94,stripped.
94,"The ""use-server"" statement works both in HTTP and TCP mode. This makes it"
94,"suitable for use with content-based inspection. For instance, a server could"
94,be selected in a farm according to the TLS SNI field. And if these servers
94,"have their weight set to zero, they will not be used for other traffic."
94,Example :
94,# intercept incoming TLS requests based on the SNI field
94,use-server www if { req_ssl_sni -i www.example.com }
94,server
94,www 192.168.0.1:443 weight 0
94,use-server mail if { req_ssl_sni -i mail.example.com }
94,server
94,mail 192.168.0.1:587 weight 0
94,use-server imap if { req_ssl_sni -i imap.example.com }
94,server
94,imap 192.168.0.1:993 weight 0
94,# all the rest is forwarded to this server
94,server
94,default 192.168.0.2:443 check
94,"See also: ""use_backend"", section 5 about server and section 7 about ACLs."
94,5. Bind and server options
94,"The ""bind"", ""server"" and ""default-server"" keywords support a number of settings"
94,depending on some build options and on the system HAProxy was built on. These
94,"settings generally each consist in one word sometimes followed by a value,"
94,"written on the same line as the ""bind"" or ""server"" line. All these options are"
94,described in this section.
94,5.1. Bind options
94,"The ""bind"" keyword supports a certain number of settings which are all passed"
94,as arguments on the same line. The order in which those arguments appear makes
94,"no importance, provided that they appear after the bind address. All of these"
94,"parameters are optional. Some of them consist in a single words (booleans),"
94,"while other ones expect a value after them. In this case, the value must be"
94,provided immediately after the setting name.
94,The currently supported settings are the following ones.
94,accept-netscaler-cip <magic number>Enforces the use of the NetScaler Client IP insertion protocol over any
94,connection accepted by any of the TCP sockets declared on the same line. The
94,NetScaler Client IP insertion protocol dictates the layer 3/4 addresses of
94,"the incoming connection to be used everywhere an address is used, with the"
94,"only exception of ""tcp-request connection"" rules which will only see the"
94,real connection address. Logs will reflect the addresses indicated in the
94,"protocol, unless it is violated, in which case the real"
94,address will still
94,be used. This keyword combined with support from external components can be
94,used as an efficient and reliable alternative to the X-Forwarded-For
94,mechanism which is not always reliable and not even always usable. See also
94,"""tcp-request connection expect-netscaler-cip"" for a finer-grained setting of"
94,which client is allowed to use the protocol.
94,accept-proxyEnforces the use of the PROXY protocol over any connection accepted by any of
94,the sockets declared on the same line. Versions 1 and 2 of the PROXY protocol
94,are supported and correctly detected. The PROXY protocol dictates the layer
94,3/4 addresses of the incoming connection to be used everywhere an address is
94,"used, with the only exception of ""tcp-request connection"" rules which will"
94,only see the real connection address. Logs will reflect the addresses
94,"indicated in the protocol, unless it is violated, in which case the real"
94,address will still be used. This keyword combined with support from external
94,components can be used as an efficient and reliable alternative to the
94,X-Forwarded-For mechanism which is not always reliable and not even always
94,"usable. See also ""tcp-request connection expect-proxy"" for a finer-grained"
94,setting of which client is allowed to use the protocol.
94,"allow-0rttAllow receiving early data when using TLSv1.3. This is disabled by default,"
94,"due to security considerations. Because it is vulnerable to replay attacks,"
94,"you should only allow if for requests that are safe to replay, ie requests"
94,"that are idempotent. You can use the ""wait-for-handshake"" action for any"
94,request that wouldn't be safe with early data.
94,alpn <protocols>This enables the TLS ALPN extension and advertises the specified protocol
94,list as supported on top of ALPN. The protocol list consists in a comma-
94,"delimited list of protocol names, for instance: ""http/1.1,http/1.0"" (without"
94,quotes). This requires that the SSL library is build with support for TLS
94,extensions enabled (check with haproxy -vv). The ALPN extension replaces the
94,initial NPN extension. ALPN is required to enable HTTP/2 on an HTTP frontend.
94,Versions of OpenSSL prior to 1.0.2 didn't support ALPN and only supposed the
94,"now obsolete NPN extension. At the time of writing this, most browsers still"
94,support both ALPN and NPN for HTTP/2 so a fallback to NPN may still work for
94,a while. But ALPN must be used whenever possible. If both HTTP/2 and HTTP/1.1
94,"are expected to be supported, both versions can be advertised, in order of"
94,"preference, like below :"
94,"bind :443 ssl crt pub.pem alpn h2,http/1.1"
94,"backlog <backlog>Sets the socket's backlog to this value. If unspecified, the frontend's"
94,"backlog is used instead, which generally defaults to the maxconn value."
94,curves <curves>This setting is only available when support for OpenSSL was built in. It sets
94,"the string describing the list of elliptic curves algorithms (""curve suite"")"
94,that are negotiated during the SSL/TLS handshake with ECDHE. The format of the
94,string is a colon-delimited list of curve name.
94,Example:
94,"""X25519:P-256"" (without quote)When ""curves"" is set, ""ecdhe"" parameter is ignored."
94,ecdhe <named curve>This setting is only available when support for OpenSSL was built in. It sets
94,"the named curve (RFC 4492) used to generate ECDH ephemeral keys. By default,"
94,used named curve is prime256v1.
94,ca-file <cafile>This setting is only available when support for OpenSSL was built in. It
94,designates a PEM file from which to load CA certificates used to verify
94,client's certificate.
94,"ca-ignore-err [all|<errorID>,...]This setting is only available when support for OpenSSL was built in."
94,Sets a comma separated list of errorIDs to ignore during verify at depth > 0.
94,"If set to 'all', all errors are ignored. SSL handshake is not aborted if an"
94,error is ignored.
94,ca-sign-file <cafile>This setting is only available when support for OpenSSL was built in. It
94,designates a PEM file containing both the CA certificate and the CA private
94,key used to create and sign server's certificates. This is a mandatory
94,setting when the dynamic generation of certificates is enabled. See
94,'generate-certificates' for details.
94,ca-sign-pass <passphrase>This setting is only available when support for OpenSSL was built in. It is
94,the CA private key passphrase. This setting is optional and used only when
94,the dynamic generation of certificates is enabled. See
94,'generate-certificates' for details.
94,ciphers <ciphers>This setting is only available when support for OpenSSL was built in. It sets
94,"the string describing the list of cipher algorithms (""cipher suite"") that are"
94,negotiated during the SSL/TLS handshake up to TLSv1.2. The format of the
94,"string is defined in ""man 1 ciphers"" from OpenSSL man pages. For background"
94,information and recommendations see e.g.
94,(https://wiki.mozilla.org/Security/Server_Side_TLS) and
94,(https://mozilla.github.io/server-side-tls/ssl-config-generator/). For TLSv1.3
94,"cipher configuration, please check the ""ciphersuitesThis keyword is available in sections :Bind optionsServer and default-server options"" keyword."
94,ciphersuites <ciphersuites>This setting is only available when support for OpenSSL was built in and
94,OpenSSL 1.1.1 or later was used to build HAProxy. It sets the string describing
94,"the list of cipher algorithms (""cipher suite"") that are negotiated during the"
94,"TLSv1.3 handshake. The format of the string is defined in ""man 1 ciphers"" from"
94,"OpenSSL man pages under the ""ciphersuitesThis keyword is available in sections :Bind optionsServer and default-server options"" section. For cipher configuration"
94,"for TLSv1.2 and earlier, please check the ""ciphersThis keyword is available in sections :Bind optionsServer and default-server options"" keyword."
94,crl-file <crlfile>This setting is only available when support for OpenSSL was built in. It
94,designates a PEM file from which to load certificate revocation list used
94,to verify client's certificate.
94,crt <cert>This setting is only available when support for OpenSSL was built in. It
94,designates a PEM file containing both the required certificates and any
94,associated private keys. This file can be built by concatenating multiple
94,PEM files into one (e.g. cat cert.pem key.pem > combined.pem). If your CA
94,"requires an intermediate certificate, this can also be concatenated into this"
94,file.
94,"If the OpenSSL used supports Diffie-Hellman, parameters present in this file"
94,are loaded.
94,"If a directory name is used instead of a PEM file, then all files found in"
94,that directory will be loaded in alphabetic order unless their name ends with
94,"'.issuer', '.ocsp' or '.sctl' (reserved extensions). This directive may be"
94,specified multiple times in order to load certificates from multiple files or
94,directories. The certificates will be presented to clients who provide a
94,valid TLS Server Name Indication field matching one of their CN or alt
94,"subjects. Wildcards are supported, where a wildcard character '*' is used"
94,instead of the first hostname component (e.g. *.example.org matches
94,www.example.org but not www.sub.example.org).
94,If no SNI is provided by the client or if the SSL library does not support
94,"TLS extensions, or if the client provides an SNI hostname which does not"
94,"match any certificate, then the first loaded certificate will be presented."
94,"This means that when loading certificates from a directory, it is highly"
94,recommended to load the default one first as a file or to ensure that it will
94,always be the first one in the directory.
94,Note that the same cert may be loaded multiple times without side effects.
94,Some CAs (such as GoDaddy) offer a drop down list of server types that do not
94,include HAProxy when obtaining a certificate. If this happens be sure to
94,choose a web server that the CA believes requires an intermediate CA (for
94,"GoDaddy, selection Apache Tomcat will get the correct bundle, but many"
94,"others, e.g. nginx, result in a wrong bundle that will not work for some"
94,clients).
94,"For each PEM file, haproxy checks for the presence of file at the same path"
94,"suffixed by "".ocsp"". If such file is found, support for the TLS Certificate"
94,"Status Request extension (also known as ""OCSP stapling"") is automatically"
94,"enabled. The content of this file is optional. If not empty, it must contain"
94,a valid OCSP Response in DER format. In order to be valid an OCSP Response
94,"must comply with the following rules: it has to indicate a good status,"
94,"it has to be a single response for the certificate of the PEM file, and it"
94,has to be valid at the moment of addition. If these rules are not respected
94,the OCSP Response is ignored and a warning is emitted. In order to
94,identify
94,"which certificate an OCSP Response applies to, the issuer's certificate is"
94,"necessary. If the issuer's certificate is not found in the PEM file, it will"
94,"be loaded from a file at the same path as the PEM file suffixed by "".issuer"""
94,if it exists otherwise it will fail with an error.
94,"For each PEM file, haproxy also checks for the presence of file at the same"
94,"path suffixed by "".sctl"". If such file is found, support for Certificate"
94,Transparency (RFC6962) TLS extension is enabled. The file must contain a
94,"valid Signed Certificate Timestamp List, as described in RFC. File is parsed"
94,"to check basic syntax, but no signatures are verified."
94,"There are cases where it is desirable to support multiple key types, e.g. RSA"
94,and ECDSA in the cipher suites offered to the clients. This allows clients
94,"that support EC certificates to be able to use EC ciphers, while"
94,"simultaneously supporting older, RSA only clients."
94,"In order to provide this functionality, multiple PEM files, each with a"
94,"different key type, are required. To associate these PEM files into a"
94,"""cert bundle"" that is recognized by haproxy, they must be named in the"
94,following way: All PEM files that are to be bundled must have the same base
94,"name, with a suffix indicating the key type. Currently, three suffixes are"
94,"supported: rsa, dsa and ecdsa. For example, if www.example.com has two PEM"
94,"files, an RSA file and an ECDSA file, they must be named: ""example.pem.rsa"""
94,"and ""example.pem.ecdsa"". The first part of the filename is arbitrary; only the"
94,"suffix matters. To load this bundle into haproxy, specify the base name only:"
94,Example :
94,bind :8443 ssl crt example.pem
94,Note that the suffix is not given to haproxy; this tells haproxy to look for
94,a cert bundle.
94,HAProxy will load all PEM files in the bundle at the same time to try to
94,support multiple key types. PEM files are combined based on Common Name
94,(CN) and Subject Alternative Name (SAN) to support SNI lookups. This means
94,"that even if you give haproxy a cert bundle, if there are no shared CN/SAN"
94,"entries in the certificates in that bundle, haproxy will not be able to"
94,provide multi-cert support.
94,Assuming bundle in the example above contained the following:
94,FilenameCNSAN
94,example.pem.rsawww.example.comrsa.example.com
94,example.pem.ecdsawww.example.comecdsa.example.com
94,"Users connecting with an SNI of ""www.example.com"" will be able"
94,to use both RSA and ECDSA cipher suites. Users connecting with an SNI of
94,"""rsa.example.com"" will only be able to use RSA cipher suites, and users"
94,"connecting with ""ecdsa.example.com"" will only be able to use ECDSA cipher"
94,"suites. With BoringSSL and Openssl >= 1.1.1 multi-cert is natively supported,"
94,no need to bundle certificates. ECDSA certificate will be preferred if client
94,support it.
94,"If a directory name is given as the <cert> argument, haproxy will"
94,automatically search and load bundled files in that directory.
94,OSCP files (.ocsp) and issuer files (.issuer) are supported with multi-cert
94,bundling. Each certificate can have its own .ocsp and .issuer file. At this
94,"time, sctl is not supported in multi-certificate bundling."
94,crt-ignore-err <errors>This setting is only available when support for OpenSSL was built in. Sets a
94,comma separated list of errorIDs to ignore during verify at depth == 0. If
94,"set to 'all', all errors are ignored. SSL handshake is not aborted if an error"
94,is ignored.
94,crt-list <file>This setting is only available when support for OpenSSL was built in. It
94,designates a list of PEM file with an optional ssl configuration and a SNI
94,"filter per certificate, with the following format for each line :"
94,<crtfile> [\[<sslbindconf> ...\]] [[!]<snifilter> ...]
94,"sslbindconf supports ""allow-0rttThis keyword is available in sections :Bind optionsServer and default-server options"", ""alpn"", ""ca-fileThis keyword is available in sections :Bind optionsServer and default-server options"", ""ciphersThis keyword is available in sections :Bind optionsServer and default-server options"","
94,"""ciphersuitesThis keyword is available in sections :Bind optionsServer and default-server options"", ""crl-fileThis keyword is available in sections :Bind optionsServer and default-server options"", ""curves"", ""ecdhe"", ""no-ca-names"", ""npn"","
94,"""verifyThis keyword is available in sections :Bind optionsServer and default-server options"" configuration. With BoringSSL and Openssl >= 1.1.1"
94,"""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"" and ""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options"" are also supported. It overrides the"
94,configuration set in bind line for the certificate.
94,Wildcards are supported in the SNI filter. Negative filters can be specified
94,"in the configuration, but they are only used as a hint, they don't do"
94,anything.
94,(this changes in newer haproxy versions) If you want to exclude a
94,"SNI from a wildcard, use this positive SNI on another line. (like in the"
94,example).
94,The certificates will be presented to clients who provide a valid TLS Server
94,Name Indication field matching one of the SNI filters. If no SNI filter is
94,"specified, the CN and alt subjects are used. This directive may be specified"
94,"multiple times. See the ""crtThis keyword is available in sections :Bind optionsServer and default-server options"" option for more information. The default"
94,"certificate is still needed to meet OpenSSL expectations. If it is not used,"
94,the 'strict-sni' option may be used.
94,"Multi-cert bundling (see ""crtThis keyword is available in sections :Bind optionsServer and default-server options"") is supported with crt-list, as long as only"
94,the base name is given in the crt-list. SNI filter will do the same work on
94,all bundled certificates. With BoringSSL and Openssl >= 1.1.1 multi-cert is
94,"natively supported, avoid multi-cert bundling. RSA and ECDSA certificates can"
94,"be declared in a row, and set different ssl and filter parameter."
94,crt-list file example:
94,cert1.pem
94,"cert2.pem [alpn h2,http/1.1]"
94,certW.pem
94,*.domain.tld !secure.domain.tld
94,certS.pem [curves X25519:P-256 ciphers ECDHE-ECDSA-AES256-GCM-SHA384] secure.domain.tld
94,defer-acceptIs an optional keyword which is supported only on certain Linux kernels. It
94,"states that a connection will only be accepted once some data arrive on it,"
94,or at worst after the first retransmit. This should be used only on protocols
94,for which the client talks first (e.g. HTTP). It can slightly improve
94,performance by ensuring that most of the request is already available when
94,"the connection is accepted. On the other hand, it will not be able to detect"
94,connections which don't talk. It is important to note that this option is
94,"broken in all kernels up to 2.6.31, as the connection is never accepted until"
94,the client talks. This can cause issues with front firewalls which would see
94,an established connection while the proxy will only see it in SYN_RECV. This
94,option is only supported on TCPv4/TCPv6 sockets and ignored by other ones.
94,expose-fd listenersThis option is only usable with the stats socket. It gives your stats socket
94,the capability to pass listeners FD to another HAProxy process.
94,"During a reload with the master-worker mode, the process is automatically"
94,reexecuted adding -x and one of the stats socket with this option.
94,"See also ""-x"" in the management guide."
94,force-sslv3This option enforces use of SSLv3 only on SSL connections instantiated from
94,this listener. SSLv3 is generally less expensive than the TLS counterparts
94,for high connection rates. This option is also available on global statement
94,"""ssl-default-bind-options"". See also ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"" and ""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options""."
94,force-tlsv10This option enforces use of TLSv1.0 only on SSL connections instantiated from
94,this listener. This option is also available on global statement
94,"""ssl-default-bind-options"". See also ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"" and ""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options""."
94,force-tlsv11This option enforces use of TLSv1.1 only on SSL connections instantiated from
94,this listener. This option is also available on global statement
94,"""ssl-default-bind-options"". See also ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"" and ""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options""."
94,force-tlsv12This option enforces use of TLSv1.2 only on SSL connections instantiated from
94,this listener. This option is also available on global statement
94,"""ssl-default-bind-options"". See also ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"" and ""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options""."
94,force-tlsv13This option enforces use of TLSv1.3 only on SSL connections instantiated from
94,this listener. This option is also available on global statement
94,"""ssl-default-bind-options"". See also ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"" and ""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options""."
94,generate-certificatesThis setting is only available when support for OpenSSL was built in. It
94,enables the dynamic SSL certificates generation. A CA certificate and its
94,private key are necessary (see 'ca-sign-file'). When HAProxy is configured as
94,"a transparent forward proxy, SSL requests generate errors because of a common"
94,name mismatch on the certificate presented to the client. With this option
94,"enabled, HAProxy will try to forge a certificate using the SNI hostname"
94,indicated by the client. This is done only if no certificate matches the SNI
94,"hostname (see 'crt-list'). If an error occurs, the default certificate is"
94,"used, else the 'strict-sni' option is set."
94,It can also be used when HAProxy is configured as a reverse proxy to ease the
94,deployment of an architecture with many backends.
94,"Creating a SSL certificate is an expensive operation, so a LRU cache is used"
94,to store forged certificates (see 'tune.ssl.ssl-ctx-cache-size'). It
94,increases the HAProxy's memory footprint to reduce latency when the same
94,certificate is used many times.
94,gid <gid>Sets the group of the UNIX sockets to the designated system gid. It can also
94,"be set by default in the global section's ""unix-bind"" statement. Note that"
94,"some platforms simply ignore this. This setting is equivalent to the ""groupThis keyword is available in sections :Process management and securityUserlistsBind options"""
94,setting except that the group ID is used instead of its name. This setting is
94,ignored by non UNIX sockets.
94,group <group>Sets the group of the UNIX sockets to the designated system group. It can
94,"also be set by default in the global section's ""unix-bind"" statement. Note"
94,that some platforms simply ignore this. This setting is equivalent to the
94,"""gidThis keyword is available in sections :Process management and securityBind options"" setting except that the group name is used instead of its gid. This"
94,setting is ignored by non UNIX sockets.
94,"id <id>Fixes the socket ID. By default, socket IDs are automatically assigned, but"
94,sometimes it is more convenient to fix them to ease monitoring. This value
94,must be strictly positive and unique within the listener/frontend. This
94,option can only be used when defining only a single socket.
94,"interface <interface>Restricts the socket to a specific interface. When specified, only packets"
94,received from that particular interface are processed by the socket. This is
94,currently only supported on Linux. The interface must be a primary system
94,"interface, not an aliased interface. It is also possible to bind multiple"
94,frontends to the same address if they are bound to different interfaces. Note
94,that binding to a network interface requires root privileges. This parameter
94,"is only compatible with TCPv4/TCPv6 sockets. When specified, return traffic"
94,"uses the same interface as inbound traffic, and its associated routing table,"
94,even if there are explicit routes through different interfaces configured.
94,This can prove useful to address asymmetric routing issues when the same
94,client IP addresses need to be able to reach frontends hosted on different
94,interfaces.
94,level <level>This setting is used with the stats sockets only to restrict the nature of
94,the commands that can be issued on the socket. It is ignored by other
94,sockets. <level> can be one of :
94,"- ""userThis keyword is available in sections :Process management and securityUserlistsBind options"" is the least privileged level; only non-sensitive stats can be"
94,"read, and no change is allowed. It would make sense on systems where it"
94,is not easy to restrict access to the socket.
94,"- ""operator"" is the default level and fits most common uses. All data can"
94,"be read, and only non-sensitive changes are permitted (e.g. clear max"
94,counters).
94,"- ""admin"" should be used with care, as everything is permitted (e.g. clear"
94,all counters).
94,severity-output <format>This setting is used with the stats sockets only to configure severity
94,level output prepended to informational feedback messages. Severity
94,"level of messages can range between 0 and 7, conforming to syslog"
94,rfc5424. Valid and successful socket commands requesting data
94,"(i.e. ""show map"", ""get acl foo"" etc.) will never have a severity level"
94,prepended. It is ignored by other sockets. <format> can be one of :
94,"- ""none"" (default) no severity level is prepended to feedback messages."
94,"- ""number"" severity level is prepended as a number."
94,"- ""string"" severity level is prepended as a string following the"
94,rfc5424 convention.
94,maxconn <maxconn>Limits the sockets to this number of concurrent connections. Extraneous
94,connections will remain in the system's backlog until a connection is
94,"released. If unspecified, the limit will be the same as the frontend's"
94,"maxconn. Note that in case of port ranges or multiple addresses, the same"
94,value will be applied to each socket. This setting enables different
94,"limitations on expensive sockets, for instance SSL entries which may easily"
94,eat all memory.
94,mode <mode>Sets the octal mode used to define access permissions on the UNIX socket. It
94,"can also be set by default in the global section's ""unix-bind"" statement."
94,Note that some platforms simply ignore this. This setting is ignored by non
94,UNIX sockets.
94,mss <maxseg>Sets the TCP Maximum Segment Size (MSS) value to be advertised on incoming
94,connections. This can be used to force a lower MSS for certain specific
94,"ports, for instance for connections passing through a VPN. Note that this"
94,relies on a kernel feature which is theoretically supported under Linux but
94,was buggy in all versions prior to 2.6.28. It may or may not work on other
94,operating systems. It may also not change the advertised value but change the
94,effective size of outgoing segments. The commonly advertised value for TCPv4
94,over Ethernet networks is 1460 = 1500(MTU) - 40(IP+TCP). If this value is
94,"positive, it will be used as the advertised MSS. If it is negative, it will"
94,indicate by how much to reduce the incoming connection's advertised MSS for
94,outgoing segments. This parameter is only compatible with TCP v4/v6 sockets.
94,"name <name>Sets an optional name for these sockets, which will be reported on the stats"
94,page.
94,"namespace <name>On Linux, it is possible to specify which network namespace a socket will"
94,belong to. This directive makes it possible to explicitly bind a listener to
94,a namespace different from the default one. Please refer to your operating
94,system's documentation to find more details about network namespaces.
94,nice <nice>Sets the 'niceness' of connections initiated from the socket. Value must be
94,"in the range -1024..1024 inclusive, and defaults to zero. Positive values"
94,means that such connections are more friendly to others and easily offer
94,"their place in the scheduler. On the opposite, negative values mean that"
94,connections want to run with a higher priority than others. The difference
94,only happens under high loads when the system is close to saturation.
94,"Negative values are appropriate for low-latency or administration services,"
94,and high values are generally recommended for CPU intensive tasks such as SSL
94,"processing or bulk transfers which are less sensible to latency. For example,"
94,it may make sense to use a positive value for an SMTP socket and a negative
94,one for an RDP socket.
94,no-ca-namesThis setting is only available when support for OpenSSL was built in. It
94,prevents from send CA names in server hello message when ca-file is used.
94,no-sslv3This setting is only available when support for OpenSSL was built in. It
94,disables support for SSLv3 on any sockets instantiated from the listener when
94,SSL is supported. Note that SSLv2 is forced disabled in the code and cannot
94,be enabled using any configuration option. This option is also available on
94,"global statement ""ssl-default-bind-options"". Use ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"" and"
94,"""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options"" instead."
94,no-tls-ticketsThis setting is only available when support for OpenSSL was built in. It
94,disables the stateless session resumption (RFC 5077 TLS Ticket
94,extension) and force to use stateful session resumption. Stateless
94,session resumption is more expensive in CPU usage. This option is also
94,"available on global statement ""ssl-default-bind-options""."
94,The TLS ticket mechanism is only used up to TLS 1.2.
94,"Forward Secrecy is compromised with TLS tickets, unless ticket keys"
94,"are periodically rotated (via reload or by using ""tls-ticket-keys"")."
94,no-tlsv10This setting is only available when support for OpenSSL was built in. It
94,disables support for TLSv1.0 on any sockets instantiated from the listener
94,when SSL is supported. Note that SSLv2 is forced disabled in the code and
94,cannot be enabled using any configuration option. This option is also
94,"available on global statement ""ssl-default-bind-options"". Use ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"""
94,"and ""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options"" instead."
94,no-tlsv11This setting is only available when support for OpenSSL was built in. It
94,disables support for TLSv1.1 on any sockets instantiated from the listener
94,when SSL is supported. Note that SSLv2 is forced disabled in the code and
94,cannot be enabled using any configuration option. This option is also
94,"available on global statement ""ssl-default-bind-options"". Use ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"""
94,"and ""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options"" instead."
94,no-tlsv12This setting is only available when support for OpenSSL was built in. It
94,disables support for TLSv1.2 on any sockets instantiated from the listener
94,when SSL is supported. Note that SSLv2 is forced disabled in the code and
94,cannot be enabled using any configuration option. This option is also
94,"available on global statement ""ssl-default-bind-options"". Use ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"""
94,"and ""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options"" instead."
94,no-tlsv13This setting is only available when support for OpenSSL was built in. It
94,disables support for TLSv1.3 on any sockets instantiated from the listener
94,when SSL is supported. Note that SSLv2 is forced disabled in the code and
94,cannot be enabled using any configuration option. This option is also
94,"available on global statement ""ssl-default-bind-options"". Use ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"""
94,"and ""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options"" instead."
94,npn <protocols>This enables the NPN TLS extension and advertises the specified protocol list
94,as supported on top of NPN. The protocol list consists in a comma-delimited
94,"list of protocol names, for instance: ""http/1.1,http/1.0"" (without quotes)."
94,This requires that the SSL library is build with support for TLS extensions
94,enabled (check with haproxy -vv). Note that the NPN extension has been
94,"replaced with the ALPN extension (see the ""alpn"" keyword), though this one is"
94,only available starting with OpenSSL 1.0.2. If HTTP/2 is desired on an older
94,"version of OpenSSL, NPN might still be used as most clients still support it"
94,at the time of writing this. It is possible to enable both NPN and ALPN
94,though it probably doesn't make any sense out of testing.
94,"prefer-client-ciphersUse the client's preference when selecting the cipher suite, by default"
94,the server's preference is enforced. This option is also available on
94,"global statement ""ssl-default-bind-options""."
94,process <process-set>[/<thread-set>]This restricts the list of processes and/or threads on which this listener is
94,allowed to run. It does not enforce any process but eliminates those which do
94,"not match. If the frontend uses a ""bind-process"" setting, the intersection"
94,between the two is applied. If in the end the listener is not allowed to run
94,"on any remaining process, a warning is emitted, and the listener will either"
94,"run on the first process of the listener if a single process was specified,"
94,or on all of its processes if multiple processes were specified. If a thread
94,"set is specified, it limits the threads allowed to process incoming"
94,"connections for this listener, for the corresponding process set. For the"
94,"unlikely case where several ranges are needed, this directive may be"
94,repeated. <process-set> and <thread-set> must use the format
94,all | odd | even | number[-[number]]
94,Ranges can be partially defined. The higher bound can be omitted. In such
94,"case, it is replaced by the corresponding maximum value. The main purpose of"
94,this directive is to be used with the stats sockets and have one different
94,socket per process. The second purpose is to have multiple bind lines sharing
94,"the same IP:port but not the same process in a listener, so that the system"
94,can distribute the incoming connections into multiple queues and allow a
94,smoother inter-process load balancing. Currently Linux 3.9 and above is known
94,"for supporting this. See also ""bind-process"" and ""nbprocThis keyword is available in sections :Process management and securityFetching samples from internal states""."
94,sslThis setting is only available when support for OpenSSL was built in. It
94,enables SSL deciphering on connections instantiated from this listener. A
94,"certificate is necessary (see ""crtThis keyword is available in sections :Bind optionsServer and default-server options"" above). All contents in the buffers will"
94,"appear in clear text, so that ACLs and HTTP processing will only have access"
94,"to deciphered contents. SSLv3 is disabled per default, use ""ssl-min-ver SSLv3"""
94,to enable it.
94,ssl-max-ver [ SSLv3 | TLSv1.0 | TLSv1.1 | TLSv1.2 | TLSv1.3 ]This option enforces use of <version> or lower on SSL connections instantiated
94,from this listener. This option is also available on global statement
94,"""ssl-default-bind-options"". See also ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options""."
94,ssl-min-ver [ SSLv3 | TLSv1.0 | TLSv1.1 | TLSv1.2 | TLSv1.3 ]This option enforces use of <version> or upper on SSL connections instantiated
94,from this listener. This option is also available on global statement
94,"""ssl-default-bind-options"". See also ""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options""."
94,strict-sniThis setting is only available when support for OpenSSL was built in. The
94,SSL/TLS negotiation is allow only if the client provided an SNI which match
94,a certificate. The default certificate is not used.
94,"See the ""crtThis keyword is available in sections :Bind optionsServer and default-server options"" option for more information."
94,tcp-ut <delay>Sets the TCP User Timeout for all incoming connections instantiated from this
94,listening socket. This option is available on Linux since version 2.6.37. It
94,allows haproxy to configure a timeout for sockets which contain data not
94,receiving an acknowledgment for the configured delay. This is especially
94,useful on long-lived connections experiencing long idle periods such as
94,"remote terminals or database connection pools, where the client and server"
94,"timeouts must remain high to allow a long period of idle, but where it is"
94,important to detect that the client has disappeared in order to release all
94,resources associated with its connection (and the server's session). The
94,argument is a delay expressed in milliseconds by default. This only works
94,"for regular TCP connections, and is ignored for other protocols."
94,tfoIs an optional keyword which is supported only on Linux kernels >= 3.7. It
94,"enables TCP Fast Open on the listening socket, which means that clients which"
94,support this feature will be able to send a request and receive a response
94,"during the 3-way handshake starting from second connection, thus saving one"
94,round-trip after the first connection. This only makes sense with protocols
94,that use high connection rates and where each round trip matters. This can
94,possibly cause issues with many firewalls which do not accept data on SYN
94,"packets, so this option should only be enabled once well tested. This option"
94,is only supported on TCPv4/TCPv6 sockets and ignored by other ones. You may
94,need to build HAProxy with USE_TFO=1 if your libc doesn't define
94,TCP_FASTOPEN.
94,tls-ticket-keys <keyfile>Sets the TLS ticket keys file to load the keys from. The keys need to be 48
94,"bytes long, encoded with base64 (ex. openssl rand -base64 48). Number of keys"
94,is specified by the TLS_TICKETS_NO build option (default 3) and at least as
94,many keys need to be present in the file. Last TLS_TICKETS_NO keys will be
94,used for decryption and the penultimate one for encryption. This enables easy
94,key rotation by just appending new key to the file and reloading the process.
94,Keys must be periodically rotated (ex. every 12h) or Perfect Forward Secrecy
94,is compromised. It is also a good idea to keep the keys off any permanent
94,storage such as hard drives (hint: use tmpfs and don't swap those files).
94,Lifetime hint can be changed using tune.ssl.timeout.
94,transparentIs an optional keyword which is supported only on certain Linux kernels. It
94,indicates that the addresses will be bound even if they do not belong to the
94,"local machine, and that packets targeting any of these addresses will be"
94,intercepted just as if the addresses were locally configured. This normally
94,requires that IP forwarding is enabled. Caution! do not use this with the
94,"default address '*', as it would redirect any traffic for the specified port."
94,This keyword is available only when HAProxy is built with USE_LINUX_TPROXY=1.
94,"This parameter is only compatible with TCPv4 and TCPv6 sockets, depending on"
94,"kernel version. Some distribution kernels include backports of the feature,"
94,so check for support with your vendor.
94,v4v6Is an optional keyword which is supported only on most recent systems
94,including Linux kernels >= 2.4.21. It is used to bind a socket to both IPv4
94,and IPv6 when it uses the default address. Doing so is sometimes necessary
94,on systems which bind to IPv6 only by default. It has no effect on non-IPv6
94,"sockets, and is overridden by the ""v6only"" option."
94,v6onlyIs an optional keyword which is supported only on most recent systems
94,including Linux kernels >= 2.4.21. It is used to bind a socket to IPv6 only
94,when it uses the default address. Doing so is sometimes preferred to doing it
94,system-wide as it is per-listener. It has no effect on non-IPv6 sockets and
94,"has precedence over the ""v4v6"" option."
94,uid <uid>Sets the owner of the UNIX sockets to the designated system uid. It can also
94,"be set by default in the global section's ""unix-bind"" statement. Note that"
94,"some platforms simply ignore this. This setting is equivalent to the ""userThis keyword is available in sections :Process management and securityUserlistsBind options"""
94,setting except that the user numeric ID is used instead of its name. This
94,setting is ignored by non UNIX sockets.
94,user <user>Sets the owner of the UNIX sockets to the designated system user. It can also
94,"be set by default in the global section's ""unix-bind"" statement. Note that"
94,"some platforms simply ignore this. This setting is equivalent to the ""uidThis keyword is available in sections :Process management and securityBind options"""
94,setting except that the user name is used instead of its uid. This setting is
94,ignored by non UNIX sockets.
94,verify [none|optional|required]This setting is only available when support for OpenSSL was built in. If set
94,"to 'none', client certificate is not requested. This is the default. In other"
94,"cases, a client certificate is requested. If the client does not provide a"
94,"certificate after the request and if 'verify' is set to 'required', then the"
94,"handshake is aborted, while it would have succeeded if set to 'optional'. The"
94,certificate provided by the client is always verified using CAs from
94,'ca-file' and optional CRLs from 'crl-file'. On verify failure the handshake
94,"is aborted, regardless of the 'verify' option, unless the error code exactly"
94,matches one of those listed with 'ca-ignore-err' or 'crt-ignore-err'.
94,5.2. Server and default-server options
94,"The ""server"" and ""default-server"" keywords support a certain number of settings"
94,which are all passed as arguments on the server line. The order in which those
94,"arguments appear does not count, and they are all optional. Some of those"
94,settings are single words (booleans) while others expect one or several values
94,"after them. In this case, the values must immediately follow the setting name."
94,"Except default-server, all those settings must be specified after the server's"
94,address if they are used:
94,server <name> <address>[:port] [settings ...]
94,default-server [settings ...]
94,"Note that all these settings are supported both by ""server"" and ""default-server"""
94,"keywords, except ""idThis keyword is available in sections :Alphabetically sorted keywords referenceBind optionsServer and default-server options"" which is only supported by ""server""."
94,The currently supported settings are the following ones.
94,"addr <ipv4|ipv6>Using the ""addr"" parameter, it becomes possible to use a different IP address"
94,"to send health-checks or to probe the agent-check. On some servers, it may be"
94,desirable to dedicate an IP address to specific component able to perform
94,complex tests which are more suitable to health-checks than the application.
94,"This parameter is ignored if the ""check"" parameter is not set. See also the"
94,"""port"" parameter."
94,agent-checkEnable an auxiliary agent check which is run independently of a regular
94,health check. An agent health check is performed by making a TCP connection
94,"to the port set by the ""agent-port"" parameter and reading an ASCII string."
94,"The string is made of a series of words delimited by spaces, tabs or commas"
94,"in any order, optionally terminated by '\r' and/or '\n', each consisting of :"
94,"- An ASCII representation of a positive integer percentage, e.g. ""75%""."
94,Values in this format will set the weight proportional to the initial
94,weight of a server as configured when haproxy starts. Note that a zero
94,"weight is reported on the stats page as ""DRAIN"" since it has the same"
94,effect on the server (it's removed from the LB farm).
94,"- The string ""maxconn:"" followed by an integer (no space between). Values"
94,in this format will set the maxconn of a server. The maximum number of
94,connections advertised needs to be multiplied by the number of load
94,balancers and different backends that use this health check to get the
94,total number of connections the server might receive. Example: maxconn:30
94,"- The word ""ready"". This will turn the server's administrative state to the"
94,"READY mode, thus canceling any DRAIN or MAINT state"
94,"- The word ""drain"". This will turn the server's administrative state to the"
94,"DRAIN mode, thus it will not accept any new connections other than those"
94,that are accepted via persistence.
94,"- The word ""maint"". This will turn the server's administrative state to the"
94,"MAINT mode, thus it will not accept any new connections at all, and health"
94,checks will be stopped.
94,"- The words ""down"", ""fail"", or ""stopped"", optionally followed by a"
94,description string after a sharp ('#'). All of these mark the server's
94,"operating state as DOWN, but since the word itself is reported on the stats"
94,"page, the difference allows an administrator to know if the situation was"
94,"expected or not : the service may intentionally be stopped, may appear up"
94,"but fail some validity tests, or may be seen as down (e.g. missing process,"
94,or port not responding).
94,"- The word ""up"" sets back the server's operating state as UP if health checks"
94,also report that the service is accessible.
94,Parameters which are not advertised by the agent are not changed. For
94,"example, an agent might be designed to monitor CPU usage and only report a"
94,"relative weight and never interact with the operating status. Similarly, an"
94,agent could be designed as an end-user interface with 3 radio buttons
94,"allowing an administrator to change only the administrative state. However,"
94,"it is important to consider that only the agent may revert its own actions,"
94,"so if a server is set to DRAIN mode or to DOWN state using the agent, the"
94,agent must implement the other equivalent actions to bring the service into
94,operations again.
94,Failure to connect to the agent is not considered an error as connectivity
94,"is tested by the regular health check which is enabled by the ""check"""
94,"parameter. Warning though, it is not a good idea to stop an agent after it"
94,"reports ""down"", since only an agent reporting ""up"" will be able to turn the"
94,server up again. Note that the CLI on the Unix stats socket is also able to
94,force an agent's result in order to work around a bogus agent if needed.
94,"Requires the ""agent-port"" parameter to be set. See also the ""agent-inter"""
94,"and ""no-agent-check"" parameters."
94,"agent-send <string>If this option is specified, haproxy will send the given string (verbatim)"
94,"to the agent server upon connection. You could, for example, encode"
94,"the backend name into this string, which would enable your agent to send"
94,different responses based on the backend. Make sure to include a '\n' if
94,you want to terminate your request with a newline.
94,"agent-inter <delay>The ""agent-inter"" parameter sets the interval between two agent checks"
94,"to <delay> milliseconds. If left unspecified, the delay defaults to 2000 ms."
94,"Just as with every other time-based parameter, it may be entered in any"
94,"other explicit unit among { us, ms, s, m, h, d }. The ""agent-inter"""
94,"parameter also serves as a timeout for agent checks ""timeout check"" is"
94,"not set. In order to reduce ""resonance"" effects when multiple servers are"
94,"hosted on the same hardware, the agent and health checks of all servers"
94,are started with a small time offset between them. It is also possible to
94,add some random noise in the agent and health checks interval using the
94,"global ""spread-checks"" keyword. This makes sense for instance when a lot"
94,of backends use the same servers.
94,"See also the ""agent-check"" and ""agent-port"" parameters."
94,"agent-addr <addr>The ""agent-addr"" parameter sets address for agent check."
94,"You can offload agent-check to another target, so you can make single place"
94,managing status and weights of servers defined in haproxy in case you can't
94,make self-aware and self-managing services. You can specify both IP or
94,"hostname, it will be resolved."
94,"agent-port <port>The ""agent-port"" parameter sets the TCP port used for agent checks."
94,"See also the ""agent-check"" and ""agent-inter"" parameters."
94,allow-0rttAllow sending early data to the server when using TLS 1.3.
94,Note that early data will be sent only if the client used early data.
94,"backupWhen ""backup"" is present on a server line, the server is only used in load"
94,balancing when all other non-backup servers are unavailable. Requests coming
94,with a persistence cookie referencing the server will always be served
94,"though. By default, only the first operational backup server is used, unless"
94,"the ""allbackups"" option is set in the backend. See also the ""no-backup"" and"
94,"""allbackups"" options."
94,ca-file <cafile>This setting is only available when support for OpenSSL was built in. It
94,designates a PEM file from which to load CA certificates used to verify
94,server's certificate.
94,"checkThis option enables health checks on the server. By default, a server is"
94,"always considered available. If ""check"" is set, the server is available when"
94,"accepting periodic TCP connections, to ensure that it is really able to serve"
94,requests. The default address and port to send the tests to are those of the
94,"server, and the default source is the same as the one defined in the"
94,"backend. It is possible to change the address using the ""addr"" parameter, the"
94,"port using the ""port"" parameter, the source address using the ""sourceThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"""
94,"address, and the interval and timers using the ""inter"", ""rise"" and ""fall"""
94,"parameters. The request method is define in the backend using the ""httpchk"","
94,"""smtpchk"", ""mysql-check"", ""pgsql-check"" and ""ssl-hello-chk"" options. Please"
94,refer to those options and parameters for more information. See also
94,"""no-check"" option."
94,check-send-proxyThis option forces emission of a PROXY protocol line with outgoing health
94,"checks, regardless of whether the server uses send-proxy or not for the"
94,"normal traffic. By default, the PROXY protocol is enabled for health checks"
94,"if it is already enabled for normal traffic and if no ""port"" nor ""addr"""
94,"directive is present. However, if such a directive is present, the"
94,"""check-send-proxy"" option needs to be used to force the use of the"
94,"protocol. See also the ""send-proxy"" option for more information."
94,check-sni <sni>This option allows you to specify the SNI to be used when doing health checks
94,over SSL. It is only possible to use a string to set <sni>. If you want to
94,"set a SNI for proxied traffic, see ""sni""."
94,"check-sslThis option forces encryption of all health checks over SSL, regardless of"
94,whether the server uses SSL or not for the normal traffic. This is generally
94,"used when an explicit ""port"" or ""addr"" directive is specified and SSL health"
94,checks are not inherited. It is important to understand that this option
94,"inserts an SSL transport layer below the checks, so that a simple TCP connect"
94,"check becomes an SSL connect, which replaces the old ssl-hello-chk. The most"
94,"common use is to send HTTPS checks by combining ""httpchk"" with SSL checks."
94,All SSL settings are common to health checks and traffic (e.g. ciphers).
94,"See the ""sslThis keyword is available in sections :Bind optionsServer and default-server options"" option for more information and ""no-check-ssl"" to disable"
94,this option.
94,ciphers <ciphers>This setting is only available when support for OpenSSL was built in. This
94,option sets the string describing the list of cipher algorithms that is
94,negotiated during the SSL/TLS handshake with the server. The format of the
94,"string is defined in ""man 1 ciphers"" from OpenSSL man pages. For background"
94,information and recommendations see e.g.
94,(https://wiki.mozilla.org/Security/Server_Side_TLS) and
94,(https://mozilla.github.io/server-side-tls/ssl-config-generator/). For TLSv1.3
94,"cipher configuration, please check the ""ciphersuitesThis keyword is available in sections :Bind optionsServer and default-server options"" keyword."
94,ciphersuites <ciphersuites>This setting is only available when support for OpenSSL was built in and
94,OpenSSL 1.1.1 or later was used to build HAProxy. This option sets the string
94,describing the list of cipher algorithms that is negotiated during the TLS
94,1.3 handshake with the server. The format of the string is defined in
94,"""man 1 ciphers"" from OpenSSL man pages under the ""ciphersuitesThis keyword is available in sections :Bind optionsServer and default-server options"" section."
94,"For cipher configuration for TLSv1.2 and earlier, please check the ""ciphersThis keyword is available in sections :Bind optionsServer and default-server options"""
94,keyword.
94,"cookie <value>The ""cookieThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server optionsFetching HTTP samples (Layer 7)"" parameter sets the cookie value assigned to the server to"
94,"<value>. This value will be checked in incoming requests, and the first"
94,"operational server possessing the same value will be selected. In return, in"
94,"cookie insertion or rewrite modes, this value will be assigned to the cookie"
94,sent to the client. There is nothing wrong in having several servers sharing
94,"the same cookie value, and it is in fact somewhat common between normal and"
94,"backup servers. See also the ""cookieThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server optionsFetching HTTP samples (Layer 7)"" keyword in backend section."
94,crl-file <crlfile>This setting is only available when support for OpenSSL was built in. It
94,designates a PEM file from which to load certificate revocation list used
94,to verify server's certificate.
94,crt <cert>This setting is only available when support for OpenSSL was built in.
94,It designates a PEM file from which to load both a certificate and the
94,associated private key. This file can be built by concatenating both PEM
94,files into one. This certificate will be sent if the server send a client
94,certificate request.
94,"disabledThe ""disabledThis keyword is available in sections :PeersAlphabetically sorted keywords referenceServer and default-server options"" keyword starts the server in the ""disabledThis keyword is available in sections :PeersAlphabetically sorted keywords referenceServer and default-server options"" state. That means"
94,"that it is marked down in maintenance mode, and no connection other than the"
94,ones allowed by persist mode will reach it. It is very well suited to setup
94,"new servers, because normal traffic will never reach them, while it is still"
94,possible to test the service by making use of the force-persist mechanism.
94,"See also ""enabledThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"" setting."
94,enabledThis option may be used as 'server' setting to reset any 'disabled'
94,setting which would have been inherited from 'default-server' directive as
94,default value.
94,It may also be used as 'default-server' setting to reset any previous
94,'default-server' 'disabled' setting.
94,"error-limit <count>If health observing is enabled, the ""error-limit"" parameter specifies the"
94,"number of consecutive errors that triggers event selected by the ""on-error"""
94,option. By default it is set to 10 consecutive errors.
94,"See also the ""check"", ""error-limit"" and ""on-error""."
94,"fall <count>The ""fall"" parameter states that a server will be considered as dead after"
94,<count> consecutive unsuccessful health checks. This value defaults to 3 if
94,"unspecified. See also the ""check"", ""inter"" and ""rise"" parameters."
94,force-sslv3This option enforces use of SSLv3 only when SSL is used to communicate with
94,the server. SSLv3 is generally less expensive than the TLS counterparts for
94,high connection rates. This option is also available on global statement
94,"""ssl-default-server-options"". See also ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"" and ssl-max-ver""."
94,force-tlsv10This option enforces use of TLSv1.0 only when SSL is used to communicate with
94,the server. This option is also available on global statement
94,"""ssl-default-server-options"". See also ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"" and ssl-max-ver""."
94,force-tlsv11This option enforces use of TLSv1.1 only when SSL is used to communicate with
94,the server. This option is also available on global statement
94,"""ssl-default-server-options"". See also ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"" and ssl-max-ver""."
94,force-tlsv12This option enforces use of TLSv1.2 only when SSL is used to communicate with
94,the server. This option is also available on global statement
94,"""ssl-default-server-options"". See also ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"" and ssl-max-ver""."
94,force-tlsv13This option enforces use of TLSv1.3 only when SSL is used to communicate with
94,the server. This option is also available on global statement
94,"""ssl-default-server-options"". See also ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"" and ssl-max-ver""."
94,id <value>Set a persistent ID for the server. This ID must be positive and unique for
94,the proxy. An unused ID will automatically be assigned if unset. The first
94,assigned value will be 1. This ID is currently only returned in statistics.
94,"init-addr {last | libc | none | <ip>},[...]*Indicate in what order the server's address should be resolved upon startup"
94,if it uses an FQDN. Attempts are made to resolve the address by applying in
94,turn each of the methods mentioned in the comma-delimited list. The first
94,method which succeeds is used. If the end of the list is reached without
94,"finding a working method, an error is thrown. Method ""last"" suggests to pick"
94,"the address which appears in the state file (see ""server-state-file""). Method"
94,"""libc"" uses the libc's internal resolver (gethostbyname() or getaddrinfo()"
94,"depending on the operating system and build options). Method ""none"""
94,specifically indicates that the server should start without any valid IP
94,address in a down state. It can be useful to ignore some DNS issues upon
94,"startup, waiting for the situation to get fixed later. Finally, an IP address"
94,(IPv4 or IPv6) may be provided. It can be the currently known address of the
94,"server (e.g. filled by a configuration generator), or the address of a dummy"
94,server used to catch old sessions and present them with a decent error
94,"message for example. When the ""first"" load balancing algorithm is used, this"
94,IP address could point to a fake server used to trigger the creation of new
94,"instances on the fly. This option defaults to ""last,libc"" indicating that the"
94,"previous address found in the state file (if any) is used first, otherwise"
94,the libc's resolver is used. This ensures continued compatibility with the
94,historic behavior.
94,Example:
94,defaults
94,# never fail on address resolution
94,"default-server init-addr last,libc,none"
94,"inter <delay>fastinter <delay>downinter <delay>The ""inter"" parameter sets the interval between two consecutive health checks"
94,"to <delay> milliseconds. If left unspecified, the delay defaults to 2000 ms."
94,"It is also possible to use ""fastinter"" and ""downinter"" to optimize delays"
94,between checks depending on the server state :
94,Server stateInterval used
94,"UP 100% (non-transitional)""inter"""
94,"Transitionally UP (going down ""fall""),Transitionally DOWN (going up ""rise""),or yet unchecked.""fastinter"" if set,""inter"" otherwise."
94,"DOWN 100% (non-transitional)""downinter"" if set,""inter"" otherwise."
94,"Just as with every other time-based parameter, they can be entered in any"
94,"other explicit unit among { us, ms, s, m, h, d }. The ""inter"" parameter also"
94,"serves as a timeout for health checks sent to servers if ""timeout check"" is"
94,"not set. In order to reduce ""resonance"" effects when multiple servers are"
94,"hosted on the same hardware, the agent and health checks of all servers"
94,are started with a small time offset between them. It is also possible to
94,add some random noise in the agent and health checks interval using the
94,"global ""spread-checks"" keyword. This makes sense for instance when a lot"
94,of backends use the same servers.
94,"maxconn <maxconn>The ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"" parameter specifies the maximal number of concurrent"
94,connections that will be sent to this server. If the number of incoming
94,"concurrent requests goes higher than this value, they will be queued, waiting"
94,for a connection to be released. This parameter is very important as it can
94,"save fragile servers from going down under extreme loads. If a ""minconn"""
94,"parameter is specified, the limit becomes dynamic. The default value is ""0"""
94,"which means unlimited. See also the ""minconn"" and ""maxqueue"" parameters, and"
94,"the backend's ""fullconn"" keyword."
94,"maxqueue <maxqueue>The ""maxqueue"" parameter specifies the maximal number of connections which"
94,"will wait in the queue for this server. If this limit is reached, next"
94,requests will be redispatched to other servers instead of indefinitely
94,waiting to be served. This will break persistence but may allow people to
94,quickly re-log in when the server they try to connect to is dying. The
94,"default value is ""0"" which means the queue is unlimited. See also the"
94,"""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"" and ""minconn"" parameters."
94,"minconn <minconn>When the ""minconn"" parameter is set, the maxconn limit becomes a dynamic"
94,limit following the backend's load. The server will always accept at least
94,"<minconn> connections, never more than <maxconn>, and the limit will be on"
94,the ramp between both values when the backend has less than <fullconn>
94,concurrent connections. This makes it possible to limit the load on the
94,"server during normal loads, but push it further for important loads without"
94,"overloading the server during exceptional loads. See also the ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"""
94,"and ""maxqueue"" parameters, as well as the ""fullconn"" backend keyword."
94,"namespace <name>On Linux, it is possible to specify which network namespace a socket will"
94,belong to. This directive makes it possible to explicitly bind a server to
94,a namespace different from the default one. Please refer to your operating
94,system's documentation to find more details about network namespaces.
94,"no-agent-checkThis option may be used as ""server"" setting to reset any ""agent-check"""
94,"setting which would have been inherited from ""default-server"" directive as"
94,default value.
94,"It may also be used as ""default-server"" setting to reset any previous"
94,"""default-server"" ""agent-check"" setting."
94,"no-backupThis option may be used as ""server"" setting to reset any ""backup"""
94,"setting which would have been inherited from ""default-server"" directive as"
94,default value.
94,"It may also be used as ""default-server"" setting to reset any previous"
94,"""default-server"" ""backup"" setting."
94,"no-checkThis option may be used as ""server"" setting to reset any ""check"""
94,"setting which would have been inherited from ""default-server"" directive as"
94,default value.
94,"It may also be used as ""default-server"" setting to reset any previous"
94,"""default-server"" ""check"" setting."
94,"no-check-sslThis option may be used as ""server"" setting to reset any ""check-ssl"""
94,"setting which would have been inherited from ""default-server"" directive as"
94,default value.
94,"It may also be used as ""default-server"" setting to reset any previous"
94,"""default-server"" ""check-ssl"" setting."
94,"no-send-proxyThis option may be used as ""server"" setting to reset any ""send-proxy"""
94,"setting which would have been inherited from ""default-server"" directive as"
94,default value.
94,"It may also be used as ""default-server"" setting to reset any previous"
94,"""default-server"" ""send-proxy"" setting."
94,"no-send-proxy-v2This option may be used as ""server"" setting to reset any ""send-proxy-v2"""
94,"setting which would have been inherited from ""default-server"" directive as"
94,default value.
94,"It may also be used as ""default-server"" setting to reset any previous"
94,"""default-server"" ""send-proxy-v2"" setting."
94,"no-send-proxy-v2-sslThis option may be used as ""server"" setting to reset any ""send-proxy-v2-ssl"""
94,"setting which would have been inherited from ""default-server"" directive as"
94,default value.
94,"It may also be used as ""default-server"" setting to reset any previous"
94,"""default-server"" ""send-proxy-v2-ssl"" setting."
94,"no-send-proxy-v2-ssl-cnThis option may be used as ""server"" setting to reset any ""send-proxy-v2-ssl-cn"""
94,"setting which would have been inherited from ""default-server"" directive as"
94,default value.
94,"It may also be used as ""default-server"" setting to reset any previous"
94,"""default-server"" ""send-proxy-v2-ssl-cn"" setting."
94,"no-sslThis option may be used as ""server"" setting to reset any ""sslThis keyword is available in sections :Bind optionsServer and default-server options"""
94,"setting which would have been inherited from ""default-server"" directive as"
94,default value.
94,"It may also be used as ""default-server"" setting to reset any previous"
94,"""default-server"" ""sslThis keyword is available in sections :Bind optionsServer and default-server options"" setting."
94,no-ssl-reuseThis option disables SSL session reuse when SSL is used to communicate with
94,the server. It will force the server to perform a full handshake for every
94,"new connection. It's probably only useful for benchmarking, troubleshooting,"
94,and for paranoid users.
94,no-sslv3This option disables support for SSLv3 when SSL is used to communicate with
94,the server. Note that SSLv2 is disabled in the code and cannot be enabled
94,"using any configuration option. Use ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"" and ""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options"" instead."
94,Supported in default-server: No
94,no-tls-ticketsThis setting is only available when support for OpenSSL was built in. It
94,disables the stateless session resumption (RFC 5077 TLS Ticket
94,extension) and force to use stateful session resumption. Stateless
94,session resumption is more expensive in CPU usage for servers. This option
94,"is also available on global statement ""ssl-default-server-options""."
94,The TLS ticket mechanism is only used up to TLS 1.2.
94,"Forward Secrecy is compromised with TLS tickets, unless ticket keys"
94,"are periodically rotated (via reload or by using ""tls-ticket-keys"")."
94,"See also ""tls-tickets""."
94,no-tlsv10This option disables support for TLSv1.0 when SSL is used to communicate with
94,the server. Note that SSLv2 is disabled in the code and cannot be enabled
94,using any configuration option. TLSv1 is more expensive than SSLv3 so it
94,often makes sense to disable it when communicating with local servers. This
94,"option is also available on global statement ""ssl-default-server-options""."
94,"Use ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"" and ""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options"" instead."
94,Supported in default-server: No
94,no-tlsv11This option disables support for TLSv1.1 when SSL is used to communicate with
94,the server. Note that SSLv2 is disabled in the code and cannot be enabled
94,using any configuration option. TLSv1 is more expensive than SSLv3 so it
94,often makes sense to disable it when communicating with local servers. This
94,"option is also available on global statement ""ssl-default-server-options""."
94,"Use ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"" and ""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options"" instead."
94,Supported in default-server: No
94,no-tlsv12This option disables support for TLSv1.2 when SSL is used to communicate with
94,the server. Note that SSLv2 is disabled in the code and cannot be enabled
94,using any configuration option. TLSv1 is more expensive than SSLv3 so it
94,often makes sense to disable it when communicating with local servers. This
94,"option is also available on global statement ""ssl-default-server-options""."
94,"Use ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"" and ""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options"" instead."
94,Supported in default-server: No
94,no-tlsv13This option disables support for TLSv1.3 when SSL is used to communicate with
94,the server. Note that SSLv2 is disabled in the code and cannot be enabled
94,using any configuration option. TLSv1 is more expensive than SSLv3 so it
94,often makes sense to disable it when communicating with local servers. This
94,"option is also available on global statement ""ssl-default-server-options""."
94,"Use ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options"" and ""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options"" instead."
94,Supported in default-server: No
94,"no-verifyhostThis option may be used as ""server"" setting to reset any ""verifyhost"""
94,"setting which would have been inherited from ""default-server"" directive as"
94,default value.
94,"It may also be used as ""default-server"" setting to reset any previous"
94,"""default-server"" ""verifyhost"" setting."
94,non-stickNever add connections allocated to this sever to a stick-table.
94,This may be used in conjunction with backup to ensure that
94,stick-table persistence is disabled for backup servers.
94,observe <mode>This option enables health adjusting based on observing communication with
94,the server. By default this functionality is disabled and enabling it also
94,"requires to enable health checks. There are two supported modes: ""layer4"" and"
94,"""layer7"". In layer4 mode, only successful/unsuccessful tcp connections are"
94,"significant. In layer7, which is only allowed for http proxies, responses"
94,"received from server are verified, like valid/wrong http code, unparsable"
94,"headers, a timeout, etc. Valid status codes include 100 to 499, 501 and 505."
94,"See also the ""check"", ""on-error"" and ""error-limit""."
94,on-error <mode>Select what should happen when enough consecutive errors are detected.
94,"Currently, four modes are available:"
94,- fastinter: force fastinter
94,"- fail-check: simulate a failed check, also forces fastinter (default)"
94,"- sudden-death: simulate a pre-fatal failed health check, one more failed"
94,"check will mark a server down, forces fastinter"
94,- mark-down: mark the server immediately down and force fastinter
94,"See also the ""check"", ""observe"" and ""error-limit""."
94,on-marked-down <action>Modify what occurs when a server is marked down.
94,Currently one action is available:
94,"- shutdown-sessions: Shutdown peer sessions. When this setting is enabled,"
94,all connections to the server are immediately terminated when the server
94,goes down. It might be used if the health check detects more complex cases
94,"than a simple connection status, and long timeouts would cause the service"
94,"to remain unresponsive for too long a time. For instance, a health check"
94,might detect that a database is stuck and that there's no chance to reuse
94,existing connections anymore. Connections killed this way are logged with
94,"a 'D' termination code (for ""Down"")."
94,Actions are disabled by default
94,on-marked-up <action>Modify what occurs when a server is marked up.
94,Currently one action is available:
94,- shutdown-backup-sessions: Shutdown sessions on all backup servers. This is
94,done only if the server is not in backup state and if it is not disabled
94,(it must have an effective weight > 0). This can be used sometimes to force
94,an active server to take all the traffic back after recovery when dealing
94,"with long sessions (e.g. LDAP, SQL, ...). Doing this can cause more trouble"
94,"than it tries to solve (e.g. incomplete transactions), so use this feature"
94,with extreme care. Sessions killed because a server comes up are logged
94,"with an 'U' termination code (for ""Up"")."
94,Actions are disabled by default
94,"port <port>Using the ""port"" parameter, it becomes possible to use a different port to"
94,"send health-checks. On some servers, it may be desirable to dedicate a port"
94,to a specific component able to perform complex tests which are more suitable
94,to health-checks than the application. It is common to run a simple script in
94,"inetd for instance. This parameter is ignored if the ""check"" parameter is not"
94,"set. See also the ""addr"" parameter."
94,"redir <prefix>The ""redir"" parameter enables the redirection mode for all GET and HEAD"
94,requests addressing this server. This means that instead of having HAProxy
94,"forward the request to the server, it will send an ""HTTP 302"" response with"
94,"the ""Location"" header composed of this prefix immediately followed by the"
94,requested URI beginning at the leading '/' of the path component. That means
94,that no trailing slash should be used after <prefix>. All invalid requests
94,"will be rejected, and all non-GET or HEAD requests will be normally served by"
94,"the server. Note that since the response is completely forged, no header"
94,"mangling nor cookie insertion is possible in the response. However, cookies in"
94,"requests are still analyzed, making this solution completely usable to direct"
94,users to a remote location in case of local disaster. Main use consists in
94,increasing bandwidth for static servers by having the clients directly
94,"connect to them. Note: never use a relative location here, it would cause a"
94,loop between the client and HAProxy!
94,Example :
94,server srv1 192.168.1.1:80 redir http://image1.mydomain.com check
94,"rise <count>The ""rise"" parameter states that a server will be considered as operational"
94,after <count> consecutive successful health checks. This value defaults to 2
94,"if unspecified. See also the ""check"", ""inter"" and ""fall"" parameters."
94,"resolve-opts <option>,<option>,...Comma separated list of options to apply to DNS resolution linked to this"
94,server.
94,Available options:
94,* allow-dup-ip
94,"By default, HAProxy prevents IP address duplication in a backend when DNS"
94,resolution at runtime is in operation.
94,"That said, for some cases, it makes sense that two servers (in the same"
94,"backend, being resolved by the same FQDN) have the same IP address."
94,"For such case, simply enable this option."
94,This is the opposite of prevent-dup-ip.
94,* prevent-dup-ip
94,Ensure HAProxy's default behavior is enforced on a server: prevent re-using
94,an IP address already set to a server in the same backend and sharing the
94,same fqdn.
94,This is the opposite of allow-dup-ip.
94,Example:
94,backend b_myapp
94,default-server init-addr none resolvers dns
94,server s1 myapp.example.com:80 check resolve-opts allow-dup-ip
94,server s2 myapp.example.com:81 check resolve-opts allow-dup-ip
94,With the option allow-dup-ip set:
94,"* if the nameserver returns a single IP address, then both servers will use"
94,"* If the nameserver returns 2 IP addresses, then each server will pick up a"
94,different address
94,Default value: not set
94,resolve-prefer <family>When DNS resolution is enabled for a server and multiple IP addresses from
94,"different families are returned, HAProxy will prefer using an IP address"
94,"from the family mentioned in the ""resolve-prefer"" parameter."
94,"Available families: ""ipv4"" and ""ipv6"""
94,Default value: ipv6
94,Example:
94,server s1 app1.domain.com:80 resolvers mydns resolve-prefer ipv6
94,"resolve-net <network>[,<network[,...]]This options prioritize th choice of an ip address matching a network. This is"
94,"useful with clouds to prefer a local ip. In some cases, a cloud high"
94,availability service can be announced with many ip addresses on many
94,"different datacenters. The latency between datacenter is not negligible, so"
94,this patch permits to prefer a local datacenter. If no address matches the
94,"configured network, another address is selected."
94,Example:
94,server s1 app1.domain.com:80 resolvers mydns resolve-net 10.0.0.0/8
94,"resolvers <id>Points to an existing ""resolversThis keyword is available in sections :Server and default-server optionsThe resolvers section"" section to resolve current server's"
94,hostname.
94,Example:
94,server s1 app1.domain.com:80 check resolvers mydns
94,See also section 5.3
94,"send-proxyThe ""send-proxy"" parameter enforces use of the PROXY protocol over any"
94,connection established to this server. The PROXY protocol informs the other
94,"end about the layer 3/4 addresses of the incoming connection, so that it can"
94,"know the client's address or the public address it accessed to, whatever the"
94,"upper layer protocol. For connections accepted by an ""accept-proxy"" or"
94,"""accept-netscaler-cip"" listener, the advertised address will be used. Only"
94,TCPv4 and TCPv6 address families are supported. Other families such as
94,"Unix sockets, will report an UNKNOWN family. Servers using this option can"
94,fully be chained to another instance of haproxy listening with an
94,"""accept-proxy"" setting. This setting must not be used if the server isn't"
94,"aware of the protocol. When health checks are sent to the server, the PROXY"
94,"protocol is automatically used when this option is set, unless there is an"
94,"explicit ""port"" or ""addr"" directive, in which case an explicit"
94,"""check-send-proxy"" directive would also be needed to use the PROXY protocol."
94,"See also the ""no-send-proxy"" option of this section and ""accept-proxy"" and"
94,"""accept-netscaler-cip"" option of the ""bind"" keyword."
94,"send-proxy-v2The ""send-proxy-v2"" parameter enforces use of the PROXY protocol version 2"
94,over any connection established to this server. The PROXY protocol informs
94,"the other end about the layer 3/4 addresses of the incoming connection, so"
94,"that it can know the client's address or the public address it accessed to,"
94,whatever the upper layer protocol. It also send ALPN information if an alpn
94,have been negotiated. This setting must not be used if the server isn't aware
94,"of this version of the protocol. See also the ""no-send-proxy-v2"" option of"
94,"this section and send-proxy"" option of the ""bind"" keyword."
94,"send-proxy-v2-sslThe ""send-proxy-v2-ssl"" parameter enforces use of the PROXY protocol version"
94,2 over any connection established to this server. The PROXY protocol informs
94,"the other end about the layer 3/4 addresses of the incoming connection, so"
94,"that it can know the client's address or the public address it accessed to,"
94,"whatever the upper layer protocol. In addition, the SSL information extension"
94,of the PROXY protocol is added to the PROXY protocol header. This setting
94,must not be used if the server isn't aware of this version of the protocol.
94,"See also the ""no-send-proxy-v2-ssl"" option of this section and the"
94,"""send-proxy-v2"" option of the ""bind"" keyword."
94,"send-proxy-v2-ssl-cnThe ""send-proxy-v2-ssl"" parameter enforces use of the PROXY protocol version"
94,2 over any connection established to this server. The PROXY protocol informs
94,"the other end about the layer 3/4 addresses of the incoming connection, so"
94,"that it can know the client's address or the public address it accessed to,"
94,"whatever the upper layer protocol. In addition, the SSL information extension"
94,"of the PROXY protocol, along along with the Common Name from the subject of"
94,"the client certificate (if any), is added to the PROXY protocol header. This"
94,setting must not be used if the server isn't aware of this version of the
94,"protocol. See also the ""no-send-proxy-v2-ssl-cn"" option of this section and"
94,"the ""send-proxy-v2"" option of the ""bind"" keyword."
94,"slowstart <start_time_in_ms>The ""slowstart"" parameter for a server accepts a value in milliseconds which"
94,indicates after how long a server which has just come back up will run at
94,"full speed. Just as with every other time-based parameter, it can be entered"
94,"in any other explicit unit among { us, ms, s, m, h, d }. The speed grows"
94,linearly from 0 to 100% during this time. The limitation applies to two
94,parameters :
94,- maxconn: the number of connections accepted by the server will grow from 1
94,"to 100% of the usual dynamic limit defined by (minconn,maxconn,fullconn)."
94,"- weight: when the backend uses a dynamic weighted algorithm, the weight"
94,"grows linearly from 1 to 100%. In this case, the weight is updated at every"
94,"health-check. For this reason, it is important that the ""inter"" parameter"
94,"is smaller than the ""slowstart"", in order to maximize the number of steps."
94,"The slowstart never applies when haproxy starts, otherwise it would cause"
94,trouble to running servers. It only applies when a server has been previously
94,seen as failed.
94,"sni <expression>The ""sni"" parameter evaluates the sample fetch expression, converts it to a"
94,string and uses the result as the host name sent in the SNI TLS extension to
94,the server. A typical use case is to send the SNI received from the client in
94,"a bridged HTTPS scenario, using the ""ssl_fc_sni"" sample fetch for the"
94,"expression, though alternatives such as req.hdr(host) can also make sense. If"
94,"""verify required"" is set (which is the recommended setting), the resulting"
94,name will also be matched against the server certificate's names. See the
94,"""verifyThis keyword is available in sections :Bind optionsServer and default-server options"" directive for more details. If you want to set a SNI for health"
94,"checks, see the ""check-sni"" directive for more details."
94,"source <addr>[:<pl>[-<ph>]] [usesrc { <addr2>[:<port2>] | client | clientip } ]source <addr>[:<port>] [usesrc { <addr2>[:<port2>] | hdr_ip(<hdr>[,<occ>]) } ]source <addr>[:<pl>[-<ph>]] [interface <name>] ...The ""sourceThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"" parameter sets the source address which will be used when"
94,connecting to the server. It follows the exact same parameters and principle
94,"as the backend ""sourceThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"" keyword, except that it only applies to the server"
94,"referencing it. Please consult the ""sourceThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"" keyword for details."
94,"Additionally, the ""sourceThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"" statement on a server line allows one to specify a"
94,source port range by indicating the lower and higher bounds delimited by a
94,dash ('-'). Some operating systems might require a valid IP address when a
94,source port range is specified. It is permitted to have the same IP/range for
94,several servers. Doing so makes it possible to bypass the maximum of 64k
94,total concurrent connections. The limit will then reach 64k connections per
94,server.
94,Since Linux 4.2/libc 2.23 IP_BIND_ADDRESS_NO_PORT is set for connections
94,specifying the source address without port(s).
94,sslThis option enables SSL ciphering on outgoing connections to the server. It
94,"is critical to verify server certificates using ""verifyThis keyword is available in sections :Bind optionsServer and default-server options"" when using SSL to"
94,"connect to servers, otherwise the communication is prone to trivial man in"
94,"the-middle attacks rendering SSL useless. When this option is used, health"
94,"checks are automatically sent in SSL too unless there is a ""port"" or an"
94,"""addr"" directive indicating the check should be sent to a different location."
94,"See the ""no-ssl"" to disable ""sslThis keyword is available in sections :Bind optionsServer and default-server options"" option and ""check-ssl"" option to force"
94,SSL health checks.
94,ssl-max-ver [ SSLv3 | TLSv1.0 | TLSv1.1 | TLSv1.2 | TLSv1.3 ]This option enforces use of <version> or lower when SSL is used to communicate
94,with the server. This option is also available on global statement
94,"""ssl-default-server-options"". See also ""ssl-min-verThis keyword is available in sections :Bind optionsServer and default-server options""."
94,ssl-min-ver [ SSLv3 | TLSv1.0 | TLSv1.1 | TLSv1.2 | TLSv1.3 ]This option enforces use of <version> or upper when SSL is used to communicate
94,with the server. This option is also available on global statement
94,"""ssl-default-server-options"". See also ""ssl-max-verThis keyword is available in sections :Bind optionsServer and default-server options""."
94,"ssl-reuseThis option may be used as ""server"" setting to reset any ""no-ssl-reuse"""
94,"setting which would have been inherited from ""default-server"" directive as"
94,default value.
94,"It may also be used as ""default-server"" setting to reset any previous"
94,"""default-server"" ""no-ssl-reuse"" setting."
94,"stickThis option may be used as ""server"" setting to reset any ""non-stick"""
94,"setting which would have been inherited from ""default-server"" directive as"
94,default value.
94,"It may also be used as ""default-server"" setting to reset any previous"
94,"""default-server"" ""non-stick"" setting."
94,tcp-ut <delay>Sets the TCP User Timeout for all outgoing connections to this server. This
94,option is available on Linux since version 2.6.37. It allows haproxy to
94,configure a timeout for sockets which contain data not receiving an
94,acknowledgment for the configured delay. This is especially useful on
94,long-lived connections experiencing long idle periods such as remote
94,"terminals or database connection pools, where the client and server timeouts"
94,"must remain high to allow a long period of idle, but where it is important to"
94,detect that the server has disappeared in order to release all resources
94,associated with its connection (and the client's session). One typical use
94,case is also to force dead server connections to die when health checks are
94,too slow or during a soft reload since health checks are then disabled. The
94,argument is a delay expressed in milliseconds by default. This only works for
94,"regular TCP connections, and is ignored for other protocols."
94,track [<proxy>/]<server>This option enables ability to set the current state of the server by tracking
94,another one. It is possible to track a server which itself tracks another
94,"server, provided that at the end of the chain, a server has health checks"
94,enabled. If <proxy> is omitted the current one is used. If disable-on-404 is
94,"used, it has to be enabled on both proxies."
94,"tls-ticketsThis option may be used as ""server"" setting to reset any ""no-tls-ticketsThis keyword is available in sections :Bind optionsServer and default-server options"""
94,"setting which would have been inherited from ""default-server"" directive as"
94,default value.
94,The TLS ticket mechanism is only used up to TLS 1.2.
94,"Forward Secrecy is compromised with TLS tickets, unless ticket keys"
94,"are periodically rotated (via reload or by using ""tls-ticket-keys"")."
94,"It may also be used as ""default-server"" setting to reset any previous"
94,"""default-server"" ""no-tls-ticketsThis keyword is available in sections :Bind optionsServer and default-server options"" setting."
94,verify [none|required]This setting is only available when support for OpenSSL was built in. If set
94,"to 'none', server certificate is not verified. In the other case, The"
94,certificate provided by the server is verified using CAs from 'ca-file' and
94,optional CRLs from 'crl-file' after having checked that the names provided in
94,the certificate's subject and subjectAlternateNames attributes match either
94,"the name passed using the ""sni"" directive, or if not provided, the static"
94,"host name passed using the ""verifyhost"" directive. When no name is found, the"
94,"certificate's names are ignored. For this reason, without SNI it's important"
94,"to use ""verifyhost"". On verification failure the handshake is aborted. It is"
94,critically important to verify server certificates when using SSL to connect
94,"to servers, otherwise the communication is prone to trivial man-in-the-middle"
94,"attacks rendering SSL totally useless. Unless ""ssl_server_verify"" appears in"
94,"the global section, ""verifyThis keyword is available in sections :Bind optionsServer and default-server options"" is set to ""required"" by default."
94,"verifyhost <hostname>This setting is only available when support for OpenSSL was built in, and"
94,only takes effect if 'verify required' is also specified. This directive sets
94,a default static hostname to check the server's certificate against when no
94,"SNI was used to connect to the server. If SNI is not used, this is the only"
94,"way to enable hostname verification. This static hostname, when set, will"
94,also be used for health checks (which cannot provide an SNI value). If none
94,"of the hostnames in the certificate match the specified hostname, the"
94,handshake is aborted. The hostnames in the server-provided certificate may
94,"include wildcards. See also ""verifyThis keyword is available in sections :Bind optionsServer and default-server options"", ""sni"" and ""no-verifyhost"" options."
94,"weight <weight>The ""weight"" parameter is used to adjust the server's weight relative to"
94,other servers. All servers will receive a load proportional to their weight
94,"relative to the sum of all weights, so the higher the weight, the higher the"
94,"load. The default weight is 1, and the maximal value is 256. A value of 0"
94,means the server will not participate in load-balancing but will still accept
94,persistent connections. If this parameter is used to distribute the load
94,"according to server's capacity, it is recommended to start with values which"
94,"can both grow and shrink, for instance between 10 and 100 to leave enough"
94,room above and below for later adjustments.
94,5.3. Server IP address resolution using DNS
94,HAProxy allows using a host name on the server line to retrieve its IP address
94,"using name servers. By default, HAProxy resolves the name when parsing the"
94,"configuration file, at startup and cache the result for the process' life."
94,"This is not sufficient in some cases, such as in Amazon where a server's IP"
94,can change after a reboot or an ELB Virtual IP can change based on current
94,workload.
94,This chapter describes how HAProxy can be configured to process server's name
94,resolution at run time.
94,"Whether run time server name resolution has been enable or not, HAProxy will"
94,carry on doing the first resolution when parsing the configuration.
94,5.3.1. Global overview
94,"As we've seen in introduction, name resolution in HAProxy occurs at two"
94,different steps of the process life:
94,"1. when starting up, HAProxy parses the server line definition and matches a"
94,host name. It uses libc functions to get the host name resolved. This
94,resolution relies on /etc/resolv.conf file.
94,"2. at run time, HAProxy performs periodically name resolutions for servers"
94,requiring DNS resolutions.
94,A few other events can trigger a name resolution at run time:
94,- when a server's health check ends up in a connection timeout: this may be
94,because the server has a new IP address. So we need to trigger a name
94,resolution to know this new IP.
94,"When using resolvers, the server name can either be a hostname, or a SRV label."
94,HAProxy considers anything that starts with an underscore as a SRV label. If a
94,"SRV label is specified, then the corresponding SRV records will be retrieved"
94,"from the DNS server, and the provided hostnames will be used. The SRV label"
94,"will be checked periodically, and if any server are added or removed, haproxy"
94,will automatically do the same.
94,A few things important to notice:
94,- all the name servers are queried in the mean time. HAProxy will process the
94,first valid response.
94,"- a resolution is considered as invalid (NX, timeout, refused), when all the"
94,servers return an error.
94,5.3.2. The resolvers section
94,This section is dedicated to host information related to name resolution in
94,HAProxy. There can be as many as resolvers section as needed. Each section can
94,contain many name servers.
94,"When multiple name servers are configured in a resolvers section, then HAProxy"
94,"uses the first valid response. In case of invalid responses, only the last one"
94,is treated. Purpose is to give the chance to a slow server to deliver a valid
94,answer after a fast faulty or outdated server.
94,"When each server returns a different error type, then only the last error is"
94,used by HAProxy. The following processing is applied on this error:
94,1. HAProxy retries the same DNS query with a new query type. The A queries are
94,switch to AAAA or the opposite. SRV queries are not concerned here. Timeout
94,errors are also excluded.
94,"2. When the fallback on the query type was done (or not applicable), HAProxy"
94,"retries the original DNS query, with the preferred query type."
94,3. HAProxy retries previous steps <resolve_retires> times. If no valid
94,"response is received after that, it stops the DNS resolution and reports"
94,the error.
94,"For example, with 2 name servers configured in a resolvers section, the"
94,following scenarios are possible:
94,"- First response is valid and is applied directly, second response is"
94,ignored
94,"- First response is invalid and second one is valid, then second response is"
94,applied
94,"- First response is a NX domain and second one a truncated response, then"
94,HAProxy retries the query with a new type
94,"- First response is a NX domain and second one is a timeout, then HAProxy"
94,retries the query with a new type
94,"- Query timed out for both name servers, then HAProxy retries it with the"
94,same query type
94,"As a DNS server may not answer all the IPs in one DNS request, haproxy keeps"
94,"a cache of previous answers, an answer will be considered obsolete after"
94,<hold obsolete> seconds without the IP returned.
94,resolvers <resolvers id>
94,Creates a new name server list labeled <resolvers id>
94,A resolvers section accept the following parameters:
94,accepted_payload_size <nb>Defines the maximum payload size accepted by HAProxy and announced to all the
94,name servers configured in this resolvers section.
94,"<nb> is in bytes. If not set, HAProxy announces 512. (minimal value defined"
94,by RFC 6891)
94,Note: the maximum allowed value is 8192.
94,nameserver <id> <ip>:<port>DNS server description:
94,<id>
94,": label of the server, should be unique"
94,<ip>
94,: IP address of the server
94,<port> : port where the DNS service actually runs
94,hold <status> <period>Defines <period> during which the last name resolution should be kept based
94,on last resolution <status>
94,"<status> : last name resolution status. Acceptable values are ""nx"","
94,"""other"", ""refused"", ""timeoutThis keyword is available in sections :MailersAlphabetically sorted keywords referenceThe resolvers section"", ""valid"", ""obsolete""."
94,<period> : interval between two successive name resolution when the last
94,answer was in <status>. It follows the HAProxy time format.
94,<period> is in milliseconds by default.
94,"Default value is 10s for ""valid"", 0s for ""obsolete"" and 30s for others."
94,resolution_pool_size <nb> (deprecated)Defines the number of resolutions available in the pool for this resolvers.
94,"If not defines, it defaults to 64. If your configuration requires more than"
94,"<nb>, then HAProxy will return an error when parsing the configuration."
94,resolve_retries <nb>Defines the number <nb> of queries to send to resolve a server name before
94,giving up.
94,Default value: 3
94,A retry occurs on name server timeout or when the full sequence of DNS query
94,type failover is over and we need to start up from the default ANY query
94,type.
94,timeout <event> <time>Defines timeouts related to name resolution
94,<event> : the event on which the <time> timeout period applies to.
94,events available are:
94,- resolve : default time to trigger name resolutions when no
94,other time applied.
94,Default value: 1s
94,- retry
94,": time between two DNS queries, when no valid response"
94,have been received.
94,Default value: 1s
94,<time>
94,: time related to the event. It follows the HAProxy time format.
94,<time> is expressed in milliseconds.
94,Example:
94,resolvers mydns
94,nameserver dns1 10.0.0.1:53
94,nameserver dns2 10.0.0.2:53
94,resolve_retries
94,timeout resolve
94,timeout retry
94,hold other
94,30s
94,hold refused
94,30s
94,hold nx
94,30s
94,hold timeout
94,30s
94,hold valid
94,10s
94,hold obsolete
94,30s
94,6. HTTP header manipulation
94,"In HTTP mode, it is possible to rewrite, add or delete some of the request and"
94,response headers based on regular expressions. It is also possible to block a
94,"request or a response if a particular header matches a regular expression,"
94,"which is enough to stop most elementary protocol attacks, and to protect"
94,against information leak from the internal network.
94,"If HAProxy encounters an ""Informational Response"" (status code 1xx), it is able"
94,"to process all rsp* rules which can allow, deny, rewrite or delete a header,"
94,but it will refuse to add a header to any such messages as this is not
94,HTTP-compliant. The reason for still processing headers in such responses is to
94,"stop and/or fix any possible information leak which may happen, for instance"
94,"because another downstream equipment would unconditionally add a header, or if"
94,"a server name appears there. When such messages are seen, normal processing"
94,still occurs on the next non-informational messages.
94,"This section covers common usage of the following keywords, described in detail"
94,in section 4.2 :
94,- reqadd
94,<string>
94,- reqallow
94,<search>
94,- reqiallow
94,<search>
94,- reqdel
94,<search>
94,- reqidel
94,<search>
94,- reqdeny
94,<search>
94,- reqideny
94,<search>
94,- reqpass
94,<search>
94,- reqipass
94,<search>
94,- reqrep
94,<search> <replace>
94,- reqirep
94,<search> <replace>
94,- reqtarpit
94,<search>
94,- reqitarpit <search>
94,- rspadd
94,<string>
94,- rspdel
94,<search>
94,- rspidel
94,<search>
94,- rspdeny
94,<search>
94,- rspideny
94,<search>
94,- rsprep
94,<search> <replace>
94,- rspirep
94,<search> <replace>
94,"With all these keywords, the same conventions are used. The <search> parameter"
94,is a POSIX extended regular expression (regex) which supports grouping through
94,parenthesis (without the backslash). Spaces and other delimiters must be
94,prefixed with a backslash ('\') to avoid confusion with a field delimiter.
94,Other characters may be prefixed with a backslash to change their meaning :
94,for a tab
94,for a carriage return (CR)
94,for a new line (LF)
94,to mark a space and differentiate it from a delimiter
94,to mark a sharp and differentiate it from a comment
94,to use a backslash in a regex
94,"\\\\ to use a backslash in the text (*2 for regex, *2 for haproxy)"
94,\xXX to write the ASCII hex code XX as in the C language
94,The <replace> parameter contains the string to be used to replace the largest
94,portion of text matching the regex. It can make use of the special characters
94,"above, and can reference a substring which is delimited by parenthesis in the"
94,"regex, by writing a backslash ('\') immediately followed by one digit from 0 to"
94,9 indicating the group position (0 designating the entire line). This practice
94,"is very common to users of the ""sed"" program."
94,The <string> parameter represents the string which will systematically be added
94,after the last header line. It can also use special character sequences above.
94,Notes related to these keywords :- these keywords are not always convenient to allow/deny based on header
94,"contents. It is strongly recommended to use ACLs with the ""block"" keyword"
94,"instead, resulting in far more flexible and manageable rules."
94,- lines are always considered as a whole. It is not possible to reference
94,a header name only or a value only. This is important because of the way
94,headers are written (notably the number of spaces after the colon).
94,"- the first line is always considered as a header, which makes it possible to"
94,"rewrite or filter HTTP requests URIs or response codes, but in turn makes"
94,it harder to distinguish between headers and request line. The regex prefix
94,"^[^\ \t]*[\ \t] matches any HTTP method followed by a space, and the prefix"
94,^[^ \t:]*: matches any header name followed by a colon.
94,"- for performances reasons, the number of characters added to a request or to"
94,a response is limited at build time to values between 1 and 4 kB. This
94,should normally be far more than enough for most usages. If it is too short
94,"on occasional usages, it is possible to gain some space by removing some"
94,useless headers before adding new ones.
94,"- keywords beginning with ""reqi"" and ""rspi"" are the same as their counterpart"
94,without the 'i' letter except that they ignore case when matching patterns.
94,"- when a request passes through a frontend then a backend, all req* rules"
94,"from the frontend will be evaluated, then all req* rules from the backend"
94,will be evaluated. The reverse path is applied to responses.
94,"- req* statements are applied after ""block"" statements, so that ""block"" is"
94,"always the first one, but before ""use_backend"" in order to permit rewriting"
94,before switching.
94,7. Using ACLs and fetching samples
94,"HAProxy is capable of extracting data from request or response streams, from"
94,"client or server information, from tables, environmental information etc..."
94,"The action of extracting such data is called fetching a sample. Once retrieved,"
94,"these samples may be used for various purposes such as a key to a stick-table,"
94,but most common usages consist in matching them against predefined constant
94,data called patterns.
94,7.1. ACL basics
94,The use of Access Control Lists (ACL) provides a flexible solution to perform
94,content switching and generally to take decisions based on content extracted
94,"from the request, the response or any environmental status. The principle is"
94,simple :
94,"- extract a data sample from a stream, table or the environment"
94,- optionally apply some format conversion to the extracted sample
94,- apply one or multiple pattern matching methods on this sample
94,- perform actions only when a pattern matches the sample
94,"The actions generally consist in blocking a request, selecting a backend, or"
94,adding a header.
94,"In order to define a test, the ""acl"" keyword is used. The syntax is :"
94,acl <aclname> <criterion> [flags] [operator] [<value>] ...
94,This creates a new ACL <aclname> or completes an existing one with new tests.
94,Those tests apply to the portion of request/response specified in <criterion>
94,and may be adjusted with optional flags [flags]. Some criteria also support
94,an operator which may be specified before the set of values. Optionally some
94,"conversion operators may be applied to the sample, and they will be specified"
94,as a comma-delimited list of keywords just after the first keyword. The values
94,"are of the type supported by the criterion, and are separated by spaces."
94,"ACL names must be formed from upper and lower case letters, digits, '-' (dash),"
94,"'_' (underscore) , '.' (dot) and ':' (colon). ACL names are case-sensitive,"
94,"which means that ""my_acl"" and ""My_Acl"" are two different ACLs."
94,There is no enforced limit to the number of ACLs. The unused ones do not affect
94,"performance, they just consume a small amount of memory."
94,"The criterion generally is the name of a sample fetch method, or one of its ACL"
94,specific declinations. The default test method is implied by the output type of
94,this sample fetch method. The ACL declinations can describe alternate matching
94,methods of a same sample fetch method. The sample fetch methods are the only
94,ones supporting a conversion.
94,Sample fetch methods return data which can be of the following types :
94,- boolean
94,- integer (signed or unsigned)
94,- IPv4 or IPv6 address
94,- string
94,- data block
94,"Converters transform any of these data into any of these. For example, some"
94,converters might convert a string to a lower-case string while other ones
94,"would turn a string to an IPv4 address, or apply a netmask to an IP address."
94,"The resulting sample is of the type of the last converter applied to the list,"
94,which defaults to the type of the sample fetch method.
94,"Each sample or converter returns data of a specific type, specified with its"
94,keyword in this documentation. When an ACL is declared using a standard sample
94,"fetch method, certain types automatically involved a default matching method"
94,which are summarized in the table below :
94,+---------------------+-----------------+
94,| Sample or converter | Default
94,output type
94,| matching method |
94,+---------------------+-----------------+
94,| boolean
94,| bool
94,+---------------------+-----------------+
94,| integer
94,| int
94,+---------------------+-----------------+
94,| ip
94,| ip
94,+---------------------+-----------------+
94,| string
94,| str
94,+---------------------+-----------------+
94,| binary
94,"| none, use ""-m"""
94,+---------------------+-----------------+
94,"Note that in order to match a binary samples, it is mandatory to specify a"
94,"matching method, see below."
94,The ACL engine can match these types against patterns of the following types :
94,- boolean
94,- integer or integer range
94,- IP address / network
94,"- string (exact, substring, suffix, prefix, subdir, domain)"
94,- regular expression
94,- hex block
94,The following ACL flags are currently supported :
94,-i : ignore case during matching of all subsequent patterns.
94,-f : load patterns from a file.
94,-m : use a specific pattern matching method
94,-n : forbid the DNS resolutions
94,-M : load the file pointed by -f like a map file.
94,-u : force the unique id of the ACL
94,-- : force end of flags. Useful when a string looks like one of the flags.
94,"The ""-f"" flag is followed by the name of a file from which all lines will be"
94,"read as individual values. It is even possible to pass multiple ""-f"" arguments"
94,if the patterns are to be loaded from multiple files. Empty lines as well as
94,lines beginning with a sharp ('#') will be ignored. All leading spaces and tabs
94,will be stripped. If it is absolutely necessary to insert a valid pattern
94,"beginning with a sharp, just prefix it with a space so that it is not taken for"
94,"a comment. Depending on the data type and match method, haproxy may load the"
94,"lines into a binary tree, allowing very fast lookups. This is true for IPv4 and"
94,"exact string matching. In this case, duplicates will automatically be removed."
94,"The ""-M"" flag allows an ACL to use a map file. If this flag is set, the file is"
94,parsed as two column file. The first column contains the patterns used by the
94,"ACL, and the second column contain the samples. The sample can be used later by"
94,a map. This can be useful in some rare cases where an ACL would just be used to
94,check for the existence of a pattern in a map before a mapping is applied.
94,"The ""-u"" flag forces the unique id of the ACL. This unique id is used with the"
94,socket interface to identify ACL and dynamically change its values. Note that a
94,file is always identified by its name even if an id is set.
94,"Also, note that the ""-i"" flag applies to subsequent entries and not to entries"
94,loaded from files preceding it. For instance :
94,acl valid-ua hdr(user-agent) -f exact-ua.lst -i -f generic-ua.lst test
94,"In this example, each line of ""exact-ua.lst"" will be exactly matched against"
94,"the ""user-agent"" header of the request. Then each line of ""generic-ua"" will be"
94,"case-insensitively matched. Then the word ""test"" will be insensitively matched"
94,as well.
94,"The ""-m"" flag is used to select a specific pattern matching method on the input"
94,sample. All ACL-specific criteria imply a pattern matching method and generally
94,"do not need this flag. However, this flag is useful with generic sample fetch"
94,methods to describe how they're going to be matched against the patterns. This
94,is required for sample fetches which return data type for which there is no
94,"obvious matching method (e.g. string or binary). When ""-m"" is specified and"
94,"followed by a pattern matching method name, this method is used instead of the"
94,default one for the criterion. This makes it possible to match contents in ways
94,"that were not initially planned, or with sample fetch methods which return a"
94,string. The matching method also affects the way the patterns are parsed.
94,"The ""-n"" flag forbids the dns resolutions. It is used with the load of ip files."
94,"By default, if the parser cannot parse ip address it considers that the parsed"
94,"string is maybe a domain name and try dns resolution. The flag ""-n"" disable this"
94,resolution. It is useful for detecting malformed ip lists. Note that if the DNS
94,"server is not reachable, the haproxy configuration parsing may last many minutes"
94,waiting fir the timeout. During this time no error messages are displayed. The
94,"flag ""-n"" disable this behavior. Note also that during the runtime, this"
94,function is disabled for the dynamic acl modifications.
94,There are some restrictions however. Not all methods can be used with all
94,"sample fetch methods. Also, if ""-m"" is used in conjunction with ""-f"", it must"
94,be placed first. The pattern matching method must be one of the following :
94,"- ""found"" : only check if the requested sample could be found in the stream,"
94,but do not compare it against any pattern. It is recommended not
94,to pass any pattern to avoid confusion. This matching method is
94,particularly useful to detect presence of certain contents such
94,"as headers, cookies, etc... even if they are empty and without"
94,comparing them to anything nor counting them.
94,"- ""boolThis keyword is available in sections :ConvertersFetching samples from internal states"""
94,: check the value as a boolean. It can only be applied to fetches
94,"which return a boolean or integer value, and takes no pattern."
94,"Value zero or false does not match, all other values do match."
94,"- ""int"""
94,: match the value as an integer. It can be used with integer and
94,"boolean samples. Boolean false is integer 0, true is integer 1."
94,"- ""ip"""
94,: match the value as an IPv4 or IPv6 address. It is compatible
94,"with IP address samples only, so it is implied and never needed."
94,"- ""bin"""
94,: match the contents against a hexadecimal string representing a
94,binary sequence. This may be used with binary or string samples.
94,"- ""len"""
94,: match the sample's length as an integer. This may be used with
94,binary or string samples.
94,"- ""str"""
94,: exact match : match the contents against a string. This may be
94,used with binary or string samples.
94,"- ""sub"""
94,: substring match : check that the contents contain at least one of
94,the provided string patterns. This may be used with binary or
94,string samples.
94,"- ""reg"""
94,: regex match : match the contents against a list of regular
94,expressions. This may be used with binary or string samples.
94,"- ""beg"""
94,: prefix match : check that the contents begin like the provided
94,string patterns. This may be used with binary or string samples.
94,"- ""end"""
94,: suffix match : check that the contents end like the provided
94,string patterns. This may be used with binary or string samples.
94,"- ""dir"""
94,: subdir match : check that a slash-delimited portion of the
94,contents exactly matches one of the provided string patterns.
94,This may be used with binary or string samples.
94,"- ""dom"""
94,: domain match : check that a dot-delimited portion of the contents
94,exactly match one of the provided string patterns. This may be
94,used with binary or string samples.
94,"For example, to quickly detect the presence of cookie ""JSESSIONID"" in an HTTP"
94,"request, it is possible to do :"
94,acl jsess_present cook(JSESSIONID) -m found
94,In order to apply a regular expression on the 500 first bytes of data in the
94,"buffer, one would use the following acl :"
94,"acl script_tag payload(0,500) -m reg -i <script>"
94,"On systems where the regex library is much slower when using ""-i"", it is"
94,"possible to convert the sample to lowercase before matching, like this :"
94,"acl script_tag payload(0,500),lower -m reg <script>"
94,"All ACL-specific criteria imply a default matching method. Most often, these"
94,criteria are composed by concatenating the name of the original sample fetch
94,"method and the matching method. For example, ""hdr_beg"" applies the ""beg"" match"
94,"to samples retrieved using the ""hdr"" fetch method. Since all ACL-specific"
94,"criteria rely on a sample fetch method, it is always possible instead to use"
94,"the original sample fetch method and the explicit matching method using ""-m""."
94,"If an alternate match is specified using ""-m"" on an ACL-specific criterion,"
94,the matching method is simply applied to the underlying sample fetch method.
94,"For example, all ACLs below are exact equivalent :"
94,acl short_form
94,hdr_beg(host)
94,www.
94,acl alternate1
94,hdr_beg(host) -m beg www.
94,acl alternate2
94,hdr_dom(host) -m beg www.
94,acl alternate3
94,hdr(host)
94,-m beg www.
94,The table below summarizes the compatibility matrix between sample or converter
94,types and the pattern types to fetch against. It indicates for each compatible
94,"combination the name of the matching method to be used, surrounded with angle"
94,"brackets "">"" and ""<"" when the method is the default one and will work by"
94,"default without ""-m""."
94,+-------------------------------------------------+
94,Input sample type
94,+----------------------+---------+---------+---------+---------+---------+
94,pattern type
94,| boolean | integer |
94,| string
94,| binary
94,+----------------------+---------+---------+---------+---------+---------+
94,| none (presence only) |
94,found
94,found
94,found
94,found
94,found
94,+----------------------+---------+---------+---------+---------+---------+
94,| none (boolean value) |>
94,bool <|
94,bool
94,bool
94,+----------------------+---------+---------+---------+---------+---------+
94,| integer (value)
94,int
94,int
94,int
94,int
94,+----------------------+---------+---------+---------+---------+---------+
94,| integer (length)
94,len
94,len
94,len
94,len
94,len
94,+----------------------+---------+---------+---------+---------+---------+
94,| IP address
94,+----------------------+---------+---------+---------+---------+---------+
94,| exact string
94,str
94,str
94,str
94,str
94,str
94,+----------------------+---------+---------+---------+---------+---------+
94,| prefix
94,beg
94,beg
94,beg
94,beg
94,beg
94,+----------------------+---------+---------+---------+---------+---------+
94,| suffix
94,end
94,end
94,end
94,end
94,end
94,+----------------------+---------+---------+---------+---------+---------+
94,| substring
94,sub
94,sub
94,sub
94,sub
94,sub
94,+----------------------+---------+---------+---------+---------+---------+
94,| subdir
94,dir
94,dir
94,dir
94,dir
94,dir
94,+----------------------+---------+---------+---------+---------+---------+
94,| domain
94,dom
94,dom
94,dom
94,dom
94,dom
94,+----------------------+---------+---------+---------+---------+---------+
94,| regex
94,reg
94,reg
94,reg
94,reg
94,reg
94,+----------------------+---------+---------+---------+---------+---------+
94,| hex block
94,bin
94,bin
94,+----------------------+---------+---------+---------+---------+---------+
94,7.1.1. Matching booleans
94,"In order to match a boolean, no value is needed and all values are ignored."
94,"Boolean matching is used by default for all fetch methods of type ""boolean""."
94,"When boolean matching is used, the fetched value is returned as-is, which means"
94,"that a boolean ""true"" will always match and a boolean ""false"" will never match."
94,"Boolean matching may also be enforced using ""-m bool"" on fetch methods which"
94,"return an integer value. Then, integer value 0 is converted to the boolean"
94,"""false"" and all other values are converted to ""true""."
94,7.1.2. Matching integers
94,Integer matching applies by default to integer fetch methods. It can also be
94,"enforced on boolean fetches using ""-m int"". In this case, ""false"" is converted"
94,"to the integer 0, and ""true"" is converted to the integer 1."
94,Integer matching also supports integer ranges and operators. Note that integer
94,matching only applies to positive values. A range is a value expressed with a
94,"lower and an upper bound separated with a colon, both of which may be omitted."
94,"For instance, ""1024:65535"" is a valid range to represent a range of"
94,"unprivileged ports, and ""1024:"" would also work. ""0:1023"" is a valid"
94,"representation of privileged ports, and "":1023"" would also work."
94,"As a special case, some ACL functions support decimal numbers which are in fact"
94,two integers separated by a dot. This is used with some version checks for
94,"instance. All integer properties apply to those decimal numbers, including"
94,ranges and operators.
94,"For an easier usage, comparison operators are also supported. Note that using"
94,operators with ranges does not make much sense and is strongly discouraged.
94,"Similarly, it does not make much sense to perform order comparisons with a set"
94,of values.
94,Available operators for integer matching are :
94,eq : true if the tested value equals at least one value
94,ge : true if the tested value is greater than or equal to at least one value
94,gt : true if the tested value is greater than at least one value
94,le : true if the tested value is less than or equal to at least one value
94,lt : true if the tested value is less than at least one value
94,"For instance, the following ACL matches any negative Content-Length header :"
94,acl negative-length hdr_val(content-length) lt 0
94,This one matches SSL versions between 3.0 and 3.1 (inclusive) :
94,acl sslv3 req_ssl_ver 3:3.1
94,7.1.3. Matching strings
94,"String matching applies to string or binary fetch methods, and exists in 6"
94,different forms :
94,- exact match
94,(-m str) : the extracted string must exactly match the
94,patterns;
94,- substring match (-m sub) : the patterns are looked up inside the
94,"extracted string, and the ACL matches if any of them is found inside;"
94,- prefix match
94,(-m beg) : the patterns are compared with the beginning of
94,"the extracted string, and the ACL matches if any of them matches."
94,- suffix match
94,(-m end) : the patterns are compared with the end of the
94,"extracted string, and the ACL matches if any of them matches."
94,- subdir match
94,(-m dir) : the patterns are looked up inside the extracted
94,"string, delimited with slashes (""/""), and the ACL matches if any of them"
94,matches.
94,- domain match
94,(-m dom) : the patterns are looked up inside the extracted
94,"string, delimited with dots ("".""), and the ACL matches if any of them"
94,matches.
94,"String matching applies to verbatim strings as they are passed, with the"
94,"exception of the backslash (""\"") which makes it possible to escape some"
94,"characters such as the space. If the ""-i"" flag is passed before the first"
94,"string, then the matching will be performed ignoring the case. In order"
94,"to match the string ""-i"", either set it second, or pass the ""--"" flag"
94,"before the first string. Same applies of course to match the string ""--""."
94,Do not use string matches for binary fetches which might contain null bytes
94,"(0x00), as the comparison stops at the occurrence of the first null byte."
94,"Instead, convert the binary fetch to a hex string with the hex converter first."
94,Example:
94,# matches if the string <tag> is present in the binary sample
94,"acl tag_found req.payload(0,0),hex -m sub 3C7461673E"
94,7.1.4. Matching regular expressions (regexes)
94,"Just like with string matching, regex matching applies to verbatim strings as"
94,"they are passed, with the exception of the backslash (""\"") which makes it"
94,"possible to escape some characters such as the space. If the ""-i"" flag is"
94,"passed before the first regex, then the matching will be performed ignoring"
94,"the case. In order to match the string ""-i"", either set it second, or pass"
94,"the ""--"" flag before the first string. Same principle applies of course to"
94,"match the string ""--""."
94,7.1.5. Matching arbitrary data blocks
94,It is possible to match some extracted samples against a binary block which may
94,"not safely be represented as a string. For this, the patterns must be passed as"
94,"a series of hexadecimal digits in an even number, when the match method is set"
94,to binary. Each sequence of two digits will represent a byte. The hexadecimal
94,digits may be used upper or lower case.
94,Example :
94,"# match ""Hello\n"" in the input stream (\x48 \x65 \x6c \x6c \x6f \x0a)"
94,"acl hello payload(0,6) -m bin 48656c6c6f0a"
94,7.1.6. Matching IPv4 and IPv6 addresses
94,IPv4 addresses values can be specified either as plain addresses or with a
94,"netmask appended, in which case the IPv4 address matches whenever it is"
94,within the network. Plain addresses may also be replaced with a resolvable
94,"host name, but this practice is generally discouraged as it makes it more"
94,"difficult to read and debug configurations. If hostnames are used, you should"
94,at least ensure that they are present in /etc/hosts so that the configuration
94,does not depend on any random DNS match at the moment the configuration is
94,parsed.
94,The dotted IPv4 address notation is supported in both regular as well as the
94,abbreviated form with all-0-octets omitted:
94,+------------------+------------------+------------------+
94,Example 1
94,Example 2
94,Example 3
94,+------------------+------------------+------------------+
94,192.168.0.1
94,10.0.0.12
94,127.0.0.1
94,192.168.1
94,10.12
94,127.1
94,192.168.0.1/22
94,10.0.0.12/8
94,127.0.0.1/8
94,192.168.1/22
94,10.12/8
94,127.1/8
94,+------------------+------------------+------------------+
94,Notice that this is different from RFC 4632 CIDR address notation in which
94,192.168.42/24 would be equivalent to 192.168.42.0/24.
94,"IPv6 may be entered in their usual form, with or without a netmask appended."
94,Only bit counts are accepted for IPv6 netmasks. In order to avoid any risk of
94,"trouble with randomly resolved IP addresses, host names are never allowed in"
94,IPv6 patterns.
94,HAProxy is also able to match IPv4 addresses with IPv6 addresses in the
94,following situations :
94,"- tested address is IPv4, pattern address is IPv4, the match applies"
94,in IPv4 using the supplied mask if any.
94,"- tested address is IPv6, pattern address is IPv6, the match applies"
94,in IPv6 using the supplied mask if any.
94,"- tested address is IPv6, pattern address is IPv4, the match applies in IPv4"
94,"using the pattern's mask if the IPv6 address matches with 2002:IPV4::,"
94,"::IPV4 or ::ffff:IPV4, otherwise it fails."
94,"- tested address is IPv4, pattern address is IPv6, the IPv4 address is first"
94,"converted to IPv6 by prefixing ::ffff: in front of it, then the match is"
94,applied in IPv6 using the supplied IPv6 mask.
94,7.2. Using ACLs to form conditions
94,Some actions are only performed upon a valid condition. A condition is a
94,combination of ACLs with operators. 3 operators are supported :
94,- AND (implicit)
94,- OR
94,"(explicit with the ""or"" keyword or the ""||"" operator)"
94,"- Negation with the exclamation mark (""!"")"
94,A condition is formed as a disjunctive form:
94,[!]acl1 [!]acl2 ... [!]acln
94,{ or [!]acl1 [!]acl2 ... [!]acln } ...
94,"Such conditions are generally used after an ""if"" or ""unless"" statement,"
94,indicating when the condition will trigger the action.
94,"For instance, to block HTTP requests to the ""*"" URL with methods other than"
94,"""OPTIONS"", as well as POST requests without content-length, and GET or HEAD"
94,"requests with a content-length greater than 0, and finally every request which"
94,is not either GET/HEAD/POST/OPTIONS !
94,acl missing_cl hdr_cnt(Content-length) eq 0
94,http-request deny if HTTP_URL_STAR !METH_OPTIONS || METH_POST missing_cl
94,http-request deny if METH_GET HTTP_CONTENT
94,http-request deny unless METH_GET or METH_POST or METH_OPTIONS
94,"To select a different backend for requests to static contents on the ""www"" site"
94,"and to every request on the ""img"", ""video"", ""download"" and ""ftp"" hosts :"
94,acl url_static
94,path_beg
94,/static /images /img /css
94,acl url_static
94,path_end
94,.gif .png .jpg .css .js
94,acl host_www
94,hdr_beg(host) -i www
94,acl host_static hdr_beg(host) -i img. video. download. ftp.
94,"# now use backend ""static"" for all static-only hosts, and for static URLs"
94,"# of host ""www"". Use backend ""www"" for the rest."
94,use_backend static if host_static or host_www url_static
94,use_backend www
94,if host_www
94,"It is also possible to form rules using ""anonymous ACLs"". Those are unnamed ACL"
94,expressions that are built on the fly without needing to be declared. They must
94,"be enclosed between braces, with a space before and after each brace (because"
94,the braces must be seen as independent words). Example :
94,The following rule :
94,acl missing_cl hdr_cnt(Content-length) eq 0
94,http-request deny if METH_POST missing_cl
94,Can also be written that way :
94,http-request deny if METH_POST { hdr_cnt(Content-length) eq 0 }
94,It is generally not recommended to use this construct because it's a lot easier
94,"to leave errors in the configuration when written that way. However, for very"
94,"simple rules matching only one source IP address for instance, it can make more"
94,sense to use them than to declare ACLs with random names. Another example of
94,good use is the following :
94,With named ACLs :
94,acl site_dead nbsrv(dynamic) lt 2
94,acl site_dead nbsrv(static)
94,lt 2
94,monitor fail
94,if site_dead
94,With anonymous ACLs :
94,monitor fail if { nbsrv(dynamic) lt 2 } || { nbsrv(static) lt 2 }
94,"See section 4.2 for detailed help on the ""http-request deny"" and ""use_backend"""
94,keywords.
94,7.3. Fetching samples
94,"Historically, sample fetch methods were only used to retrieve data to match"
94,"against patterns using ACLs. With the arrival of stick-tables, a new class of"
94,"sample fetch methods was created, most often sharing the same syntax as their"
94,"ACL counterpart. These sample fetch methods are also known as ""fetches"". As"
94,"of now, ACLs and fetches have converged. All ACL fetch methods have been made"
94,"available as fetch methods, and ACLs may use any sample fetch method as well."
94,This section details all available sample fetch methods and their output type.
94,Some sample fetch methods have deprecated aliases that are used to maintain
94,compatibility with existing configurations. They are then explicitly marked as
94,deprecated and should not be used in new setups.
94,"The ACL derivatives are also indicated when available, with their respective"
94,matching methods. These ones all have a well defined default pattern matching
94,"method, so it is never necessary (though allowed) to pass the ""-m"" option to"
94,indicate how the sample will be matched using ACLs.
94,"As indicated in the sample type versus matching compatibility matrix above,"
94,"when using a generic sample fetch method in an ACL, the ""-m"" option is"
94,"mandatory unless the sample type is one of boolean, integer, IPv4 or IPv6. When"
94,"the same keyword exists as an ACL keyword and as a standard fetch method, the"
94,ACL engine will automatically pick the ACL-only one by default.
94,"Some of these keywords support one or multiple mandatory arguments, and one or"
94,multiple optional arguments. These arguments are strongly typed and are checked
94,when the configuration is parsed so that there is no risk of running with an
94,incorrect argument (e.g. an unresolved backend name). Fetch function arguments
94,are passed between parenthesis and are delimited by commas. When an argument
94,"is optional, it will be indicated below between square brackets ('[ ]'). When"
94,"all arguments are optional, the parenthesis may be omitted."
94,"Thus, the syntax of a standard sample fetch method is one of the following :"
94,- name
94,- name(arg1)
94,"- name(arg1,arg2)"
94,7.3.1. Converters
94,Sample fetch methods may be combined with transformations to be applied on top
94,"of the fetched sample (also called ""converters""). These combinations form what"
94,"is called ""sample expressions"" and the result is a ""sample"". Initially this"
94,"was only supported by ""stick on"" and ""stick store-request"" directives but this"
94,"has now be extended to all places where samples may be used (ACLs, log-format,"
94,"unique-id-format, add-header, ...)."
94,These transformations are enumerated as a series of specific keywords after the
94,sample fetch method. These keywords may equally be appended immediately after
94,"the fetch keyword's argument, delimited by a comma. These keywords can also"
94,support some arguments (e.g. a netmask) which must be passed in parenthesis.
94,A certain category of converters are bitwise and arithmetic operators which
94,support performing basic operations on integers. Some bitwise operations are
94,"supported (and, or, xor, cpl) and some arithmetic operations are supported"
94,"(add, sub, mul, div, mod, neg). Some comparators are provided (odd, even, not,"
94,bool) which make it possible to report a match without having to write an ACL.
94,The currently available list of transformation keywords include :
94,"51d.single(<prop>[,<prop>*])Returns values for the properties requested as a string, where values are"
94,"separated by the delimiter specified with ""51degrees-property-separator""."
94,The device is identified using the User-Agent header passed to the
94,"converter. The function can be passed up to five property names, and if a"
94,"property name can't be found, the value ""NoData"" is returned."
94,Example :
94,"# Here the header ""X-51D-DeviceTypeMobileTablet"" is added to the request,"
94,# containing values for the three properties requested by using the
94,# User-Agent passed to the converter.
94,frontend http-in
94,bind *:8081
94,default_backend servers
94,http-request set-header X-51D-DeviceTypeMobileTablet \
94,"%[req.fhdr(User-Agent),51d.single(DeviceType,IsMobile,IsTablet)]"
94,"add(<value>)Adds <value> to the input value of type signed integer, and returns the"
94,result as a signed integer. <value> can be a numeric value or a variable
94,name. The name of the variable starts with an indication about its scope. The
94,scopes allowed are:
94,"""proc"" : the variable is shared with the whole process"
94,"""sess"" : the variable is shared with the whole session"
94,"""txn"""
94,: the variable is shared with the transaction (request and response)
94,"""req"""
94,: the variable is shared only during request processing
94,"""res"""
94,: the variable is shared only during response processing
94,This prefix is followed by a name. The separator is a '.'. The name may only
94,"contain characters 'a-z', 'A-Z', '0-9', '.' and '_'."
94,"and(<value>)Performs a bitwise ""AND"" between <value> and the input value of type signed"
94,"integer, and returns the result as an signed integer. <value> can be a"
94,numeric value or a variable name. The name of the variable starts with an
94,indication about its scope. The scopes allowed are:
94,"""proc"" : the variable is shared with the whole process"
94,"""sess"" : the variable is shared with the whole session"
94,"""txn"""
94,: the variable is shared with the transaction (request and response)
94,"""req"""
94,: the variable is shared only during request processing
94,"""res"""
94,: the variable is shared only during response processing
94,This prefix is followed by a name. The separator is a '.'. The name may only
94,"contain characters 'a-z', 'A-Z', '0-9', '.' and '_'."
94,b64decConverts (decodes) a base64 encoded input string to its binary
94,representation. It performs the inverse operation of base64().
94,base64Converts a binary input sample to a base64 string. It is used to log or
94,transfer binary content in a way that can be reliably transferred (e.g.
94,an SSL ID can be copied in a header).
94,boolReturns a boolean TRUE if the input value of type signed integer is
94,"non-null, otherwise returns FALSE. Used in conjunction with and(), it can be"
94,used to report true/false for bit testing on input values (e.g. verify the
94,presence of a flag).
94,"bytes(<offset>[,<length>])Extracts some bytes from an input binary sample. The result is a binary"
94,sample starting at an offset (in bytes) of the original sample and
94,optionally truncated at the given length.
94,"cplTakes the input value of type signed integer, applies a ones-complement"
94,(flips all bits) and returns the result as an signed integer.
94,crc32([<avalanche>])Hashes a binary input sample into an unsigned 32-bit quantity using the CRC32
94,"hash function. Optionally, it is possible to apply a full avalanche hash"
94,function to the output if the optional <avalanche> argument equals 1. This
94,converter uses the same functions as used by the various hash-based load
94,"balancing algorithms, so it will provide exactly the same results. It is"
94,provided for compatibility with other software which want a CRC32 to be
94,"computed on some input keys, so it follows the most common implementation as"
94,"found in Ethernet, Gzip, PNG, etc... It is slower than the other algorithms"
94,but may provide a better or at least less predictable distribution. It must
94,not be used for security purposes as a 32-bit hash is trivial to break. See
94,"also ""djb2"", ""sdbm"", ""wt6"" and the ""hash-type"" directive."
94,"da-csv-conv(<prop>[,<prop>*])Asks the DeviceAtlas converter to identify the User Agent string passed on"
94,"input, and to emit a string made of the concatenation of the properties"
94,"enumerated in argument, delimited by the separator defined by the global"
94,"keyword ""deviceatlas-property-separator"", or by default the pipe character"
94,('|'). There's a limit of 12 different properties imposed by the haproxy
94,configuration language.
94,Example:
94,frontend www
94,bind *:8881
94,default_backend servers
94,"http-request set-header X-DeviceAtlas-Data %[req.fhdr(User-Agent),da-csv(primaryHardwareType,osName,osVersion,browserName,browserVersion,browserRenderingEngine)]"
94,debugThis converter is used as debug tool. It dumps on screen the content and the
94,type of the input sample. The sample is returned as is on its output. This
94,converter only exists when haproxy was built with debugging enabled.
94,"div(<value>)Divides the input value of type signed integer by <value>, and returns the"
94,"result as an signed integer. If <value> is null, the largest unsigned"
94,integer is returned (typically 2^63-1). <value> can be a numeric value or a
94,variable name. The name of the variable starts with an indication about its
94,scope. The scopes allowed are:
94,"""proc"" : the variable is shared with the whole process"
94,"""sess"" : the variable is shared with the whole session"
94,"""txn"""
94,: the variable is shared with the transaction (request and response)
94,"""req"""
94,: the variable is shared only during request processing
94,"""res"""
94,: the variable is shared only during response processing
94,This prefix is followed by a name. The separator is a '.'. The name may only
94,"contain characters 'a-z', 'A-Z', '0-9', '.' and '_'."
94,djb2([<avalanche>])Hashes a binary input sample into an unsigned 32-bit quantity using the DJB2
94,"hash function. Optionally, it is possible to apply a full avalanche hash"
94,function to the output if the optional <avalanche> argument equals 1. This
94,converter uses the same functions as used by the various hash-based load
94,"balancing algorithms, so it will provide exactly the same results. It is"
94,"mostly intended for debugging, but can be used as a stick-table entry to"
94,collect rough statistics. It must not be used for security purposes as a
94,"32-bit hash is trivial to break. See also ""crc32"", ""sdbm"", ""wt6"" and the"
94,"""hash-type"" directive."
94,evenReturns a boolean TRUE if the input value of type signed integer is even
94,"otherwise returns FALSE. It is functionally equivalent to ""not,and(1),bool""."
94,"field(<index>,<delimiters>)Extracts the substring at the given index considering given delimiters from"
94,an input string. Indexes start at 1 and delimiters are a string formatted
94,list of chars.
94,hexConverts a binary input sample to a hex string containing two hex digits per
94,input byte. It is used to log or transfer hex dumps of some binary input data
94,in a way that can be reliably transferred (e.g. an SSL ID can be copied in a
94,header).
94,hex2iConverts a hex string containing two hex digits per input byte to an
94,"integer. If the input value can not be converted, then zero is returned."
94,http_date([<offset>])Converts an integer supposed to contain a date since epoch to a string
94,representing this date in a format suitable for use in HTTP header fields. If
94,"an offset value is specified, then it is a number of seconds that is added to"
94,the date before the conversion is operated. This is particularly useful to
94,"emit Date header fields, Expires values in responses when combined with a"
94,"positive offset, or Last-Modified values when the offset is negative."
94,in_table(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, a boolean false"
94,is returned. Otherwise a boolean true is returned. This can be used to verify
94,the presence of a certain key in a table tracking some elements (e.g. whether
94,or not a source IP address or an Authorization header was already seen).
94,"ipmask(<mask>)Apply a mask to an IPv4 address, and use the result for lookups and storage."
94,This can be used to make all hosts within a certain mask to share the same
94,table entries and as such use the same server. The mask can be passed in
94,dotted form (e.g. 255.255.255.0) or in CIDR form (e.g. 24).
94,json([<input-code>])Escapes the input string and produces an ASCII output string ready to use as a
94,JSON string. The converter tries to decode the input string according to the
94,"<input-code> parameter. It can be ""ascii"", ""utf8"", ""utf8s"", ""utf8p"" or"
94,"""utf8ps"". The ""ascii"" decoder never fails. The ""utf8"" decoder detects 3 types"
94,of errors:
94,"- bad UTF-8 sequence (lone continuation byte, bad number of continuation"
94,"bytes, ...)"
94,"- invalid range (the decoded value is within a UTF-8 prohibited range),"
94,- code overlong (the value is encoded with more bytes than necessary).
94,"The UTF-8 JSON encoding can produce a ""too long value"" error when the UTF-8"
94,character is greater than 0xffff because the JSON string escape specification
94,only authorizes 4 hex digits for the value encoding. The UTF-8 decoder exists
94,"in 4 variants designated by a combination of two suffix letters : ""p"" for"
94,"""permissive"" and ""s"" for ""silently ignore"". The behaviors of the decoders"
94,are :
94,"- ""ascii"""
94,: never fails;
94,"- ""utf8"""
94,: fails on any detected errors;
94,"- ""utf8s"""
94,": never fails, but removes characters corresponding to errors;"
94,"- ""utf8p"""
94,": accepts and fixes the overlong errors, but fails on any other"
94,error;
94,"- ""utf8ps"" : never fails, accepts and fixes the overlong errors, but removes"
94,characters corresponding to the other errors.
94,This converter is particularly useful for building properly escaped JSON for
94,logging to servers which consume JSON-formatted traffic logs.
94,Example:
94,capture request header Host len 15
94,capture request header user-agent len 150
94,"log-format &#x27;{""ip"":""%[src]"",""user-agent"":""%[capture.req.hdr(1),json(utf8s)]""}'"
94,Input request from client 127.0.0.1:
94,GET / HTTP/1.0
94,"User-Agent: Very ""Ugly"" UA 1/2"
94,Output log:
94,"{""ip"":""127.0.0.1"",""user-agent"":""Very \""Ugly\"" UA 1\/2""}"
94,"language(<value>[,<default>])Returns the value with the highest q-factor from a list as extracted from the"
94,"""accept-language"" header using ""req.fhdr"". Values with no q-factor have a"
94,q-factor of 1. Values with a q-factor of 0 are dropped. Only values which
94,belong to the list of semi-colon delimited <values> will be considered. The
94,"argument <value> syntax is ""lang[;lang[;lang[;...]]]"". If no value matches the"
94,"given list and a default value is provided, it is returned. Note that language"
94,names may have a variant after a dash ('-'). If this variant is present in the
94,"list, it will be matched, but if it is not, only the base language is checked."
94,"The match is case-sensitive, and the output string is always one of those"
94,"provided in arguments. The ordering of arguments is meaningless, only the"
94,"ordering of the values in the request counts, as the first value among"
94,multiple sharing the same q-factor is used.
94,Example :
94,# this configuration switches to the backend matching a
94,# given language based on the request :
94,"acl es req.fhdr(accept-language),language(es;fr;en) -m str es"
94,"acl fr req.fhdr(accept-language),language(es;fr;en) -m str fr"
94,"acl en req.fhdr(accept-language),language(es;fr;en) -m str en"
94,use_backend spanish if es
94,use_backend french
94,if fr
94,use_backend english if en
94,default_backend choose_your_language
94,lowerConvert a string sample to lower case. This can only be placed after a string
94,sample fetch function or after a transformation keyword returning a string
94,type. The result is of type string.
94,"ltime(<format>[,<offset>])Converts an integer supposed to contain a date since epoch to a string"
94,representing this date in local time using a format defined by the <format>
94,string using strftime(3). The purpose is to allow any date format to be used
94,in logs. An optional <offset> in seconds may be applied to the input date
94,(positive or negative). See the strftime() man page for the format supported
94,by your operating system. See also the utime converter.
94,Example :
94,"# Emit two colons, one with the local time and another with ip:port"
94,# e.g.
94,20140710162350 127.0.0.1:57325
94,"log-format %[date,ltime(%Y%m%d%H%M%S)]\ %ci:%cp"
94,"map(<map_file>[,<default_value>])map_<match_type>(<map_file>[,<default_value>])"
94,"map_<match_type>_<output_type>(<map_file>[,<default_value>])"
94,"Search the input value from <map_file> using the <match_type> matching method,"
94,and return the associated value converted to the type <output_type>. If the
94,"input value cannot be found in the <map_file>, the converter returns the"
94,"<default_value>. If the <default_value> is not set, the converter fails and"
94,"acts as if no input value could be fetched. If the <match_type> is not set, it"
94,"defaults to ""str"". Likewise, if the <output_type> is not set, it defaults to"
94,"""str"". For convenience, the ""map"" keyword is an alias for ""map_str"" and maps a"
94,string to another string.
94,It is important to avoid overlapping between the keys : IP addresses and
94,"strings are stored in trees, so the first of the finest match will be used."
94,"Other keys are stored in lists, so the first matching occurrence will be used."
94,The following array contains the list of all map functions available sorted by
94,"input type, match type and output type."
94,input typematch methodoutput type stroutput type intoutput type ip
94,strstrmap_strmap_str_intmap_str_ip
94,strbegmap_begmap_beg_intmap_end_ip
94,strsubmap_submap_sub_intmap_sub_ip
94,strdirmap_dirmap_dir_intmap_dir_ip
94,strdommap_dommap_dom_intmap_dom_ip
94,strendmap_endmap_end_intmap_end_ip
94,strregmap_regmap_reg_intmap_reg_ip
94,strregmap_regmmap_reg_intmap_reg_ip
94,intintmap_intmap_int_intmap_int_ip
94,ipipmap_ipmap_ip_intmap_ip_ip
94,"The special map called ""map_regm"" expect matching zone in the regular"
94,"expression and modify the output replacing back reference (like ""\1"") by"
94,the corresponding match text.
94,The file contains one key + value per line. Lines which start with '#' are
94,"ignored, just like empty lines. Leading tabs and spaces are stripped. The key"
94,"is then the first ""word"" (series of non-space/tabs characters), and the value"
94,is what follows this series of space/tab till the end of the line excluding
94,trailing spaces/tabs.
94,Example :
94,# this is a comment and is ignored
94,2.22.246.0/23
94,United Kingdom
94,<-><-----------><--><------------><---->
94,`- trailing spaces ignored
94,`---------- value
94,`-------------------- middle spaces ignored
94,`---------------------------- key
94,`------------------------------------ leading spaces ignored
94,"mod(<value>)Divides the input value of type signed integer by <value>, and returns the"
94,"remainder as an signed integer. If <value> is null, then zero is returned."
94,<value> can be a numeric value or a variable name. The name of the variable
94,starts with an indication about its scope. The scopes allowed are:
94,"""proc"" : the variable is shared with the whole process"
94,"""sess"" : the variable is shared with the whole session"
94,"""txn"""
94,: the variable is shared with the transaction (request and response)
94,"""req"""
94,: the variable is shared only during request processing
94,"""res"""
94,: the variable is shared only during response processing
94,This prefix is followed by a name. The separator is a '.'. The name may only
94,"contain characters 'a-z', 'A-Z', '0-9', '.' and '_'."
94,"mul(<value>)Multiplies the input value of type signed integer by <value>, and returns"
94,"the product as an signed integer. In case of overflow, the largest possible"
94,value for the sign is returned so that the operation doesn't wrap around.
94,<value> can be a numeric value or a variable name. The name of the variable
94,starts with an indication about its scope. The scopes allowed are:
94,"""proc"" : the variable is shared with the whole process"
94,"""sess"" : the variable is shared with the whole session"
94,"""txn"""
94,: the variable is shared with the transaction (request and response)
94,"""req"""
94,: the variable is shared only during request processing
94,"""res"""
94,: the variable is shared only during response processing
94,This prefix is followed by a name. The separator is a '.'. The name may only
94,"contain characters 'a-z', 'A-Z', '0-9', '.' and '_'."
94,"nbsrvTakes an input value of type string, interprets it as a backend name and"
94,returns the number of usable servers in that backend. Can be used in places
94,"where we want to look up a backend from a dynamic name, like a result of a"
94,map lookup.
94,"negTakes the input value of type signed integer, computes the opposite value,"
94,and returns the remainder as an signed integer. 0 is identity. This operator
94,is provided for reversed subtracts : in order to subtract the input from a
94,"constant, simply perform a ""neg,add(value)""."
94,notReturns a boolean FALSE if the input value of type signed integer is
94,"non-null, otherwise returns TRUE. Used in conjunction with and(), it can be"
94,used to report true/false for bit testing on input values (e.g. verify the
94,absence of a flag).
94,oddReturns a boolean TRUE if the input value of type signed integer is odd
94,"otherwise returns FALSE. It is functionally equivalent to ""and(1),bool""."
94,"or(<value>)Performs a bitwise ""OR"" between <value> and the input value of type signed"
94,"integer, and returns the result as an signed integer. <value> can be a"
94,numeric value or a variable name. The name of the variable starts with an
94,indication about its scope. The scopes allowed are:
94,"""proc"" : the variable is shared with the whole process"
94,"""sess"" : the variable is shared with the whole session"
94,"""txn"""
94,: the variable is shared with the transaction (request and response)
94,"""req"""
94,: the variable is shared only during request processing
94,"""res"""
94,: the variable is shared only during response processing
94,This prefix is followed by a name. The separator is a '.'. The name may only
94,"contain characters 'a-z', 'A-Z', '0-9', '.' and '_'."
94,"regsub(<regex>,<subst>[,<flags>])Applies a regex-based substitution to the input string. It does the same"
94,"operation as the well-known ""sed"" utility with ""s/<regex>/<subst>/"". By"
94,default it will replace in the input string the first occurrence of the
94,largest part matching the regular expression <regex> with the substitution
94,string <subst>. It is possible to replace all occurrences instead by adding
94,"the flag ""g"" in the third argument <flags>. It is also possible to make the"
94,"regex case insensitive by adding the flag ""i"" in <flags>. Since <flags> is a"
94,"string, it is made up from the concatenation of all desired flags. Thus if"
94,"both ""i"" and ""g"" are desired, using ""gi"" or ""ig"" will have the same effect."
94,It is important to note that due to the current limitations of the
94,"configuration parser, some characters such as closing parenthesis, closing"
94,square brackets or comma are not possible to use in the arguments. The first
94,use of this converter is to replace certain characters or sequence of
94,characters with other ones.
94,Example :
94,"# de-duplicate ""/"" in header ""x-path""."
94,# input:
94,x-path: /////a///b/c/xzxyz/
94,# output: x-path: /a/b/c/xzxyz/
94,"http-request set-header x-path %[hdr(x-path),regsub(/+,/,g)]"
94,capture-req(<id>)Capture the string entry in the request slot <id> and returns the entry as
94,"is. If the slot doesn't exist, the capture fails silently."
94,"See also: ""declare capture"", ""http-request capture"", ""http-response capture"", ""capture.req.hdr"" and ""capture.res.hdr"" (sample fetches)."
94,capture-res(<id>)Capture the string entry in the response slot <id> and returns the entry as
94,"is. If the slot doesn't exist, the capture fails silently."
94,"See also: ""declare capture"", ""http-request capture"", ""http-response capture"", ""capture.req.hdr"" and ""capture.res.hdr"" (sample fetches)."
94,sdbm([<avalanche>])
94,Hashes a binary input sample into an unsigned 32-bit quantity using the SDBM
94,"hash function. Optionally, it is possible to apply a full avalanche hash"
94,function to the output if the optional <avalanche> argument equals 1. This
94,converter uses the same functions as used by the various hash-based load
94,"balancing algorithms, so it will provide exactly the same results. It is"
94,"mostly intended for debugging, but can be used as a stick-table entry to"
94,collect rough statistics. It must not be used for security purposes as a
94,"32-bit hash is trivial to break. See also ""crc32"", ""djb2"", ""wt6"" and the"
94,"""hash-type"" directive."
94,set-var(<var name>)
94,Sets a variable with the input content and returns the content on the output
94,as-is. The variable keeps the value and the associated input type. The name of
94,the variable starts with an indication about its scope. The scopes allowed are:
94,"""proc"" : the variable is shared with the whole process"
94,"""sess"" : the variable is shared with the whole session"
94,"""txn"""
94,: the variable is shared with the transaction (request and
94,"response),"
94,"""req"""
94,": the variable is shared only during request processing,"
94,"""res"""
94,: the variable is shared only during response processing.
94,This prefix is followed by a name. The separator is a '.'. The name may only
94,"contain characters 'a-z', 'A-Z', '0-9', '.' and '_'."
94,sha1Converts a binary input sample to a SHA1 digest. The result is a binary
94,sample with length of 20 bytes.
94,"sub(<value>)Subtracts <value> from the input value of type signed integer, and returns"
94,the result as an signed integer. Note: in order to subtract the input from
94,"a constant, simply perform a ""neg,add(value)"". <value> can be a numeric value"
94,or a variable name. The name of the variable starts with an indication about
94,its scope. The scopes allowed are:
94,"""proc"" : the variable is shared with the whole process"
94,"""sess"" : the variable is shared with the whole session"
94,"""txn"""
94,: the variable is shared with the transaction (request and
94,"response),"
94,"""req"""
94,": the variable is shared only during request processing,"
94,"""res"""
94,: the variable is shared only during response processing.
94,This prefix is followed by a name. The separator is a '.'. The name may only
94,"contain characters 'a-z', 'A-Z', '0-9', '.' and '_'."
94,table_bytes_in_rate(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, integer value zero"
94,is returned. Otherwise the converter returns the average client-to-server
94,"bytes rate associated with the input sample in the designated table, measured"
94,in amount of bytes over the period configured in the table. See also the
94,sc_bytes_in_rate sample fetch keyword.
94,table_bytes_out_rate(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, integer value zero"
94,is returned. Otherwise the converter returns the average server-to-client
94,"bytes rate associated with the input sample in the designated table, measured"
94,in amount of bytes over the period configured in the table. See also the
94,sc_bytes_out_rate sample fetch keyword.
94,table_conn_cnt(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, integer value zero"
94,is returned. Otherwise the converter returns the cumulative number of incoming
94,connections associated with the input sample in the designated table. See
94,also the sc_conn_cnt sample fetch keyword.
94,table_conn_cur(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, integer value zero"
94,is returned. Otherwise the converter returns the current amount of concurrent
94,tracked connections associated with the input sample in the designated table.
94,See also the sc_conn_cur sample fetch keyword.
94,table_conn_rate(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, integer value zero"
94,is returned. Otherwise the converter returns the average incoming connection
94,rate associated with the input sample in the designated table. See also the
94,sc_conn_rate sample fetch keyword.
94,table_gpt0(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, boolean value zero"
94,is returned. Otherwise the converter returns the current value of the first
94,general purpose tag associated with the input sample in the designated table.
94,See also the sc_get_gpt0 sample fetch keyword.
94,table_gpc0(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, integer value zero"
94,is returned. Otherwise the converter returns the current value of the first
94,general purpose counter associated with the input sample in the designated
94,table. See also the sc_get_gpc0 sample fetch keyword.
94,table_gpc0_rate(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, integer value zero"
94,is returned. Otherwise the converter returns the frequency which the gpc0
94,"counter was incremented over the configured period in the table, associated"
94,with the input sample in the designated table. See also the sc_get_gpc0_rate
94,sample fetch keyword.
94,table_http_err_cnt(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, integer value zero"
94,is returned. Otherwise the converter returns the cumulative number of HTTP
94,errors associated with the input sample in the designated table. See also the
94,sc_http_err_cnt sample fetch keyword.
94,table_http_err_rate(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, integer value zero"
94,is returned. Otherwise the average rate of HTTP errors associated with the
94,"input sample in the designated table, measured in amount of errors over the"
94,period configured in the table. See also the sc_http_err_rate sample fetch
94,keyword.
94,table_http_req_cnt(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, integer value zero"
94,is returned. Otherwise the converter returns the cumulative number of HTTP
94,requests associated with the input sample in the designated table. See also
94,the sc_http_req_cnt sample fetch keyword.
94,table_http_req_rate(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, integer value zero"
94,is returned. Otherwise the average rate of HTTP requests associated with the
94,"input sample in the designated table, measured in amount of requests over the"
94,period configured in the table. See also the sc_http_req_rate sample fetch
94,keyword.
94,table_kbytes_in(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, integer value zero"
94,is returned. Otherwise the converter returns the cumulative number of client-
94,"to-server data associated with the input sample in the designated table,"
94,"measured in kilobytes. The test is currently performed on 32-bit integers,"
94,which limits values to 4 terabytes. See also the sc_kbytes_in sample fetch
94,keyword.
94,table_kbytes_out(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, integer value zero"
94,is returned. Otherwise the converter returns the cumulative number of server-
94,"to-client data associated with the input sample in the designated table,"
94,"measured in kilobytes. The test is currently performed on 32-bit integers,"
94,which limits values to 4 terabytes. See also the sc_kbytes_out sample fetch
94,keyword.
94,table_server_id(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, integer value zero"
94,is returned. Otherwise the converter returns the server ID associated with
94,the input sample in the designated table. A server ID is associated to a
94,"sample by a ""stickThis keyword is available in sections :Alphabetically sorted keywords referenceServer and default-server options"" rule when a connection to a server succeeds. A server ID"
94,zero means that no server is associated with this key.
94,table_sess_cnt(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, integer value zero"
94,is returned. Otherwise the converter returns the cumulative number of incoming
94,sessions associated with the input sample in the designated table. Note that
94,a session here refers to an incoming connection being accepted by the
94,"""tcp-request connection"" rulesets. See also the sc_sess_cnt sample fetch"
94,keyword.
94,table_sess_rate(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, integer value zero"
94,is returned. Otherwise the converter returns the average incoming session
94,rate associated with the input sample in the designated table. Note that a
94,session here refers to an incoming connection being accepted by the
94,"""tcp-request connection"" rulesets. See also the sc_sess_rate sample fetch"
94,keyword.
94,table_trackers(<table>)Uses the string representation of the input sample to perform a look up in
94,"the specified table. If the key is not found in the table, integer value zero"
94,is returned. Otherwise the converter returns the current amount of concurrent
94,connections tracking the same key as the input sample in the designated
94,table. It differs from table_conn_cur in that it does not rely on any stored
94,"information but on the table's reference count (the ""use"" value which is"
94,"returned by ""show table"" on the CLI). This may sometimes be more suited for"
94,layer7 tracking. It can be used to tell a server how many concurrent
94,connections there are from a given address for example. See also the
94,sc_trackers sample fetch keyword.
94,upperConvert a string sample to upper case. This can only be placed after a string
94,sample fetch function or after a transformation keyword returning a string
94,type. The result is of type string.
94,url_dec([<in_form>])
94,Takes an url-encoded string provided as input and returns the decoded version
94,as output. The input and the output are of type string. If the <in_form>
94,"argument is set to a non-zero integer value, the input string is assumed to"
94,be part of a form or query string and the '+' character will be turned into a
94,space (' '). Otherwise this will only happen after a question mark indicating
94,a query string ('?').
94,unset-var(<var name>)
94,Unsets a variable if the input content is defined. The name of the variable
94,starts with an indication about its scope. The scopes allowed are:
94,"""proc"" : the variable is shared with the whole process"
94,"""sess"" : the variable is shared with the whole session"
94,"""txn"""
94,: the variable is shared with the transaction (request and
94,"response),"
94,"""req"""
94,": the variable is shared only during request processing,"
94,"""res"""
94,: the variable is shared only during response processing.
94,This prefix is followed by a name. The separator is a '.'. The name may only
94,"contain characters 'a-z', 'A-Z', '0-9', '.' and '_'."
94,"utime(<format>[,<offset>])Converts an integer supposed to contain a date since epoch to a string"
94,representing this date in UTC time using a format defined by the <format>
94,string using strftime(3). The purpose is to allow any date format to be used
94,in logs. An optional <offset> in seconds may be applied to the input date
94,(positive or negative). See the strftime() man page for the format supported
94,by your operating system. See also the ltime converter.
94,Example :
94,"# Emit two colons, one with the UTC time and another with ip:port"
94,# e.g.
94,20140710162350 127.0.0.1:57325
94,"log-format %[date,utime(%Y%m%d%H%M%S)]\ %ci:%cp"
94,"word(<index>,<delimiters>)Extracts the nth word considering given delimiters from an input string."
94,Indexes start at 1 and delimiters are a string formatted list of chars.
94,wt6([<avalanche>])Hashes a binary input sample into an unsigned 32-bit quantity using the WT6
94,"hash function. Optionally, it is possible to apply a full avalanche hash"
94,function to the output if the optional <avalanche> argument equals 1. This
94,converter uses the same functions as used by the various hash-based load
94,"balancing algorithms, so it will provide exactly the same results. It is"
94,"mostly intended for debugging, but can be used as a stick-table entry to"
94,collect rough statistics. It must not be used for security purposes as a
94,"32-bit hash is trivial to break. See also ""crc32"", ""djb2"", ""sdbm"", and the"
94,"""hash-type"" directive."
94,"xor(<value>)Performs a bitwise ""XOR"" (exclusive OR) between <value> and the input value"
94,"of type signed integer, and returns the result as an signed integer."
94,<value> can be a numeric value or a variable name. The name of the variable
94,starts with an indication about its scope. The scopes allowed are:
94,"""proc"" : the variable is shared with the whole process"
94,"""sess"" : the variable is shared with the whole session"
94,"""txn"""
94,: the variable is shared with the transaction (request and
94,"response),"
94,"""req"""
94,": the variable is shared only during request processing,"
94,"""res"""
94,: the variable is shared only during response processing.
94,This prefix is followed by a name. The separator is a '.'. The name may only
94,"contain characters 'a-z', 'A-Z', '0-9', '.' and '_'."
94,xxh32([<seed>])Hashes a binary input sample into an unsigned 32-bit quantity using the 32-bit
94,variant of the XXHash hash function. This hash supports a seed which defaults
94,to zero but a different value maybe passed as the <seed> argument. This hash
94,is known to be very good and very fast so it can be used to hash URLs and/or
94,URL parameters for use as stick-table keys to collect statistics with a low
94,"collision rate, though care must be taken as the algorithm is not considered"
94,as cryptographically secure.
94,xxh64([<seed>])Hashes a binary input sample into a signed 64-bit quantity using the 64-bit
94,variant of the XXHash hash function. This hash supports a seed which defaults
94,to zero but a different value maybe passed as the <seed> argument. This hash
94,is known to be very good and very fast so it can be used to hash URLs and/or
94,URL parameters for use as stick-table keys to collect statistics with a low
94,"collision rate, though care must be taken as the algorithm is not considered"
94,as cryptographically secure.
94,7.3.2. Fetching samples from internal states
94,A first set of sample fetch methods applies to internal information which does
94,not even relate to any client information. These ones are sometimes used with
94,"""monitor-fail"" directives to report an internal status to external watchers."
94,The sample fetch methods described in this section are usable anywhere.
94,"always_false : booleanAlways returns the boolean ""false"" value. It may be used with ACLs as a"
94,temporary replacement for another one when adjusting configurations.
94,"always_true : booleanAlways returns the boolean ""true"" value. It may be used with ACLs as a"
94,temporary replacement for another one when adjusting configurations.
94,avg_queue([<backend>]) : integerReturns the total number of queued connections of the designated backend
94,divided by the number of active servers. The current backend is used if no
94,"backend is specified. This is very similar to ""queue"" except that the size of"
94,"the farm is considered, in order to give a more accurate measurement of the"
94,time it may take for a new connection to be processed. The main usage is with
94,ACL to return a sorry page to new users when it becomes certain they will get
94,"a degraded service, or to pass to the backend servers in a header so that"
94,they decide to work in degraded mode or to disable some functions to speed up
94,the processing a bit. Note that in the event there would not be any active
94,"server anymore, twice the number of queued connections would be considered as"
94,"the measured value. This is a fair estimate, as we expect one server to get"
94,"back soon anyway, but we still prefer to send new traffic to another backend"
94,"if in better shape. See also the ""queue"", ""be_conn"", and ""be_sess_rate"""
94,sample fetches.
94,"be_conn([<backend>]) : integerApplies to the number of currently established connections on the backend,"
94,possibly including the connection being evaluated. If no backend name is
94,"specified, the current one is used. But it is also possible to check another"
94,backend. It can be used to use a specific farm when the nominal one is full.
94,"See also the ""fe_conn"", ""queue"" and ""be_sess_rate"" criteria."
94,be_sess_rate([<backend>]) : integerReturns an integer value corresponding to the sessions creation rate on the
94,"backend, in number of new sessions per second. This is used with ACLs to"
94,switch to an alternate backend when an expensive or fragile one reaches too
94,"high a session rate, or to limit abuse of service (e.g. prevent sucking of an"
94,online dictionary). It can also be useful to add this element to logs using a
94,log-format directive.
94,Example :
94,# Redirect to an error page if the dictionary is requested too often
94,backend dynamic
94,mode http
94,acl being_scanned be_sess_rate gt 100
94,redirect location /denied.html if being_scanned
94,bin(<hex>) : binReturns a binary chain. The input is the hexadecimal representation
94,of the string.
94,"bool(<bool>) : boolReturns a boolean value. <bool> can be 'true', 'false', '1' or '0'."
94,'false' and '0' are the same. 'true' and '1' are the same.
94,connslots([<backend>]) : integerReturns an integer value corresponding to the number of connection slots
94,"still available in the backend, by totaling the maximum amount of"
94,connections on all servers and the maximum queue size. This is probably only
94,used with ACLs.
94,"The basic idea here is to be able to measure the number of connection ""slots"""
94,"still available (connection + queue), so that anything beyond that (intended"
94,"usage; see ""use_backend"" keyword) can be redirected to a different backend."
94,"'connslots' = number of available server connection slots, + number of"
94,available server queue slots.
94,"Note that while ""fe_conn"" may be used, ""connslots"" comes in especially"
94,"useful when you have a case of traffic going to one single ip, splitting into"
94,multiple backends (perhaps using ACLs to do name-based load balancing) and
94,"you want to be able to differentiate between different backends, and their"
94,"available ""connslots"". Also, whereas ""nbsrvThis keyword is available in sections :ConvertersFetching samples from internal states"" only measures servers that are"
94,"actually *down*, this fetch is more fine-grained and looks into the number of"
94,"available connection slots as well. See also ""queue"" and ""avg_queue""."
94,"OTHER CAVEATS AND NOTES: at this point in time, the code does not take care"
94,"of dynamic connections. Also, if any of the server maxconn, or maxqueue is 0,"
94,"then this fetch clearly does not make sense, in which case the value returned"
94,will be -1.
94,date([<offset>]) : integerReturns the current date as the epoch (number of seconds since 01/01/1970).
94,"If an offset value is specified, then it is a number of seconds that is added"
94,to the current date before returning the value. This is particularly useful
94,"to compute relative dates, as both positive and negative offsets are allowed."
94,It is useful combined with the http_date converter.
94,Example :
94,# set an expires header to now+1 hour in every response
94,"http-response set-header Expires %[date(3600),http_date]"
94,"distcc_body(<token>[,<occ>]) : binaryParses a distcc message and returns the body associated to occurrence #<occ>"
94,"of the token <token>. Occurrences start at 1, and when unspecified, any may"
94,match though in practice only the first one is checked for now. This can be
94,used to extract file names or arguments in files built using distcc through
94,haproxy. Please refer to distcc's protocol documentation for the complete
94,list of supported tokens.
94,"distcc_param(<token>[,<occ>]) : integerParses a distcc message and returns the parameter associated to occurrence"
94,"#<occ> of the token <token>. Occurrences start at 1, and when unspecified,"
94,any may match though in practice only the first one is checked for now. This
94,"can be used to extract certain information such as the protocol version, the"
94,file size or the argument in files built using distcc through haproxy.
94,Another use case consists in waiting for the start of the preprocessed file
94,contents before connecting to the server to avoid keeping idle connections.
94,Please refer to distcc's protocol documentation for the complete list of
94,supported tokens.
94,Example :
94,# wait up to 20s for the pre-processed file to be uploaded
94,tcp-request inspect-delay 20s
94,tcp-request content accept if { distcc_param(DOTI) -m found }
94,# send large files to the big farm
94,use_backend big_farm if { distcc_param(DOTI) gt 1000000 }
94,env(<name>) : stringReturns a string containing the value of environment variable <name>. As a
94,"reminder, environment variables are per-process and are sampled when the"
94,process starts. This can be useful to pass some information to a next hop
94,"server, or with ACLs to take specific action when the process is started a"
94,certain way.
94,Examples :
94,# Pass the Via header to next hop with the local hostname in it
94,http-request add-header Via 1.1\ %[env(HOSTNAME)]
94,# reject cookie-less requests when the STOP environment variable is set
94,http-request deny if !{ cook(SESSIONID) -m found } { env(STOP) -m found }
94,"fe_conn([<frontend>]) : integerReturns the number of currently established connections on the frontend,"
94,possibly including the connection being evaluated. If no frontend name is
94,"specified, the current one is used. But it is also possible to check another"
94,"frontend. It can be used to return a sorry page before hard-blocking, or to"
94,use a specific backend to drain new requests when the farm is considered
94,full. This is mostly used with ACLs but can also be used to pass some
94,"statistics to servers in HTTP headers. See also the ""dst_conn"", ""be_conn"","
94,"""fe_sess_rate"" fetches."
94,fe_req_rate([<frontend>]) : integerReturns an integer value corresponding to the number of HTTP requests per
94,"second sent to a frontend. This number can differ from ""fe_sess_rate"" in"
94,situations where client-side keep-alive is enabled.
94,fe_sess_rate([<frontend>]) : integerReturns an integer value corresponding to the sessions creation rate on the
94,"frontend, in number of new sessions per second. This is used with ACLs to"
94,limit the incoming session rate to an acceptable range in order to prevent
94,"abuse of service at the earliest moment, for example when combined with other"
94,layer 4 ACLs in order to force the clients to wait a bit for the rate to go
94,down below the limit. It can also be useful to add this element to logs using
94,"a log-format directive. See also the ""rate-limit sessions"" directive for use"
94,in frontends.
94,Example :
94,# This frontend limits incoming mails to 10/s with a max of 100
94,"# concurrent connections. We accept any connection below 10/s, and"
94,# force excess clients to wait for 100 ms. Since clients are limited to
94,"# 100 max, there cannot be more than 10 incoming mails per second."
94,frontend mail
94,bind :25
94,mode tcp
94,maxconn 100
94,acl too_fast fe_sess_rate ge 10
94,tcp-request inspect-delay 100ms
94,tcp-request content accept if ! too_fast
94,tcp-request content accept if WAIT_END
94,hostname : stringReturns the system hostname.
94,int(<integer>) : signed integerReturns a signed integer.
94,ipv4(<ipv4>) : ipv4Returns an ipv4.
94,ipv6(<ipv6>) : ipv6Returns an ipv6.
94,meth(<method>) : methodReturns a method.
94,nbproc : integerReturns an integer value corresponding to the number of processes that were
94,"started (it equals the global ""nbprocThis keyword is available in sections :Process management and securityFetching samples from internal states"" setting). This is useful for logging"
94,and debugging purposes.
94,nbsrv([<backend>]) : integerReturns an integer value corresponding to the number of usable servers of
94,either the current backend or the named backend. This is mostly used with
94,ACLs but can also be useful when added to logs. This is normally used to
94,switch to an alternate backend when the number of servers is too low to
94,to handle some load. It is useful to report a failure when combined with
94,"""monitor fail""."
94,proc : integerReturns an integer value corresponding to the position of the process calling
94,"the function, between 1 and global.nbproc. This is useful for logging and"
94,debugging purposes.
94,"queue([<backend>]) : integerReturns the total number of queued connections of the designated backend,"
94,including all the connections in server queues. If no backend name is
94,"specified, the current one is used, but it is also possible to check another"
94,one. This is useful with ACLs or to pass statistics to backend servers. This
94,"can be used to take actions when queuing goes above a known level, generally"
94,indicating a surge of traffic or a massive slowdown on the servers. One
94,possible action could be to reject new users but still accept old ones. See
94,"also the ""avg_queue"", ""be_conn"", and ""be_sess_rate"" fetches."
94,"rand([<range>]) : integerReturns a random integer value within a range of <range> possible values,"
94,"starting at zero. If the range is not specified, it defaults to 2^32, which"
94,gives numbers between 0 and 4294967295. It can be useful to pass some values
94,"needed to take some routing decisions for example, or just for debugging"
94,purposes. This random must not be used for security purposes.
94,srv_conn([<backend>/]<server>) : integerReturns an integer value corresponding to the number of currently established
94,"connections on the designated server, possibly including the connection being"
94,"evaluated. If <backend> is omitted, then the server is looked up in the"
94,current backend. It can be used to use a specific farm when one server is
94,"full, or to inform the server about our view of the number of active"
94,"connections with it. See also the ""fe_conn"", ""be_conn"" and ""queue"" fetch"
94,methods.
94,"srv_is_up([<backend>/]<server>) : booleanReturns true when the designated server is UP, and false when it is either"
94,"DOWN or in maintenance mode. If <backend> is omitted, then the server is"
94,looked up in the current backend. It is mainly used to take action based on
94,an external status reported via a health check (e.g. a geographical site's
94,availability). Another possible use which is more of a hack consists in
94,using dummy servers as boolean variables that can be enabled or disabled from
94,"the CLI, so that rules depending on those ACLs can be tweaked in realtime."
94,srv_queue([<backend>/]<server>) : integerReturns an integer value corresponding to the number of connections currently
94,"pending in the designated server's queue. If <backend> is omitted, then the"
94,server is looked up in the current backend. It can sometimes be used together
94,"with the ""use-server"" directive to force to use a known faster server when it"
94,"is not much loaded. See also the ""srv_conn"", ""avg_queue"" and ""queue"" sample"
94,fetch methods.
94,srv_sess_rate([<backend>/]<server>) : integerReturns an integer corresponding to the sessions creation rate on the
94,"designated server, in number of new sessions per second. If <backend> is"
94,"omitted, then the server is looked up in the current backend. This is mostly"
94,used with ACLs but can make sense with logs too. This is used to switch to an
94,alternate backend when an expensive or fragile one reaches too high a session
94,"rate, or to limit abuse of service (e.g. prevent latent requests from"
94,overloading servers).
94,Example :
94,# Redirect to a separate back
94,acl srv1_full srv_sess_rate(be1/srv1) gt 50
94,acl srv2_full srv_sess_rate(be1/srv2) gt 50
94,use_backend be2 if srv1_full or srv2_full
94,stopping : booleanReturns TRUE if the process calling the function is currently stopping. This
94,"can be useful for logging, or for relaxing certain checks or helping close"
94,certain connections upon graceful shutdown.
94,str(<string>) : stringReturns a string.
94,table_avl([<table>]) : integerReturns the total number of available entries in the current proxy's
94,stick-table or in the designated stick-table. See also table_cnt.
94,table_cnt([<table>]) : integerReturns the total number of entries currently in use in the current proxy's
94,stick-table or in the designated stick-table. See also src_conn_cnt and
94,table_avl for other entry counting methods.
94,thread : integerReturns an integer value corresponding to the position of the thread calling
94,"the function, between 0 and (global.nbthread-1). This is useful for logging"
94,and debugging purposes.
94,"var(<var-name>) : undefinedReturns a variable with the stored type. If the variable is not set, the"
94,sample fetch fails. The name of the variable starts with an indication
94,about its scope. The scopes allowed are:
94,"""proc"" : the variable is shared with the whole process"
94,"""sess"" : the variable is shared with the whole session"
94,"""txn"""
94,: the variable is shared with the transaction (request and
94,"response),"
94,"""req"""
94,": the variable is shared only during request processing,"
94,"""res"""
94,: the variable is shared only during response processing.
94,This prefix is followed by a name. The separator is a '.'. The name may only
94,"contain characters 'a-z', 'A-Z', '0-9', '.' and '_'."
94,7.3.3. Fetching samples at Layer 4
94,The layer 4 usually describes just the transport layer which in haproxy is
94,"closest to the connection, where no content is yet made available. The fetch"
94,"methods described here are usable as low as the ""tcp-request connection"" rule"
94,sets unless they require some future information. Those generally include
94,"TCP/IP addresses and ports, as well as elements from stick-tables related to"
94,"the incoming connection. For retrieving a value from a sticky counters, the"
94,"counter number can be explicitly set as 0, 1, or 2 using the pre-defined"
94,"""sc0_"", ""sc1_"", or ""sc2_"" prefix. These three pre-defined prefixes can only be"
94,"used if MAX_SESS_STKCTR value does not exceed 3, otherwise the counter number"
94,"can be specified as the first integer argument when using the ""sc_"" prefix."
94,"Starting from ""sc_0"" to ""sc_N"" where N is (MAX_SESS_STKCTR-1). An optional"
94,"table may be specified with the ""sc*"" form, in which case the currently"
94,tracked key will be looked up into this alternate table instead of the table
94,currently being tracked.
94,be_id : integerReturns an integer containing the current backend's id. It can be used in
94,frontends with responses to check which backend processed the request.
94,be_name : stringReturns a string containing the current backend's name. It can be used in
94,frontends with responses to check which backend processed the request.
94,"dst : ipThis is the destination IPv4 address of the connection on the client side,"
94,which is the address the client connected to. It can be useful when running
94,in transparent mode. It is of type IP and works on both IPv4 and IPv6 tables.
94,"On IPv6 tables, IPv4 address is mapped to its IPv6 equivalent, according to"
94,RFC 4291. When the incoming connection passed through address translation or
94,"redirection involving connection tracking, the original destination address"
94,"before the redirection will be reported. On Linux systems, the source and"
94,destination may seldom appear reversed if the nf_conntrack_tcp_loose sysctl
94,"is set, because a late response may reopen a timed out connection and switch"
94,what is believed to be the source and the destination.
94,dst_conn : integerReturns an integer value corresponding to the number of currently established
94,connections on the same socket including the one being evaluated. It is
94,normally used with ACLs but can as well be used to pass the information to
94,servers in an HTTP header or in logs. It can be used to either return a sorry
94,"page before hard-blocking, or to use a specific backend to drain new requests"
94,when the socket is considered saturated. This offers the ability to assign
94,different limits to different listening ports or addresses. See also the
94,"""fe_conn"" and ""be_conn"" fetches."
94,dst_is_local : booleanReturns true if the destination address of the incoming connection is local
94,"to the system, or false if the address doesn't exist on the system, meaning"
94,that it was intercepted in transparent mode. It can be useful to apply
94,certain rules by default to forwarded traffic and other rules to the traffic
94,targeting the real address of the machine. For example the stats page could
94,"be delivered only on this address, or SSH access could be locally redirected."
94,"Please note that the check involves a few system calls, so it's better to do"
94,it only once per connection.
94,dst_port : integerReturns an integer value corresponding to the destination TCP port of the
94,"connection on the client side, which is the port the client connected to."
94,"This might be used when running in transparent mode, when assigning dynamic"
94,"ports to some clients for a whole application session, to stick all users to"
94,"a same server, or to pass the destination port information to a server using"
94,an HTTP header.
94,"fc_http_major : integerReports the front connection's HTTP major version encoding, which may be 1"
94,"for HTTP/0.9 to HTTP/1.1 or 2 for HTTP/2. Note, this is based on the on-wire"
94,encoding and not on the version present in the request header.
94,fc_rcvd_proxy : booleanReturns true if the client initiated the connection with a PROXY protocol
94,header.
94,fc_rtt(<unit>) : integerReturns the Round Trip Time (RTT) measured by the kernel for the client
94,"connection. <unit> is facultative, by default the unit is milliseconds. <unit>"
94,"can be set to ""ms"" for milliseconds or ""us"" for microseconds. If the server"
94,"connection is not established, if the connection is not TCP or if the"
94,"operating system does not support TCP_INFO, for example Linux kernels before"
94,"2.4, the sample fetch fails."
94,fc_rttvar(<unit>) : integerReturns the Round Trip Time (RTT) variance measured by the kernel for the
94,"client connection. <unit> is facultative, by default the unit is milliseconds."
94,"<unit> can be set to ""ms"" for milliseconds or ""us"" for microseconds. If the"
94,"server connection is not established, if the connection is not TCP or if the"
94,"operating system does not support TCP_INFO, for example Linux kernels before"
94,"2.4, the sample fetch fails."
94,fc_unacked : integerReturns the unacked counter measured by the kernel for the client connection.
94,"If the server connection is not established, if the connection is not TCP or"
94,"if the operating system does not support TCP_INFO, for example Linux kernels"
94,"before 2.4, the sample fetch fails."
94,fc_sacked : integerReturns the sacked counter measured by the kernel for the client connection.
94,"If the server connection is not established, if the connection is not TCP or"
94,"if the operating system does not support TCP_INFO, for example Linux kernels"
94,"before 2.4, the sample fetch fails."
94,fc_retrans : integerReturns the retransmits counter measured by the kernel for the client
94,"connection. If the server connection is not established, if the connection is"
94,"not TCP or if the operating system does not support TCP_INFO, for example"
94,"Linux kernels before 2.4, the sample fetch fails."
94,fc_fackets : integerReturns the fack counter measured by the kernel for the client
94,"connection. If the server connection is not established, if the connection is"
94,"not TCP or if the operating system does not support TCP_INFO, for example"
94,"Linux kernels before 2.4, the sample fetch fails."
94,fc_lost : integerReturns the lost counter measured by the kernel for the client
94,"connection. If the server connection is not established, if the connection is"
94,"not TCP or if the operating system does not support TCP_INFO, for example"
94,"Linux kernels before 2.4, the sample fetch fails."
94,fc_reordering : integerReturns the reordering counter measured by the kernel for the client
94,"connection. If the server connection is not established, if the connection is"
94,"not TCP or if the operating system does not support TCP_INFO, for example"
94,"Linux kernels before 2.4, the sample fetch fails."
94,fe_id : integerReturns an integer containing the current frontend's id. It can be used in
94,"backends to check from which frontend it was called, or to stick all users"
94,coming via a same frontend to the same server.
94,fe_name : stringReturns a string containing the current frontend's name. It can be used in
94,"backends to check from which frontend it was called, or to stick all users"
94,coming via a same frontend to the same server.
94,"sc_bytes_in_rate(<ctr>[,<table>]) : integersc0_bytes_in_rate([<table>]) : integersc1_bytes_in_rate([<table>]) : integersc2_bytes_in_rate([<table>]) : integerReturns the average client-to-server bytes rate from the currently tracked"
94,"counters, measured in amount of bytes over the period configured in the"
94,table. See also src_bytes_in_rate.
94,"sc_bytes_out_rate(<ctr>[,<table>]) : integersc0_bytes_out_rate([<table>]) : integersc1_bytes_out_rate([<table>]) : integersc2_bytes_out_rate([<table>]) : integerReturns the average server-to-client bytes rate from the currently tracked"
94,"counters, measured in amount of bytes over the period configured in the"
94,table. See also src_bytes_out_rate.
94,"sc_clr_gpc0(<ctr>[,<table>]) : integersc0_clr_gpc0([<table>]) : integersc1_clr_gpc0([<table>]) : integersc2_clr_gpc0([<table>]) : integerClears the first General Purpose Counter associated to the currently tracked"
94,"counters, and returns its previous value. Before the first invocation, the"
94,"stored value is zero, so first invocation will always return zero. This is"
94,typically used as a second ACL in an expression in order to mark a connection
94,when a first ACL was verified :
94,Example:
94,# block if 5 consecutive requests continue to come faster than 10 sess
94,"# per second, and reset the counter as soon as the traffic slows down."
94,acl abuse sc0_http_req_rate gt 10
94,acl kill
94,sc0_inc_gpc0 gt 5
94,acl save
94,sc0_clr_gpc0 ge 0
94,tcp-request connection accept if !abuse save
94,tcp-request connection reject if abuse kill
94,"sc_conn_cnt(<ctr>[,<table>]) : integersc0_conn_cnt([<table>]) : integersc1_conn_cnt([<table>]) : integersc2_conn_cnt([<table>]) : integerReturns the cumulative number of incoming connections from currently tracked"
94,counters. See also src_conn_cnt.
94,"sc_conn_cur(<ctr>[,<table>]) : integersc0_conn_cur([<table>]) : integersc1_conn_cur([<table>]) : integersc2_conn_cur([<table>]) : integerReturns the current amount of concurrent connections tracking the same"
94,tracked counters. This number is automatically incremented when tracking
94,begins and decremented when tracking stops. See also src_conn_cur.
94,"sc_conn_rate(<ctr>[,<table>]) : integersc0_conn_rate([<table>]) : integersc1_conn_rate([<table>]) : integersc2_conn_rate([<table>]) : integerReturns the average connection rate from the currently tracked counters,"
94,measured in amount of connections over the period configured in the table.
94,See also src_conn_rate.
94,"sc_get_gpc0(<ctr>[,<table>]) : integersc0_get_gpc0([<table>]) : integersc1_get_gpc0([<table>]) : integersc2_get_gpc0([<table>]) : integerReturns the value of the first General Purpose Counter associated to the"
94,currently tracked counters. See also src_get_gpc0 and sc/sc0/sc1/sc2_inc_gpc0.
94,"sc_get_gpt0(<ctr>[,<table>]) : integersc0_get_gpt0([<table>]) : integersc1_get_gpt0([<table>]) : integersc2_get_gpt0([<table>]) : integerReturns the value of the first General Purpose Tag associated to the"
94,currently tracked counters. See also src_get_gpt0.
94,"sc_gpc0_rate(<ctr>[,<table>]) : integersc0_gpc0_rate([<table>]) : integersc1_gpc0_rate([<table>]) : integersc2_gpc0_rate([<table>]) : integerReturns the average increment rate of the first General Purpose Counter"
94,associated to the currently tracked counters. It reports the frequency
94,which the gpc0 counter was incremented over the configured period. See also
94,"src_gpc0_rate, sc/sc0/sc1/sc2_get_gpc0, and sc/sc0/sc1/sc2_inc_gpc0. Note"
94,"that the ""gpc0_rate"" counter must be stored in the stick-table for a value to"
94,"be returned, as ""gpc0"" only holds the event count."
94,"sc_http_err_cnt(<ctr>[,<table>]) : integersc0_http_err_cnt([<table>]) : integersc1_http_err_cnt([<table>]) : integersc2_http_err_cnt([<table>]) : integerReturns the cumulative number of HTTP errors from the currently tracked"
94,counters. This includes the both request errors and 4xx error responses.
94,See also src_http_err_cnt.
94,"sc_http_err_rate(<ctr>[,<table>]) : integersc0_http_err_rate([<table>]) : integersc1_http_err_rate([<table>]) : integersc2_http_err_rate([<table>]) : integerReturns the average rate of HTTP errors from the currently tracked counters,"
94,measured in amount of errors over the period configured in the table. This
94,includes the both request errors and 4xx error responses. See also
94,src_http_err_rate.
94,"sc_http_req_cnt(<ctr>[,<table>]) : integersc0_http_req_cnt([<table>]) : integersc1_http_req_cnt([<table>]) : integersc2_http_req_cnt([<table>]) : integerReturns the cumulative number of HTTP requests from the currently tracked"
94,"counters. This includes every started request, valid or not. See also"
94,src_http_req_cnt.
94,"sc_http_req_rate(<ctr>[,<table>]) : integersc0_http_req_rate([<table>]) : integersc1_http_req_rate([<table>]) : integersc2_http_req_rate([<table>]) : integerReturns the average rate of HTTP requests from the currently tracked"
94,"counters, measured in amount of requests over the period configured in"
94,"the table. This includes every started request, valid or not. See also"
94,src_http_req_rate.
94,"sc_inc_gpc0(<ctr>[,<table>]) : integersc0_inc_gpc0([<table>]) : integersc1_inc_gpc0([<table>]) : integersc2_inc_gpc0([<table>]) : integerIncrements the first General Purpose Counter associated to the currently"
94,"tracked counters, and returns its new value. Before the first invocation,"
94,"the stored value is zero, so first invocation will increase it to 1 and will"
94,return 1. This is typically used as a second ACL in an expression in order
94,to mark a connection when a first ACL was verified :
94,Example:
94,acl abuse sc0_http_req_rate gt 10
94,acl kill
94,sc0_inc_gpc0 gt 0
94,tcp-request connection reject if abuse kill
94,"sc_kbytes_in(<ctr>[,<table>]) : integersc0_kbytes_in([<table>]) : integersc1_kbytes_in([<table>]) : integersc2_kbytes_in([<table>]) : integerReturns the total amount of client-to-server data from the currently tracked"
94,"counters, measured in kilobytes. The test is currently performed on 32-bit"
94,"integers, which limits values to 4 terabytes. See also src_kbytes_in."
94,"sc_kbytes_out(<ctr>[,<table>]) : integersc0_kbytes_out([<table>]) : integersc1_kbytes_out([<table>]) : integersc2_kbytes_out([<table>]) : integerReturns the total amount of server-to-client data from the currently tracked"
94,"counters, measured in kilobytes. The test is currently performed on 32-bit"
94,"integers, which limits values to 4 terabytes. See also src_kbytes_out."
94,"sc_sess_cnt(<ctr>[,<table>]) : integersc0_sess_cnt([<table>]) : integersc1_sess_cnt([<table>]) : integersc2_sess_cnt([<table>]) : integerReturns the cumulative number of incoming connections that were transformed"
94,"into sessions, which means that they were accepted by a ""tcp-request"
94,"connection"" rule, from the currently tracked counters. A backend may count"
94,more sessions than connections because each connection could result in many
94,backend sessions if some HTTP keep-alive is performed over the connection
94,with the client. See also src_sess_cnt.
94,"sc_sess_rate(<ctr>[,<table>]) : integersc0_sess_rate([<table>]) : integersc1_sess_rate([<table>]) : integersc2_sess_rate([<table>]) : integerReturns the average session rate from the currently tracked counters,"
94,measured in amount of sessions over the period configured in the table. A
94,"session is a connection that got past the early ""tcp-request connection"""
94,rules. A backend may count more sessions than connections because each
94,connection could result in many backend sessions if some HTTP keep-alive is
94,performed over the connection with the client. See also src_sess_rate.
94,"sc_tracked(<ctr>[,<table>]) : booleansc0_tracked([<table>]) : booleansc1_tracked([<table>]) : booleansc2_tracked([<table>]) : booleanReturns true if the designated session counter is currently being tracked by"
94,the current session. This can be useful when deciding whether or not we want
94,to set some values in a header passed to the server.
94,"sc_trackers(<ctr>[,<table>]) : integersc0_trackers([<table>]) : integersc1_trackers([<table>]) : integersc2_trackers([<table>]) : integerReturns the current amount of concurrent connections tracking the same"
94,tracked counters. This number is automatically incremented when tracking
94,begins and decremented when tracking stops. It differs from sc0_conn_cur in
94,that it does not rely on any stored information but on the table's reference
94,"count (the ""use"" value which is returned by ""show table"" on the CLI). This"
94,may sometimes be more suited for layer7 tracking. It can be used to tell a
94,server how many concurrent connections there are from a given address for
94,example.
94,so_id : integerReturns an integer containing the current listening socket's id. It is useful
94,"in frontends involving many ""bind"" lines, or to stick all users coming via a"
94,same socket to the same server.
94,src : ipThis is the source IPv4 address of the client of the session. It is of type
94,"IP and works on both IPv4 and IPv6 tables. On IPv6 tables, IPv4 addresses are"
94,"mapped to their IPv6 equivalent, according to RFC 4291. Note that it is the"
94,"TCP-level source address which is used, and not the address of a client"
94,"behind a proxy. However if the ""accept-proxy"" or ""accept-netscaler-cip"" bind"
94,"directive is used, it can be the address of a client behind another"
94,PROXY-protocol compatible component for all rule sets except
94,"""tcp-request connection"" which sees the real address. When the incoming"
94,connection passed through address translation or redirection involving
94,"connection tracking, the original destination address before the redirection"
94,"will be reported. On Linux systems, the source and destination may seldom"
94,"appear reversed if the nf_conntrack_tcp_loose sysctl is set, because a late"
94,response may reopen a timed out connection and switch what is believed to be
94,the source and the destination.
94,Example:
94,# add an HTTP header in requests with the originating address' country
94,"http-request set-header X-Country %[src,map_ip(geoip.lst)]"
94,src_bytes_in_rate([<table>]) : integerReturns the average bytes rate from the incoming connection's source address
94,"in the current proxy's stick-table or in the designated stick-table, measured"
94,in amount of bytes over the period configured in the table. If the address is
94,"not found, zero is returned. See also sc/sc0/sc1/sc2_bytes_in_rate."
94,src_bytes_out_rate([<table>]) : integerReturns the average bytes rate to the incoming connection's source address in
94,"the current proxy's stick-table or in the designated stick-table, measured in"
94,amount of bytes over the period configured in the table. If the address is
94,"not found, zero is returned. See also sc/sc0/sc1/sc2_bytes_out_rate."
94,src_clr_gpc0([<table>]) : integerClears the first General Purpose Counter associated to the incoming
94,connection's source address in the current proxy's stick-table or in the
94,"designated stick-table, and returns its previous value. If the address is not"
94,"found, an entry is created and 0 is returned. This is typically used as a"
94,second ACL in an expression in order to mark a connection when a first ACL
94,was verified :
94,Example:
94,# block if 5 consecutive requests continue to come faster than 10 sess
94,"# per second, and reset the counter as soon as the traffic slows down."
94,acl abuse src_http_req_rate gt 10
94,acl kill
94,src_inc_gpc0 gt 5
94,acl save
94,src_clr_gpc0 ge 0
94,tcp-request connection accept if !abuse save
94,tcp-request connection reject if abuse kill
94,src_conn_cnt([<table>]) : integerReturns the cumulative number of connections initiated from the current
94,incoming connection's source address in the current proxy's stick-table or in
94,"the designated stick-table. If the address is not found, zero is returned."
94,See also sc/sc0/sc1/sc2_conn_cnt.
94,src_conn_cur([<table>]) : integerReturns the current amount of concurrent connections initiated from the
94,current incoming connection's source address in the current proxy's
94,"stick-table or in the designated stick-table. If the address is not found,"
94,zero is returned. See also sc/sc0/sc1/sc2_conn_cur.
94,src_conn_rate([<table>]) : integerReturns the average connection rate from the incoming connection's source
94,"address in the current proxy's stick-table or in the designated stick-table,"
94,measured in amount of connections over the period configured in the table. If
94,"the address is not found, zero is returned. See also sc/sc0/sc1/sc2_conn_rate."
94,src_get_gpc0([<table>]) : integerReturns the value of the first General Purpose Counter associated to the
94,incoming connection's source address in the current proxy's stick-table or in
94,"the designated stick-table. If the address is not found, zero is returned."
94,See also sc/sc0/sc1/sc2_get_gpc0 and src_inc_gpc0.
94,src_get_gpt0([<table>]) : integerReturns the value of the first General Purpose Tag associated to the
94,incoming connection's source address in the current proxy's stick-table or in
94,"the designated stick-table. If the address is not found, zero is returned."
94,See also sc/sc0/sc1/sc2_get_gpt0.
94,src_gpc0_rate([<table>]) : integerReturns the average increment rate of the first General Purpose Counter
94,associated to the incoming connection's source address in the current proxy's
94,stick-table or in the designated stick-table. It reports the frequency
94,which the gpc0 counter was incremented over the configured period. See also
94,"sc/sc0/sc1/sc2_gpc0_rate, src_get_gpc0, and sc/sc0/sc1/sc2_inc_gpc0. Note"
94,"that the ""gpc0_rate"" counter must be stored in the stick-table for a value to"
94,"be returned, as ""gpc0"" only holds the event count."
94,src_http_err_cnt([<table>]) : integerReturns the cumulative number of HTTP errors from the incoming connection's
94,source address in the current proxy's stick-table or in the designated
94,stick-table. This includes the both request errors and 4xx error responses.
94,"See also sc/sc0/sc1/sc2_http_err_cnt. If the address is not found, zero is"
94,returned.
94,src_http_err_rate([<table>]) : integerReturns the average rate of HTTP errors from the incoming connection's source
94,"address in the current proxy's stick-table or in the designated stick-table,"
94,measured in amount of errors over the period configured in the table. This
94,includes the both request errors and 4xx error responses. If the address is
94,"not found, zero is returned. See also sc/sc0/sc1/sc2_http_err_rate."
94,src_http_req_cnt([<table>]) : integerReturns the cumulative number of HTTP requests from the incoming connection's
94,source address in the current proxy's stick-table or in the designated stick-
94,"table. This includes every started request, valid or not. If the address is"
94,"not found, zero is returned. See also sc/sc0/sc1/sc2_http_req_cnt."
94,src_http_req_rate([<table>]) : integerReturns the average rate of HTTP requests from the incoming connection's
94,source address in the current proxy's stick-table or in the designated stick-
94,"table, measured in amount of requests over the period configured in the"
94,"table. This includes every started request, valid or not. If the address is"
94,"not found, zero is returned. See also sc/sc0/sc1/sc2_http_req_rate."
94,src_inc_gpc0([<table>]) : integerIncrements the first General Purpose Counter associated to the incoming
94,connection's source address in the current proxy's stick-table or in the
94,"designated stick-table, and returns its new value. If the address is not"
94,"found, an entry is created and 1 is returned. See also sc0/sc2/sc2_inc_gpc0."
94,This is typically used as a second ACL in an expression in order to mark a
94,connection when a first ACL was verified :
94,Example:
94,acl abuse src_http_req_rate gt 10
94,acl kill
94,src_inc_gpc0 gt 0
94,tcp-request connection reject if abuse kill
94,src_is_local : booleanReturns true if the source address of the incoming connection is local to the
94,"system, or false if the address doesn't exist on the system, meaning that it"
94,comes from a remote machine. Note that UNIX addresses are considered local.
94,It can be useful to apply certain access restrictions based on where the
94,client comes from (e.g. require auth or https for remote machines). Please
94,"note that the check involves a few system calls, so it's better to do it only"
94,once per connection.
94,src_kbytes_in([<table>]) : integerReturns the total amount of data received from the incoming connection's
94,source address in the current proxy's stick-table or in the designated
94,"stick-table, measured in kilobytes. If the address is not found, zero is"
94,"returned. The test is currently performed on 32-bit integers, which limits"
94,values to 4 terabytes. See also sc/sc0/sc1/sc2_kbytes_in.
94,src_kbytes_out([<table>]) : integerReturns the total amount of data sent to the incoming connection's source
94,"address in the current proxy's stick-table or in the designated stick-table,"
94,"measured in kilobytes. If the address is not found, zero is returned. The"
94,"test is currently performed on 32-bit integers, which limits values to 4"
94,terabytes. See also sc/sc0/sc1/sc2_kbytes_out.
94,src_port : integerReturns an integer value corresponding to the TCP source port of the
94,"connection on the client side, which is the port the client connected from."
94,Usage of this function is very limited as modern protocols do not care much
94,about source ports nowadays.
94,src_sess_cnt([<table>]) : integerReturns the cumulative number of connections initiated from the incoming
94,connection's source IPv4 address in the current proxy's stick-table or in the
94,"designated stick-table, that were transformed into sessions, which means that"
94,"they were accepted by ""tcp-request"" rules. If the address is not found, zero"
94,is returned. See also sc/sc0/sc1/sc2_sess_cnt.
94,src_sess_rate([<table>]) : integerReturns the average session rate from the incoming connection's source
94,"address in the current proxy's stick-table or in the designated stick-table,"
94,measured in amount of sessions over the period configured in the table. A
94,"session is a connection that went past the early ""tcp-request"" rules. If the"
94,"address is not found, zero is returned. See also sc/sc0/sc1/sc2_sess_rate."
94,src_updt_conn_cnt([<table>]) : integerCreates or updates the entry associated to the incoming connection's source
94,address in the current proxy's stick-table or in the designated stick-table.
94,"This table must be configured to store the ""conn_cnt"" data type, otherwise"
94,"the match will be ignored. The current count is incremented by one, and the"
94,"expiration timer refreshed. The updated count is returned, so this match"
94,can't return zero. This was used to reject service abusers based on their
94,"source address. Note: it is recommended to use the more complete ""track-sc*"""
94,"actions in ""tcp-request"" rules instead."
94,Example :
94,# This frontend limits incoming SSH connections to 3 per 10 second for
94,"# each source address, and rejects excess connections until a 10 second"
94,# silence is observed. At most 20 addresses are tracked.
94,listen ssh
94,bind :22
94,mode tcp
94,maxconn 100
94,stick-table type ip size 20 expire 10s store conn_cnt
94,tcp-request content reject if { src_updt_conn_cnt gt 3 }
94,server local 127.0.0.1:22
94,srv_id : integerReturns an integer containing the server's id when processing the response.
94,"While it's almost only used with ACLs, it may be used for logging or"
94,debugging.
94,7.3.4. Fetching samples at Layer 5
94,The layer 5 usually describes just the session layer which in haproxy is
94,"closest to the session once all the connection handshakes are finished, but"
94,when no content is yet made available. The fetch methods described here are
94,"usable as low as the ""tcp-request content"" rule sets unless they require some"
94,future information. Those generally include the results of SSL negotiations.
94,"51d.all(<prop>[,<prop>*]) : stringReturns values for the properties requested as a string, where values are"
94,"separated by the delimiter specified with ""51degrees-property-separator""."
94,The device is identified using all the important HTTP headers from the
94,"request. The function can be passed up to five property names, and if a"
94,"property name can't be found, the value ""NoData"" is returned."
94,Example :
94,"# Here the header ""X-51D-DeviceTypeMobileTablet"" is added to the request"
94,# containing the three properties requested using all relevant headers from
94,# the request.
94,frontend http-in
94,bind *:8081
94,default_backend servers
94,http-request set-header X-51D-DeviceTypeMobileTablet \
94,"%[51d.all(DeviceType,IsMobile,IsTablet)]"
94,ssl_bc : booleanReturns true when the back connection was made via an SSL/TLS transport
94,layer and is locally deciphered. This means the outgoing connection was made
94,"other a server with the ""sslThis keyword is available in sections :Bind optionsServer and default-server options"" option."
94,ssl_bc_alg_keysize : integerReturns the symmetric cipher key size supported in bits when the outgoing
94,connection was made over an SSL/TLS transport layer.
94,ssl_bc_cipher : stringReturns the name of the used cipher when the outgoing connection was made
94,over an SSL/TLS transport layer.
94,ssl_bc_protocol : stringReturns the name of the used protocol when the outgoing connection was made
94,over an SSL/TLS transport layer.
94,"ssl_bc_unique_id : binaryWhen the outgoing connection was made over an SSL/TLS transport layer,"
94,returns the TLS unique ID as defined in RFC5929 section 3. The unique id
94,"can be encoded to base64 using the converter: ""ssl_bc_unique_id,base64""."
94,ssl_bc_session_id : binaryReturns the SSL ID of the back connection when the outgoing connection was
94,made over an SSL/TLS transport layer. It is useful to log if we want to know
94,if session was reused or not.
94,ssl_bc_use_keysize : integerReturns the symmetric cipher key size used in bits when the outgoing
94,connection was made over an SSL/TLS transport layer.
94,"ssl_c_ca_err : integerWhen the incoming connection was made over an SSL/TLS transport layer,"
94,returns the ID of the first error detected during verification of the client
94,"certificate at depth > 0, or 0 if no error was encountered during this"
94,verification process. Please refer to your SSL library's documentation to
94,find the exhaustive list of error codes.
94,"ssl_c_ca_err_depth : integerWhen the incoming connection was made over an SSL/TLS transport layer,"
94,returns the depth in the CA chain of the first error detected during the
94,"verification of the client certificate. If no error is encountered, 0 is"
94,returned.
94,ssl_c_der : binaryReturns the DER formatted certificate presented by the client when the
94,incoming connection was made over an SSL/TLS transport layer. When used for
94,"an ACL, the value(s) to match against can be passed in hexadecimal form."
94,"ssl_c_err : integerWhen the incoming connection was made over an SSL/TLS transport layer,"
94,"returns the ID of the first error detected during verification at depth 0, or"
94,0 if no error was encountered during this verification process. Please refer
94,to your SSL library's documentation to find the exhaustive list of error
94,codes.
94,"ssl_c_i_dn([<entry>[,<occ>]]) : stringWhen the incoming connection was made over an SSL/TLS transport layer,"
94,returns the full distinguished name of the issuer of the certificate
94,"presented by the client when no <entry> is specified, or the value of the"
94,first given entry found from the beginning of the DN. If a positive/negative
94,"occurrence number is specified as the optional second argument, it returns"
94,the value of the nth given entry value from the beginning/end of the DN.
94,"For instance, ""ssl_c_i_dn(OU,2)"" the second organization unit, and"
94,"""ssl_c_i_dn(CN)"" retrieves the common name."
94,ssl_c_key_alg : stringReturns the name of the algorithm used to generate the key of the certificate
94,presented by the client when the incoming connection was made over an SSL/TLS
94,transport layer.
94,ssl_c_notafter : stringReturns the end date presented by the client as a formatted string
94,YYMMDDhhmmss[Z] when the incoming connection was made over an SSL/TLS
94,transport layer.
94,ssl_c_notbefore : stringReturns the start date presented by the client as a formatted string
94,YYMMDDhhmmss[Z] when the incoming connection was made over an SSL/TLS
94,transport layer.
94,"ssl_c_s_dn([<entry>[,<occ>]]) : stringWhen the incoming connection was made over an SSL/TLS transport layer,"
94,returns the full distinguished name of the subject of the certificate
94,"presented by the client when no <entry> is specified, or the value of the"
94,first given entry found from the beginning of the DN. If a positive/negative
94,"occurrence number is specified as the optional second argument, it returns"
94,the value of the nth given entry value from the beginning/end of the DN.
94,"For instance, ""ssl_c_s_dn(OU,2)"" the second organization unit, and"
94,"""ssl_c_s_dn(CN)"" retrieves the common name."
94,ssl_c_serial : binaryReturns the serial of the certificate presented by the client when the
94,incoming connection was made over an SSL/TLS transport layer. When used for
94,"an ACL, the value(s) to match against can be passed in hexadecimal form."
94,ssl_c_sha1 : binaryReturns the SHA-1 fingerprint of the certificate presented by the client when
94,the incoming connection was made over an SSL/TLS transport layer. This can be
94,"used to stick a client to a server, or to pass this information to a server."
94,"Note that the output is binary, so if you want to pass that signature to the"
94,"server, you need to encode it in hex or base64, such as in the example below:"
94,Example:
94,"http-request set-header X-SSL-Client-SHA1 %[ssl_c_sha1,hex]"
94,ssl_c_sig_alg : stringReturns the name of the algorithm used to sign the certificate presented by
94,the client when the incoming connection was made over an SSL/TLS transport
94,layer.
94,ssl_c_used : booleanReturns true if current SSL session uses a client certificate even if current
94,"connection uses SSL session resumption. See also ""ssl_fc_has_crt""."
94,ssl_c_verify : integerReturns the verify result error ID when the incoming connection was made over
94,"an SSL/TLS transport layer, otherwise zero if no error is encountered. Please"
94,refer to your SSL library's documentation for an exhaustive list of error
94,codes.
94,ssl_c_version : integerReturns the version of the certificate presented by the client when the
94,incoming connection was made over an SSL/TLS transport layer.
94,ssl_f_der : binaryReturns the DER formatted certificate presented by the frontend when the
94,incoming connection was made over an SSL/TLS transport layer. When used for
94,"an ACL, the value(s) to match against can be passed in hexadecimal form."
94,"ssl_f_i_dn([<entry>[,<occ>]]) : stringWhen the incoming connection was made over an SSL/TLS transport layer,"
94,returns the full distinguished name of the issuer of the certificate
94,"presented by the frontend when no <entry> is specified, or the value of the"
94,first given entry found from the beginning of the DN. If a positive/negative
94,"occurrence number is specified as the optional second argument, it returns"
94,the value of the nth given entry value from the beginning/end of the DN.
94,"For instance, ""ssl_f_i_dn(OU,2)"" the second organization unit, and"
94,"""ssl_f_i_dn(CN)"" retrieves the common name."
94,ssl_f_key_alg : stringReturns the name of the algorithm used to generate the key of the certificate
94,presented by the frontend when the incoming connection was made over an
94,SSL/TLS transport layer.
94,ssl_f_notafter : stringReturns the end date presented by the frontend as a formatted string
94,YYMMDDhhmmss[Z] when the incoming connection was made over an SSL/TLS
94,transport layer.
94,ssl_f_notbefore : stringReturns the start date presented by the frontend as a formatted string
94,YYMMDDhhmmss[Z] when the incoming connection was made over an SSL/TLS
94,transport layer.
94,"ssl_f_s_dn([<entry>[,<occ>]]) : stringWhen the incoming connection was made over an SSL/TLS transport layer,"
94,returns the full distinguished name of the subject of the certificate
94,"presented by the frontend when no <entry> is specified, or the value of the"
94,first given entry found from the beginning of the DN. If a positive/negative
94,"occurrence number is specified as the optional second argument, it returns"
94,the value of the nth given entry value from the beginning/end of the DN.
94,"For instance, ""ssl_f_s_dn(OU,2)"" the second organization unit, and"
94,"""ssl_f_s_dn(CN)"" retrieves the common name."
94,ssl_f_serial : binaryReturns the serial of the certificate presented by the frontend when the
94,incoming connection was made over an SSL/TLS transport layer. When used for
94,"an ACL, the value(s) to match against can be passed in hexadecimal form."
94,ssl_f_sha1 : binaryReturns the SHA-1 fingerprint of the certificate presented by the frontend
94,when the incoming connection was made over an SSL/TLS transport layer. This
94,can be used to know which certificate was chosen using SNI.
94,ssl_f_sig_alg : stringReturns the name of the algorithm used to sign the certificate presented by
94,the frontend when the incoming connection was made over an SSL/TLS transport
94,layer.
94,ssl_f_version : integerReturns the version of the certificate presented by the frontend when the
94,incoming connection was made over an SSL/TLS transport layer.
94,ssl_fc : booleanReturns true when the front connection was made via an SSL/TLS transport
94,layer and is locally deciphered. This means it has matched a socket declared
94,"with a ""bind"" line having the ""sslThis keyword is available in sections :Bind optionsServer and default-server options"" option."
94,Example :
94,"# This passes ""X-Proto: https"" to servers when client connects over SSL"
94,listen http-https
94,bind :80
94,bind :443 ssl crt /etc/haproxy.pem
94,http-request add-header X-Proto https if { ssl_fc }
94,ssl_fc_alg_keysize : integerReturns the symmetric cipher key size supported in bits when the incoming
94,connection was made over an SSL/TLS transport layer.
94,ssl_fc_alpn : stringThis extracts the Application Layer Protocol Negotiation field from an
94,incoming connection made via a TLS transport layer and locally deciphered by
94,haproxy. The result is a string containing the protocol name advertised by
94,the client. The SSL library must have been built with support for TLS
94,extensions enabled (check haproxy -vv). Note that the TLS ALPN extension is
94,"not advertised unless the ""alpn"" keyword on the ""bind"" line specifies a"
94,"protocol list. Also, nothing forces the client to pick a protocol from this"
94,"list, any other one may be requested. The TLS ALPN extension is meant to"
94,"replace the TLS NPN extension. See also ""ssl_fc_npn""."
94,ssl_fc_cipher : stringReturns the name of the used cipher when the incoming connection was made
94,over an SSL/TLS transport layer.
94,ssl_fc_cipherlist_bin : binaryReturns the binary form of the client hello cipher list. The maximum returned
94,value length is according with the value of
94,"""tune.ssl.capture-cipherlist-size""."
94,ssl_fc_cipherlist_hex : stringReturns the binary form of the client hello cipher list encoded as
94,hexadecimal. The maximum returned value length is according with the value of
94,"""tune.ssl.capture-cipherlist-size""."
94,ssl_fc_cipherlist_str : stringReturns the decoded text form of the client hello cipher list. The maximum
94,number of ciphers returned is according with the value of
94,"""tune.ssl.capture-cipherlist-size"". Note that this sample-fetch is only"
94,"available with OpenSSL >= 1.0.2. If the function is not enabled, this"
94,"sample-fetch returns the hash like ""ssl_fc_cipherlist_xxh""."
94,ssl_fc_cipherlist_xxh : integerReturns a xxh64 of the cipher list. This hash can be return only is the value
94,"""tune.ssl.capture-cipherlist-size"" is set greater than 0, however the hash"
94,take in account all the data of the cipher list.
94,ssl_fc_has_crt : booleanReturns true if a client certificate is present in an incoming connection over
94,SSL/TLS transport layer. Useful if 'verify' statement is set to 'optional'.
94,"Note: on SSL session resumption with Session ID or TLS ticket, client"
94,certificate is not present in the current connection but may be retrieved
94,"from the cache or the ticket. So prefer ""ssl_c_used"" if you want to check if"
94,current SSL session uses a client certificate.
94,"ssl_fc_has_early : booleanReturns true if early data were sent, and the handshake didn't happen yet. As"
94,"it has security implications, it is useful to be able to refuse those, or"
94,wait until the handshake happened.
94,ssl_fc_has_sni : booleanThis checks for the presence of a Server Name Indication TLS extension (SNI)
94,in an incoming connection was made over an SSL/TLS transport layer. Returns
94,true when the incoming connection presents a TLS SNI field. This requires
94,that the SSL library is build with support for TLS extensions enabled (check
94,haproxy -vv).
94,ssl_fc_is_resumed : booleanReturns true if the SSL/TLS session has been resumed through the use of
94,SSL session cache or TLS tickets on an incoming connection over an SSL/TLS
94,transport layer.
94,ssl_fc_npn : stringThis extracts the Next Protocol Negotiation field from an incoming connection
94,made via a TLS transport layer and locally deciphered by haproxy. The result
94,is a string containing the protocol name advertised by the client. The SSL
94,library must have been built with support for TLS extensions enabled (check
94,haproxy -vv). Note that the TLS NPN extension is not advertised unless the
94,"""npn"" keyword on the ""bind"" line specifies a protocol list. Also, nothing"
94,"forces the client to pick a protocol from this list, any other one may be"
94,requested. Please note that the TLS NPN extension was replaced with ALPN.
94,ssl_fc_protocol : stringReturns the name of the used protocol when the incoming connection was made
94,over an SSL/TLS transport layer.
94,"ssl_fc_unique_id : binaryWhen the incoming connection was made over an SSL/TLS transport layer,"
94,returns the TLS unique ID as defined in RFC5929 section 3. The unique id
94,"can be encoded to base64 using the converter: ""ssl_bc_unique_id,base64""."
94,ssl_fc_session_id : binaryReturns the SSL ID of the front connection when the incoming connection was
94,made over an SSL/TLS transport layer. It is useful to stick a given client to
94,a server. It is important to note that some browsers refresh their session ID
94,every few minutes.
94,ssl_fc_sni : stringThis extracts the Server Name Indication TLS extension (SNI) field from an
94,incoming connection made via an SSL/TLS transport layer and locally
94,deciphered by haproxy. The result (when present) typically is a string
94,matching the HTTPS host name (253 chars or less). The SSL library must have
94,been built with support for TLS extensions enabled (check haproxy -vv).
94,"This fetch is different from ""req_ssl_sni"" above in that it applies to the"
94,connection being deciphered by haproxy and not to SSL contents being blindly
94,"forwarded. See also ""ssl_fc_sni_end"" and ""ssl_fc_sni_reg"" below. This"
94,requires that the SSL library is build with support for TLS extensions
94,enabled (check haproxy -vv).
94,ACL derivatives :
94,ssl_fc_sni_end : suffix match
94,ssl_fc_sni_reg : regex match
94,ssl_fc_use_keysize : integerReturns the symmetric cipher key size used in bits when the incoming
94,connection was made over an SSL/TLS transport layer.
94,7.3.5. Fetching samples from buffer contents (Layer 6)
94,Fetching samples from buffer contents is a bit different from the previous
94,sample fetches above because the sampled data are ephemeral. These data can
94,only be used when they're available and will be lost when they're forwarded.
94,"For this reason, samples fetched from buffer contents during a request cannot"
94,"be used in a response for example. Even while the data are being fetched, they"
94,can change. Sometimes it is necessary to set some delays or combine multiple
94,"sample fetch methods to ensure that the expected data are complete and usable,"
94,"for example through TCP request content inspection. Please see the ""tcp-request"
94,"content"" keyword for more detailed information on the subject."
94,"payload(<offset>,<length>) : binary (deprecated)This is an alias for ""req.payload"" when used in the context of a request (e.g."
94,"""stick on"", ""stick match""), and for ""res.payload"" when used in the context of"
94,"a response such as in ""stick store response""."
94,"payload_lv(<offset1>,<length>[,<offset2>]) : binary (deprecated)This is an alias for ""req.payload_lv"" when used in the context of a request"
94,"(e.g. ""stick on"", ""stick match""), and for ""res.payload_lv"" when used in the"
94,"context of a response such as in ""stick store response""."
94,req.hdrs : stringReturns the current request headers as string including the last empty line
94,separating headers from the request body. The last empty line can be used to
94,detect a truncated header block. This sample fetch is useful for some SPOE
94,headers analyzers and for advanced logging.
94,req.hdrs_bin : binaryReturns the current request headers contained in preparsed binary form. This
94,is useful for offloading some processing with SPOE. Each string is described
94,by a length followed by the number of bytes indicated in the length. The
94,length is represented using the variable integer encoding detailed in the
94,SPOE documentation. The end of the list is marked by a couple of empty header
94,names and values (length of 0 for both).
94,*(<str:header-name><str:header-value>)<empty string><empty string>
94,int:
94,refer to the SPOE documentation for the encoding
94,str:
94,<int:length><bytes>
94,req.len : integerreq_len : integer (deprecated)Returns an integer value corresponding to the number of bytes present in the
94,request buffer. This is mostly used in ACL. It is important to understand
94,that this test does not return false as long as the buffer is changing. This
94,means that a check with equality to zero will almost always immediately match
94,"at the beginning of the session, while a test for more data will wait for"
94,that data to come in and return false only when haproxy is certain that no
94,more data will come in. This test was designed to be used with TCP request
94,content inspection.
94,"req.payload(<offset>,<length>) : binaryThis extracts a binary block of <length> bytes and starting at byte <offset>"
94,"in the request buffer. As a special case, if the <length> argument is zero,"
94,the the whole buffer from <offset> to the end is extracted. This can be used
94,with ACLs in order to check for the presence of some content in a buffer at
94,any location.
94,ACL alternatives :
94,"payload(<offset>,<length>) : hex binary match"
94,"req.payload_lv(<offset1>,<length>[,<offset2>]) : binaryThis extracts a binary block whose size is specified at <offset1> for <length>"
94,"bytes, and which starts at <offset2> if specified or just after the length in"
94,the request buffer. The <offset2> parameter also supports relative offsets if
94,prepended with a '+' or '-' sign.
94,ACL alternatives :
94,"payload_lv(<offset1>,<length>[,<offset2>]) : hex binary match"
94,Example :
94,"please consult the example from the ""stick store-response"" keyword."
94,req.proto_http : booleanreq_proto_http : boolean (deprecated)Returns true when data in the request buffer look like HTTP and correctly
94,parses as such. It is the same parser as the common HTTP request parser which
94,is used so there should be no surprises. The test does not match until the
94,"request is complete, failed or timed out. This test may be used to report the"
94,"protocol in TCP logs, but the biggest use is to block TCP request analysis"
94,"until a complete HTTP request is present in the buffer, for example to track"
94,a header.
94,Example:
94,"# track request counts per ""base"" (concatenation of Host+URL)"
94,tcp-request inspect-delay 10s
94,tcp-request content reject if !HTTP
94,tcp-request content track-sc0 base table req-rate
94,"req.rdp_cookie([<name>]) : stringrdp_cookie([<name>]) : string (deprecated)When the request buffer looks like the RDP protocol, extracts the RDP cookie"
94,"<name>, or any cookie if unspecified. The parser only checks for the first"
94,"cookie, as illustrated in the RDP protocol specification. The cookie name is"
94,"case insensitive. Generally the ""MSTS"" cookie name will be used, as it can"
94,contain the user name of the client connecting to the server if properly
94,"configured on the client. The ""MSTSHASH"" cookie is often used as well for"
94,session stickiness to servers.
94,"This differs from ""balance rdp-cookie"" in that any balancing algorithm may be"
94,used and thus the distribution of clients to backend servers is not linked to
94,a hash of the RDP cookie. It is envisaged that using a balancing algorithm
94,"such as ""balance roundrobin"" or ""balance leastconn"" will lead to a more even"
94,"distribution of clients to backend servers than the hash used by ""balance"
94,"rdp-cookie""."
94,ACL derivatives :
94,req_rdp_cookie([<name>]) : exact string match
94,Example :
94,listen tse-farm
94,bind 0.0.0.0:3389
94,# wait up to 5s for an RDP cookie in the request
94,tcp-request inspect-delay 5s
94,tcp-request content accept if RDP_COOKIE
94,# apply RDP cookie persistence
94,persist rdp-cookie
94,# Persist based on the mstshash cookie
94,# This is only useful makes sense if
94,# balance rdp-cookie is not used
94,stick-table type string size 204800
94,stick on req.rdp_cookie(mstshash)
94,server srv1 1.1.1.1:3389
94,server srv1 1.1.1.2:3389
94,"See also : ""balance rdp-cookie"", ""persist rdp-cookie"", ""tcp-request"" and the ""req_rdp_cookie"" ACL."
94,"req.rdp_cookie_cnt([name]) : integerrdp_cookie_cnt([name]) : integer (deprecated)Tries to parse the request buffer as RDP protocol, then returns an integer"
94,corresponding to the number of RDP cookies found. If an optional cookie name
94,"is passed, only cookies matching this name are considered. This is mostly"
94,used in ACL.
94,ACL derivatives :
94,req_rdp_cookie_cnt([<name>]) : integer match
94,req.ssl_ec_ext : booleanReturns a boolean identifying if client sent the Supported Elliptic Curves
94,"Extension as defined in RFC4492, section 5.1. within the SSL ClientHello"
94,message. This can be used to present ECC compatible clients with EC
94,"certificate and to use RSA for all others, on the same IP address. Note that"
94,this only applies to raw contents found in the request buffer and not to
94,"contents deciphered via an SSL data layer, so this will not work with ""bind"""
94,"lines having the ""sslThis keyword is available in sections :Bind optionsServer and default-server options"" option."
94,req.ssl_hello_type : integerreq_ssl_hello_type : integer (deprecated)Returns an integer value containing the type of the SSL hello message found
94,in the request buffer if the buffer contains data that parse as a complete
94,SSL (v3 or superior) client hello message. Note that this only applies to raw
94,contents found in the request buffer and not to contents deciphered via an
94,"SSL data layer, so this will not work with ""bind"" lines having the ""sslThis keyword is available in sections :Bind optionsServer and default-server options"""
94,option. This is mostly used in ACL to detect presence of an SSL hello message
94,that is supposed to contain an SSL session ID usable for stickiness.
94,req.ssl_sni : stringreq_ssl_sni : string (deprecated)Returns a string containing the value of the Server Name TLS extension sent
94,by a client in a TLS stream passing through the request buffer if the buffer
94,contains data that parse as a complete SSL (v3 or superior) client hello
94,message. Note that this only applies to raw contents found in the request
94,"buffer and not to contents deciphered via an SSL data layer, so this will not"
94,"work with ""bind"" lines having the ""sslThis keyword is available in sections :Bind optionsServer and default-server options"" option. SNI normally contains the"
94,name of the host the client tries to connect to (for recent browsers). SNI is
94,useful for allowing or denying access to certain hosts when SSL/TLS is used
94,by the client. This test was designed to be used with TCP request content
94,"inspection. If content switching is needed, it is recommended to first wait"
94,"for a complete client hello (type 1), like in the example below. See also"
94,"""ssl_fc_sni""."
94,ACL derivatives :
94,req_ssl_sni : exact string match
94,Examples :
94,# Wait for a client hello for at most 5 seconds
94,tcp-request inspect-delay 5s
94,tcp-request content accept if { req_ssl_hello_type 1 }
94,use_backend bk_allow if { req_ssl_sni -f allowed_sites }
94,default_backend bk_sorry_page
94,req.ssl_st_ext : integerReturns 0 if the client didn't send a SessionTicket TLS Extension (RFC5077)
94,Returns 1 if the client sent SessionTicket TLS Extension
94,Returns 2 if the client also sent non-zero length TLS SessionTicket
94,Note that this only applies to raw contents found in the request buffer and
94,"not to contents deciphered via an SSL data layer, so this will not work with"
94,"""bind"" lines having the ""sslThis keyword is available in sections :Bind optionsServer and default-server options"" option. This can for example be used to detect"
94,"whether the client sent a SessionTicket or not and stick it accordingly, if"
94,no SessionTicket then stick on SessionID or don't stick as there's no server
94,side state is there when SessionTickets are in use.
94,req.ssl_ver : integerreq_ssl_ver : integer (deprecated)Returns an integer value containing the version of the SSL/TLS protocol of a
94,stream present in the request buffer. Both SSLv2 hello messages and SSLv3
94,messages are supported. TLSv1 is announced as SSL version 3.1. The value is
94,"composed of the major version multiplied by 65536, added to the minor"
94,version. Note that this only applies to raw contents found in the request
94,"buffer and not to contents deciphered via an SSL data layer, so this will not"
94,"work with ""bind"" lines having the ""sslThis keyword is available in sections :Bind optionsServer and default-server options"" option. The ACL version of the test"
94,matches against a decimal notation in the form MAJOR.MINOR (e.g. 3.1). This
94,fetch is mostly used in ACL.
94,ACL derivatives :
94,req_ssl_ver : decimal match
94,res.len : integerReturns an integer value corresponding to the number of bytes present in the
94,response buffer. This is mostly used in ACL. It is important to understand
94,that this test does not return false as long as the buffer is changing. This
94,means that a check with equality to zero will almost always immediately match
94,"at the beginning of the session, while a test for more data will wait for"
94,that data to come in and return false only when haproxy is certain that no
94,more data will come in. This test was designed to be used with TCP response
94,content inspection.
94,"res.payload(<offset>,<length>) : binaryThis extracts a binary block of <length> bytes and starting at byte <offset>"
94,"in the response buffer. As a special case, if the <length> argument is zero,"
94,the the whole buffer from <offset> to the end is extracted. This can be used
94,with ACLs in order to check for the presence of some content in a buffer at
94,any location.
94,"res.payload_lv(<offset1>,<length>[,<offset2>]) : binaryThis extracts a binary block whose size is specified at <offset1> for <length>"
94,"bytes, and which starts at <offset2> if specified or just after the length in"
94,the response buffer. The <offset2> parameter also supports relative offsets
94,if prepended with a '+' or '-' sign.
94,Example :
94,"please consult the example from the ""stick store-response"" keyword."
94,res.ssl_hello_type : integerrep_ssl_hello_type : integer (deprecated)Returns an integer value containing the type of the SSL hello message found
94,in the response buffer if the buffer contains data that parses as a complete
94,SSL (v3 or superior) hello message. Note that this only applies to raw
94,contents found in the response buffer and not to contents deciphered via an
94,"SSL data layer, so this will not work with ""server"" lines having the ""sslThis keyword is available in sections :Bind optionsServer and default-server options"""
94,option. This is mostly used in ACL to detect presence of an SSL hello message
94,that is supposed to contain an SSL session ID usable for stickiness.
94,"wait_end : booleanThis fetch either returns true when the inspection period is over, or does"
94,"not fetch. It is only used in ACLs, in conjunction with content analysis to"
94,avoid returning a wrong verdict early. It may also be used to delay some
94,"actions, such as a delayed reject for some special addresses. Since it either"
94,"stops the rules evaluation or immediately returns true, it is recommended to"
94,use this acl as the last one in a rule. Please note that the default ACL
94,"""WAIT_END"" is always usable without prior declaration. This test was designed"
94,to be used with TCP request content inspection.
94,Examples :
94,# delay every incoming request by 2 seconds
94,tcp-request inspect-delay 2s
94,tcp-request content accept if WAIT_END
94,# don't immediately tell bad guys they are rejected
94,tcp-request inspect-delay 10s
94,acl goodguys src 10.0.0.0/24
94,acl badguys
94,src 10.0.1.0/24
94,tcp-request content accept if goodguys
94,tcp-request content reject if badguys WAIT_END
94,tcp-request content reject
94,7.3.6. Fetching HTTP samples (Layer 7)
94,"It is possible to fetch samples from HTTP contents, requests and responses."
94,This application layer is also called layer 7. It is only possible to fetch the
94,data in this section when a full HTTP request or response has been parsed from
94,its respective request or response buffer. This is always the case with all
94,"HTTP specific rules and for sections running with ""mode http"". When using TCP"
94,"content inspection, it may be necessary to support an inspection delay in order"
94,to let the request or response come in first. These fetches may require a bit
94,"more CPU resources than the layer 4 ones, but not much since the request and"
94,response are indexed.
94,base : stringThis returns the concatenation of the first Host header and the path part of
94,"the request, which starts at the first slash and ends before the question"
94,mark. It can be useful in virtual hosted environments to detect URL abuses as
94,well as to improve shared caches efficiency. Using this with a limited size
94,stick table also allows one to collect statistics about most commonly
94,requested objects by host/path. With ACLs it can allow simple content
94,"switching rules involving the host and the path at the same time, such as"
94,"""www.example.com/favicon.ico"". See also ""path"" and ""uri""."
94,ACL derivatives :
94,base
94,: exact string match
94,base_beg : prefix match
94,base_dir : subdir match
94,base_dom : domain match
94,base_end : suffix match
94,base_len : length match
94,base_reg : regex match
94,base_sub : substring match
94,"base32 : integerThis returns a 32-bit hash of the value returned by the ""base"" fetch method"
94,above. This is useful to track per-URL activity on high traffic sites without
94,"having to store all URLs. Instead a shorter hash is stored, saving a lot of"
94,memory. The output type is an unsigned integer. The hash function used is
94,"SDBM with full avalanche on the output. Technically, base32 is exactly equal"
94,"to ""base,sdbm(1)""."
94,base32+src : binaryThis returns the concatenation of the base32 fetch above and the src fetch
94,"below. The resulting type is of type binary, with a size of 8 or 20 bytes"
94,"depending on the source address family. This can be used to track per-IP,"
94,per-URL counters.
94,"capture.req.hdr(<idx>) : stringThis extracts the content of the header captured by the ""capture request"
94,"header"", idx is the position of the capture keyword in the configuration."
94,"See also: ""capture request header""."
94,capture.req.method : stringThis extracts the METHOD of an HTTP request. It can be used in both request
94,"and response. Unlike ""method"", it can be used in both request and response"
94,because it's allocated.
94,"capture.req.uri : stringThis extracts the request's URI, which starts at the first slash and ends"
94,"before the first space in the request (without the host part). Unlike ""path"""
94,"and ""url"", it can be used in both request and response because it's"
94,allocated.
94,"capture.req.ver : stringThis extracts the request's HTTP version and returns either ""HTTP/1.0"" or"
94,"""HTTP/1.1"". Unlike ""req.ver"", it can be used in both request, response, and"
94,logs because it relies on a persistent flag.
94,"capture.res.hdr(<idx>) : stringThis extracts the content of the header captured by the ""capture response"
94,"header"", idx is the position of the capture keyword in the configuration."
94,The first entry is an index of 0.
94,"See also: ""capture response header"""
94,"capture.res.ver : stringThis extracts the response's HTTP version and returns either ""HTTP/1.0"" or"
94,"""HTTP/1.1"". Unlike ""res.ver"", it can be used in logs because it relies on a"
94,persistent flag.
94,req.body : binaryThis returns the HTTP request's available body as a block of data. It
94,requires that the request body has been buffered made available using
94,"""option http-buffer-request"". In case of chunked-encoded body, currently only"
94,the first chunk is analyzed.
94,req.body_param([<name>) : stringThis fetch assumes that the body of the POST request is url-encoded. The user
94,"can check if the ""content-type"" contains the value"
94,"""application/x-www-form-urlencoded"". This extracts the first occurrence of the"
94,"parameter <name> in the body, which ends before '&'. The parameter name is"
94,"case-sensitive. If no name is given, any parameter will match, and the first"
94,one will be returned. The result is a string corresponding to the value of the
94,parameter <name> as presented in the request body (no URL decoding is
94,performed). Note that the ACL version of this fetch iterates over multiple
94,parameters and will iteratively report all parameters values if no name is
94,given.
94,req.body_len : integerThis returns the length of the HTTP request's available body in bytes. It may
94,be lower than the advertised length if the body is larger than the buffer. It
94,requires that the request body has been buffered made available using
94,"""option http-buffer-request""."
94,req.body_size : integerThis returns the advertised length of the HTTP request's body in bytes. It
94,"will represent the advertised Content-Length header, or the size of the first"
94,"chunk in case of chunked encoding. In order to parse the chunks, it requires"
94,that the request body has been buffered made available using
94,"""option http-buffer-request""."
94,"req.cook([<name>]) : stringcook([<name>]) : string (deprecated)This extracts the last occurrence of the cookie name <name> on a ""Cookie"""
94,"header line from the request, and returns its value as string. If no name is"
94,"specified, the first cookie value is returned. When used with ACLs, all"
94,matching cookies are evaluated. Spaces around the name and the value are
94,ignored as requested by the Cookie header specification (RFC6265). The cookie
94,"name is case-sensitive. Empty cookies are valid, so an empty cookie may very"
94,"well return an empty value if it is present. Use the ""found"" match to detect"
94,presence. Use the res.cook() variant for response cookies sent by the server.
94,ACL derivatives :
94,cook([<name>])
94,: exact string match
94,cook_beg([<name>]) : prefix match
94,cook_dir([<name>]) : subdir match
94,cook_dom([<name>]) : domain match
94,cook_end([<name>]) : suffix match
94,cook_len([<name>]) : length match
94,cook_reg([<name>]) : regex match
94,cook_sub([<name>]) : substring match
94,req.cook_cnt([<name>]) : integercook_cnt([<name>]) : integer (deprecated)Returns an integer value representing the number of occurrences of the cookie
94,"<name> in the request, or all cookies if <name> is not specified."
94,"req.cook_val([<name>]) : integercook_val([<name>]) : integer (deprecated)This extracts the last occurrence of the cookie name <name> on a ""Cookie"""
94,"header line from the request, and converts its value to an integer which is"
94,"returned. If no name is specified, the first cookie value is returned. When"
94,"used in ACLs, all matching names are iterated over until a value matches."
94,"cookie([<name>]) : string (deprecated)This extracts the last occurrence of the cookie name <name> on a ""Cookie"""
94,"header line from the request, or a ""Set-Cookie"" header from the response, and"
94,returns its value as a string. A typical use is to get multiple clients
94,sharing a same profile use the same server. This can be similar to what
94,"""appsession"" did with the ""request-learn"" statement, but with support for"
94,multi-peer synchronization and state keeping across restarts. If no name is
94,"specified, the first cookie value is returned. This fetch should not be used"
94,anymore and should be replaced by req.cook() or res.cook() instead as it
94,ambiguously uses the direction based on the context where it is used.
94,"hdr([<name>[,<occ>]]) : stringThis is equivalent to req.hdr() when used on requests, and to res.hdr() when"
94,used on responses. Please refer to these respective fetches for more details.
94,"In case of doubt about the fetch direction, please use the explicit ones."
94,"Note that contrary to the hdr() sample fetch method, the hdr_* ACL keywords"
94,unambiguously apply to the request headers.
94,"req.fhdr(<name>[,<occ>]) : stringThis extracts the last occurrence of header <name> in an HTTP request. When"
94,"used from an ACL, all occurrences are iterated over until a match is found."
94,"Optionally, a specific occurrence might be specified as a position number."
94,"Positive values indicate a position from the first occurrence, with 1 being"
94,"the first one. Negative values indicate positions relative to the last one,"
94,with -1 being the last one. It differs from req.hdr() in that any commas
94,present in the value are returned and are not used as delimiters. This is
94,sometimes useful with headers such as User-Agent.
94,req.fhdr_cnt([<name>]) : integerReturns an integer value representing the number of occurrences of request
94,"header field name <name>, or the total number of header fields if <name> is"
94,"not specified. Contrary to its req.hdr_cnt() cousin, this function returns"
94,the number of full line headers and does not stop on commas.
94,"req.hdr([<name>[,<occ>]]) : stringThis extracts the last occurrence of header <name> in an HTTP request. When"
94,"used from an ACL, all occurrences are iterated over until a match is found."
94,"Optionally, a specific occurrence might be specified as a position number."
94,"Positive values indicate a position from the first occurrence, with 1 being"
94,"the first one. Negative values indicate positions relative to the last one,"
94,with -1 being the last one. A typical use is with the X-Forwarded-For header
94,"once converted to IP, associated with an IP stick-table. The function"
94,considers any comma as a delimiter for distinct values. If full-line headers
94,"are desired instead, use req.fhdr(). Please carefully check RFC7231 to know"
94,"how certain headers are supposed to be parsed. Also, some of them are case"
94,insensitive (e.g. Connection).
94,ACL derivatives :
94,"hdr([<name>[,<occ>]])"
94,: exact string match
94,"hdr_beg([<name>[,<occ>]]) : prefix match"
94,"hdr_dir([<name>[,<occ>]]) : subdir match"
94,"hdr_dom([<name>[,<occ>]]) : domain match"
94,"hdr_end([<name>[,<occ>]]) : suffix match"
94,"hdr_len([<name>[,<occ>]]) : length match"
94,"hdr_reg([<name>[,<occ>]]) : regex match"
94,"hdr_sub([<name>[,<occ>]]) : substring match"
94,req.hdr_cnt([<name>]) : integerhdr_cnt([<header>]) : integer (deprecated)Returns an integer value representing the number of occurrences of request
94,"header field name <name>, or the total number of header field values if"
94,<name> is not specified. It is important to remember that one header line may
94,count as several headers if it has several values. The function considers any
94,comma as a delimiter for distinct values. If full-line headers are desired
94,"instead, req.fhdr_cnt() should be used instead. With ACLs, it can be used to"
94,"detect presence, absence or abuse of a specific header, as well as to block"
94,request smuggling attacks by rejecting requests which contain more than one
94,"of certain headers. See ""req.hdr"" for more information on header matching."
94,"req.hdr_ip([<name>[,<occ>]]) : iphdr_ip([<name>[,<occ>]]) : ip (deprecated)This extracts the last occurrence of header <name> in an HTTP request,"
94,converts it to an IPv4 or IPv6 address and returns this address. When used
94,"with ACLs, all occurrences are checked, and if <name> is omitted, every value"
94,"of every header is checked. Optionally, a specific occurrence might be"
94,specified as a position number. Positive values indicate a position from the
94,"first occurrence, with 1 being the first one. Negative values indicate"
94,"positions relative to the last one, with -1 being the last one. A typical use"
94,is with the X-Forwarded-For and X-Client-IP headers.
94,"req.hdr_val([<name>[,<occ>]]) : integerhdr_val([<name>[,<occ>]]) : integer (deprecated)This extracts the last occurrence of header <name> in an HTTP request, and"
94,"converts it to an integer value. When used with ACLs, all occurrences are"
94,"checked, and if <name> is omitted, every value of every header is checked."
94,"Optionally, a specific occurrence might be specified as a position number."
94,"Positive values indicate a position from the first occurrence, with 1 being"
94,"the first one. Negative values indicate positions relative to the last one,"
94,with -1 being the last one. A typical use is with the X-Forwarded-For header.
94,http_auth(<userlist>) : booleanReturns a boolean indicating whether the authentication data received from
94,the client match a username & password stored in the specified userlist. This
94,fetch function is not really useful outside of ACLs. Currently only http
94,basic auth is supported.
94,http_auth_group(<userlist>) : stringReturns a string corresponding to the user name found in the authentication
94,data received from the client if both the user name and password are valid
94,according to the specified userlist. The main purpose is to use it in ACLs
94,where it is then checked whether the user belongs to any group within a list.
94,This fetch function is not really useful outside of ACLs. Currently only http
94,basic auth is supported.
94,ACL derivatives :
94,http_auth_group(<userlist>) : group ...
94,Returns true when the user extracted from the request and whose password is
94,valid according to the specified userlist belongs to at least one of the
94,groups.
94,http_first_req : booleanReturns true when the request being processed is the first one of the
94,connection. This can be used to add or remove headers that may be missing
94,"from some requests when a request is not the first one, or to help grouping"
94,requests in the logs.
94,method : integer + stringReturns an integer value corresponding to the method in the HTTP request. For
94,"example, ""GET"" equals 1 (check sources to establish the matching). Value 9"
94,"means ""other method"" and may be converted to a string extracted from the"
94,"stream. This should not be used directly as a sample, this is only meant to"
94,"be used from ACLs, which transparently convert methods from patterns to these"
94,integer + string values. Some predefined ACL already check for most common
94,methods.
94,ACL derivatives :
94,method : case insensitive method match
94,Example :
94,# only accept GET and HEAD requests
94,acl valid_method method GET HEAD
94,http-request deny if ! valid_method
94,"path : stringThis extracts the request's URL path, which starts at the first slash and"
94,ends before the question mark (without the host part). A typical use is with
94,"prefetch-capable caches, and with portals which need to aggregate multiple"
94,information from databases and keep them in caches. Note that with outgoing
94,"caches, it would be wiser to use ""url"" instead. With ACLs, it's typically"
94,"used to match exact file names (e.g. ""/login.php""), or directory parts using"
94,"the derivative forms. See also the ""url"" and ""base"" fetch methods."
94,ACL derivatives :
94,path
94,: exact string match
94,path_beg : prefix match
94,path_dir : subdir match
94,path_dom : domain match
94,path_end : suffix match
94,path_len : length match
94,path_reg : regex match
94,path_sub : substring match
94,"query : stringThis extracts the request's query string, which starts after the first"
94,"question mark. If no question mark is present, this fetch returns nothing. If"
94,"a question mark is present but nothing follows, it returns an empty string."
94,This means it's possible to easily know whether a query string is present
94,"using the ""found"" matching method. This fetch is the complement of ""path"""
94,which stops before the question mark.
94,req.hdr_names([<delim>]) : stringThis builds a string made from the concatenation of all header names as they
94,appear in the request when the rule is evaluated. The default delimiter is
94,"the comma (',') but it may be overridden as an optional argument <delim>. In"
94,"this case, only the first character of <delim> is considered."
94,"req.ver : stringreq_ver : string (deprecated)Returns the version string from the HTTP request, for example ""1.1"". This can"
94,"be useful for logs, but is mostly there for ACL. Some predefined ACL already"
94,check for versions 1.0 and 1.1.
94,ACL derivatives :
94,req_ver : exact string match
94,"res.comp : booleanReturns the boolean ""true"" value if the response has been compressed by"
94,"HAProxy, otherwise returns boolean ""false"". This may be used to add"
94,information in the logs.
94,res.comp_algo : stringReturns a string containing the name of the algorithm used if the response
94,"was compressed by HAProxy, for example : ""deflate"". This may be used to add"
94,some information in the logs.
94,"res.cook([<name>]) : stringscook([<name>]) : string (deprecated)This extracts the last occurrence of the cookie name <name> on a ""Set-Cookie"""
94,"header line from the response, and returns its value as string. If no name is"
94,"specified, the first cookie value is returned."
94,ACL derivatives :
94,scook([<name>] : exact string match
94,res.cook_cnt([<name>]) : integerscook_cnt([<name>]) : integer (deprecated)Returns an integer value representing the number of occurrences of the cookie
94,"<name> in the response, or all cookies if <name> is not specified. This is"
94,mostly useful when combined with ACLs to detect suspicious responses.
94,"res.cook_val([<name>]) : integerscook_val([<name>]) : integer (deprecated)This extracts the last occurrence of the cookie name <name> on a ""Set-Cookie"""
94,"header line from the response, and converts its value to an integer which is"
94,"returned. If no name is specified, the first cookie value is returned."
94,"res.fhdr([<name>[,<occ>]]) : stringThis extracts the last occurrence of header <name> in an HTTP response, or of"
94,"the last header if no <name> is specified. Optionally, a specific occurrence"
94,might be specified as a position number. Positive values indicate a position
94,"from the first occurrence, with 1 being the first one. Negative values"
94,"indicate positions relative to the last one, with -1 being the last one. It"
94,differs from res.hdr() in that any commas present in the value are returned
94,"and are not used as delimiters. If this is not desired, the res.hdr() fetch"
94,should be used instead. This is sometimes useful with headers such as Date or
94,Expires.
94,res.fhdr_cnt([<name>]) : integerReturns an integer value representing the number of occurrences of response
94,"header field name <name>, or the total number of header fields if <name> is"
94,"not specified. Contrary to its res.hdr_cnt() cousin, this function returns"
94,the number of full line headers and does not stop on commas. If this is not
94,"desired, the res.hdr_cnt() fetch should be used instead."
94,"res.hdr([<name>[,<occ>]]) : stringshdr([<name>[,<occ>]]) : string (deprecated)This extracts the last occurrence of header <name> in an HTTP response, or of"
94,"the last header if no <name> is specified. Optionally, a specific occurrence"
94,might be specified as a position number. Positive values indicate a position
94,"from the first occurrence, with 1 being the first one. Negative values"
94,"indicate positions relative to the last one, with -1 being the last one. This"
94,can be useful to learn some data into a stick-table. The function considers
94,"any comma as a delimiter for distinct values. If this is not desired, the"
94,res.fhdr() fetch should be used instead.
94,ACL derivatives :
94,"shdr([<name>[,<occ>]])"
94,: exact string match
94,"shdr_beg([<name>[,<occ>]]) : prefix match"
94,"shdr_dir([<name>[,<occ>]]) : subdir match"
94,"shdr_dom([<name>[,<occ>]]) : domain match"
94,"shdr_end([<name>[,<occ>]]) : suffix match"
94,"shdr_len([<name>[,<occ>]]) : length match"
94,"shdr_reg([<name>[,<occ>]]) : regex match"
94,"shdr_sub([<name>[,<occ>]]) : substring match"
94,res.hdr_cnt([<name>]) : integershdr_cnt([<name>]) : integer (deprecated)Returns an integer value representing the number of occurrences of response
94,"header field name <name>, or the total number of header fields if <name> is"
94,not specified. The function considers any comma as a delimiter for distinct
94,"values. If this is not desired, the res.fhdr_cnt() fetch should be used"
94,instead.
94,"res.hdr_ip([<name>[,<occ>]]) : ipshdr_ip([<name>[,<occ>]]) : ip (deprecated)This extracts the last occurrence of header <name> in an HTTP response,"
94,"convert it to an IPv4 or IPv6 address and returns this address. Optionally, a"
94,specific occurrence might be specified as a position number. Positive values
94,"indicate a position from the first occurrence, with 1 being the first one."
94,"Negative values indicate positions relative to the last one, with -1 being"
94,the last one. This can be useful to learn some data into a stick table.
94,res.hdr_names([<delim>]) : stringThis builds a string made from the concatenation of all header names as they
94,appear in the response when the rule is evaluated. The default delimiter is
94,"the comma (',') but it may be overridden as an optional argument <delim>. In"
94,"this case, only the first character of <delim> is considered."
94,"res.hdr_val([<name>[,<occ>]]) : integershdr_val([<name>[,<occ>]]) : integer (deprecated)This extracts the last occurrence of header <name> in an HTTP response, and"
94,"converts it to an integer value. Optionally, a specific occurrence might be"
94,specified as a position number. Positive values indicate a position from the
94,"first occurrence, with 1 being the first one. Negative values indicate"
94,"positions relative to the last one, with -1 being the last one. This can be"
94,useful to learn some data into a stick table.
94,"res.ver : stringresp_ver : string (deprecated)Returns the version string from the HTTP response, for example ""1.1"". This"
94,"can be useful for logs, but is mostly there for ACL."
94,ACL derivatives :
94,resp_ver : exact string match
94,"set-cookie([<name>]) : string (deprecated)This extracts the last occurrence of the cookie name <name> on a ""Set-Cookie"""
94,header line from the response and uses the corresponding value to match. This
94,"can be comparable to what ""appsession"" did with default options, but with"
94,support for multi-peer synchronization and state keeping across restarts.
94,"This fetch function is deprecated and has been superseded by the ""res.cook"""
94,fetch. This keyword will disappear soon.
94,"status : integerReturns an integer containing the HTTP status code in the HTTP response, for"
94,"example, 302. It is mostly used within ACLs and integer ranges, for example,"
94,to remove any Location header if the response is not a 3xx.
94,unique-id : stringReturns the unique-id attached to the request. The directive
94,"""unique-id-format"" must be set. If it is not set, the unique-id sample fetch"
94,"fails. Note that the unique-id is usually used with HTTP requests, however this"
94,"sample fetch can be used with other protocols. Obviously, if it is used with"
94,"other protocols than HTTP, the unique-id-format directive must not contain"
94,HTTP parts. See: unique-id-format and unique-id-header
94,url : stringThis extracts the request's URL as presented in the request. A typical use is
94,"with prefetch-capable caches, and with portals which need to aggregate"
94,"multiple information from databases and keep them in caches. With ACLs, using"
94,"""path"" is preferred over using ""url"", because clients may send a full URL as"
94,"is normally done with proxies. The only real use is to match ""*"" which does"
94,"not match in ""path"", and for which there is already a predefined ACL. See"
94,"also ""path"" and ""base""."
94,ACL derivatives :
94,url
94,: exact string match
94,url_beg : prefix match
94,url_dir : subdir match
94,url_dom : domain match
94,url_end : suffix match
94,url_len : length match
94,url_reg : regex match
94,url_sub : substring match
94,url_ip : ipThis extracts the IP address from the request's URL when the host part is
94,"presented as an IP address. Its use is very limited. For instance, a"
94,monitoring system might use this field as an alternative for the source IP in
94,"order to test what path a given source address would follow, or to force an"
94,entry in a table for a given source address. With ACLs it can be used to
94,"restrict access to certain systems through a proxy, for example when combined"
94,"with option ""http_proxy""."
94,url_port : integerThis extracts the port part from the request's URL. Note that if the port is
94,"not specified in the request, port 80 is assumed. With ACLs it can be used to"
94,"restrict access to certain systems through a proxy, for example when combined"
94,"with option ""http_proxy""."
94,"urlp([<name>[,<delim>]]) : stringurl_param([<name>[,<delim>]]) : stringThis extracts the first occurrence of the parameter <name> in the query"
94,"string, which begins after either '?' or <delim>, and which ends before '&',"
94,"';' or <delim>. The parameter name is case-sensitive. If no name is given,"
94,"any parameter will match, and the first one will be returned. The result is"
94,a string corresponding to the value of the parameter <name> as presented in
94,the request (no URL decoding is performed). This can be used for session
94,"stickiness based on a client ID, to extract an application cookie passed as a"
94,"URL parameter, or in ACLs to apply some checks. Note that the ACL version of"
94,this fetch iterates over multiple parameters and will iteratively report all
94,parameters values if no name is given
94,ACL derivatives :
94,"urlp(<name>[,<delim>])"
94,: exact string match
94,"urlp_beg(<name>[,<delim>]) : prefix match"
94,"urlp_dir(<name>[,<delim>]) : subdir match"
94,"urlp_dom(<name>[,<delim>]) : domain match"
94,"urlp_end(<name>[,<delim>]) : suffix match"
94,"urlp_len(<name>[,<delim>]) : length match"
94,"urlp_reg(<name>[,<delim>]) : regex match"
94,"urlp_sub(<name>[,<delim>]) : substring match"
94,Example :
94,# match http://example.com/foo?PHPSESSIONID=some_id
94,stick on urlp(PHPSESSIONID)
94,# match http://example.com/foo;JSESSIONID=some_id
94,"stick on urlp(JSESSIONID,;)"
94,"urlp_val([<name>[,<delim>]]) : integerSee ""urlp"" above. This one extracts the URL parameter <name> in the request"
94,and converts it to an integer value. This can be used for session stickiness
94,"based on a user ID for example, or with ACLs to match a page number or price."
94,url32 : integerThis returns a 32-bit hash of the value obtained by concatenating the first
94,Host header and the whole URL including parameters (not only the path part of
94,"the request, as in the ""base32"" fetch above). This is useful to track per-URL"
94,"activity. A shorter hash is stored, saving a lot of memory. The output type"
94,is an unsigned integer.
94,"url32+src : binaryThis returns the concatenation of the ""url32"" fetch and the ""src"" fetch. The"
94,"resulting type is of type binary, with a size of 8 or 20 bytes depending on"
94,"the source address family. This can be used to track per-IP, per-URL counters."
94,7.4. Pre-defined ACLs
94,Some predefined ACLs are hard-coded so that they do not have to be declared in
94,every frontend which needs them. They all have their names in upper case in
94,order to avoid confusion. Their equivalence is provided below.
94,ACL nameEquivalent toUsage
94,FALSEalways_falsenever match
94,HTTPreq_proto_httpmatch if protocol is valid HTTP
94,HTTP_1.0req_ver 1.0match HTTP version 1.0
94,HTTP_1.1req_ver 1.1match HTTP version 1.1
94,HTTP_CONTENThdr_val(content-length) gt 0match an existing content-length
94,HTTP_URL_ABSurl_reg ^[^/:]*://match absolute URL with scheme
94,"HTTP_URL_SLASHurl_beg /match URL beginning with ""/"""
94,HTTP_URL_STARurl
94,"*match URL equal to ""*"""
94,LOCALHOSTsrc 127.0.0.1/8match connection from local host
94,METH_CONNECTmethod
94,CONNECTmatch HTTP CONNECT method
94,METH_DELETEmethod
94,DELETEmatch HTTP DELETE method
94,METH_GETmethod
94,GET HEADmatch HTTP GET or HEAD method
94,METH_HEADmethod
94,HEADmatch HTTP HEAD method
94,METH_OPTIONSmethod
94,OPTIONSmatch HTTP OPTIONS method
94,METH_POSTmethod
94,POSTmatch HTTP POST method
94,METH_PUTmethod
94,PUTmatch HTTP PUT method
94,METH_TRACEmethod
94,TRACEmatch HTTP TRACE method
94,RDP_COOKIEreq_rdp_cookie_cnt gt 0match presence of an RDP cookie
94,REQ_CONTENTreq_len gt 0match data in the request buffer
94,TRUEalways_truealways match
94,WAIT_ENDwait_endwait for end of content analysis
94,8. Logging
94,One of HAProxy's strong points certainly lies is its precise logs. It probably
94,"provides the finest level of information available for such a product, which is"
94,very important for troubleshooting complex environments. Standard information
94,"provided in logs include client ports, TCP/HTTP state timers, precise session"
94,"state at termination and precise termination cause, information about decisions"
94,"to direct traffic to a server, and of course the ability to capture arbitrary"
94,headers.
94,"In order to improve administrators reactivity, it offers a great transparency"
94,"about encountered problems, both internal and external, and it is possible to"
94,send logs to different sources at the same time with different level filters :
94,"- global process-level logs (system errors, start/stop, etc..)"
94,"- per-instance system and internal errors (lack of resource, bugs, ...)"
94,"- per-instance external troubles (servers up/down, max connections)"
94,"- per-instance activity (client connections), either at the establishment or"
94,at the termination.
94,"- per-request control of log-level, e.g."
94,http-request set-log-level silent if sensitive_request
94,The ability to distribute different levels of logs to different log servers
94,allow several production teams to interact and to fix their problems as soon
94,"as possible. For example, the system team might monitor system-wide errors,"
94,while the application team might be monitoring the up/down for their servers in
94,"real time, and the security team might analyze the activity logs with one hour"
94,delay.
94,8.1. Log levels
94,"TCP and HTTP connections can be logged with information such as the date, time,"
94,"source IP address, destination address, connection duration, response times,"
94,"HTTP request, HTTP return code, number of bytes transmitted, conditions"
94,"in which the session ended, and even exchanged cookies values. For example"
94,track a particular user's problems. All messages may be sent to up to two
94,"syslog servers. Check the ""logThis keyword is available in sections :Process management and securityAlphabetically sorted keywords reference"" keyword in section 4.2 for more information"
94,about log facilities.
94,8.2. Log formats
94,HAProxy supports 5 log formats. Several fields are common between these formats
94,and will be detailed in the following sections. A few of them may vary
94,"slightly with the configuration, due to indicators specific to certain"
94,options. The supported formats are as follows :
94,"- the default format, which is very basic and very rarely used. It only"
94,provides very basic information about the incoming connection at the moment
94,"it is accepted : source IP:port, destination IP:port, and frontend-name."
94,This mode will eventually disappear so it will not be described to great
94,extents.
94,"- the TCP format, which is more advanced. This format is enabled when ""option"
94,"tcplog"" is set on the frontend. HAProxy will then usually wait for the"
94,connection to terminate before logging. This format provides much richer
94,"information, such as timers, connection counts, queue size, etc... This"
94,format is recommended for pure TCP proxies.
94,"- the HTTP format, which is the most advanced for HTTP proxying. This format"
94,"is enabled when ""option httplog"" is set on the frontend. It provides the"
94,same information as the TCP format with some HTTP-specific fields such as
94,"the request, the status code, and captures of headers and cookies. This"
94,format is recommended for HTTP proxies.
94,"- the CLF HTTP format, which is equivalent to the HTTP format, but with the"
94,"fields arranged in the same order as the CLF format. In this mode, all"
94,"timers, captures, flags, etc... appear one per field after the end of the"
94,"common fields, in the same order they appear in the standard HTTP format."
94,"- the custom log format, allows you to make your own log line."
94,Next sections will go deeper into details for each of these formats. Format
94,"specification will be performed on a ""field"" basis. Unless stated otherwise, a"
94,field is a portion of text delimited by any number of spaces. Since syslog
94,"servers are susceptible of inserting fields at the beginning of a line, it is"
94,always assumed that the first field is the one containing the process name and
94,identifier.
94,"Note : Since log lines may be quite long, the log examples in sections below"
94,might be broken into multiple lines. The example log lines will be
94,prefixed with 3 closing angle brackets ('>>>') and each time a log is
94,"broken into multiple lines, each non-final line will end with a"
94,backslash ('\') and the next line will start indented by two characters.
94,8.2.1. Default log format
94,This format is used when no specific option is set. The log is emitted as soon
94,as the connection is accepted. One should note that this currently is the only
94,format which logs the request's destination IP and ports.
94,Example :
94,listen www
94,mode http
94,log global
94,server srv1 127.0.0.1:8000
94,>>> Feb
94,6 12:12:09 localhost \
94,haproxy[14385]: Connect from 10.0.1.2:33312 to 10.0.3.31:8012 \
94,(www/HTTP)
94,Field
94,Format
94,Extract from the example above
94,process_name '[' pid ']:'
94,haproxy[14385]:
94,'Connect from'
94,Connect from
94,source_ip ':' source_port
94,10.0.1.2:33312
94,'to'
94,destination_ip ':' destination_port
94,10.0.3.31:8012
94,'(' frontend_name '/' mode ')'
94,(www/HTTP)
94,Detailed fields description :
94,"- ""source_ip"" is the IP address of the client which initiated the connection."
94,"- ""source_port"" is the TCP port of the client which initiated the connection."
94,"- ""destination_ip"" is the IP address the client connected to."
94,"- ""destination_port"" is the TCP port the client connected to."
94,"- ""frontend_name"" is the name of the frontend (or listener) which received"
94,and processed the connection.
94,"- ""mode is the mode the frontend is operating (TCP or HTTP)."
94,"In case of a UNIX socket, the source and destination addresses are marked as"
94,"""unix:"" and the ports reflect the internal ID of the socket which accepted the"
94,connection (the same ID as reported in the stats).
94,It is advised not to use this deprecated format for newer installations as it
94,will eventually disappear.
94,8.2.2. TCP log format
94,"The TCP format is used when ""option tcplog"" is specified in the frontend, and"
94,is the recommended format for pure TCP proxies. It provides a lot of precious
94,information for troubleshooting. Since this format includes timers and byte
94,"counts, the log is normally emitted at the end of the session. It can be"
94,"emitted earlier if ""option logasap"" is specified, which makes sense in most"
94,environments with long sessions such as remote terminals. Sessions which match
94,"the ""monitor"" rules are never logged. It is also possible not to emit logs for"
94,"sessions for which no data were exchanged between the client and the server, by"
94,"specifying ""option dontlognull"" in the frontend. Successful connections will"
94,"not be logged if ""option dontlog-normal"" is specified in the frontend. A few"
94,"fields may slightly vary depending on some configuration options, those are"
94,marked with a star ('*') after the field name below.
94,Example :
94,frontend fnt
94,mode tcp
94,option tcplog
94,log global
94,default_backend bck
94,backend bck
94,server srv1 127.0.0.1:8000
94,>>> Feb
94,6 12:12:56 localhost \
94,haproxy[14387]: 10.0.1.2:33313 [06/Feb/2009:12:12:51.443] fnt \
94,bck/srv1 0/0/5007 212 -- 0/0/0/0/3 0/0
94,Field
94,Format
94,Extract from the example above
94,process_name '[' pid ']:'
94,haproxy[14387]:
94,client_ip ':' client_port
94,10.0.1.2:33313
94,'[' accept_date ']'
94,[06/Feb/2009:12:12:51.443]
94,frontend_name
94,fnt
94,backend_name '/' server_name
94,bck/srv1
94,Tw '/' Tc '/' Tt*
94,0/0/5007
94,bytes_read*
94,212
94,termination_state
94,actconn '/' feconn '/' beconn '/' srv_conn '/' retries*
94,0/0/0/0/3
94,srv_queue '/' backend_queue
94,0/0
94,Detailed fields description :
94,"- ""client_ip"" is the IP address of the client which initiated the TCP"
94,connection to haproxy. If the connection was accepted on a UNIX socket
94,"instead, the IP address would be replaced with the word ""unix"". Note that"
94,"when the connection is accepted on a socket configured with ""accept-proxy"""
94,"and the PROXY protocol is correctly used, or with a ""accept-netscaler-cip"""
94,"and the NetScaler Client IP insertion protocol is correctly used, then the"
94,logs will reflect the forwarded connection's information.
94,"- ""client_port"" is the TCP port of the client which initiated the connection."
94,"If the connection was accepted on a UNIX socket instead, the port would be"
94,"replaced with the ID of the accepting socket, which is also reported in the"
94,stats interface.
94,"- ""accept_date"" is the exact date when the connection was received by haproxy"
94,(which might be very slightly different from the date observed on the
94,network if there was some queuing in the system's backlog). This is usually
94,the same date which may appear in any upstream firewall's log. When used in
94,"HTTP mode, the accept_date field will be reset to the first moment the"
94,connection is ready to receive a new request (end of previous response for
94,"HTTP/1, immediately after previous request for HTTP/2)."
94,"- ""frontend_name"" is the name of the frontend (or listener) which received"
94,and processed the connection.
94,"- ""backend_name"" is the name of the backend (or listener) which was selected"
94,to manage the connection to the server. This will be the same as the
94,"frontend if no switching rule has been applied, which is common for TCP"
94,applications.
94,"- ""server_name"" is the name of the last server to which the connection was"
94,"sent, which might differ from the first one if there were connection errors"
94,and a redispatch occurred. Note that this server belongs to the backend
94,which processed the request. If the connection was aborted before reaching
94,"a server, ""<NOSRV>"" is indicated instead of a server name."
94,"- ""Tw"" is the total time in milliseconds spent waiting in the various queues."
94,"It can be ""-1"" if the connection was aborted before reaching the queue."
94,"See ""Timers"" below for more details."
94,"- ""Tc"" is the total time in milliseconds spent waiting for the connection to"
94,"establish to the final server, including retries. It can be ""-1"" if the"
94,connection was aborted before a connection could be established. See
94,"""Timers"" below for more details."
94,"- ""Tt"" is the total time in milliseconds elapsed between the accept and the"
94,"last close. It covers all possible processing. There is one exception, if"
94,"""option logasap"" was specified, then the time counting stops at the moment"
94,"the log is emitted. In this case, a '+' sign is prepended before the value,"
94,"indicating that the final one will be larger. See ""Timers"" below for more"
94,details.
94,"- ""bytes_read"" is the total number of bytes transmitted from the server to"
94,"the client when the log is emitted. If ""option logasap"" is specified, the"
94,this value will be prefixed with a '+' sign indicating that the final one
94,"may be larger. Please note that this value is a 64-bit counter, so log"
94,analysis tools must be able to handle it without overflowing.
94,"- ""termination_state"" is the condition the session was in when the session"
94,"ended. This indicates the session state, which side caused the end of"
94,"session to happen, and for what reason (timeout, error, ...). The normal"
94,"flags should be ""--"", indicating the session was closed by either end with"
94,"no data remaining in buffers. See below ""Session state at disconnection"""
94,for more details.
94,"- ""actconn"" is the total number of concurrent connections on the process when"
94,the session was logged. It is useful to detect when some per-process system
94,"limits have been reached. For instance, if actconn is close to 512 when"
94,"multiple connection errors occur, chances are high that the system limits"
94,the process to use a maximum of 1024 file descriptors and that all of them
94,"are used. See section 3 ""Global parameters"" to find how to tune the system."
94,"- ""feconn"" is the total number of concurrent connections on the frontend when"
94,the session was logged. It is useful to estimate the amount of resource
94,"required to sustain high loads, and to detect when the frontend's ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"""
94,"has been reached. Most often when this value increases by huge jumps, it is"
94,"because there is congestion on the backend servers, but sometimes it can be"
94,caused by a denial of service attack.
94,"- ""beconn"" is the total number of concurrent connections handled by the"
94,backend when the session was logged. It includes the total number of
94,concurrent connections active on servers as well as the number of
94,connections pending in queues. It is useful to estimate the amount of
94,additional servers needed to support high loads for a given application.
94,"Most often when this value increases by huge jumps, it is because there is"
94,"congestion on the backend servers, but sometimes it can be caused by a"
94,denial of service attack.
94,"- ""srv_conn"" is the total number of concurrent connections still active on"
94,the server when the session was logged. It can never exceed the server's
94,"configured ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"" parameter. If this value is very often close or equal"
94,"to the server's ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"", it means that traffic regulation is involved a"
94,"lot, meaning that either the server's maxconn value is too low, or that"
94,there aren't enough servers to process the load with an optimal response
94,"time. When only one of the server's ""srv_conn"" is high, it usually means"
94,that this server has some trouble causing the connections to take longer to
94,be processed than on other servers.
94,"- ""retries"" is the number of connection retries experienced by this session"
94,"when trying to connect to the server. It must normally be zero, unless a"
94,server is being stopped at the same moment the connection was attempted.
94,Frequent retries generally indicate either a network problem between
94,"haproxy and the server, or a misconfigured system backlog on the server"
94,preventing new connections from being queued. This field may optionally be
94,"prefixed with a '+' sign, indicating that the session has experienced a"
94,redispatch after the maximal retry count has been reached on the initial
94,"server. In this case, the server name appearing in the log is the one the"
94,"connection was redispatched to, and not the first one, though both may"
94,sometimes be the same in case of hashing for instance. So as a general rule
94,"of thumb, when a '+' is present in front of the retry count, this count"
94,should not be attributed to the logged server.
94,"- ""srv_queue"" is the total number of requests which were processed before"
94,this one in the server queue. It is zero when the request has not gone
94,through the server queue. It makes it possible to estimate the approximate
94,server's response time by dividing the time spent in queue by the number of
94,requests in the queue. It is worth noting that if a session experiences a
94,"redispatch and passes through two server queues, their positions will be"
94,cumulative. A request should not pass through both the server queue and the
94,backend queue unless a redispatch occurs.
94,"- ""backend_queue"" is the total number of requests which were processed before"
94,this one in the backend's global queue. It is zero when the request has not
94,gone through the global queue. It makes it possible to estimate the average
94,"queue length, which easily translates into a number of missing servers when"
94,"divided by a server's ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"" parameter. It is worth noting that if a"
94,"session experiences a redispatch, it may pass twice in the backend's queue,"
94,and then both positions will be cumulative. A request should not pass
94,through both the server queue and the backend queue unless a redispatch
94,occurs.
94,8.2.3. HTTP log format
94,The HTTP format is the most complete and the best suited for HTTP proxies. It
94,"is enabled by when ""option httplog"" is specified in the frontend. It provides"
94,the same level of information as the TCP format with additional features which
94,"are specific to the HTTP protocol. Just like the TCP format, the log is usually"
94,"emitted at the end of the session, unless ""option logasap"" is specified, which"
94,generally only makes sense for download sites. A session which matches the
94,"""monitor"" rules will never logged. It is also possible not to log sessions for"
94,"which no data were sent by the client by specifying ""option dontlognull"" in the"
94,"frontend. Successful connections will not be logged if ""option dontlog-normal"""
94,is specified in the frontend.
94,"Most fields are shared with the TCP log, some being different. A few fields may"
94,slightly vary depending on some configuration options. Those ones are marked
94,with a star ('*') after the field name below.
94,Example :
94,frontend http-in
94,mode http
94,option httplog
94,log global
94,default_backend bck
94,backend static
94,server srv1 127.0.0.1:8000
94,>>> Feb
94,6 12:14:14 localhost \
94,haproxy[14389]: 10.0.1.2:33317 [06/Feb/2009:12:14:14.655] http-in \
94,static/srv1 10/0/30/69/109 200 2750 - - ---- 1/1/1/1/0 0/0 {1wt.eu} \
94,"{} ""GET /index.html HTTP/1.1"""
94,Field
94,Format
94,Extract from the example above
94,process_name '[' pid ']:'
94,haproxy[14389]:
94,client_ip ':' client_port
94,10.0.1.2:33317
94,'[' request_date ']'
94,[06/Feb/2009:12:14:14.655]
94,frontend_name
94,http-in
94,backend_name '/' server_name
94,static/srv1
94,TR '/' Tw '/' Tc '/' Tr '/' Ta*
94,10/0/30/69/109
94,status_code
94,200
94,bytes_read*
94,2750
94,captured_request_cookie
94,captured_response_cookie
94,termination_state
94,----
94,actconn '/' feconn '/' beconn '/' srv_conn '/' retries*
94,1/1/1/1/0
94,srv_queue '/' backend_queue
94,0/0
94,'{' captured_request_headers* '}'
94,{haproxy.1wt.eu}
94,'{' captured_response_headers* '}'
94,"'""' http_request '""'"
94,"""GET /index.html HTTP/1.1"""
94,Detailed fields description :
94,"- ""client_ip"" is the IP address of the client which initiated the TCP"
94,connection to haproxy. If the connection was accepted on a UNIX socket
94,"instead, the IP address would be replaced with the word ""unix"". Note that"
94,"when the connection is accepted on a socket configured with ""accept-proxy"""
94,"and the PROXY protocol is correctly used, or with a ""accept-netscaler-cip"""
94,"and the NetScaler Client IP insertion protocol is correctly used, then the"
94,logs will reflect the forwarded connection's information.
94,"- ""client_port"" is the TCP port of the client which initiated the connection."
94,"If the connection was accepted on a UNIX socket instead, the port would be"
94,"replaced with the ID of the accepting socket, which is also reported in the"
94,stats interface.
94,"- ""request_date"" is the exact date when the first byte of the HTTP request"
94,was received by haproxy (log field %tr).
94,"- ""frontend_name"" is the name of the frontend (or listener) which received"
94,and processed the connection.
94,"- ""backend_name"" is the name of the backend (or listener) which was selected"
94,to manage the connection to the server. This will be the same as the
94,frontend if no switching rule has been applied.
94,"- ""server_name"" is the name of the last server to which the connection was"
94,"sent, which might differ from the first one if there were connection errors"
94,and a redispatch occurred. Note that this server belongs to the backend
94,which processed the request. If the request was aborted before reaching a
94,"server, ""<NOSRV>"" is indicated instead of a server name. If the request was"
94,"intercepted by the stats subsystem, ""<STATS>"" is indicated instead."
94,"- ""TR"" is the total time in milliseconds spent waiting for a full HTTP"
94,request from the client (not counting body) after the first byte was
94,"received. It can be ""-1"" if the connection was aborted before a complete"
94,request could be received or the a bad request was received. It should
94,always be very small because a request generally fits in one single packet.
94,Large times here generally indicate network issues between the client and
94,"haproxy or requests being typed by hand. See section 8.4 ""Timing Events"""
94,for more details.
94,"- ""Tw"" is the total time in milliseconds spent waiting in the various queues."
94,"It can be ""-1"" if the connection was aborted before reaching the queue."
94,"See section 8.4 ""Timing Events"" for more details."
94,"- ""Tc"" is the total time in milliseconds spent waiting for the connection to"
94,"establish to the final server, including retries. It can be ""-1"" if the"
94,request was aborted before a connection could be established. See section
94,"8.4 ""Timing Events"" for more details."
94,"- ""Tr"" is the total time in milliseconds spent waiting for the server to send"
94,"a full HTTP response, not counting data. It can be ""-1"" if the request was"
94,aborted before a complete response could be received. It generally matches
94,"the server's processing time for the request, though it may be altered by"
94,the amount of data sent by the client to the server. Large times here on
94,"""GET"" requests generally indicate an overloaded server. See section 8.4"
94,"""Timing Events"" for more details."
94,"- ""Ta"" is the time the request remained active in haproxy, which is the total"
94,time in milliseconds elapsed between the first byte of the request was
94,received and the last byte of response was sent. It covers all possible
94,processing except the handshake (see Th) and idle time (see Ti). There is
94,"one exception, if ""option logasap"" was specified, then the time counting"
94,"stops at the moment the log is emitted. In this case, a '+' sign is"
94,"prepended before the value, indicating that the final one will be larger."
94,"See section 8.4 ""Timing Events"" for more details."
94,"- ""status_code"" is the HTTP status code returned to the client. This status"
94,"is generally set by the server, but it might also be set by haproxy when"
94,the server cannot be reached or when its response is blocked by haproxy.
94,"- ""bytes_read"" is the total number of bytes transmitted to the client when"
94,"the log is emitted. This does include HTTP headers. If ""option logasap"" is"
94,"specified, the this value will be prefixed with a '+' sign indicating that"
94,the final one may be larger. Please note that this value is a 64-bit
94,"counter, so log analysis tools must be able to handle it without"
94,overflowing.
94,"- ""captured_request_cookie"" is an optional ""name=value"" entry indicating that"
94,the client had this cookie in the request. The cookie name and its maximum
94,"length are defined by the ""capture cookie"" statement in the frontend"
94,configuration. The field is a single dash ('-') when the option is not
94,"set. Only one cookie may be captured, it is generally used to track session"
94,ID exchanges between a client and a server to detect session crossing
94,"between clients due to application bugs. For more details, please consult"
94,"the section ""Capturing HTTP headers and cookies"" below."
94,"- ""captured_response_cookie"" is an optional ""name=value"" entry indicating"
94,that the server has returned a cookie with its response. The cookie name
94,"and its maximum length are defined by the ""capture cookie"" statement in the"
94,frontend configuration. The field is a single dash ('-') when the option is
94,"not set. Only one cookie may be captured, it is generally used to track"
94,session ID exchanges between a client and a server to detect session
94,"crossing between clients due to application bugs. For more details, please"
94,"consult the section ""Capturing HTTP headers and cookies"" below."
94,"- ""termination_state"" is the condition the session was in when the session"
94,"ended. This indicates the session state, which side caused the end of"
94,"session to happen, for what reason (timeout, error, ...), just like in TCP"
94,"logs, and information about persistence operations on cookies in the last"
94,"two characters. The normal flags should begin with ""--"", indicating the"
94,session was closed by either end with no data remaining in buffers. See
94,"below ""Session state at disconnection"" for more details."
94,"- ""actconn"" is the total number of concurrent connections on the process when"
94,the session was logged. It is useful to detect when some per-process system
94,"limits have been reached. For instance, if actconn is close to 512 or 1024"
94,"when multiple connection errors occur, chances are high that the system"
94,limits the process to use a maximum of 1024 file descriptors and that all
94,"of them are used. See section 3 ""Global parameters"" to find how to tune the"
94,system.
94,"- ""feconn"" is the total number of concurrent connections on the frontend when"
94,the session was logged. It is useful to estimate the amount of resource
94,"required to sustain high loads, and to detect when the frontend's ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"""
94,"has been reached. Most often when this value increases by huge jumps, it is"
94,"because there is congestion on the backend servers, but sometimes it can be"
94,caused by a denial of service attack.
94,"- ""beconn"" is the total number of concurrent connections handled by the"
94,backend when the session was logged. It includes the total number of
94,concurrent connections active on servers as well as the number of
94,connections pending in queues. It is useful to estimate the amount of
94,additional servers needed to support high loads for a given application.
94,"Most often when this value increases by huge jumps, it is because there is"
94,"congestion on the backend servers, but sometimes it can be caused by a"
94,denial of service attack.
94,"- ""srv_conn"" is the total number of concurrent connections still active on"
94,the server when the session was logged. It can never exceed the server's
94,"configured ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"" parameter. If this value is very often close or equal"
94,"to the server's ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"", it means that traffic regulation is involved a"
94,"lot, meaning that either the server's maxconn value is too low, or that"
94,there aren't enough servers to process the load with an optimal response
94,"time. When only one of the server's ""srv_conn"" is high, it usually means"
94,that this server has some trouble causing the requests to take longer to be
94,processed than on other servers.
94,"- ""retries"" is the number of connection retries experienced by this session"
94,"when trying to connect to the server. It must normally be zero, unless a"
94,server is being stopped at the same moment the connection was attempted.
94,Frequent retries generally indicate either a network problem between
94,"haproxy and the server, or a misconfigured system backlog on the server"
94,preventing new connections from being queued. This field may optionally be
94,"prefixed with a '+' sign, indicating that the session has experienced a"
94,redispatch after the maximal retry count has been reached on the initial
94,"server. In this case, the server name appearing in the log is the one the"
94,"connection was redispatched to, and not the first one, though both may"
94,sometimes be the same in case of hashing for instance. So as a general rule
94,"of thumb, when a '+' is present in front of the retry count, this count"
94,should not be attributed to the logged server.
94,"- ""srv_queue"" is the total number of requests which were processed before"
94,this one in the server queue. It is zero when the request has not gone
94,through the server queue. It makes it possible to estimate the approximate
94,server's response time by dividing the time spent in queue by the number of
94,requests in the queue. It is worth noting that if a session experiences a
94,"redispatch and passes through two server queues, their positions will be"
94,cumulative. A request should not pass through both the server queue and the
94,backend queue unless a redispatch occurs.
94,"- ""backend_queue"" is the total number of requests which were processed before"
94,this one in the backend's global queue. It is zero when the request has not
94,gone through the global queue. It makes it possible to estimate the average
94,"queue length, which easily translates into a number of missing servers when"
94,"divided by a server's ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"" parameter. It is worth noting that if a"
94,"session experiences a redispatch, it may pass twice in the backend's queue,"
94,and then both positions will be cumulative. A request should not pass
94,through both the server queue and the backend queue unless a redispatch
94,occurs.
94,"- ""captured_request_headers"" is a list of headers captured in the request due"
94,"to the presence of the ""capture request header"" statement in the frontend."
94,"Multiple headers can be captured, they will be delimited by a vertical bar"
94,"('|'). When no capture is enabled, the braces do not appear, causing a"
94,shift of remaining fields. It is important to note that this field may
94,"contain spaces, and that using it requires a smarter log parser than when"
94,"it's not used. Please consult the section ""Capturing HTTP headers and"
94,"cookies"" below for more details."
94,"- ""captured_response_headers"" is a list of headers captured in the response"
94,"due to the presence of the ""capture response header"" statement in the"
94,"frontend. Multiple headers can be captured, they will be delimited by a"
94,"vertical bar ('|'). When no capture is enabled, the braces do not appear,"
94,causing a shift of remaining fields. It is important to note that this
94,"field may contain spaces, and that using it requires a smarter log parser"
94,"than when it's not used. Please consult the section ""Capturing HTTP headers"
94,"and cookies"" below for more details."
94,"- ""http_request"" is the complete HTTP request line, including the method,"
94,request and HTTP version string. Non-printable characters are encoded (see
94,"below the section ""Non-printable characters""). This is always the last"
94,"field, and it is always delimited by quotes and is the only one which can"
94,"contain quotes. If new fields are added to the log format, they will be"
94,added before this field. This field might be truncated if the request is
94,huge and does not fit in the standard syslog buffer (1024 characters). This
94,is the reason why this field must always remain the last one.
94,8.2.4. Custom log format
94,The directive log-format allows you to customize the logs in http mode and tcp
94,mode. It takes a string as argument.
94,HAProxy understands some log format variables. % precedes log format variables.
94,"Variables can take arguments using braces ('{}'), and multiple arguments are"
94,separated by commas within the braces. Flags may be added or removed by
94,prefixing them with a '+' or '-' sign.
94,"Special variable ""%o"" may be used to propagate its flags to all other"
94,variables on the same format string. This is particularly handy with quoted
94,"(""Q"") and escaped (""E"") string formats."
94,If a variable is named between square brackets ('[' .. ']') then it is used
94,as a sample expression rule (see section 7.3). This it useful to add some
94,"less common information such as the client's SSL certificate's DN, or to log"
94,the key that would be used to store an entry into a stick table.
94,Note: spaces must be escaped. A space character is considered as a separator.
94,"In order to emit a verbatim '%', it must be preceded by another '%' resulting"
94,in '%%'. HAProxy will automatically merge consecutive separators.
94,"Note: when using the RFC5424 syslog message format, the characters '""',"
94,'\' and ']' inside PARAM-VALUE should be escaped with '\' as prefix (see
94,https://tools.ietf.org/html/rfc5424#section-6.3.3 for more details). In
94,"such cases, the use of the flag ""E"" should be considered."
94,Flags are :
94,* Q: quote a string
94,"* X: hexadecimal representation (IPs, Ports, %Ts, %rt, %pid)"
94,"* E: escape characters '""', '\' and ']' in a string with '\' as prefix"
94,(intended purpose is for the RFC5424 structured-data log formats)
94,Example:
94,log-format %T\ %t\ Some\ Text
94,log-format %{+Q}o\ %t\ %s\ %{-Q}r
94,"log-format-sd %{+Q,+E}o\ [exampleSDID@1234\ header=%[capture.req.hdr(0)]]"
94,"At the moment, the default HTTP format is defined this way :"
94,"log-format ""%ci:%cp [%tr] %ft %b/%s %TR/%Tw/%Tc/%Tr/%Ta %ST %B %CC \"
94,"%CS %tsc %ac/%fc/%bc/%sc/%rc %sq/%bq %hr %hs %{+Q}r"""
94,the default CLF format is defined this way :
94,"log-format ""%{+Q}o %{-Q}ci - - [%trg] %r %ST %B \""\"" \""\"" %cp \"
94,%ms %ft %b %s %TR %Tw %Tc %Tr %Ta %tsc %ac %fc \
94,"%bc %sc %rc %sq %bq %CC %CS %hrl %hsl"""
94,and the default TCP format is defined this way :
94,"log-format ""%ci:%cp [%t] %ft %b/%s %Tw/%Tc/%Tt %B %ts \"
94,"%ac/%fc/%bc/%sc/%rc %sq/%bq"""
94,Please refer to the table below for currently defined variables :
94,+---+------+-----------------------------------------------+-------------+
94,| R | var
94,| field name (8.2.2 and 8.2.3 for description)
94,| type
94,+---+------+-----------------------------------------------+-------------+
94,| %o
94,"| special variable, apply flags on all next var |"
94,+---+------+-----------------------------------------------+-------------+
94,| %B
94,| bytes_read
94,(from server to client)
94,| numeric
94,| H | %CC
94,| captured_request_cookie
94,| string
94,| H | %CS
94,| captured_response_cookie
94,| string
94,| %H
94,| hostname
94,| string
94,| H | %HM
94,| HTTP method (ex: POST)
94,| string
94,| H | %HP
94,| HTTP request URI without query string (path)
94,| string
94,| H | %HQ
94,| HTTP request URI query string (ex: ?bar=baz)
94,| string
94,| H | %HU
94,| HTTP request URI (ex: /foo?bar=baz)
94,| string
94,| H | %HV
94,| HTTP version (ex: HTTP/1.0)
94,| string
94,| %ID
94,| unique-id
94,| string
94,| %ST
94,| status_code
94,| numeric
94,| %T
94,| gmt_date_time
94,| date
94,| %Ta
94,| Active time of the request (from TR to end)
94,| numeric
94,| %Tc
94,| Tc
94,| numeric
94,| %Td
94,| Td = Tt - (Tq + Tw + Tc + Tr)
94,| numeric
94,| %Tl
94,| local_date_time
94,| date
94,| %Th
94,"| connection handshake time (SSL, PROXY proto)"
94,| numeric
94,| H | %Ti
94,| idle time before the HTTP request
94,| numeric
94,| H | %Tq
94,| Th + Ti + TR
94,| numeric
94,| H | %TR
94,| time to receive the full request from 1st byte| numeric
94,| H | %Tr
94,| Tr (response time)
94,| numeric
94,| %Ts
94,| timestamp
94,| numeric
94,| %Tt
94,| Tt
94,| numeric
94,| %Tw
94,| Tw
94,| numeric
94,| %U
94,| bytes_uploaded
94,(from client to server)
94,| numeric
94,| %ac
94,| actconn
94,| numeric
94,| %b
94,| backend_name
94,| string
94,| %bc
94,| beconn
94,(backend concurrent connections)
94,| numeric
94,| %bi
94,| backend_source_ip
94,(connecting address)
94,| IP
94,| %bp
94,| backend_source_port
94,(connecting address)
94,| numeric
94,| %bq
94,| backend_queue
94,| numeric
94,| %ci
94,| client_ip
94,(accepted address)
94,| IP
94,| %cp
94,| client_port
94,(accepted address)
94,| numeric
94,| %f
94,| frontend_name
94,| string
94,| %fc
94,| feconn
94,(frontend concurrent connections)
94,| numeric
94,| %fi
94,| frontend_ip
94,(accepting address)
94,| IP
94,| %fp
94,| frontend_port
94,(accepting address)
94,| numeric
94,| %ft
94,| frontend_name_transport ('~' suffix for SSL)
94,| string
94,| %lc
94,| frontend_log_counter
94,| numeric
94,| %hr
94,| captured_request_headers default style
94,| string
94,| %hrl | captured_request_headers CLF style
94,| string list |
94,| %hs
94,| captured_response_headers default style
94,| string
94,| %hsl | captured_response_headers CLF style
94,| string list |
94,| %ms
94,| accept date milliseconds (left-padded with 0) | numeric
94,| %pid | PID
94,| numeric
94,| H | %r
94,| http_request
94,| string
94,| %rc
94,| retries
94,| numeric
94,| %rt
94,| request_counter (HTTP req or TCP session)
94,| numeric
94,| %s
94,| server_name
94,| string
94,| %sc
94,| srv_conn
94,(server concurrent connections)
94,| numeric
94,| %si
94,| server_IP
94,(target address)
94,| IP
94,| %sp
94,| server_port
94,(target address)
94,| numeric
94,| %sq
94,| srv_queue
94,| numeric
94,| S | %sslc| ssl_ciphers (ex: AES-SHA)
94,| string
94,| S | %sslv| ssl_version (ex: TLSv1)
94,| string
94,| %t
94,| date_time
94,(with millisecond resolution)
94,| date
94,| H | %tr
94,| date_time of HTTP request
94,| date
94,| H | %trg | gmt_date_time of start of HTTP request
94,| date
94,| H | %trl | local_date_time of start of HTTP request
94,| date
94,| %ts
94,| termination_state
94,| string
94,| H | %tsc | termination_state with cookie status
94,| string
94,+---+------+-----------------------------------------------+-------------+
94,R = Restrictions : H = mode http only ; S = SSL only
94,8.2.5. Error log format
94,When an incoming connection fails due to an SSL handshake or an invalid PROXY
94,"protocol header, haproxy will log the event using a shorter, fixed line format."
94,"By default, logs are emitted at the LOG_INFO level, unless the option"
94,"""log-separate-errors"" is set in the backend, in which case the LOG_ERR level"
94,will be used. Connections on which no data are exchanged (e.g. probes) are not
94,"logged if the ""dontlognull"" option is set."
94,The format looks like this :
94,>>> Dec
94,3 18:27:14 localhost \
94,haproxy[6103]: 127.0.0.1:56059 [03/Dec/2012:17:35:10.380] frt/f1: \
94,Connection error during SSL handshake
94,Field
94,Format
94,Extract from the example above
94,process_name '[' pid ']:'
94,haproxy[6103]:
94,client_ip ':' client_port
94,127.0.0.1:56059
94,'[' accept_date ']'
94,[03/Dec/2012:17:35:10.380]
94,"frontend_name ""/"" bind_name "":"""
94,frt/f1:
94,message
94,Connection error during SSL handshake
94,These fields just provide minimal information to help debugging connection
94,failures.
94,8.3. Advanced logging options
94,Some advanced logging options are often looked for but are not easy to find out
94,just by looking at the various options. Here is an entry point for the few
94,options which can enable better logging. Please refer to the keywords reference
94,for more information about their usage.
94,8.3.1. Disabling logging of external tests
94,It is quite common to have some monitoring tools perform health checks on
94,haproxy. Sometimes it will be a layer 3 load-balancer such as LVS or any
94,"commercial load-balancer, and sometimes it will simply be a more complete"
94,"monitoring system such as Nagios. When the tests are very frequent, users often"
94,ask how to disable logging for those checks. There are three possibilities :
94,"- if connections come from everywhere and are just TCP probes, it is often"
94,"desired to simply disable logging of connections without data exchange, by"
94,"setting ""option dontlognull"" in the frontend. It also disables logging of"
94,"port scans, which may or may not be desired."
94,"- if the connection come from a known source network, use ""monitor-net"" to"
94,declare this network as monitoring only. Any host in this network will then
94,"only be able to perform health checks, and their requests will not be"
94,logged. This is generally appropriate to designate a list of equipment
94,such as other load-balancers.
94,"- if the tests are performed on a known URI, use ""monitor-uri"" to declare"
94,this URI as dedicated to monitoring. Any host sending this request will
94,"only get the result of a health-check, and the request will not be logged."
94,8.3.2. Logging before waiting for the session to terminate
94,The problem with logging at end of connection is that you have no clue about
94,"what is happening during very long sessions, such as remote terminal sessions"
94,or large file downloads. This problem can be worked around by specifying
94,"""option logasap"" in the frontend. HAProxy will then log as soon as possible,"
94,"just before data transfer begins. This means that in case of TCP, it will still"
94,"log the connection status to the server, and in case of HTTP, it will log just"
94,"after processing the server headers. In this case, the number of bytes reported"
94,is the number of header bytes sent to the client. In order to avoid confusion
94,"with normal logs, the total time field and the number of bytes are prefixed"
94,with a '+' sign which means that real numbers are certainly larger.
94,8.3.3. Raising log level upon errors
94,"Sometimes it is more convenient to separate normal traffic from errors logs,"
94,for instance in order to ease error monitoring from log files. When the option
94,"""log-separate-errors"" is used, connections which experience errors, timeouts,"
94,"retries, redispatches or HTTP status codes 5xx will see their syslog level"
94,"raised from ""info"" to ""err"". This will help a syslog daemon store the log in"
94,a separate file. It is very important to keep the errors in the normal traffic
94,"file too, so that log ordering is not altered. You should also be careful if"
94,you already have configured your syslog daemon to store all logs higher than
94,"""notice"" in an ""admin"" file, because the ""err"" level is higher than ""notice""."
94,8.3.4. Disabling logging of successful connections
94,"Although this may sound strange at first, some large sites have to deal with"
94,multiple thousands of logs per second and are experiencing difficulties keeping
94,them intact for a long time or detecting errors within them. If the option
94,"""dontlog-normal"" is set on the frontend, all normal connections will not be"
94,"logged. In this regard, a normal connection is defined as one without any"
94,"error, timeout, retry nor redispatch. In HTTP, the status code is checked too,"
94,and a response with a status 5xx is not considered normal and will be logged
94,"too. Of course, doing is is really discouraged as it will remove most of the"
94,useful information from the logs. Do this only if you have no other
94,alternative.
94,8.4. Timing events
94,Timers provide a great help in troubleshooting network problems. All values are
94,reported in milliseconds (ms). These timers should be used in conjunction with
94,"the session termination flags. In TCP mode with ""option tcplog"" set on the"
94,"frontend, 3 control points are reported under the form ""Tw/Tc/Tt"", and in HTTP"
94,"mode, 5 control points are reported under the form ""TR/Tw/Tc/Tr/Ta"". In"
94,"addition, three other measures are provided, ""Th"", ""Ti"", and ""Tq""."
94,Timings events in HTTP mode:
94,first request
94,2nd request
94,|<-------------------------------->|<-------------- ...
94,tr ...
94,---|----|----|----|----|----|----|----|----|--
94,: Th
94,Td : Ti
94,...
94,:<---- Tq ---->:
94,:<-------------- Tt -------------->:
94,:<--------- Ta --------->:
94,Timings events in TCP mode:
94,TCP session
94,|<----------------->|
94,---|----|----|----|----|---
94,| Th
94,Td |
94,|<------ Tt ------->|
94,- Th: total time to accept tcp connection and execute handshakes for low level
94,"protocols. Currently, these protocols are proxy-protocol and SSL. This may"
94,only happen once during the whole connection's lifetime. A large time here
94,may indicate that the client only pre-established the connection without
94,"speaking, that it is experiencing network issues preventing it from"
94,"completing a handshake in a reasonable time (e.g. MTU issues), or that an"
94,SSL handshake was very expensive to compute. Please note that this time is
94,"reported only before the first request, so it is safe to average it over"
94,all request to calculate the amortized value. The second and subsequent
94,request will always report zero here.
94,- Ti: is the idle time before the HTTP request (HTTP mode only). This timer
94,counts between the end of the handshakes and the first byte of the HTTP
94,"request. When dealing with a second request in keep-alive mode, it starts"
94,to count after the end of the transmission the previous response. When a
94,"multiplexed protocol such as HTTP/2 is used, it starts to count immediately"
94,after the previous request. Some browsers pre-establish connections to a
94,"server in order to reduce the latency of a future request, and keep them"
94,pending until they need it. This delay will be reported as the idle time. A
94,value of -1 indicates that nothing was received on the connection.
94,- TR: total time to get the client request (HTTP mode only). It's the time
94,elapsed between the first bytes received and the moment the proxy received
94,"the empty line marking the end of the HTTP headers. The value ""-1"""
94,indicates that the end of headers has never been seen. This happens when
94,the client closes prematurely or times out. This time is usually very short
94,since most requests fit in a single packet. A large time may indicate a
94,request typed by hand during a test.
94,- Tq: total time to get the client request from the accept date or since the
94,emission of the last byte of the previous response (HTTP mode only). It's
94,"exactly equal to Th + Ti + TR unless any of them is -1, in which case it"
94,returns -1 as well. This timer used to be very useful before the arrival of
94,HTTP keep-alive and browsers' pre-connect feature. It's recommended to drop
94,"it in favor of TR nowadays, as the idle time adds a lot of noise to the"
94,reports.
94,- Tw: total time spent in the queues waiting for a connection slot. It
94,"accounts for backend queue as well as the server queues, and depends on the"
94,"queue size, and the time needed for the server to complete previous"
94,"requests. The value ""-1"" means that the request was killed before reaching"
94,"the queue, which is generally what happens with invalid or denied requests."
94,- Tc: total time to establish the TCP connection to the server. It's the time
94,"elapsed between the moment the proxy sent the connection request, and the"
94,"moment it was acknowledged by the server, or between the TCP SYN packet and"
94,"the matching SYN/ACK packet in return. The value ""-1"" means that the"
94,connection never established.
94,- Tr: server response time (HTTP mode only). It's the time elapsed between
94,the moment the TCP connection was established to the server and the moment
94,the server sent its complete response headers. It purely shows its request
94,"processing time, without the network overhead due to the data transmission."
94,"It is worth noting that when the client has data to send to the server, for"
94,"instance during a POST request, the time already runs, and this can distort"
94,"apparent response time. For this reason, it's generally wise not to trust"
94,too much this field for POST requests initiated from clients behind an
94,"untrusted network. A value of ""-1"" here means that the last the response"
94,"header (empty line) was never seen, most likely because the server timeout"
94,stroke before the server managed to process the request.
94,"- Ta: total active time for the HTTP request, between the moment the proxy"
94,received the first byte of the request header and the emission of the last
94,"byte of the response body. The exception is when the ""logasap"" option is"
94,"specified. In this case, it only equals (TR+Tw+Tc+Tr), and is prefixed with"
94,"a '+' sign. From this field, we can deduce ""Td"", the data transmission time,"
94,by subtracting other timers when valid :
94,Td = Ta - (TR + Tw + Tc + Tr)
94,"Timers with ""-1"" values have to be excluded from this equation. Note that"
94,"""Ta"" can never be negative."
94,"- Tt: total session duration time, between the moment the proxy accepted it"
94,"and the moment both ends were closed. The exception is when the ""logasap"""
94,"option is specified. In this case, it only equals (Th+Ti+TR+Tw+Tc+Tr), and"
94,"is prefixed with a '+' sign. From this field, we can deduce ""Td"", the data"
94,"transmission time, by subtracting other timers when valid :"
94,Td = Tt - (Th + Ti + TR + Tw + Tc + Tr)
94,"Timers with ""-1"" values have to be excluded from this equation. In TCP"
94,"mode, ""Ti"", ""Tq"" and ""Tr"" have to be excluded too. Note that ""Tt"" can never"
94,"be negative and that for HTTP, Tt is simply equal to (Th+Ti+Ta)."
94,These timers provide precious indications on trouble causes. Since the TCP
94,"protocol defines retransmit delays of 3, 6, 12... seconds, we know for sure"
94,that timers close to multiples of 3s are nearly always related to lost packets
94,"due to network problems (wires, negotiation, congestion). Moreover, if ""Ta"" or"
94,"""Tt"" is close to a timeout value specified in the configuration, it often means"
94,that a session has been aborted on timeout.
94,Most common cases :
94,"- If ""Th"" or ""Ti"" are close to 3000, a packet has probably been lost between"
94,the client and the proxy. This is very rare on local networks but might
94,happen when clients are on far remote networks and send large requests. It
94,may happen that values larger than usual appear here without any network
94,"cause. Sometimes, during an attack or just after a resource starvation has"
94,"ended, haproxy may accept thousands of connections in a few milliseconds."
94,The time spent accepting these connections will inevitably slightly delay
94,"processing of other connections, and it can happen that request times in the"
94,order of a few tens of milliseconds are measured after a few thousands of
94,new connections have been accepted at once. Using one of the keep-alive
94,"modes may display larger idle times since ""Ti"" measures the time spent"
94,waiting for additional requests.
94,"- If ""Tc"" is close to 3000, a packet has probably been lost between the"
94,server and the proxy during the server connection phase. This value should
94,"always be very low, such as 1 ms on local networks and less than a few tens"
94,of ms on remote networks.
94,"- If ""Tr"" is nearly always lower than 3000 except some rare values which seem"
94,"to be the average majored by 3000, there are probably some packets lost"
94,between the proxy and the server.
94,"- If ""Ta"" is large even for small byte counts, it generally is because"
94,neither the client nor the server decides to close the connection while
94,haproxy is running in tunnel mode and both have agreed on a keep-alive
94,"connection mode. In order to solve this issue, it will be needed to specify"
94,one of the HTTP options to manipulate keep-alive or close options on either
94,the frontend or the backend. Having the smallest possible 'Ta' or 'Tt' is
94,"important when connection regulation is used with the ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"" option on"
94,"the servers, since no new connection will be sent to the server until"
94,another one is released.
94,Other noticeable HTTP log cases ('xx' means any value to be ignored) :
94,TR/Tw/Tc/Tr/+Ta
94,"The ""option logasap"" is present on the frontend and the log"
94,was emitted before the data phase. All the timers are valid
94,"except ""Ta"" which is shorter than reality."
94,-1/xx/xx/xx/Ta
94,The client was not able to send a complete request in time
94,or it aborted too early. Check the session termination flags
94,"then ""timeout http-request"" and ""timeout client"" settings."
94,TR/-1/xx/xx/Ta
94,"It was not possible to process the request, maybe because"
94,"servers were out of order, because the request was invalid"
94,or forbidden by ACL rules. Check the session termination
94,flags.
94,TR/Tw/-1/xx/Ta
94,The connection could not establish on the server. Either it
94,actively refused it or it timed out after Ta-(TR+Tw) ms.
94,"Check the session termination flags, then check the"
94,"""timeout connect"" setting. Note that the tarpit action might"
94,"return similar-looking patterns, with ""Tw"" equal to the time"
94,the client connection was maintained open.
94,TR/Tw/Tc/-1/Ta
94,The server has accepted the connection but did not return
94,"a complete response in time, or it closed its connection"
94,unexpectedly after Ta-(TR+Tw+Tc) ms. Check the session
94,"termination flags, then check the ""timeout server"" setting."
94,8.5. Session state at disconnection
94,TCP and HTTP logs provide a session termination indicator in the
94,"""termination_state"" field, just before the number of active connections. It is"
94,"2-characters long in TCP mode, and is extended to 4 characters in HTTP mode,"
94,each of which has a special meaning :
94,"- On the first character, a code reporting the first event which caused the"
94,session to terminate :
94,C : the TCP session was unexpectedly aborted by the client.
94,"S : the TCP session was unexpectedly aborted by the server, or the"
94,server explicitly refused it.
94,"P : the session was prematurely aborted by the proxy, because of a"
94,"connection limit enforcement, because a DENY filter was matched,"
94,because of a security check which detected and blocked a dangerous
94,error in server response which might have caused information leak
94,(e.g. cacheable cookie).
94,L : the session was locally processed by haproxy and was not passed to
94,a server. This is what happens for stats and redirects.
94,"R : a resource on the proxy has been exhausted (memory, sockets, source"
94,"ports, ...). Usually, this appears during the connection phase, and"
94,system logs should contain a copy of the precise error. If this
94,"happens, it must be considered as a very serious anomaly which"
94,should be fixed as soon as possible by any means.
94,I : an internal error was identified by the proxy during a self-check.
94,"This should NEVER happen, and you are encouraged to report any log"
94,"containing this, because this would almost certainly be a bug. It"
94,would be wise to preventively restart the process after such an
94,"event too, in case it would be caused by memory corruption."
94,D : the session was killed by haproxy because the server was detected
94,as down and was configured to kill all connections when going down.
94,U : the session was killed by haproxy on this backup server because an
94,active server was detected as up and was configured to kill all
94,backup connections when going up.
94,K : the session was actively killed by an admin operating on haproxy.
94,c : the client-side timeout expired while waiting for the client to
94,send or receive data.
94,s : the server-side timeout expired while waiting for the server to
94,send or receive data.
94,"- : normal session completion, both the client and the server closed"
94,with nothing left in the buffers.
94,"- on the second character, the TCP or HTTP session state when it was closed :"
94,"R : the proxy was waiting for a complete, valid REQUEST from the client"
94,(HTTP mode only). Nothing was sent to any server.
94,Q : the proxy was waiting in the QUEUE for a connection slot. This can
94,only happen when servers have a 'maxconn' parameter set. It can
94,also happen in the global queue after a redispatch consecutive to
94,a failed attempt to connect to a dying server. If no redispatch is
94,"reported, then no connection attempt was made to any server."
94,C : the proxy was waiting for the CONNECTION to establish on the
94,server. The server might at most have noticed a connection attempt.
94,"H : the proxy was waiting for complete, valid response HEADERS from the"
94,server (HTTP only).
94,D : the session was in the DATA phase.
94,L : the proxy was still transmitting LAST data to the client while the
94,server had already finished. This one is very rare as it can only
94,happen when the client dies while receiving the last packets.
94,T : the request was tarpitted. It has been held open with the client
94,"during the whole ""timeout tarpit"" duration or until the client"
94,"closed, both of which will be reported in the ""Tw"" timer."
94,- : normal session completion after end of data transfer.
94,- the third character tells whether the persistence cookie was provided by
94,the client (only in HTTP mode) :
94,N : the client provided NO cookie. This is usually the case for new
94,"visitors, so counting the number of occurrences of this flag in the"
94,logs generally indicate a valid trend for the site frequentation.
94,I : the client provided an INVALID cookie matching no known server.
94,"This might be caused by a recent configuration change, mixed"
94,"cookies between HTTP/HTTPS sites, persistence conditionally"
94,"ignored, or an attack."
94,"D : the client provided a cookie designating a server which was DOWN,"
94,"so either ""option persist"" was used and the client was sent to"
94,"this server, or it was not set and the client was redispatched to"
94,another server.
94,"V : the client provided a VALID cookie, and was sent to the associated"
94,server.
94,"E : the client provided a valid cookie, but with a last date which was"
94,"older than what is allowed by the ""maxidle"" cookie parameter, so"
94,the cookie is consider EXPIRED and is ignored. The request will be
94,redispatched just as if there was no cookie.
94,"O : the client provided a valid cookie, but with a first date which was"
94,"older than what is allowed by the ""maxlife"" cookie parameter, so"
94,the cookie is consider too OLD and is ignored. The request will be
94,redispatched just as if there was no cookie.
94,U : a cookie was present but was not used to select the server because
94,some other server selection mechanism was used instead (typically a
94,"""use-server"" rule)."
94,- : does not apply (no cookie set in configuration).
94,- the last character reports what operations were performed on the persistence
94,cookie returned by the server (only in HTTP mode) :
94,"N : NO cookie was provided by the server, and none was inserted either."
94,"I : no cookie was provided by the server, and the proxy INSERTED one."
94,"Note that in ""cookie insert"" mode, if the server provides a cookie,"
94,"it will still be overwritten and reported as ""I"" here."
94,U : the proxy UPDATED the last date in the cookie that was presented by
94,"the client. This can only happen in insert mode with ""maxidle"". It"
94,happens every time there is activity at a different date than the
94,"date indicated in the cookie. If any other change happens, such as"
94,"a redispatch, then the cookie will be marked as inserted instead."
94,P : a cookie was PROVIDED by the server and transmitted as-is.
94,"R : the cookie provided by the server was REWRITTEN by the proxy, which"
94,"happens in ""cookie rewrite"" or ""cookie prefix"" modes."
94,D : the cookie provided by the server was DELETED by the proxy.
94,- : does not apply (no cookie set in configuration).
94,The combination of the two first flags gives a lot of information about what
94,"was happening when the session terminated, and why it did terminate. It can be"
94,"helpful to detect server saturation, network troubles, local system resource"
94,"starvation, attacks, etc..."
94,The most common termination flags combinations are indicated below. They are
94,"alphabetically sorted, with the lowercase set just after the upper case for"
94,easier finding and understanding.
94,Flags
94,Reason
94,Normal termination.
94,The client aborted before the connection could be established to the
94,server. This can happen when haproxy tries to connect to a recently
94,"dead (or unchecked) server, and the client aborts while haproxy is"
94,"waiting for the server to respond or for ""timeout connect"" to expire."
94,The client unexpectedly aborted during data transfer. This can be
94,"caused by a browser crash, by an intermediate equipment between the"
94,"client and haproxy which decided to actively break the connection,"
94,"by network routing issues between the client and haproxy, or by a"
94,keep-alive session between the server and the client terminated first
94,by the client.
94,The client did not send nor acknowledge any data for as long as the
94,"""timeout client"" delay. This is often caused by network failures on"
94,"the client side, or the client simply leaving the net uncleanly."
94,The client aborted while waiting for the server to start responding.
94,It might be the server taking too long to respond or the client
94,clicking the 'Stop' button too fast.
94,"The ""timeout client"" stroke while waiting for client data during a"
94,POST request. This is sometimes caused by too large TCP MSS values
94,for PPPoE networks which cannot transport full-sized packets. It can
94,also happen when client timeout is smaller than server timeout and
94,the server takes too long to respond.
94,"The client aborted while its session was queued, waiting for a server"
94,with enough empty slots to accept it. It might be that either all the
94,servers were saturated or that the assigned server was taking too
94,long a time to respond.
94,The client aborted before sending a full HTTP request. Most likely
94,"the request was typed by hand using a telnet client, and aborted"
94,too early. The HTTP status code is likely a 400 here. Sometimes this
94,might also be caused by an IDS killing the connection between haproxy
94,"and the client. ""option http-ignore-probes"" can be used to ignore"
94,connections without any data transfer.
94,"The ""timeout http-request"" stroke before the client sent a full HTTP"
94,request. This is sometimes caused by too large TCP MSS values on the
94,client side for PPPoE networks which cannot transport full-sized
94,"packets, or by clients sending requests by hand and not typing fast"
94,"enough, or forgetting to enter the empty line at the end of the"
94,"request. The HTTP status code is likely a 408 here. Note: recently,"
94,"some browsers started to implement a ""pre-connect"" feature consisting"
94,in speculatively connecting to some recently visited web sites just
94,in case the user would like to visit them. This results in many
94,"connections being established to web sites, which end up in 408"
94,"Request Timeout if the timeout strikes first, or 400 Bad Request when"
94,the browser decides to close them first. These ones pollute the log
94,and feed the error counters. Some versions of some browsers have even
94,been reported to display the error code. It is possible to work
94,"around the undesirable effects of this behavior by adding ""option"
94,"http-ignore-probes"" in the frontend, resulting in connections with"
94,zero data transfer to be totally ignored. This will definitely hide
94,the errors of people experiencing connectivity issues though.
94,The client aborted while its session was tarpitted. It is important to
94,"check if this happens on valid requests, in order to be sure that no"
94,"wrong tarpit rules have been written. If a lot of them happen, it"
94,"might make sense to lower the ""timeout tarpit"" value to something"
94,"closer to the average reported ""Tw"" timer, in order not to consume"
94,resources for just a few attackers.
94,The request was intercepted and locally handled by haproxy. Generally
94,it means that this was a redirect or a stats request.
94,The server or an equipment between it and haproxy explicitly refused
94,the TCP connection (the proxy received a TCP RST or an ICMP message
94,"in return). Under some circumstances, it can also be the network"
94,"stack telling the proxy that the server is unreachable (e.g. no route,"
94,"or no ARP response on local network). When this happens in HTTP mode,"
94,the status code is likely a 502 or 503 here.
94,"The ""timeout connect"" stroke before a connection to the server could"
94,"complete. When this happens in HTTP mode, the status code is likely a"
94,503 or 504 here.
94,The connection to the server died with an error during the data
94,transfer. This usually means that haproxy has received an RST from
94,the server or an ICMP message from an intermediate equipment while
94,exchanging data with the server. This can be caused by a server crash
94,or by a network issue on an intermediate equipment.
94,The server did not send nor acknowledge any data for as long as the
94,"""timeout server"" setting during the data phase. This is often caused"
94,"by too short timeouts on L4 equipment before the server (firewalls,"
94,"load-balancers, ...), as well as keep-alive sessions maintained"
94,between the client and the server expiring first on haproxy.
94,"The server aborted before sending its full HTTP response headers, or"
94,it crashed while processing the request. Since a server aborting at
94,"this moment is very rare, it would be wise to inspect its logs to"
94,control whether it crashed and why. The logged request may indicate a
94,"small set of faulty requests, demonstrating bugs in the application."
94,Sometimes this might also be caused by an IDS killing the connection
94,between haproxy and the server.
94,"The ""timeout server"" stroke before the server could return its"
94,"response headers. This is the most common anomaly, indicating too"
94,"long transactions, probably caused by server or database saturation."
94,"The immediate workaround consists in increasing the ""timeout server"""
94,"setting, but it is important to keep in mind that the user experience"
94,will suffer from these long response times. The only long term
94,solution is to fix the application.
94,The session spent too much time in queue and has been expired. See
94,"the ""timeout queue"" and ""timeout connect"" settings to find out how to"
94,fix this if it happens too often. If it often happens massively in
94,"short periods, it may indicate general problems on the affected"
94,"servers due to I/O or database congestion, or saturation caused by"
94,external attacks.
94,The proxy refused to establish a connection to the server because the
94,process' socket limit has been reached while attempting to connect.
94,"The global ""maxconnThis keyword is available in sections :Performance tuningAlphabetically sorted keywords referenceBind optionsServer and default-server options"" parameter may be increased in the configuration"
94,so that it does not happen anymore. This status is very rare and
94,"might happen when the global ""ulimit-n"" parameter is forced by hand."
94,The proxy blocked an incorrectly formatted chunked encoded message in
94,"a request or a response, after the server has emitted its headers. In"
94,"most cases, this will indicate an invalid message from the server to"
94,the client. HAProxy supports chunk sizes of up to 2GB - 1 (2147483647
94,bytes). Any larger size will be considered as an error.
94,"The proxy blocked the server's response, because it was invalid,"
94,"incomplete, dangerous (cache control), or matched a security filter."
94,"In any case, an HTTP 502 error is sent to the client. One possible"
94,cause for this error is an invalid syntax in an HTTP header name
94,containing unauthorized characters. It is also possible but quite
94,"rare, that the proxy blocked a chunked-encoding request from the"
94,"client due to an invalid syntax, before the server responded. In this"
94,"case, an HTTP 400 error is sent to the client and reported in the"
94,logs.
94,"The proxy blocked the client's HTTP request, either because of an"
94,"invalid HTTP syntax, in which case it returned an HTTP 400 error to"
94,"the client, or because a deny filter matched, in which case it"
94,returned an HTTP 403 error.
94,The proxy blocked the client's request and has tarpitted its
94,connection before returning it a 500 server error. Nothing was sent
94,to the server. The connection was maintained open for as long as
94,"reported by the ""Tw"" timer field."
94,"A local resource has been exhausted (memory, sockets, source ports)"
94,preventing the connection to the server from establishing. The error
94,logs will tell precisely what was missing. This is very rare and can
94,only be solved by proper system tuning.
94,The combination of the two last flags gives a lot of information about how
94,"persistence was handled by the client, the server and by haproxy. This is very"
94,"important to troubleshoot disconnections, when users complain they have to"
94,re-authenticate. The commonly encountered flags are :
94,Persistence cookie is not enabled.
94,"No cookie was provided by the client, none was inserted in the"
94,"response. For instance, this can be in insert mode with ""postonly"""
94,set on a GET request.
94,"A cookie designating an invalid server was provided by the client,"
94,a valid one was inserted in the response. This typically happens when
94,"a ""server"" entry is removed from the configuration, since its cookie"
94,value can be presented by a client when no other server knows it.
94,"No cookie was provided by the client, one was inserted in the"
94,response. This typically happens for first requests from every user
94,"in ""insert"" mode, which makes it an easy way to count real users."
94,"A cookie was provided by the client, none was inserted in the"
94,response. This happens for most responses for which the client has
94,already got a cookie.
94,"A cookie was provided by the client, with a last visit date which is"
94,"not completely up-to-date, so an updated cookie was provided in"
94,"response. This can also happen if there was no date at all, or if"
94,"there was a date but the ""maxidle"" parameter was not set, so that the"
94,cookie can be switched to unlimited time.
94,"A cookie was provided by the client, with a last visit date which is"
94,"too old for the ""maxidle"" parameter, so the cookie was ignored and a"
94,new cookie was inserted in the response.
94,"A cookie was provided by the client, with a first visit date which is"
94,"too old for the ""maxlife"" parameter, so the cookie was ignored and a"
94,new cookie was inserted in the response.
94,"The server designated by the cookie was down, a new server was"
94,selected and a new cookie was emitted in the response.
94,The server designated by the cookie was not marked dead but could not
94,"be reached. A redispatch happened and selected another one, which was"
94,then advertised in the response.
94,8.6. Non-printable characters
94,In order not to cause trouble to log analysis tools or terminals during log
94,"consulting, non-printable characters are not sent as-is into log files, but are"
94,"converted to the two-digits hexadecimal representation of their ASCII code,"
94,prefixed by the character '#'. The only characters that can be logged without
94,"being escaped are comprised between 32 and 126 (inclusive). Obviously, the"
94,"escape character '#' itself is also encoded to avoid any ambiguity (""#23""). It"
94,"is the same for the character '""' which becomes ""#22"", as well as '{', '|' and"
94,'}' when logging headers.
94,"Note that the space character (' ') is not encoded in headers, which can cause"
94,issues for tools relying on space count to locate fields. A typical header
94,"containing spaces is ""User-Agent""."
94,"Last, it has been observed that some syslog daemons such as syslog-ng escape"
94,"the quote ('""') with a backslash ('\'). The reverse operation can safely be"
94,performed since no quote may appear anywhere else in the logs.
94,8.7. Capturing HTTP cookies
94,Cookie capture simplifies the tracking a complete user session. This can be
94,"achieved using the ""capture cookie"" statement in the frontend. Please refer to"
94,"section 4.2 for more details. Only one cookie can be captured, and the same"
94,"cookie will simultaneously be checked in the request (""Cookie:"" header) and in"
94,"the response (""Set-Cookie:"" header). The respective values will be reported in"
94,"the HTTP logs at the ""captured_request_cookie"" and ""captured_response_cookie"""
94,locations (see section 8.2.3 about HTTP log format). When either cookie is
94,"not seen, a dash ('-') replaces the value. This way, it's easy to detect when a"
94,"user switches to a new session for example, because the server will reassign it"
94,a new cookie. It is also possible to detect if a server unexpectedly sets a
94,"wrong cookie to a client, leading to session crossing."
94,Examples :
94,"# capture the first cookie whose name starts with ""ASPSESSION"""
94,capture cookie ASPSESSION len 32
94,"# capture the first cookie whose name is exactly ""vgnvisitor"""
94,capture cookie vgnvisitor= len 32
94,8.8. Capturing HTTP headers
94,Header captures are useful to track unique request identifiers set by an upper
94,"proxy, virtual host names, user-agents, POST content-length, referrers, etc. In"
94,"the response, one can search for information about the response length, how the"
94,"server asked the cache to behave, or an object location during a redirection."
94,"Header captures are performed using the ""capture request header"" and ""capture"
94,"response header"" statements in the frontend. Please consult their definition in"
94,section 4.2 for more details.
94,It is possible to include both request headers and response headers at the same
94,"time. Non-existent headers are logged as empty strings, and if one header"
94,"appears more than once, only its last occurrence will be logged. Request headers"
94,"are grouped within braces '{' and '}' in the same order as they were declared,"
94,and delimited with a vertical bar '|' without any space. Response headers
94,"follow the same representation, but are displayed after a space following the"
94,request headers block. These blocks are displayed just before the HTTP request
94,in the logs.
94,"As a special case, it is possible to specify an HTTP header capture in a TCP"
94,frontend. The purpose is to enable logging of headers which will be parsed in
94,an HTTP backend if the request is then switched to this HTTP backend.
94,Example :
94,# This instance chains to the outgoing proxy
94,listen proxy-out
94,mode http
94,option httplog
94,option logasap
94,log global
94,server cache1 192.168.1.1:3128
94,# log the name of the virtual server
94,capture request
94,header Host len 20
94,# log the amount of data uploaded during a POST
94,capture request
94,header Content-Length len 10
94,# log the beginning of the referrer
94,capture request
94,header Referer len 20
94,# server name (useful for outgoing proxies only)
94,capture response header Server len 20
94,"# logging the content-length is useful with ""option logasap"""
94,capture response header Content-Length len 10
94,# log the expected cache behavior on the response
94,capture response header Cache-Control len 8
94,# the Via header will report the next proxy's name
94,capture response header Via len 20
94,# log the URL location during a redirection
94,capture response header Location len 20
94,>>> Aug
94,9 20:26:09 localhost \
94,haproxy[2022]: 127.0.0.1:34014 [09/Aug/2004:20:26:09] proxy-out \
94,proxy-out/cache1 0/0/0/162/+162 200 +350 - - ---- 0/0/0/0/0 0/0 \
94,{fr.adserver.yahoo.co||http://fr.f416.mail.} {|864|private||} \
94,"""GET http://fr.adserver.yahoo.com/"""
94,>>> Aug
94,9 20:30:46 localhost \
94,haproxy[2022]: 127.0.0.1:34020 [09/Aug/2004:20:30:46] proxy-out \
94,proxy-out/cache1 0/0/0/182/+182 200 +279 - - ---- 0/0/0/0/0 0/0 \
94,{w.ods.org||} {Formilux/0.1.8|3495|||} \
94,"""GET http://trafic.1wt.eu/ HTTP/1.1"""
94,>>> Aug
94,9 20:30:46 localhost \
94,haproxy[2022]: 127.0.0.1:34028 [09/Aug/2004:20:30:46] proxy-out \
94,proxy-out/cache1 0/0/2/126/+128 301 +223 - - ---- 0/0/0/0/0 0/0 \
94,{www.sytadin.equipement.gouv.fr||http://trafic.1wt.eu/} \
94,{Apache|230|||http://www.sytadin.} \
94,"""GET http://www.sytadin.equipement.gouv.fr/ HTTP/1.1"""
94,8.9. Examples of logs
94,These are real-world examples of logs accompanied with an explanation. Some of
94,them have been made up by hand. The syslog part has been removed for better
94,reading. Their sole purpose is to explain how to decipher them.
94,>>> haproxy[674]: 127.0.0.1:33318 [15/Oct/2003:08:31:57.130] px-http \
94,px-http/srv1 6559/0/7/147/6723 200 243 - - ---- 5/3/3/1/0 0/0 \
94,"""HEAD / HTTP/1.0"""
94,=> long request (6.5s) entered by hand through 'telnet'. The server replied
94,"in 147 ms, and the session ended normally ('----')"
94,>>> haproxy[674]: 127.0.0.1:33319 [15/Oct/2003:08:31:57.149] px-http \
94,px-http/srv1 6559/1230/7/147/6870 200 243 - - ---- 324/239/239/99/0 \
94,"0/9 ""HEAD / HTTP/1.0"""
94,"=> Idem, but the request was queued in the global queue behind 9 other"
94,"requests, and waited there for 1230 ms."
94,>>> haproxy[674]: 127.0.0.1:33320 [15/Oct/2003:08:32:17.654] px-http \
94,px-http/srv1 9/0/7/14/+30 200 +243 - - ---- 3/3/3/1/0 0/0 \
94,"""GET /image.iso HTTP/1.0"""
94,"=> request for a long data transfer. The ""logasap"" option was specified, so"
94,the log was produced just before transferring data. The server replied in
94,"14 ms, 243 bytes of headers were sent to the client, and total time from"
94,accept to first data byte is 30 ms.
94,>>> haproxy[674]: 127.0.0.1:33320 [15/Oct/2003:08:32:17.925] px-http \
94,px-http/srv1 9/0/7/14/30 502 243 - - PH-- 3/2/2/0/0 0/0 \
94,"""GET /cgi-bin/bug.cgi? HTTP/1.0"""
94,"=> the proxy blocked a server response either because of an ""rspdeny"" or"
94,"""rspideny"" filter, or because the response was improperly formatted and"
94,"not HTTP-compliant, or because it blocked sensitive information which"
94,"risked being cached. In this case, the response is replaced with a ""502"
94,"bad gateway"". The flags (""PH--"") tell us that it was haproxy who decided"
94,to return the 502 and not the server.
94,>>> haproxy[18113]: 127.0.0.1:34548 [15/Oct/2003:15:18:55.798] px-http \
94,"px-http/<NOSRV> -1/-1/-1/-1/8490 -1 0 - - CR-- 2/2/2/0/0 0/0 """""
94,"=> the client never completed its request and aborted itself (""C---"") after"
94,"8.5s, while the proxy was waiting for the request headers (""-R--"")."
94,Nothing was sent to any server.
94,>>> haproxy[18113]: 127.0.0.1:34549 [15/Oct/2003:15:19:06.103] px-http \
94,"px-http/<NOSRV> -1/-1/-1/-1/50001 408 0 - - cR-- 2/2/2/0/0 0/0 """""
94,"=> The client never completed its request, which was aborted by the"
94,"time-out (""c---"") after 50s, while the proxy was waiting for the request"
94,"headers (""-R--""). Nothing was sent to any server, but the proxy could"
94,send a 408 return code to the client.
94,>>> haproxy[18989]: 127.0.0.1:34550 [15/Oct/2003:15:24:28.312] px-tcp \
94,px-tcp/srv1 0/0/5007 0 cD 0/0/0/0/0 0/0
94,"=> This log was produced with ""option tcplog"". The client timed out after"
94,"5 seconds (""c----"")."
94,>>> haproxy[18989]: 10.0.0.1:34552 [15/Oct/2003:15:26:31.462] px-http \
94,px-http/srv1 3183/-1/-1/-1/11215 503 0 - - SC-- 205/202/202/115/3 \
94,"0/0 ""HEAD / HTTP/1.0"""
94,"=> The request took 3s to complete (probably a network problem), and the"
94,connection to the server failed ('SC--') after 4 attempts of 2 seconds
94,"(config says 'retries 3'), and no redispatch (otherwise we would have"
94,"seen ""/+3""). Status code 503 was returned to the client. There were 115"
94,"connections on this server, 202 connections on this proxy, and 205 on"
94,the global process. It is possible that the server refused the
94,connection because of too many already established.
94,9. Supported filters
94,Here are listed officially supported filters with the list of parameters they
94,"accept. Depending on compile options, some of these filters might be"
94,unavailable. The list of available filters is reported in haproxy -vv.
94,"See also : ""filterThis keyword is available in sections :Alphabetically sorted keywords referenceTraceHTTP compressionStream Processing Offload Engine (SPOE)"""
94,9.1. Trace
94,filter trace [name <name>] [random-parsing] [random-forwarding] [hexdump]
94,Arguments:<name>
94,is an arbitrary name that will be reported in
94,"messages. If no name is provided, ""TRACE"" is used."
94,<random-parsing>
94,enables the random parsing of data exchanged between
94,"the client and the server. By default, this filter"
94,"parses all available data. With this parameter, it"
94,only parses a random amount of the available data.
94,<random-forwarding>
94,enables the random forwarding of parsed data. By
94,"default, this filter forwards all previously parsed"
94,"data. With this parameter, it only forwards a random"
94,amount of the parsed data.
94,<hexdump>
94,dumps all forwarded data to the server and the client.
94,This filter can be used as a base to develop new filters. It defines all
94,callbacks and print a message on the standard error stream (stderr) with useful
94,information for all of them. It may be useful to debug the activity of other
94,"filters or, quite simply, HAProxy's activity."
94,Using <random-parsing> and/or <random-forwarding> parameters is a good way to
94,tests the behavior of a filter that parses data exchanged between a client and
94,a server by adding some latencies in the processing.
94,9.2. HTTP compression
94,"filter compressionThe HTTP compression has been moved in a filter in HAProxy 1.7. ""compression"""
94,keyword must still be used to enable and configure the HTTP compression. And
94,"when no other filter is used, it is enough. But it is mandatory to explicitly"
94,use a filter line to enable the HTTP compression when two or more filters are
94,used for the same listener/frontend/backend. This is important to know the
94,filters evaluation order.
94,"See also : ""compression"""
94,9.3. Stream Processing Offload Engine (SPOE)
94,filter spoe [engine <name>] config <file>
94,Arguments :<name>
94,is the engine name that will be used to find the right scope in
94,"the configuration file. If not provided, all the file will be"
94,parsed.
94,<file>
94,is the path of the engine configuration file. This file can
94,"contain configuration of several engines. In this case, each"
94,part must be placed in its own scope.
94,The Stream Processing Offload Engine (SPOE) is a filter communicating with
94,external components. It allows the offload of some specifics processing on the
94,streams in tiered applications. These external components and information
94,"exchanged with them are configured in dedicated files, for the main part. It"
94,"also requires dedicated backends, defined in HAProxy configuration."
94,"SPOE communicates with external components using an in-house binary protocol,"
94,the Stream Processing Offload Protocol (SPOP).
94,"For all information about the SPOE configuration and the SPOP specification, see"
94,"""doc/SPOE.txt""."
94,Important note:
94,The SPOE filter is highly experimental for now and was not heavily
94,tested. It is really not production ready. So use it carefully.
94,10. Cache
94,"HAProxy provides a cache, which was designed to perform cache on small objects"
94,"(favicon, css...). This is a minimalist low-maintenance cache which runs in"
94,RAM.
94,"The cache is based on a memory which is shared between processes and threads,"
94,this memory is split in blocks of 1k.
94,"If an object is not used anymore, it can be deleted to store a new object"
94,independently of its expiration date. The oldest objects are deleted first
94,when we try to allocate a new one.
94,The cache uses a hash of the host header and the URI as the key.
94,It's possible to view the status of a cache using the Unix socket command
94,"""show cache"" consult section 9.3 ""Unix Socket commands"" of Management Guide"
94,for more details.
94,"When an object is delivered from the cache, the server name in the log is"
94,"replaced by ""<CACHE>""."
94,10.1. Limitation
94,The cache won't store and won't deliver objects in these cases:
94,- If the response is not a 200
94,- If the response contains a Vary header
94,- If the response does not contain a Content-Length header or if the
94,Content-Length + the headers size is greater than a buffer size - the
94,reserve.
94,- If the response is not cacheable
94,- If the request is not a GET
94,- If the HTTP version of the request is smaller than 1.1
94,- If the request contains an Authorization header
94,"Caution!: Due to the current limitation of the filters, it is not recommended"
94,to use the cache with other filters. Using them can cause undefined behavior
94,if they modify the response (compression for example).
94,10.2. Setup
94,"To setup a cache, you must define a cache section and use it in a proxy with"
94,the corresponding http-request and response actions.
94,10.2.1. Cache section
94,"cache <name>Declare a cache section, allocate a shared cache memory named <name>, the"
94,size of cache is mandatory.
94,total-max-size <megabytes>Define the size in RAM of the cache in megabytes. This size is split in
94,blocks of 1kB which are used by the cache entries. Its maximum value is 4095.
94,max-age <seconds>Define the maximum expiration duration. The expiration is set has the lowest
94,value between the s-maxage or max-age (in this order) directive in the
94,Cache-Control response header and this value. The default value is 60
94,"seconds, which means that you can't cache an object more than 60 seconds by"
94,default.
94,10.2.2. Proxy section
94,http-request cache-use <name> [ { if | unless } <condition> ]Try to deliver a cached object from the cache <name>. This directive is also
94,mandatory to store the cache as it calculates the cache hash. If you want to
94,use a condition for both storage and delivering that's a good idea to put it
94,after this one.
94,http-response cache-store <name> [ { if | unless } <condition> ]Store an http-response within the cache. The storage of the response headers
94,"is done at this step, which means you can use others http-response actions"
94,to modify headers before or after the storage of the response. This action
94,is responsible for the setup of the cache storage filter.
94,Example:
94,backend bck1
94,mode http
94,http-request cache-use foobar
94,http-response cache-store foobar
94,server srv1 127.0.0.1:80
94,cache foobar
94,total-max-size 4
94,max-age 240
94,HAProxy 1.8.27 – Configuration Manual
94,"2020/11/06, willy tarreau"
98,IIS Tuning for Connectwise Automate: Server Tuning Guide
98,Skip to content
98,Search for:
98,HomeBlogIntegrationsReverse ProxyEnterprise Package ManagementDatabase TunerDatabase CommanderDatabase Index PacksDatabase PermissionerServicesConsultingDatabase TuneupMySQL PatchingDocumentationResourcesLabtech Database SchemaTime Recovery Calculator
98,support@automationtheory.org FacebookYoutube
98,support@automationtheory.org FacebookYoutube
98,Automation Theory
98,Turning Good Theory into PracticeHomeBlogIntegrationsReverse ProxyEnterprise Package ManagementDatabase TunerDatabase CommanderDatabase Index PacksDatabase PermissionerServicesConsultingDatabase TuneupMySQL PatchingDocumentationResourcesLabtech Database SchemaTime Recovery Calculator
98,"IIS Tuning for Connectwise AutomateHomePerformanceIIS Tuning for Connectwise Automate Connectwise Automate leverages Microsoft’s IIS webserver to facilitate application communications. Like other components of the Automate stack, IIS needs proper configuration to perform well under the heavy load of an RMM system. Below you’ll find the core IIS tuning config we use here at Automation Theory whenever we’re doing consulting for partners.As we explore IIS tuning for Connectwise Automate it’s important to note that we’re going in reverse order; we’re starting with the smallest structures and working towards the largest; tuning lower-level items first simplifies things as we work up the stack.Application Pool RecyclingIIS has an application pool feature known as recycling. This restarts the worker processes on a regular interval, and the original intent was to address memory leaks in web applications where modification of the code wasn’t possible. While we have seen excessive memory usage with Automate worker processes occasionally, the true utility of this setting lies elsewhere.The Automate application uses connection pooling when communicating with MySQL. The application will open up to 500 connections to MySQL and leave them open to facilitate the rapid processing of database requests. This is a best practice, but historically the pooled connections don’t always get closed, and for larger partners, this will result in a server that regularly hits the limit for MySQL connections, and the Automate app freezing/crashing.Recycling the IIS worker processes closes these connections and prevents the database from being overwhelmed. The exact setting to correct this behavior will vary between servers, but recycling the IIS application pool every 60-90 minutes is adequate for most partners. While the recycle is happening the application will appear to hang for ~30 seconds. If this would be disruptive to daily operations it’s possible to specify a set of times for recycling to occur to ensure it happens outside of business hours.Worker ProcessesInside of IIS a thread known as a worker process handles the incoming web requests. The default number of worker processes is 1, and this will be fine for smaller Automate instances. However, the theoretical limit for simultaneous web requests (worst case scenario) would approach ~33% of the agent count (there are so many variables at play; please treat this as a loose number and use PowerShell to measure TCP connections on your webserver). For a 300 agent server, it’s quite plausible that a single thread could handle 100 simultaneous requests. However, the case becomes much less plausible for a 3000 agent server to efficiently serve 1000 simultaneous requests with a single thread.# PowerShell to measure current web requests"
98,"Get-NetTCPConnection -LocalPort 443 | Measure-ObjectSo, obviously bigger servers will need more threads, but how many? The old school logic when doing IIS tuning for Connectwise Automate is to have half as many worker processes as you have CPU cores (so a server with 8 cores/vCPUs would have 4 worker threads). The idea behind this is to prevent resource starvation. However, there are settings inside of IIS for controlling CPU usage by the worker threads, and they provide much more granular control over the resource utilization.For most partners starting with the old school method is advisable, but if performance is still lacking (or the threads/connections ratio is still disproportionate) it would be worth gradually adding more worker processes (with the IIS resource controls) until an optimal balance is reached. As with other concepts in resource allocation, it’s worth noting that more is not always better, and there is a point of diminishing returns (the tipping point is normally when worker processes exceed core count). It’s also worth noting that additional worker processes will open additional database connections, so care should be taken to prevent hitting the max_connections threshold.Queue LengthIIS application pools also have a property known as queue length. This is simply the number of pending web requests the server will queue before returning the HTTP 503 status code (service not available). All requests pass through this queue first as they are processed by the webserver (there are also other queues, but they are out of scope for our discussion here). The queue length plays a role in determining the total number of concurrent connections; as any request that a worker process can’t accept immediately remains in the queue.In an ideal world, the webserver always can process incoming requests rapidly, and this queue never contains a large volume of requests. However, as discussed in our blog post here, there are inherent performance issues with the Automate database — and that can result in web requests waiting for the database, and thus the queue filling up. Because of this tendency, the Automate installer sets the queue length to 11,000 each on the “Labtech” and “CwaRestApi” application pools — a full 11x greater than the default.It’s important to note cause and effect bidirectionally as we look at tuning this setting. In most cases, a high queue length is caused by contention in MySQL. This makes IIS unable to process requests, and the queue fills. However, once MySQL recovers, the queued requests begin to flood in and the application response will still be poor until the server catches up. It’s during these times that the Control Center will lock up and be unresponsive.The million-dollar question of course is: when doing IIS tuning for Connectwise Automate, what should the queue length be? In the spirit of the proper use of the queue, we’d suggest setting it to a count that could hold ~60 seconds worth of requests (normally this is ~30% of the agent count). If it takes longer than 60 seconds to process a web request that indicates that there is a real issue, and it would make sense to start returning the 503 status code.As a simple test to gauge this, set Performance Monitor to watch the queue size and recycle the application pool. This is normally lower than the 30% of agents count, but it’s representative of what a short processing delay on the server should look like. If your queue length is comfortably larger than this amount then all is well. For smaller Automate instances where 30% of the agent count would be less than the default of 1000 sticking with the default setting is the best course of action.Connection LimitIIS also has a connection limit for all requests to a website on the server. The default value for this is 4,294,967,295, and it’s a setting that normally doesn’t get much attention, but it did get the spotlight during the 2020.7 Automate patch. This setting is defined on the website level, and it is the funnel that feeds all of the different application pools.So, why does this setting default to the number of IPv4 addresses in existence? This value is set to the upper limit by default to prevent connections from being denied by default (the developers are hedging a bet that the whole internet won’t be accessing an IIS server simultaneously). However, the idea is that this value could be set to make sure that the sum of all connections to the server doesn’t exceed the available resources.If this setting is left to the default, the connection limits at the application pool layer are the controlling factor. The danger here is that it’s possible for the sum of the application pool connections to be greater than the number of connections the server can accommodate. This appears to be the issue with the 2020.7 patch, where the queue length on the application pool was too large, and thus the website connection limit was used to prevent overloading of the server. Obviously, the conditions of the patch were a rather special case, but in general, it is of benefit to partners to configure their servers to be tolerant against connection spikes.When doing IIS tuning for Connectwise Automate this value should be set to the max number of connections the server can process — however, there are a lot of variables in that calculation. As a starting point, it’s advisable to set this to the sum of the application pool queues once they are properly scaled, and work down from there. Please keep in mind that this value will be the cap of agent communications, web interface users, and API calls — and the normal count of those will be different depending on user count, integrations, and how Automate is used in your particular environment.We hope that this has been helpful for you. Here at Automation Theory, we’re certified MySQL DBAs dedicated exclusively to the Connectwise Automate software stack. Be sure to check out our integrations and services."
98,Please leave this field emptyWant to get the latest from our blog delivered to your inbox?Check your inbox or spam folder to confirm your email! Posted on
98,"August 22, 2020August 31, 2020"
98,Tags
98,Automate Performance
98,Categories
98,Performance
98,Post Author: Jeremy OaksPost navigationPrevious PostTop 5 myths about the Automate DatabaseNext PostMySQL 5.6
98,End of Life: What Connectwise Automate admins need to knowFacebookYoutubePrivacy PolicyTerms and Conditions Copyright © 2021 All Rights Reserved.
99,Connect to a PolarDB for MySQL cluster - User Guide| Alibaba Cloud Documentation Center
99,Document Center
99,PolarDB for MySQL Cloud Native Database
99,Product Updates
99,Release notes
99,Release notes of the PolarDB kernel
99,Product Introduction
99,Overview
99,Product editions
99,Archive Database
99,Overview
99,Usage instructions
99,Benefits
99,Architecture
99,Case Studies
99,Online games: XD.com
99,Glossary
99,Limits
99,Product Billing
99,Billable items
99,Billing methods
99,Overview
99,Billing method 1: pay-as-you-go
99,Billing method 2: subscription
99,Purchase procedures
99,Purchase a pay-as-you-go cluster
99,Purchase a subscription cluster
99,Purchase a storage plan
99,Change the billing method from subscription to pay-as-you-go
99,Change the billing method from pay-as-you-go to subscription
99,Instructions for purchase
99,Manage storage plans
99,View the deducted capacity of a storage plan
99,Renew or upgrade a storage plan
99,Expiration or overdue payments
99,Renew subscription clusters
99,Manual renewal
99,Auto-renewal
99,View bills
99,Billing rules of PolarDB for MySQL
99,Specifications of compute nodes
99,Billing rules of compute nodes
99,Billing rules of pay-as-you-go compute nodes
99,Billing rules of subscription compute nodes
99,Comparison
99,Storage pricing
99,Pricing of data backups that exceed the free quota
99,Pricing of SQL Explorer (optional)
99,Configuration change fees
99,FAQ
99,Storage plans
99,Scale-out and scale-in
99,Quick Start
99,Data Migration or Synchronization
99,Overview of data migration or synchronization solutions
99,[Considerations] Migrate data from MySQL 5.7 to PolarDB for MySQL 8.0
99,Migrate data from ApsaraDB for RDS to ApsaraDB for PolarDB
99,Create a PolarDB for MySQL cluster from an ApsaraDB RDS for MySQL instance
99,Create a PolarDB for MySQL cluster by using the Clone from RDS method
99,Migrate data from an ApsaraDB RDS for MySQL instance to a PolarDB for MySQL cluster
99,Migrate data between ApsaraDB for PolarDB
99,Migrate data between PolarDB for MySQL clusters
99,Migrate data from other databases to Apsara for PolarDB
99,Migrate data from a user-created MySQL database to a PolarDB for MySQL cluster
99,Migrate data from an Amazon Aurora MySQL cluster to a PolarDB for MySQL cluster
99,Migrate data from ApsaraDB for PolarDB to other databases
99,Migrate data from an Apsara PolarDB for MySQL cluster to an ApsaraDB RDS for MySQL
99,instance
99,Migrate data from an Apsara PolarDB for MySQL cluster to a user-created MySQL database
99,Synchronize data between ApsaraDB for PolarDB
99,Configure one-way data synchronization between Apsara PolarDB for MySQL clusters
99,Configure two-way data synchronization between Apsara PolarDB for MySQL clusters
99,Synchronize data between ApsaraDB for PolarDB and other databases
99,Synchronize data from an ApsaraDB RDS for MySQL instance to an Apsara PolarDB for MySQL cluster
99,Synchronize data from an Apsara PolarDB for MySQL cluster to an ApsaraDB RDS for MySQL instance
99,"Synchronize data from a user-created MySQL database connected over Express Connect,"
99,"VPN Gateway, or Smart Access Gateway to a PolarDB for MySQL cluster"
99,Synchronize data from a PolarDB for MySQL cluster to an AnalyticDB for PostgreSQL
99,instance
99,Synchronize data from a user-created MySQL database hosted on ECS to an Apsara PolarDB for MySQL cluster
99,Synchronize data from a PolarDB for MySQL cluster to a user-created Kafka cluster
99,Synchronize data from a PolarDB for MySQL cluster to an AnalyticDB for MySQL cluster
99,User Guide
99,Overview
99,Features
99,Resource plans
99,Connect to PolarDB
99,View or apply for an endpoint
99,Modify or delete an endpoint
99,Connect to a PolarDB for MySQL cluster
99,Private domain names
99,Database proxy
99,Introduction
99,Read/write splitting
99,Create a custom cluster endpoint
99,Consistency levels
99,Transaction splitting
99,Connection pool
99,Persistent connection
99,Manage a cluster endpoint
99,FAQ
99,Account Management
99,Account overview
99,Register and log on to an Alibaba Cloud account
99,Authorize RAM users to manage PolarDB by using custom policies
99,Create and authorize a RAM user
99,Create a database account
99,Manage database accounts for a cluster
99,Account permissions
99,Databases
99,Deployment Architecture
99,Deploy a cluster across zones and change the primary zone
99,Switch over services between primary and read-only nodes
99,Global database networks
99,Overview
99,Technical architecture
99,Typical scenarios
99,Best practices for deploying a GDN across regions
99,Create and release a GDN
99,Add and remove secondary clusters
99,Connect to a GDN
99,Elastic upgrade and downgrade
99,Change specifications
99,Add or remove read-only nodes
99,Perform a temporary upgrade
99,Data Security and Encryption
99,Configure TDE
99,Configure a whitelist for a cluster
99,Configure SSL encryption
99,Backup and restore
99,Back up data
99,Restore data
99,Restore data of specific databases and tables
99,FAQ about the backup feature
99,Cluster recycle bin
99,Clone a cluster
99,Monitoring and optimization
99,Diagnosis
99,Autonomy center
99,Session Management
99,Real-time Monitoring
99,Storage analysis
99,Deadlock analysis
99,Diagnostic reports
99,Performance Insight
99,Monitor the performance of clusters and nodes
99,Manage alert rules
99,Slow SQL queries
99,SQL Explorer
99,More operations
99,Specify cluster parameters
99,Enable binary logging
99,Set a maintenance window
99,Restart nodes
99,Pending events
99,View the database storage usage
99,Release a cluster
99,Tags
99,Bind a tag
99,Filter clusters by tag
99,View tags bound to a cluster
99,Unbind a tag
99,Kernel Features
99,Overview of Kernel
99,Kernel compatibility
99,Release notes of the PolarDB kernel
99,Version of the PolarDB database kernel
99,Advanced kernel features
99,Upgrade versions
99,Optimization of DDL execution
99,Instant ADD COLUMN
99,Async Metadata Lock Replication
99,DDL physical replication optimization
99,Parallel DDL
99,Parallel query
99,Parallel query
99,DOP policy
99,Hash joins in parallel queries
99,Semijoins in parallel queries
99,ROLLUP
99,Parallel hints
99,Inconsistencies between the results of parallel execution and serial execution and
99,limits on parallel queries
99,Examples of parallel queries
99,High concurrency optimization
99,Statement Concurrency Control
99,Inventory Hint
99,Statement Queue
99,Hot row optimization
99,Thread Pool
99,Performance monitoring
99,Performance Agent
99,Resource Manager
99,Other features
99,Fast Query Cache
99,Statement Outline
99,Recycle bin
99,Returning
99,Performance White Paper
99,OLTP performance tests
99,Performance of PolarDB for MySQL 8.0 (Cluster Edition)
99,Performance test results of PolarDB for MySQL 5.7 (Cluster Edition)
99,Performance test results of PolarDB for MySQL 5.6 (Cluster Edition)
99,Performance comparison between Archive Database and Cluster Edition in PolarDB for MySQL 8.0
99,Comparison with ApsaraDB RDS for MySQL
99,Parallel query performance (OLAP)
99,Guidelines for performance comparison
99,API Reference
99,API overview
99,Use RAM for resource authorization
99,RAM role linked to Apsara PolarDB
99,Call methods
99,Common parameters
99,Request structures
99,Signatures
99,Regions
99,DescribeRegions
99,Cluster management
99,CreateDBCluster
99,DeleteDBCluster
99,DescribeDBClusters
99,DescribeDBClusterAttribute
99,DescribeTasks
99,ModifyDBClusterDescription
99,ModifyDBClusterMaintainTime
99,Deployment architecture
99,ModifyDBClusterPrimaryZone
99,FailoverDBCluster
99,DescribeGlobalDatabaseNetworks
99,Renewal management
99,DescribeAutoRenewAttribute
99,ModifyAutoRenewAttribute
99,DescribeDBClusterAvailableResources
99,Kernel
99,DescribeDBClusterVersion
99,UpgradeDBClusterVersion
99,Data Security
99,Whitelist management
99,ModifyDBClusterAccessWhitelist
99,DescribeDBClusterAccessWhitelist
99,SSL encryption
99,DescribeDBClusterSSL
99,ModifyDBClusterSSL
99,TDE
99,DescribeDBClusterTDE
99,ModifyDBClusterTDE
99,Node management
99,CreateDBNodes
99,ModifyDBNodeClass
99,RestartDBNode
99,DeleteDBNodes
99,Cluster parameters
99,DescribeDBClusterParameters
99,ModifyDBClusterParameters
99,DescribeParameterTemplates
99,CreateParameterGroup
99,DescribeParameterGroups
99,DescribeParameterGroup
99,DeleteParameterGroup
99,Connection points
99,CreateDBEndpointAddress
99,CreateDBClusterEndpoint
99,DescribeDBClusterEndpoints
99,ModifyDBClusterEndpoint
99,ModifyDBEndpointAddress
99,DeleteDBEndpointAddress
99,DeleteDBClusterEndpoint
99,Logs
99,DescribeSlowLogRecords
99,DescribeDBClusterAuditLogCollector
99,ModifyDBClusterAuditLogCollector
99,Account management
99,CreateAccount
99,CheckAccountName
99,DescribeAccounts
99,ModifyAccountDescription
99,GrantAccountPrivilege
99,RevokeAccountPrivilege
99,ResetAccount
99,DeleteAccount
99,ModifyAccountPassword
99,Database management
99,CreateDatabase
99,DescribeDatabases
99,ModifyDBDescription
99,DescribeCharacterSetName
99,CheckDBName
99,DeleteDatabase
99,Backup management
99,CreateBackup
99,DescribeBackups
99,DescribeBackupTasks
99,DescribeBackupLogs
99,DescribeDetachedBackups
99,DescribeDBClustersWithBackups
99,DeleteBackup
99,Backup policy
99,DescribeBackupPolicy
99,DescribeLogBackupPolicy
99,ModifyBackupPolicy
99,ModifyLogBackupPolicy
99,Restore management
99,DescribeMetaList
99,RestoreTable
99,Data migration from RDS
99,DescribeDBClusterMigration
99,ModifyDBClusterMigration
99,CloseDBClusterMigration
99,Tag management
99,TagResources
99,UntagResources
99,ListTagResources
99,Pending events
99,DescribePendingMaintenanceAction
99,DescribePendingMaintenanceActions
99,ModifyPendingMaintenanceAction
99,Scheduled tasks
99,DescribeScheduleTasks
99,CancelScheduleTasks
99,Monitoring management
99,DescribeDBNodePerformance
99,DescribeDBClusterPerformance
99,DescribeDBClusterMonitor
99,ModifyDBClusterMonitor
99,Appendixes
99,Client error codes
99,Cluster status
99,Character set tables
99,Performance metric monitoring
99,Download the SDK
99,FAQ
99,All Products
99,Search
99,Document Center
99,PolarDB for MySQL Cloud Native Database
99,User Guide
99,Connect to PolarDB
99,Connect to a PolarDB for MySQL cluster
99,all-products-head
99,This Product
99,This Product
99,All Products
99,Connect to a PolarDB for MySQL cluster
99,Document Center
99,Connect to a PolarDB for MySQL cluster
99,"Last Updated: Jan 19, 2021"
99,This topic describes how to use Data Management Service (DMS) or a MySQL client to
99,connect to a PolarDB for MySQL cluster.
99,Prerequisites
99,A privileged account or standard account is created for a PolarDB for MySQL cluster.
99,"For more information, see Create a database account."
99,Use DMS to connect to a PolarDB for MySQL cluster
99,DMS is a visualized data management service provided by Alibaba Cloud. It provides
99,"various management services, such as data management, schema management, access control,"
99,"BI charts, data trends, data tracking, performance optimization, and server management."
99,"DMS supports relational databases such as MySQL, SQL Server, and PostgreSQL, as well"
99,as NoSQL databases such as MongoDB and Redis. DMS also supports the management of
99,Linux servers.
99,Log on to the PolarDB console.
99,"In the upper-left corner of the console, select the region where the cluster resides."
99,"Find the cluster, and then click the cluster ID."
99,"In the upper-right corner of the Overview page, click Log On to Database."
99,"In the dialog box that appears, specify Database Account and Database Password that are created in the PolarDB for MySQL cluster, and click Login."
99,Note The account that is used to log on must have the permissions on the specified database.
99,"Otherwise, you cannot find the specified database in the left-side navigation pane"
99,of the DMS console. For more information about how to modify the permissions of a
99,"database account, see Modify the permissions of a standard account."
99,"After you log on to DMS, refresh the page. In the left-side navigation pane, click"
99,Logged in instance.
99,"On the Logged in instance page, click the cluster name and double-click the database name. Then, you can manage"
99,the database.
99,Use a client to connect to a PolarDB for MySQL cluster
99,"You can use a MySQL client to connect a PolarDB cluster. In this topic, HeidiSQL is used."
99,Start the HeidiSQL client.
99,"In the lower-left corner of the session manager, click New."
99,Enter the information of the PolarDB cluster to be connected. The following table describes the parameters.
99,Parameter
99,Description
99,Network Type
99,The network type of the database. Select MariaDB or MySQL (TCP/IP).
99,Hostname/IP
99,Enter the public or internal endpoint of the PolarDB cluster.
99,If the client runs on an Elastic Compute Service (ECS) instance that is deployed in
99,"the same region and has the same network type as the PolarDB cluster, use the internal endpoint. For example, if the ECS instance and PolarDB cluster are deployed in a virtual private cloud (VPC) in the China (Hangzhou) region,"
99,you can use the internal endpoint to establish a secure and fast connection.
99,Use the public endpoint for other scenarios.
99,"To view the endpoint and port information of the PolarDB cluster, perform the following steps:"
99,Log on to the PolarDB console.
99,"In the upper-left corner of the page, select the region where the cluster that you"
99,want to manage is deployed.
99,Find the cluster and click the cluster ID.
99,You can view the endpoint and port information on the Overview page.
99,User
99,The account that is used to connect to the PolarDB cluster.
99,Password
99,The password of the account.
99,Port
99,The port number of the public or internal endpoint for the PolarDB cluster. The default port number is 3306.
99,"Click Open. If the connection information is valid, you can connect to the cluster."
99,Use the command-line interface (CLI) to connect to a PolarDB for MySQL cluster
99,"If MySQL is installed on your server, you can run the following command in the command-line"
99,interface (CLI) to connect to a PolarDB for MySQL cluster:
99,mysql -h<Endpoint> -P<Port> -u<Username> -p<Password> -D<Database>
99,Parameter
99,Description
99,Example
99,"The public or internal endpoint of the PolarDB cluster. For more information, see View or apply for an endpoint."
99,pc-bpxxxxxxxxxxxxxx.mysql.polardb.rds.aliyuncs.com
99,The port number of the PolarDB cluster.
99,"If you use the internal endpoint, you must enter the internal port number of the PolarDB cluster."
99,"If you use the public endpoint, you must enter the public port number of the PolarDB cluster."
99,Note
99,The default port number is 3306.
99,"If you use the default port, you do not need to specify this parameter."
99,3306
99,The account that is used to connect to the PolarDB cluster.
99,root
99,The password of the account.
99,Note This parameter is optional.
99,"If you do not specify this parameter, you are required to enter the password when"
99,you connect to the cluster.
99,"If you specify this parameter, do not enter a space character between -p and the password."
99,Password233
99,The name of the database to which you want to connect.
99,Note
99,This parameter is optional.
99,You can enter only the database name and exclude -D.
99,mysql
99,Troubleshooting
99,The IP address whitelist is error.
99,"The default whitelist contains only the IP address 127.0.0.1. 127.0.0.1 indicates that no IP address is allowed to access the PolarDB cluster. Therefore, you must add IP addresses to the whitelist. For more information,"
99,see Configure a whitelist for a cluster.
99,The whitelist is set to 0.0.0.0. The valid format is 0.0.0.0/0.
99,Notice 0.0.0.0/0 indicates that all IP addresses are allowed to access the PolarDB cluster
99,"The public IP addresses that you add to the whitelist are invalid. For example, the"
99,public IP address may be a dynamic IP address. The tools or websites that are used
99,to query the public IP addresses provide invalid IP addresses.
99,The wrong internal or public endpoint is used.
99,The connection fails when you use an internal endpoint to connect over the Internet
99,or use a public endpoint to connect through an internal network.
99,"Use the required endpoint. If you want to connect to the PolarDB cluster through an internal network, you must use the internal endpoint of the PolarDB cluster. If you want to connect to the PolarDB cluster over the Internet, you must use the public endpoint of the PolarDB cluster."
99,The network types of the ECS instance and the PolarDB cluster are different. The ECS instance is deployed in the classic network while
99,the PolarDB cluster is deployed in a VPC.
99,"Solution 1 (recommended): Migrate the ECS instance to the same VPC. For more information,"
99,see Migrate ECS instances.
99,Note The ECS instance and the PolarDB cluster must be deployed in the same VPC to communicate
99,with each other through the internal network.
99,Solution 2: Use the ClassicLink feature to establish an internal network connection between the ECS instance in the
99,classic network and the PolarDB cluster in the VPC.
99,Solution 3: Connect the ECS instance to the PolarDB cluster over the Internet by using the public endpoint of the cluster. This solution
99,"provides the lowest security, stability, and performance."
99,Previous: Modify or delete an endpoint
99,Next: Private domain names
99,How helpful was this page?
99,What might be the problems?
99,More suggestions?
99,Send Feedback
99,Thank you! We've received your
99,feedback.
99,Free Trial
99,Free Trial
